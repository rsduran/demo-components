[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/73890-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A financial services company needs to aggregate daily stock trade data from the exchanges into a data store. The company requires that data be streamed directly into the data store, but also occasionally allows data to be modified using SQL. The solution should integrate complex, analytic queries running with minimal latency. The solution must provide a business intelligence dashboard that enables viewing of the top contributors to anomalies in stock prices.<br>Which solution meets the company's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to stream data to Amazon S3. Use Amazon Athena as a data source for Amazon QuickSight to create a business intelligence dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon QuickSight to create a business intelligence dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon QuickSight to create a business intelligence dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to stream data to Amazon S3. Use Amazon Athena as a data source for Amazon QuickSight to create a business intelligence dashboard."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-20T08:16:00.000Z",
        "voteCount": 17,
        "content": "complex, analytic queries running with minimal latency = REDSHIFT\nKDF load data into redshift\nAnswer = C"
      },
      {
        "date": "2024-03-19T23:04:00.000Z",
        "voteCount": 13,
        "content": "C is a good answer, but it is slower compared to data stream (B). Kinesis data firehose has to buffer stream data (thus near-realtime only), and then copy to s3 bucket as a staging area, then issue COPY command to Insert into Redshift table. Whereas Kinesis data stream just transfer data directly into the data warehouse via materialized view(s).\nSecond, \n\nLink to back my claim up: https://docs.google.com/document/d/1Zre9fB3Q21829_lqxrDaLrP5W0k89cEL6ykuT2pjCDE"
      },
      {
        "date": "2024-09-24T22:06:00.000Z",
        "voteCount": 1,
        "content": "C. Use Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon to create a business intelligence dashboard.\nC is valid\n\n"
      },
      {
        "date": "2024-09-06T01:06:00.000Z",
        "voteCount": 9,
        "content": "the keywords \"complex, analytic queries running with minimal latency\""
      },
      {
        "date": "2024-03-16T00:15:00.000Z",
        "voteCount": 2,
        "content": "the keywords \"complex, analytic queries running with minimal latency\""
      },
      {
        "date": "2024-02-26T18:25:00.000Z",
        "voteCount": 2,
        "content": "\"but also occasionally allows data to be modified using SQL\". This is only possible in redshift and redshift does not support kinesis stream directly , so firehouse and then redshift will work as source for quick sight"
      },
      {
        "date": "2024-01-17T10:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is now B as you can alter data in the materialized view when ingesting from stream.  \"You can now connect to and access the data from the stream using SQL and simplify your data pipelines by creating materialized views directly on top of the stream. The materialized views can also include SQL transforms as part of your ELT (extract, load and transform) pipeline.\""
      },
      {
        "date": "2024-01-09T06:59:00.000Z",
        "voteCount": 1,
        "content": "Can Kinesis Data Streams directly write to S3?"
      },
      {
        "date": "2023-11-15T05:40:00.000Z",
        "voteCount": 1,
        "content": "D is a best. KFH provides near-real time data, so it's latency is little higher than that of KDS. Athena is the best data visualizer for complex and raw data. Since the data are in s3, it already appears in athena. Redshift is a data warehouse and can't reciev raw data."
      },
      {
        "date": "2023-11-11T17:15:00.000Z",
        "voteCount": 3,
        "content": "can't use kds to deliver to s3 so must use kdf. use athena or redshift"
      },
      {
        "date": "2023-10-24T05:28:00.000Z",
        "voteCount": 1,
        "content": "KDS cant connect datahouse directly.\nThis is reason why  We have to use KDF."
      },
      {
        "date": "2023-10-14T07:40:00.000Z",
        "voteCount": 2,
        "content": "The key is \"ntegrate complex, analytic queries running with minimal latency.\""
      },
      {
        "date": "2023-09-17T23:55:00.000Z",
        "voteCount": 1,
        "content": "Definitely C. While you can stream data from KDS into Redshift, you cannot modify the materialised view using SQL, you can only run selects on a materialised view. S3 on the other hand doesn't support data modification using Athena."
      },
      {
        "date": "2023-08-31T00:09:00.000Z",
        "voteCount": 5,
        "content": "C is a good answer, but it is slower compared to data stream (B). Kinesis data firehose has to buffer stream data (thus near-realtime only), and then copy to s3 bucket as a staging area, then issue COPY command to Insert into Redshift table. Whereas Kinesis data stream just transfer data directly into the data warehouse via materialized view(s). \nSecond, the question requires the stream be transform-able via SQL, Data firehose do support transformation of data yes, but only via Lambda blueprints; Data stream transfer into Materialized View, which is written in SQL to convert Json body of the stream data, you can add more SQL here to transform it as you need - so Data Stream satisfies the requirement - albeit with lots of limitations.\n\nLink to back my claim up: https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html"
      },
      {
        "date": "2023-08-30T03:23:00.000Z",
        "voteCount": 2,
        "content": "corect answer is D is wrong ?"
      },
      {
        "date": "2023-07-31T14:44:00.000Z",
        "voteCount": 1,
        "content": "it's a C"
      },
      {
        "date": "2023-06-08T21:33:00.000Z",
        "voteCount": 2,
        "content": "There is a new feature of Redshift, supporting read KDS directly into a materialized view, which support lowest latency and make B quite promising. But it might not meet the modify by SQL requirements. So, this make C be the best choice."
      },
      {
        "date": "2023-07-11T08:52:00.000Z",
        "voteCount": 1,
        "content": "\"You don't have to send data to an Amazon Kinesis Data Firehose delivery stream, because with streaming ingestion, data can be sent directly from Kinesis Data Streams to a materialized view in an Amazon Redshift database.\" https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n\nyes, I just think what you say. The problem in this question is de modifying by SQL, and that is not possible with KDS."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/27804-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A financial company hosts a data lake in Amazon S3 and a data warehouse on an Amazon Redshift cluster. The company uses Amazon QuickSight to build dashboards and wants to secure access from its on-premises Active Directory to Amazon QuickSight.<br>How should the data be secured?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Active Directory connector and single sign-on (SSO) in a corporate network environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a VPC endpoint to connect to Amazon S3 from Amazon QuickSight and an IAM role to authenticate Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a secure connection by creating an S3 endpoint to connect Amazon QuickSight and a VPC endpoint to connect to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace Amazon QuickSight and Amazon Redshift in the security group and use an Amazon S3 endpoint to connect Amazon QuickSight to Amazon S3."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T10:50:00.000Z",
        "voteCount": 40,
        "content": "Answer is A - \"Use an AD connector and SSO in a corporate environment\",\nKey point of question is to \u201cSecure access from its on-premise AD to Quicksight\".\n\u2022\tQuicksight Enterprise edition allows for connecting to AD / using AD groups, SSO, row-level security, encryption at rest ... etc"
      },
      {
        "date": "2021-10-06T15:40:00.000Z",
        "voteCount": 16,
        "content": "Option A. Quicksight Enterprise edition allows for connecting through AD connector."
      },
      {
        "date": "2024-09-06T01:06:00.000Z",
        "voteCount": 10,
        "content": "Option A. Quicksight Enterprise edition allows for connecting through AD connector."
      },
      {
        "date": "2024-01-09T08:09:00.000Z",
        "voteCount": 1,
        "content": "Considering the need to secure access from an on-premises Active Directory to Amazon QuickSight, Option A (Use an Active Directory Connector and Single Sign-On (SSO) in a Corporate Network Environment) is the most appropriate. It directly addresses the requirement of integrating QuickSight with the company\u2019s Active Directory, allowing for controlled and secure access to QuickSight dashboards using corporate credentials. This approach ensures that access to QuickSight is managed in alignment with the company's existing security policies and user management practices. While it doesn't explicitly mention securing data, integrating QuickSight with SSO and Active Directory is a crucial step in overall access security, especially in a corporate environment."
      },
      {
        "date": "2023-10-14T07:39:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-08-02T10:23:00.000Z",
        "voteCount": 1,
        "content": "Its a me A"
      },
      {
        "date": "2023-07-31T15:31:00.000Z",
        "voteCount": 1,
        "content": "it's an A"
      },
      {
        "date": "2023-06-08T21:38:00.000Z",
        "voteCount": 1,
        "content": "Is this single selection? Both A and D seems correct to me."
      },
      {
        "date": "2023-05-01T06:57:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-21T01:37:00.000Z",
        "voteCount": 3,
        "content": "S3 cannot be placed inside VPC. Redshift can be placed inside VPC. So here connecting Quicksight to S3 doesn't need anything else. Quicksight enterprise edition allows AD connection and answer is A"
      },
      {
        "date": "2023-03-11T04:31:00.000Z",
        "voteCount": 2,
        "content": "A. Use an Active Directory connector and single sign-on (SSO) in a corporate network environment is the correct solution to secure access from an on-premises Active Directory to Amazon QuickSight. This solution allows the users to log in with their existing corporate credentials and enables IT administrators to manage access to Amazon QuickSight using their on-premises Active Directory. Amazon QuickSight supports various authentication methods, including SSO, OpenID Connect, and SAML 2.0, making it easy to integrate with existing authentication solutions."
      },
      {
        "date": "2023-02-16T09:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is A \"Use an AD connector and SSO in a corporate environment\""
      },
      {
        "date": "2023-01-17T22:43:00.000Z",
        "voteCount": 3,
        "content": "The question target on How should the data be secured?\nso in this case, it should be B right? option A talks only about the connecting via Active Directory connector; i doubt does it focus on the actual question"
      },
      {
        "date": "2022-11-03T06:06:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A as QuickSight access using Active Directory can be implemented using an Active Directory connector and single sign-on (SSO) in a corporate network environment.\nOptions B, C &amp; D are wrong as they do not target the QuickSight authentication requirement but focus on QuickSight access to S3 and Redshift."
      },
      {
        "date": "2022-10-26T21:57:00.000Z",
        "voteCount": 3,
        "content": "The Answer seems to be A, but wondering why the answer mentioned as B. it worries me!"
      },
      {
        "date": "2022-08-21T07:56:00.000Z",
        "voteCount": 2,
        "content": "A is the answer"
      },
      {
        "date": "2022-08-21T07:55:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/74136-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A real estate company has a mission-critical application using Apache HBase in Amazon EMR. Amazon EMR is configured with a single master node. The company has over 5 TB of data stored on an Hadoop Distributed File System (HDFS). The company wants a cost-effective solution to make its HBase data highly available.<br>Which architectural pattern meets company's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spot Instances for core and task nodes and a Reserved Instance for the EMR master node. Configure the EMR cluster with multiple master nodes. Schedule automated snapshots using Amazon EventBridge.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data on an EMR File System (EMRFS) instead of HDFS. Enable EMRFS consistent view. Create an EMR HBase cluster with multiple master nodes. Point the HBase root directory to an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data on an EMR File System (EMRFS) instead of HDFS and enable EMRFS consistent view. Run two separate EMR clusters in two different Availability Zones. Point both clusters to the same HBase root directory in the same Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data on an EMR File System (EMRFS) instead of HDFS and enable EMRFS consistent view. Create a primary EMR HBase cluster with multiple master nodes. Create a secondary EMR HBase read-replica cluster in a separate Availability Zone. Point both clusters to the same HBase root directory in the same Amazon S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 71,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-04T04:48:00.000Z",
        "voteCount": 36,
        "content": "D is correct as Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously.\n\nA  is incorrect because using Spot EC2 instances for both of your core and task nodes could potentially cause downtime. Although this solution is the most cost-effective, it certainly doesn\u2019t provide the highest availability for Amazon EMR.\n\nB is incorrect. While an EMR cluster with multiple master nodes can survive scenarios in which a primary master node fails, it is not, however, tolerant of Availability Zone failures.\n\nC is wrong as It's not possible for two primary clusters to be linked to the same root directory at the same time. Take note that only one active cluster at a time can use the same HBase root directory in Amazon S3. The best way to implement this is to launch a primary EMR cluster and a secondary (read-replica) EMR cluster, since using two primary clusters is not supported."
      },
      {
        "date": "2022-12-26T19:16:00.000Z",
        "voteCount": 5,
        "content": "The answer is D.\nUdemy course  by  Bonso has the same Logic."
      },
      {
        "date": "2022-07-01T20:09:00.000Z",
        "voteCount": 14,
        "content": "If we strictly want high availability then answer should be \"D\". But to be cost effective it only needs to go from current HDFS to S3, to make the data more available than before. Read replica is the next step if we want availability over master node crashes, etc. And it comes with additional cost. So I also suggest ans \"B\""
      },
      {
        "date": "2024-09-06T01:06:00.000Z",
        "voteCount": 10,
        "content": "Option D provides a robust and cost-effective solution that meets the company's requirements for making its HBase data highly available while leveraging Amazon EMR's capabilities effectively."
      },
      {
        "date": "2024-02-20T07:17:00.000Z",
        "voteCount": 3,
        "content": "Option D provides a robust and cost-effective solution that meets the company's requirements for making its HBase data highly available while leveraging Amazon EMR's capabilities effectively."
      },
      {
        "date": "2024-02-15T01:49:00.000Z",
        "voteCount": 3,
        "content": "the answer here is D."
      },
      {
        "date": "2024-02-13T17:13:00.000Z",
        "voteCount": 2,
        "content": "D because it requires HA deploying two different AZ"
      },
      {
        "date": "2023-10-14T07:44:00.000Z",
        "voteCount": 3,
        "content": "as cloudlearnerhere explains \"\"D is correct as Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously\""
      },
      {
        "date": "2023-07-31T15:37:00.000Z",
        "voteCount": 1,
        "content": "B for cost\nHbase data availability is satisfied by EMRFS"
      },
      {
        "date": "2023-05-01T06:58:00.000Z",
        "voteCount": 3,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-12-29T11:52:00.000Z",
        "voteCount": 1,
        "content": "Did you clear using this dumps ?"
      },
      {
        "date": "2023-04-21T01:39:00.000Z",
        "voteCount": 2,
        "content": "EMR is Single availability zone cluster which means we need to setup cluster in different avz for high availability. Two primary cluster is not an option. So answer is D"
      },
      {
        "date": "2023-04-20T15:50:00.000Z",
        "voteCount": 2,
        "content": "this recent aws documentation stateshttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-plan-consistent-view.html indicates consistent views are not supported and is not needed since 2020 . So yes D seems accurate or best answer but these questions are outdated and given how fast features change in AWS , this question certainly would be worded differently ."
      },
      {
        "date": "2023-04-07T10:43:00.000Z",
        "voteCount": 1,
        "content": "Has to be option B , because it says HBASE data to be highly available which is already satisfied by EMRFS. It doesn't talk about cluster availability directly anywhere also considering the costs option D can be eliminated compared to B."
      },
      {
        "date": "2023-04-03T19:21:00.000Z",
        "voteCount": 1,
        "content": "Just to tell you why not B.\n\nEnabling EMRFS consistent view and pointing the HBase root directory to an Amazon S3 bucket are two different concepts, but they are related in this scenario.\n\nEMRFS (EMR File System) is a file system interface that allows EMR clusters to access data stored in Amazon S3 in the same way as data stored on HDFS. By enabling EMRFS consistent view, EMR ensures that all nodes in the cluster see a consistent view of data stored in S3, which is important for applications like HBase that require strong consistency.\n\nOn the other hand, pointing the HBase root directory to an S3 bucket means that HBase tables and metadata are stored in S3, rather than on HDFS. This allows HBase to take advantage of the durability and scalability of S3, while still providing low-latency access to data.\n\nSo, in option B, the company is using both EMRFS and S3. EMRFS is used to provide a consistent view of data stored in S3, while HBase is configured to store its tables and metadata in S3."
      },
      {
        "date": "2023-03-11T04:36:00.000Z",
        "voteCount": 3,
        "content": "D. Store the data on an EMR File System (EMRFS) instead of HDFS and enable EMRFS consistent view. Create a primary EMR HBase cluster with multiple master nodes. Create a secondary EMR HBase read-replica cluster in a separate Availability Zone. Point both clusters to the same HBase root directory in the same Amazon S3 bucket.\n\nThis solution provides a cost-effective way to make HBase data highly available by creating a primary EMR HBase cluster with multiple master nodes and a secondary EMR HBase read-replica cluster in a separate Availability Zone. By storing data on EMRFS and enabling EMRFS consistent view, both clusters can access the same data stored on an Amazon S3 bucket. This eliminates the need to store data redundantly and reduces costs. The use of multiple master nodes improves HBase availability and reliability. If the primary cluster fails, the secondary read-replica cluster can continue to serve read traffic."
      },
      {
        "date": "2023-01-31T16:12:00.000Z",
        "voteCount": 3,
        "content": "D based on Bonso Udemy Course"
      },
      {
        "date": "2022-12-12T22:24:00.000Z",
        "voteCount": 1,
        "content": "Data highly available.\nD is not cost effective."
      },
      {
        "date": "2022-10-16T17:42:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/73892-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A software company hosts an application on AWS, and new features are released weekly. As part of the application testing process, a solution must be developed that analyzes logs from each Amazon EC2 instance to ensure that the application is working as expected after each deployment. The collection and analysis solution should be highly available with the ability to display new information with minimal delays.<br>Which method should the company use to collect and analyze the logs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable detailed monitoring on Amazon EC2, use Amazon CloudWatch agent to store logs in Amazon S3, and use Amazon Athena for fast, interactive log analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Kinesis Producer Library (KPL) agent on Amazon EC2 to collect and send data to Kinesis Data Streams to further push the data to Amazon OpenSearch Service (Amazon Elasticsearch Service) and visualize using Amazon QuickSight.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Kinesis Producer Library (KPL) agent on Amazon EC2 to collect and send data to Kinesis Data Firehose to further push the data to Amazon OpenSearch Service (Amazon Elasticsearch Service) and OpenSearch Dashboards (Kibana).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch subscriptions to get access to a real-time feed of logs and have the logs delivered to Amazon Kinesis Data Streams to further push the data to Amazon OpenSearch Service (Amazon Elasticsearch Service) and OpenSearch Dashboards (Kibana)."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-08T02:47:00.000Z",
        "voteCount": 21,
        "content": "I don't understand why everyone is choosing C. First of all, KPL does not send data to Kinesis Firehose, it sends data to Kinesis Data Streams, so C is very much incorrect. Second, the term KPL agent, there is no such thing. We would install Kinesis Agent on EC2 and not KPL Agent, so B and C are incorrect. In Option D, you see that it is using Cloudwatch logs which already offers what the customer wants... and implementing an Opensearch with Kibana for it would be overengineering and duplicating the same solution in another tool, duplicating data and cost. In option A it is using CloudWatch logs and Athena, which is easy to configure and works well. The answer does not say it, but the customer could use Cloudwatch Dashboards and Cloudwatch Metrics generated from the log stream. Activating EC2 detailed monitoring is not necessary though. Another thing: you guys are saying that Data Streams cannot deliver data to ES, but actually by using a Lambda you can pretty much do this... This question is very strange... But, by elimination, I would stay with option A."
      },
      {
        "date": "2022-12-12T23:15:00.000Z",
        "voteCount": 6,
        "content": "KPL to Firehose is possible.\nhttps://docs.aws.amazon.com/streams/latest/dev/kpl-with-firehose.html"
      },
      {
        "date": "2023-01-11T08:29:00.000Z",
        "voteCount": 9,
        "content": "Direct KPL to Firehose is not possible. The above doc says \nKPL --&gt; Data streams --&gt; Firehose. \nCorrect if I am wrong here."
      },
      {
        "date": "2023-07-14T04:07:00.000Z",
        "voteCount": 1,
        "content": "Ability to display is missing in Athena"
      },
      {
        "date": "2022-09-30T01:25:00.000Z",
        "voteCount": 3,
        "content": "Agree, kinesis agent can write to firehose, KPL can't. So A"
      },
      {
        "date": "2022-10-20T22:11:00.000Z",
        "voteCount": 2,
        "content": "why not D?"
      },
      {
        "date": "2022-11-14T01:10:00.000Z",
        "voteCount": 3,
        "content": "You are right, changed to D!"
      },
      {
        "date": "2023-06-29T02:04:00.000Z",
        "voteCount": 2,
        "content": "The problem with D is that kinesis data stream cannot write directly to opensearch. Only via lambda. Thats why it seams there is not correct answer... unless C is to be interpreted as kinesis agent.\nhttps://opensearch.org/docs/1.1/opensearch/data-streams/"
      },
      {
        "date": "2022-10-16T21:30:00.000Z",
        "voteCount": 5,
        "content": "But it 'should be highly available with the ability to display new information with minimal delays.' D is real time solution, while A is not."
      },
      {
        "date": "2024-09-06T01:07:00.000Z",
        "voteCount": 9,
        "content": "D is correct. CloudWatch subscription is realtime. Push logs to Kinesis Firehose or Streams which can push into ElasticSearch for aggregation and dashboard.\nCloudWatch-Subs -&gt; Amazon Streams - &gt; Firehose -&gt; ElasticSearch -&gt; Dashbaord"
      },
      {
        "date": "2024-02-28T09:39:00.000Z",
        "voteCount": 2,
        "content": "C is correct, KDS cannot send data to OpenSearch as stated in answer D. Also answer D does not mention how the logs will get ingested from the EC2 instances to CloudWatch. logs."
      },
      {
        "date": "2023-12-11T09:41:00.000Z",
        "voteCount": 1,
        "content": "real-time capabilities are not a strict requirement and minimal delay is the primary consideration, then using Kinesis might indeed be considered overkill for your specific use case. In such scenarios, a simpler and cost-effective solution, such as Option A (CloudWatch + S3 + Athena), could be more suitable. This architecture allows for periodic log analysis without the need for real-time streaming.\n\nOption A provides a straightforward setup with CloudWatch for log collection, S3 for storage, and Athena for fast and interactive log analytics. It is a serverless solution that can meet your requirements while minimizing complexity.\n\nConsider your specific needs, the frequency of log analysis, and the trade-offs between simplicity, cost, and real-time capabilities when making your decision. If real-time insights are unnecessary, a less complex solution like Option A might be more appropriate."
      },
      {
        "date": "2023-12-10T09:58:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2023-11-24T01:50:00.000Z",
        "voteCount": 2,
        "content": "Very weird one:\nI'm also inbetween C &amp; D but bot are making no sense because of:\nC: KPL can't write into Firehose. The normal process flow would be KPL -&gt; KDS -&gt; KDF.\nD: Is weird because they KDS can't write directly to OpenSearch. The normal process flow would be CloudWatch -&gt; KDS -&gt; Lambda -&gt; OpenSearch. In addition to that, the question is talking about logs of the application itself and not metrics/logs of the EC2 - Instance (I'm not a native english speaker but this is how I've understood the question). I don't think that CloudWatch is the right tool for that. \n\nIn the end I go with C (hoping KPL agent is somewhat of a typo and they mean just Agent)."
      },
      {
        "date": "2023-10-31T11:51:00.000Z",
        "voteCount": 1,
        "content": "KPL cannot send data directly to KDF.\n \n\"You can send data to your Kinesis Data Firehose Delivery stream using different types of sources: You can use a Kinesis data stream, the Kinesis Agent, or the Kinesis Data Firehose API using the AWS SDK. You can also use Amazon CloudWatch Logs, CloudWatch Events, or AWS IoT as your data source.\"\n\nSource: https://docs.aws.amazon.com/firehose/latest/dev/basic-write.html#\n\nSo, C is incorrect. D would be 100% correct IF it had KDF instead of KDS. I am guessing it's a typo or who knows what !! I am gonna pretend this question won't show up in the exam :)"
      },
      {
        "date": "2023-10-14T07:58:00.000Z",
        "voteCount": 1,
        "content": "D. I thought in a different option but:\nA. \"detailed monitoring on Amazon EC2\" is for metrics, and report 1 minute period, so is NOT real time\nBC. Do not exist KPL agent\nD. Is the real time option"
      },
      {
        "date": "2023-10-09T22:53:00.000Z",
        "voteCount": 1,
        "content": "So what is the answer here?"
      },
      {
        "date": "2023-10-08T07:05:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D, although in reality none is correct. I had this same question but instead of KDS it was Kinesis Data Firehose. Answer A B C were also among the answers and were all incorrect. Answer C will cause some delays while CloudWatch subscriptions are near-real time"
      },
      {
        "date": "2023-09-29T15:20:00.000Z",
        "voteCount": 2,
        "content": "C - i guess correct\nD - says KDS --&gt; OpenSearch does not work."
      },
      {
        "date": "2023-09-09T05:04:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html#integrations-kinesis"
      },
      {
        "date": "2023-08-02T10:51:00.000Z",
        "voteCount": 2,
        "content": "Clearly C, \nD can't be correct, Kinesis Data Streams cant send data directly to OpenSearch"
      },
      {
        "date": "2023-08-02T10:54:00.000Z",
        "voteCount": 1,
        "content": "Apparently CloudWatch logs can send data to KDS, who knew https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html"
      },
      {
        "date": "2023-08-02T10:54:00.000Z",
        "voteCount": 1,
        "content": "So answer is D"
      },
      {
        "date": "2023-08-02T10:55:00.000Z",
        "voteCount": 1,
        "content": "Actually no, its C, since Kinesis Data Stream cannot send logs to OpenSearch haha"
      },
      {
        "date": "2023-08-01T10:23:00.000Z",
        "voteCount": 1,
        "content": "going w D"
      },
      {
        "date": "2023-07-16T13:37:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2023-07-08T02:12:00.000Z",
        "voteCount": 3,
        "content": "I think D is not correct, as Kinesis Data Streams can't push directly to OpenSearch Service: \"You can still use other sources to load streaming data, such as Amazon Kinesis Data Firehose and Amazon CloudWatch Logs, which have built-in support for OpenSearch Service. Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers.\" You need Lambda to do that. \nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html\nSo C is the correct for me. \nYes, Amazon Kinesis Producer Library (KPL) agent does not exist, but i think they are refering just to the agent https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html"
      },
      {
        "date": "2023-05-01T07:00:00.000Z",
        "voteCount": 2,
        "content": "D:: I passed the test"
      },
      {
        "date": "2023-07-08T02:07:00.000Z",
        "voteCount": 3,
        "content": "Kinesis Data Streams can't push directly to OpenSearch Service"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/27799-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analyst is using AWS Glue to organize, cleanse, validate, and format a 200 GB dataset. The data analyst triggered the job to run with the Standard worker type. After 3 hours, the AWS Glue job status is still RUNNING. Logs from the job run show no error codes. The data analyst wants to improve the job execution time without overprovisioning.<br>Which actions should the data analyst take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job bookmarks in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the executor- cores job parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the spark.yarn.executor.memoryOverhead job parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job bookmarks in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the num- executors job parameter."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-02T07:26:00.000Z",
        "voteCount": 15,
        "content": "Answer: B \nB. Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.\n\nSimilar question is there in Jon Bonso's practice exam."
      },
      {
        "date": "2022-11-04T04:53:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is B as job metrics can be used to estimate the number of DPUs needed.\n\nOptions A &amp; D are wrong as Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data.\n\n\nOptions A &amp; D are wrong as Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data."
      },
      {
        "date": "2023-10-14T08:00:00.000Z",
        "voteCount": 1,
        "content": "B. Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter."
      },
      {
        "date": "2023-07-31T15:47:00.000Z",
        "voteCount": 1,
        "content": "It's a B"
      },
      {
        "date": "2023-05-01T07:02:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-11T03:49:00.000Z",
        "voteCount": 5,
        "content": "B. Enable job metrics in AWS Glue to estimate the number of data processing units (DPUs). Based on the profiled metrics, increase the value of the maximum capacity job parameter.\n\nThe data analyst should enable job metrics in AWS Glue to estimate the number of data processing units (DPUs) and profile the job to understand its resource requirements. Based on the profiled metrics, the data analyst should increase the value of the maximum capacity job parameter. This parameter controls the maximum number of DPUs that the job can use. By increasing the maximum capacity, the job can use more resources and complete faster without overprovisioning. Enabling job bookmarks can help with incremental processing but will not directly improve job execution time. Increasing the value of the executor-cores job parameter or the spark.yarn.executor.memoryOverhead job parameter may improve performance, but these parameters depend on the specific job requirements and are not directly related to the job's resource utilization. Similarly, increasing the num-executors job parameter will not directly improve job execution time."
      },
      {
        "date": "2022-07-25T21:48:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      },
      {
        "date": "2021-11-04T05:45:00.000Z",
        "voteCount": 2,
        "content": "\uc815\ub2f5:B\n\ubd81\ub9c8\ud06c\ub294 \uc0c1\ud0dc\ub97c \ud45c\uc2dc\ud558\ub294\uac83\uc774\uae30\ub54c\ubb38\uc5d0 A,D\ub294 \uc81c\uc678.\n\uba54\ud2b8\ub9ad\uc744 \ud1b5\ud574\uc11c dpu\uc801\uc815 \uac1c\uc218\ub97c \ucd94\uc815\ud558\uace0, \ucd5c\ub300\uc6a9\ub7c9\uc744 \ub298\ub9ac\uba74 \ub429\ub2c8\ub2e4.\n\uc624\ubc84\ud5e4\ub4dc \ud30c\ub77c\ubbf8\ud130\ub294 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud588\uc744\ub54c \uc218\uc815\ud574\uc57c\ud558\uc9c0\ub9cc, \ubb38\uc81c\ub294 3\uc2dc\uac04\ub3d9\uc548 \ub7ec\ub2dd\uc0c1\ud0dc\uc774\uae30\ub54c\ubb38\uc5d0 \uc5d0\ub7ec\uac00 \ub09c\uac83\uc740 \uc544\ub2c8\ubbc0\ub85c C \uc81c\uc678\nhttps://docs.aws.amazon.com/ko_kr/glue/latest/dg/monitor-debug-capacity.html#monitor-debug-capacity-fix"
      },
      {
        "date": "2024-01-08T07:46:00.000Z",
        "voteCount": 1,
        "content": "Clear like water"
      },
      {
        "date": "2021-11-03T19:16:00.000Z",
        "voteCount": 1,
        "content": "I suggest taking Jon Bonso's practice exams too."
      },
      {
        "date": "2021-11-03T02:38:00.000Z",
        "voteCount": 3,
        "content": "This question is quite confused because for Glue version 2.0 jobs, you cannot instead specify a Maximum capacity. Instead, you should specify a Worker type and the Number of workers. However since A, D (no such parameters) and C (memoryOverhead not help in this case) are wrong, the best choice is B"
      },
      {
        "date": "2021-10-31T17:05:00.000Z",
        "voteCount": 1,
        "content": "Ans B\nA and D = wrong, the name \u201cbookmark\u201d suggests persistency of a in-progress state, and so it is, used to track processed data, not for scaling. C = wrong, although you can set this parameter, the job metrics won\u2019t help you, and this parameter won\u2019t help long job running times because that was due to lack of computational power not memory."
      },
      {
        "date": "2021-10-30T11:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2021-10-18T16:56:00.000Z",
        "voteCount": 1,
        "content": "B. \n\nA, D. Bookmark is not used for monitoring ETL job status."
      },
      {
        "date": "2021-10-17T15:13:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-14T06:13:00.000Z",
        "voteCount": 4,
        "content": "B.\nB and C can make sense but C will be right only if the job returned the error spark.yarn.executor.memoryOverhead.\nIf no error, then the job is just taking too long so increase the max capacity\nFor AWS Glue version 2.0 jobs, you cannot instead specify a Maximum capacity. Instead, you should specify a Worker type and the Number of workers. \nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html"
      },
      {
        "date": "2021-10-13T11:25:00.000Z",
        "voteCount": 1,
        "content": "b is correct!"
      },
      {
        "date": "2021-10-12T17:26:00.000Z",
        "voteCount": 1,
        "content": "Option B for sure. We can eliminate the two options with Bookmarks and spark.yarn.executor.memoryOverhead has nothing to do with Glue."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/27694-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has a business unit uploading .csv files to an Amazon S3 bucket. The company's data platform team has set up an AWS Glue crawler to do discovery, and create tables and schemas. An AWS Glue job writes processed data from the created tables to an Amazon Redshift database. The AWS Glue job handles column mapping and creating the Amazon Redshift table appropriately. When the AWS Glue job is rerun for any reason in a day, duplicate records are introduced into the Amazon Redshift table.<br>Which solution will update the Redshift table without duplicates when jobs are rerun?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the AWS Glue job to copy the rows into a staging table. Add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the previously inserted data into a MySQL database in the AWS Glue job. Perform an upsert operation in MySQL, and copy the results to the Amazon Redshift table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Spark's DataFrame dropDuplicates() API to eliminate duplicates and then write the data to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue ResolveChoice built-in transform to select the most recent value of the column."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T14:16:00.000Z",
        "voteCount": 19,
        "content": "Answer should be A according to the link provided. Thoughts?"
      },
      {
        "date": "2021-12-21T20:44:00.000Z",
        "voteCount": 6,
        "content": "Indeed A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/sql-commands-redshift-glue-job/"
      },
      {
        "date": "2021-11-05T09:55:00.000Z",
        "voteCount": 15,
        "content": "B is wrong. We don't need a staging DB here which is costly and moreover MySQL is not the right choice.\nC. dropDuplicates() is used to remove duplicate records in the Spark not destination DB\nD. ResolveChoice is to cast data with unidentified data type to a specified data type and also work on Spark not destination DB.\n\nA is the answer"
      },
      {
        "date": "2023-07-31T15:50:00.000Z",
        "voteCount": 1,
        "content": "It's n A"
      },
      {
        "date": "2023-05-21T06:20:00.000Z",
        "voteCount": 2,
        "content": "To me A looks correct answer, check this link\n\nhttps://stackoverflow.com/questions/52397646/aws-glue-to-redshift-duplicate-data"
      },
      {
        "date": "2023-05-01T07:03:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-11T03:59:00.000Z",
        "voteCount": 4,
        "content": "A. Modify the AWS Glue job to copy the rows into a staging table. Add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class.\n\nTo update the Redshift table without duplicates when AWS Glue jobs are rerun, the company should modify the AWS Glue job to copy the rows into a staging table. The job should then add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class. This approach ensures that the data written to the Redshift table does not contain any duplicates, and the table only contains the latest data.\n\nLoading the previously inserted data into a MySQL database and performing an upsert operation may be a feasible approach but adds complexity to the architecture. Using Spark's dropDuplicates() API to eliminate duplicates may not always work correctly when dealing with large datasets. Using the ResolveChoice built-in transform is used for handling schema changes in a column, not for removing duplicates."
      },
      {
        "date": "2023-03-04T08:46:00.000Z",
        "voteCount": 1,
        "content": "With option B, it is to copy the RedShift data into SQL and back to RedShift.\n\nOption A is simpler"
      },
      {
        "date": "2023-02-02T14:42:00.000Z",
        "voteCount": 1,
        "content": "A, for sure"
      },
      {
        "date": "2022-11-27T19:18:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer - A\n\nB is is incorrect because you can't use the COPY command to copy data directly from a MySQL database into Amazon Redshift. A workaround for this is to move the MySQL data into Amazon S3 and use AWS Glue as a staging table to perform the upsert operation. Since this method requires more effort, it is not the best approach to solve the problem."
      },
      {
        "date": "2022-11-04T04:56:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is A as Redshift does not support merge or upsert on the single table. However, a staging table can be created and data merged with the main table.\n\nOption B is wrong as a staging DB as MySQL is not required.\n\nOption C is wrong as dropDuplicates() is used to remove duplicate records in the Spark and not destination DB.\n\nOption D is wrong as ResolveChoice is to cast data with an unidentified data type to a specified data type. It does not handle duplicates."
      },
      {
        "date": "2022-07-27T23:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-10-21T14:21:00.000Z",
        "voteCount": 1,
        "content": "Got confused with C as dataframe.dropDuplicates() also will work, but as per the given question, we have to stick to AWS Glue job, thus Answer is A."
      },
      {
        "date": "2022-05-21T05:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-01-14T00:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-11-03T20:27:00.000Z",
        "voteCount": 3,
        "content": "Answer: A. Modify the AWS Glue job to copy the rows into a staging table. Add SQL commands to replace the existing rows in the main table as postactions in the DynamicFrameWriter class."
      },
      {
        "date": "2021-11-03T18:51:00.000Z",
        "voteCount": 2,
        "content": "The answer is A."
      },
      {
        "date": "2021-10-31T17:39:00.000Z",
        "voteCount": 1,
        "content": "A should be the right answer. I found a link that helps to explain why. https://aws.amazon.com/pt/premiumsupport/knowledge-center/sql-commands-redshift-glue-job/"
      },
      {
        "date": "2021-10-30T22:59:00.000Z",
        "voteCount": 4,
        "content": "A is the right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/28263-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A streaming application is reading data from Amazon Kinesis Data Streams and immediately writing the data to an Amazon S3 bucket every 10 seconds. The application is reading data from hundreds of shards. The batch interval cannot be changed due to a separate requirement. The data is being accessed by Amazon<br>Athena. Users are seeing degradation in query performance as time progresses.<br>Which action can help improve query performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge the files in Amazon S3 to form larger files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in Kinesis Data Streams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more memory and CPU capacity to the streaming application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the files to multiple S3 buckets."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T17:03:00.000Z",
        "voteCount": 41,
        "content": "It should be A, large number of small files ins3 will slow down reads"
      },
      {
        "date": "2021-09-28T04:30:00.000Z",
        "voteCount": 5,
        "content": "Yeap, I agree its A."
      },
      {
        "date": "2021-12-21T01:50:00.000Z",
        "voteCount": 2,
        "content": "You can speed up your queries dramatically by compressing your data, provided that files are splittable or of an optimal size (optimal S3 file size is between 200MB-1GB). Smaller data sizes mean less network traffic between Amazon S3 to Athena."
      },
      {
        "date": "2021-10-08T20:23:00.000Z",
        "voteCount": 10,
        "content": "Merge the files in Amazon S3 to form larger files will definitely increase read performance. So option A is the right choice."
      },
      {
        "date": "2024-02-27T16:45:00.000Z",
        "voteCount": 1,
        "content": "Everyone is saying A, which is write but why because 1000's shard and per shard capacity is 1 mb , So 1000's of files per second . which require merge to improve the query performance."
      },
      {
        "date": "2023-07-31T15:51:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-05-01T07:04:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-05-14T10:36:00.000Z",
        "voteCount": 1,
        "content": "what is right answer"
      },
      {
        "date": "2023-03-31T02:04:00.000Z",
        "voteCount": 1,
        "content": "A. This bit of AWS documentation: https://docs.aws.amazon.com/athena/latest/ug/performance-tuning-s3-throttling.html says \"If possible, avoid having a large number of small files. Amazon S3 has a limit of 5500 requests per second, and your Athena queries share this same limit. If you scan millions of small objects in a single query, your query will likely be throttled by Amazon S3.\""
      },
      {
        "date": "2023-03-11T04:09:00.000Z",
        "voteCount": 4,
        "content": "A. Merge the files in Amazon S3 to form larger files.\n\nTo improve query performance when using Amazon Athena to access data from an Amazon S3 bucket, the streaming application should merge the files in S3 to form larger files. When the streaming application writes data to S3 every 10 seconds, it creates small files, which can lead to a large number of small files over time. This can lead to performance degradation in Athena queries as more small files mean more metadata needs to be scanned, and more file operations are required to read data. By merging small files into larger files, the number of files in the bucket can be reduced, which can significantly improve Athena query performance.\n\nIncreasing the number of shards in Kinesis Data Streams, adding more memory and CPU capacity to the streaming application, or writing files to multiple S3 buckets are not directly related to the issue of degraded query performance in Athena."
      },
      {
        "date": "2023-03-04T09:00:00.000Z",
        "voteCount": 1,
        "content": "s3 has a limit of 5500 requests per second, combining reduces the requests\n\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html"
      },
      {
        "date": "2022-11-04T04:59:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is A as merging files to form a bigger file can help optimize and improve query performance.\nOption B is wrong as increasing shards would only increase the ingestion flow.\n\nOptions C &amp; D are wrong as it does not improve Athena's query performance."
      },
      {
        "date": "2022-10-14T11:01:00.000Z",
        "voteCount": 2,
        "content": "Merging small files into larger files will reduce the number of compute activities and speed up the process"
      },
      {
        "date": "2022-09-05T17:35:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2022-07-19T22:24:00.000Z",
        "voteCount": 2,
        "content": "Answer should be A"
      },
      {
        "date": "2022-07-14T08:26:00.000Z",
        "voteCount": 3,
        "content": "A \nas merging small files into one large file will result in less meta data to maintain for the Data Catalog to maintain which results in Athena to scan data faster"
      },
      {
        "date": "2022-06-19T01:16:00.000Z",
        "voteCount": 2,
        "content": "Things can be done to increase performance of Athena are use columnar formats, use small number of large files, use partitions. So the answer should be A."
      },
      {
        "date": "2022-05-22T01:57:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A"
      },
      {
        "date": "2022-03-17T17:31:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      },
      {
        "date": "2022-02-13T03:31:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer. merge small files into larger files works as expected"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/27696-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company uses Amazon OpenSearch Service (Amazon Elasticsearch Service) to store and analyze its website clickstream data. The company ingests 1 TB of data daily using Amazon Kinesis Data Firehose and stores one day's worth of data in an Amazon ES cluster.<br>The company has very slow query performance on the Amazon ES index and occasionally sees errors from Kinesis Data Firehose when attempting to write to the index. The Amazon ES cluster has 10 nodes running a single index and 3 dedicated master nodes. Each data node has 1.5 TB of Amazon EBS storage attached and the cluster is configured with 1,000 shards. Occasionally, JVMMemoryPressure errors are found in the cluster logs.<br>Which solution will improve the performance of Amazon ES?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory of the Amazon ES master nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the number of Amazon ES data nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the number of Amazon ES shards for the index.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of Amazon ES shards for the index."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-08T19:51:00.000Z",
        "voteCount": 29,
        "content": "IThink its C.\nRefer the below link \nhttps://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/"
      },
      {
        "date": "2021-10-09T09:40:00.000Z",
        "voteCount": 10,
        "content": "I agree with Option C:\nUnbalanced shard allocations across nodes or too many shards in a cluster can cause JVMMemoryPressue.\n\nResolution - Reduce the number of shards by deleting old or unused indices.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/"
      },
      {
        "date": "2023-07-31T15:54:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-05-01T07:05:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-11T04:24:00.000Z",
        "voteCount": 1,
        "content": "C. Decrease the number of Amazon ES shards for the index.\n\nTo improve the performance of Amazon ES in this scenario, the number of shards for the index should be decreased. Currently, the index has 1,000 shards, which is likely causing high overhead and slowing down query performance. In general, it's recommended to have 20-30 GB of data per shard for efficient indexing and query performance in Amazon ES. However, having too many shards can lead to inefficient resource utilization and slow query performance.\n\nAdditionally, since the cluster is configured with 3 dedicated master nodes, increasing the memory of the master nodes may not have a significant impact on performance. Decreasing the number of data nodes may also not be an effective solution, as this could reduce the capacity of the cluster to handle the 1 TB of daily data ingestion.\n\nIncreasing the number of shards for the index would further exacerbate the performance issues, as more shards would lead to more overhead and slower query performance."
      },
      {
        "date": "2022-11-27T03:41:00.000Z",
        "voteCount": 2,
        "content": "According to this link \nhttps://aws.amazon.com/premiumsupport/knowledge-center/opensearch-high-jvm-memory-pressure/\nIt clearly states that this issue can be caused due to \"Unbalanced shard allocations across nodes or too many shards in a cluster.\""
      },
      {
        "date": "2022-11-04T05:06:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as one of the causes of JVMMemoryPressure error can be Unbalanced shard allocations across nodes or too many shards in a cluster. and can be resolved by reducing the number of shards by deleting old or unused indices.\n\nOption A is wrong  because dedicated master nodes are only used to increase cluster stability. Therefore, this option won't help you improve the performance of the cluster.\n\nOption B is incorrect because these nodes carry all the data in your indexes (storage) and do all the processing for your requests (CPU). If you decrease the number of data nodes, the performance of the cluster still won't improve.\n\nOption D  is incorrect. The JVMMemoryPressure error signifies that there is an unbalanced shard allocations across nodes. This means that there are too many shards in the Amazon ES cluster and not the other way around. To improve the performance of the cluster, you must decrease the number of shards."
      },
      {
        "date": "2022-07-26T19:28:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-03-19T09:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is C."
      },
      {
        "date": "2021-12-19T19:54:00.000Z",
        "voteCount": 1,
        "content": "Ans - C\nI got this question for my certification I gave on Dec 9th 2021."
      },
      {
        "date": "2021-11-21T07:23:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2021-11-06T14:40:00.000Z",
        "voteCount": 2,
        "content": "Answer: C. Decrease the number of Amazon ES shards for the index.\nMemory pressure in the JVM can result if:\n\tYou have unbalanced shard allocations across nodes\n\tYou have too many shards in a cluster\n\nFewer shards can yield better performance if JVMMemoryPressure errors\nare encountered\n\tDelete old or unused indices"
      },
      {
        "date": "2021-11-05T14:42:00.000Z",
        "voteCount": 6,
        "content": "C is correct. from documentation --\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/sizing-domains.html\nHere the shard size is 1.5*1000 (GB)/1000 (number of shards)= 1.5 GB which is  much less than recommended size of shards.\n\nThe overarching goal of choosing a number of shards is to distribute an index evenly across all data nodes in the cluster. However, these shards shouldn't be too large or too numerous. A good rule of thumb is to try to keep shard size between 10\u201350 GiB. Large shards can make it difficult for Elasticsearch to recover from failure, but because each shard uses some amount of CPU and memory, having too many small shards can cause performance issues and out of memory errors. In other words, shards should be small enough that the underlying Amazon ES instance can handle them, but not so small that they place needless strain on the hardware."
      },
      {
        "date": "2021-11-03T12:18:00.000Z",
        "voteCount": 3,
        "content": "C is the right answer"
      },
      {
        "date": "2021-11-01T03:05:00.000Z",
        "voteCount": 1,
        "content": "The question says the domain is running one index, if so how can we assume there are old unused indices. shouldn't we reindex to adjust the number of shards?"
      },
      {
        "date": "2021-10-30T10:14:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-10-30T00:11:00.000Z",
        "voteCount": 3,
        "content": "From link https://aws.amazon.com/premiumsupport/knowledge-center/high-jvm-memory-pressure-elasticsearch/\n\nYou can resolve high JVM memory pressure issues by reducing traffic to the cluster. To reduce traffic to the cluster, follow these best practices:\nReduce the number of shards by deleting old or unused indices."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/27699-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A manufacturing company has been collecting IoT sensor data from devices on its factory floor for a year and is storing the data in Amazon Redshift for daily analysis. A data analyst has determined that, at an expected ingestion rate of about 2 TB per day, the cluster will be undersized in less than 4 months. A long-term solution is needed. The data analyst has indicated that most queries only reference the most recent 13 months of data, yet there are also quarterly reports that need to query all the data generated from the past 7 years. The chief technology officer (CTO) is concerned about the costs, administrative effort, and performance of a long-term solution.<br>Which solution should the data analyst use to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a daily job in AWS Glue to UNLOAD records older than 13 months to Amazon S3 and delete those records from Amazon Redshift. Create an external table in Amazon Redshift to point to the S3 location. Use Amazon Redshift Spectrum to join to data that is older than 13 months.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the Amazon Redshift cluster. Restore the cluster to a new cluster using dense storage nodes with additional storage capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute a CREATE TABLE AS SELECT (CTAS) statement to move records that are older than 13 months to quarterly partitioned data in Amazon Redshift Spectrum backed by Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnload all the tables in Amazon Redshift to an Amazon S3 bucket using S3 Intelligent-Tiering. Use AWS Glue to crawl the S3 bucket location to create external tables in an AWS Glue Data Catalog. Create an Amazon EMR cluster using Auto Scaling for any daily analytics needs, and use Amazon Athena for the quarterly reports, with both using the same AWS Glue Data Catalog."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T17:19:00.000Z",
        "voteCount": 53,
        "content": "Option A; We have implemented this to save cost ."
      },
      {
        "date": "2021-10-02T11:57:00.000Z",
        "voteCount": 20,
        "content": "B is not correct because snapshotting will save costs but not solve problem of cluster being undersized\nC is not correct because - CTAS is not used to move data to S3 via spectrum. CTAS Creates a new table based on a query. The owner of this table is the user that issues the command. \nD is incorrect because EMR cannot be used as Data Warehouse solution And they do not need interactive query with Athena.\nA is correct because that exactly specifies how to move data to Redshift spectrum and reduce cluster space: https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html"
      },
      {
        "date": "2024-02-12T22:37:00.000Z",
        "voteCount": 1,
        "content": "Def A, to save cost and less admin."
      },
      {
        "date": "2023-07-31T15:56:00.000Z",
        "voteCount": 1,
        "content": "its an A"
      },
      {
        "date": "2023-05-01T07:06:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-31T02:13:00.000Z",
        "voteCount": 2,
        "content": "A. The Udemy course by Stephane Maarek and Frank Kane has a really similar question in the practice exam."
      },
      {
        "date": "2022-11-04T05:12:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A as the AWS Glue job can be used to offload the data older than 13 months from Redshift to S3. 13 months data can be queried from Redshift, while 7 years data in S3 can be queried using Redshift Spectrum.\n\n\n\nOption B is wrong as this would increase the cost further and would not scale far.\n\nOption C is wrong as CTAS is not used to move data to S3 via the spectrum. CTAS creates a new table based on a query. The owner of this table is the user that issues the command.\n\nOption D is wrong as EMR would increase the administrative effort as compared to Redshift."
      },
      {
        "date": "2022-10-27T07:15:00.000Z",
        "voteCount": 2,
        "content": "I am wondering why in the portal the correct ans is given as B.  who validated and gives the right ans here?"
      },
      {
        "date": "2022-09-05T17:57:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\nhttps://d1.awsstatic.com/whitepapers/amazon-redshift-cost-optimization.pdf"
      },
      {
        "date": "2022-07-21T19:45:00.000Z",
        "voteCount": 1,
        "content": "Answer-A"
      },
      {
        "date": "2022-05-22T01:39:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A"
      },
      {
        "date": "2022-04-30T15:44:00.000Z",
        "voteCount": 1,
        "content": "Answer-A"
      },
      {
        "date": "2022-03-12T09:25:00.000Z",
        "voteCount": 3,
        "content": "A ticks all the boxes"
      },
      {
        "date": "2021-11-20T06:51:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2021-11-07T06:37:00.000Z",
        "voteCount": 1,
        "content": "B = wrong, this will not solve either cost or scale problem. C = wrong, to create table on S3 you use CREATE EXTERNAL TABLE not CTAS, also this does not remove older data. D = wrong, nonsense."
      },
      {
        "date": "2021-10-20T20:55:00.000Z",
        "voteCount": 2,
        "content": "The answer is A."
      },
      {
        "date": "2021-10-19T16:56:00.000Z",
        "voteCount": 1,
        "content": "When reading the Post : https://aws.amazon.com/blogs/big-data/amazon-redshift-dense-compute-dc2-nodes-deliver-twice-the-performance-as-dc1-at-the-same-price/, Option B Makes More sense.. any thoughts.."
      },
      {
        "date": "2021-10-25T10:18:00.000Z",
        "voteCount": 1,
        "content": "It's not cost effective.."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/28715-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An insurance company has raw data in JSON format that is sent without a predefined schedule through an Amazon Kinesis Data Firehose delivery stream to an<br>Amazon S3 bucket. An AWS Glue crawler is scheduled to run every 8 hours to update the schema in the data catalog of the tables stored in the S3 bucket. Data analysts analyze the data using Apache Spark SQL on Amazon EMR set up with AWS Glue Data Catalog as the metastore. Data analysts say that, occasionally, the data they receive is stale. A data engineer needs to provide access to the most up-to-date data.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external schema based on the AWS Glue Data Catalog on the existing Amazon Redshift cluster to query new data in Amazon S3 with Amazon Redshift Spectrum.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch Events with the rate (1 hour) expression to execute the AWS Glue crawler every hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the AWS CLI, modify the execution schedule of the AWS Glue crawler from 8 hours to 1 minute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the AWS Glue crawler from an AWS Lambda function triggered by an S3:ObjectCreated:* event notification on the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T17:22:00.000Z",
        "voteCount": 37,
        "content": "Answer: D\nData analysts analyze the data using Apache Spark SQL on Amazon EMR for the data stored on S3 in JSON format. \nInput JSON file landing in S3 triggers a Lambda which invokes Glue Crawler."
      },
      {
        "date": "2023-09-19T03:57:00.000Z",
        "voteCount": 2,
        "content": "D is correct out of all these answer , but at the same time running a crawler on bucket for just landing of one object. Is it a good idea ?"
      },
      {
        "date": "2021-09-21T06:35:00.000Z",
        "voteCount": 9,
        "content": "A is on demand (triggered by hand). B minimum time required is 1 hr. C is 1 minute based on the cron schedule syntax. For D, it could reach to sub-minute level since it watches S3 new data events.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html#RateExpressions"
      },
      {
        "date": "2024-02-20T13:52:00.000Z",
        "voteCount": 1,
        "content": "Option A is not directly related to the issue of schema updates and would not address the staleness of data in the AWS Glue Data Catalog. Option B increases the frequency of crawls but still may not provide real-time updates. Option C is not practical or cost-effective due to the excessive number of crawler runs it would trigger, and the AWS Glue crawler cannot be scheduled to run every minute. Option D provides a dynamic, event-driven solution that ensures data analysts have access to the most current data available."
      },
      {
        "date": "2023-07-31T16:00:00.000Z",
        "voteCount": 1,
        "content": "its a D"
      },
      {
        "date": "2023-05-12T05:31:00.000Z",
        "voteCount": 1,
        "content": "Why triggering glue crawler can give us the latest data? Isn\u2019t it only updating metastore?"
      },
      {
        "date": "2023-09-19T03:55:00.000Z",
        "voteCount": 2,
        "content": "yes metastore , but every filename in s3 bucket need to registered in metastore to pick the latest data."
      },
      {
        "date": "2023-05-01T07:07:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2024-02-12T22:41:00.000Z",
        "voteCount": 1,
        "content": "why D?"
      },
      {
        "date": "2023-02-24T08:58:00.000Z",
        "voteCount": 3,
        "content": "very curious to know every answer so far what it says it incorrect as discussion revleas something else, why?"
      },
      {
        "date": "2022-11-04T05:18:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D as it is event-driven and would load the data as soon as the object-created event is triggered.\n\nOption A is wrong as this is still manual and on-demand.\n\nOption B is wrong as the refresh interval is still 1 hr.\n\nOption C is wrong as the minimum precision for the schedule is 5 mins."
      },
      {
        "date": "2022-09-05T18:07:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2022-07-25T21:01:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2022-05-21T04:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-04-30T16:12:00.000Z",
        "voteCount": 1,
        "content": "Answer-D"
      },
      {
        "date": "2022-03-20T04:40:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-11-20T08:11:00.000Z",
        "voteCount": 1,
        "content": "The answer is D."
      },
      {
        "date": "2021-11-06T10:02:00.000Z",
        "voteCount": 1,
        "content": "D seems correct, but could potentially be an expensive solution."
      },
      {
        "date": "2021-11-05T21:56:00.000Z",
        "voteCount": 2,
        "content": "Although D is correct answer, the answer should mention SQS. The crawler will not run fast enough to catch up with objects created."
      },
      {
        "date": "2021-11-05T02:50:00.000Z",
        "voteCount": 2,
        "content": "This is a textbook question. A = wrong, won\u2019t work because schema will update every 8 hours. B = wrong, not most up-to-date. C = wrong, minimum schedule is 5 minutes.\n\nhttps://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/27698-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company that produces network devices has millions of users. Data is collected from the devices on an hourly basis and stored in an Amazon S3 data lake.<br>The company runs analyses on the last 24 hours of data flow logs for abnormality detection and to troubleshoot and resolve user issues. The company also analyzes historical logs dating back 2 years to discover patterns and look for improvement opportunities.<br>The data flow logs contain many metrics, such as date, timestamp, source IP, and target IP. There are about 10 billion events every day.<br>How should this data be stored for optimal performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Apache ORC partitioned by date and sorted by source IP\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn compressed .csv partitioned by date and sorted by source IP",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Apache Parquet partitioned by source IP and sorted by date",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn compressed nested JSON partitioned by source IP and sorted by date"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T04:16:00.000Z",
        "voteCount": 60,
        "content": "A. \nBD dropped due to row based format.\nChoosing between ORC and Parquet format would be tough since their performance is very close. However, data are supposed to partitioned by date then sorted by source IP, so C dropped."
      },
      {
        "date": "2021-09-24T15:35:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-10-01T11:20:00.000Z",
        "voteCount": 12,
        "content": "ORC and Parquet are ideal here. But the data should be partitioned by Date and sorted on IP and not the other way round. So option A is the right choice."
      },
      {
        "date": "2024-02-12T22:46:00.000Z",
        "voteCount": 2,
        "content": "ideal choices will be ORC and Parquet, first choice being Apache Parquet, but here we have to consider partition, and partition by Date then sort on IP is best way to store data."
      },
      {
        "date": "2023-08-02T11:46:00.000Z",
        "voteCount": 1,
        "content": "Between A and C, if we have more IPs than Dates.\nI would go with A since analysis is performed on a daily schedule and anomalies are detected on a time interval."
      },
      {
        "date": "2023-07-31T16:02:00.000Z",
        "voteCount": 1,
        "content": "its an A"
      },
      {
        "date": "2023-05-01T07:09:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-12T23:53:00.000Z",
        "voteCount": 1,
        "content": "Option A: In Apache ORC partitioned by date and sorted by source IP\n\nPartitioning the data by date allows for faster query performance and efficient data retrieval based on time periods.\nSorting the data by source IP enables efficient filtering and joins on that attribute.\nOverall, ORC partitioned by date and sorted by source IP would provide efficient storage and querying of the data."
      },
      {
        "date": "2023-02-21T22:18:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best. The analysis is done on last 24 hours of data. Hence, sorting by IP may not be ideal."
      },
      {
        "date": "2022-11-04T05:25:00.000Z",
        "voteCount": 3,
        "content": "A is the right answer as the company does daily analysis, so it only needs to look at the data generated for a given date\n\nC is wrong as partitioning by source IP is incorrect for this use case, and partitioning by date is optimal.\n\nB &amp; D, Both the above options are not columnar storage formats, they are row-based formats that are not optimal for big data retrievals for complex analytical queries."
      },
      {
        "date": "2022-07-20T23:11:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2022-07-04T02:54:00.000Z",
        "voteCount": 1,
        "content": "Agree with  \"zanhsieh\""
      },
      {
        "date": "2022-06-14T11:19:00.000Z",
        "voteCount": 1,
        "content": "Columnar data is faster such as ORC and Parquet, answer is  A"
      },
      {
        "date": "2022-05-22T01:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2022-04-30T14:12:00.000Z",
        "voteCount": 1,
        "content": "Answer : A"
      },
      {
        "date": "2022-01-29T02:02:00.000Z",
        "voteCount": 2,
        "content": "For previous 24 hours,  sorted by date from C is not helpful. Sorted by timestamp makes sense."
      },
      {
        "date": "2021-11-05T21:31:00.000Z",
        "voteCount": 5,
        "content": "Answer is A. In Apache ORC partitioned by date and sorted by source IP.\nBecause the company analyzes historical logs dating back 2 years and also past 24 hours data.\nHence the Data should be partitioned based on Date and sorted by IP and not the other way around.\nORC is columnar hence preferred data format."
      },
      {
        "date": "2021-11-03T20:42:00.000Z",
        "voteCount": 2,
        "content": "B and D = wrong, use columnar format. C = wrong, partition by date so historical data and be separated."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/27697-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A banking company is currently using an Amazon Redshift cluster with dense storage (DS) nodes to store sensitive data. An audit found that the cluster is unencrypted. Compliance requirements state that a database with sensitive data must be encrypted through a hardware security module (HSM) with automated key rotation.<br>Which combination of steps is required to achieve compliance? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a trusted connection with HSM using a client and server certificate with automatic key rotation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the cluster with an HSM encryption option and automatic key rotation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new HSM-encrypted Amazon Redshift cluster and migrate the data to the new cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable HSM with key rotation through the AWS CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) encryption in the HSM."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T04:34:00.000Z",
        "voteCount": 48,
        "content": "Answer should be A and C. Using HSM you have to create a new cluster (that eliminates B). See link below, it clearly states \"You can't enable hardware security module (HSM) encryption by modifying the cluster. Instead, create a new, HSM-encrypted cluster and migrate your data to the new cluster\"\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html\n\nIn the same link it says you have create certificates. \n\nMy thinking that its not D, its because it can be already configured when you are settinp up the cluster. (option C)"
      },
      {
        "date": "2021-10-08T15:15:00.000Z",
        "voteCount": 2,
        "content": "I dont agree with you on c...... that site you referenced says \"When you modify your cluster to enable KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster. \" also see https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html"
      },
      {
        "date": "2021-10-14T20:42:00.000Z",
        "voteCount": 3,
        "content": "I see now why C is correct ---  \"To migrate an unencrypted cluster to a cluster encrypted using a hardware security module (HSM), you create a new encrypted cluster and move your data to the new cluster. So I agree C is correct"
      },
      {
        "date": "2021-09-26T21:07:00.000Z",
        "voteCount": 15,
        "content": "Answer: A, C\n\nWhen you use an HSM, you must use client and server certificates to configure a trusted connection between Amazon Redshift and your HSM.\n\nReference link:\nhttps://docs.amazonaws.cn/en_us/redshift/latest/mgmt/security-key-management.html  \n\nTo migrate an unencrypted cluster to a cluster encrypted using a hardware security module (HSM), you create a new encrypted cluster and move your data to the new cluster.\n\nReference link: \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html"
      },
      {
        "date": "2024-03-25T19:55:00.000Z",
        "voteCount": 1,
        "content": "Bing is answering C and D. By this explanation \n\nOption A suggests setting up a trusted connection with HSM using a client and server certificate with automatic key rotation. While this is a valid method for some systems, it\u2019s not directly applicable to Amazon Redshift. Redshift doesn\u2019t support this method for enabling encryption.\nOption C is correct because Amazon Redshift doesn\u2019t allow you to modify an existing cluster to use HSM encryption. You would need to create a new HSM-encrypted Redshift cluster and migrate the data to it.\nOption D is also correct. Once the new HSM-encrypted Redshift cluster is set up, you can enable HSM with key rotation through the AWS CLI."
      },
      {
        "date": "2023-07-31T16:05:00.000Z",
        "voteCount": 1,
        "content": "It's AC"
      },
      {
        "date": "2023-05-01T07:10:00.000Z",
        "voteCount": 1,
        "content": "AC: I passed the test"
      },
      {
        "date": "2022-11-04T05:28:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A &amp; C as Redshift does not allow encrypting existing cluster using HSM and there needs to be trust connection established between Redshift and HSM.\n\nOptions B &amp; D are wrong as You can enable encryption when you launch your cluster, or you can modify an unencrypted cluster to use AWS Key Management Service (AWS KMS) encryption.\n\nOption E is wrong as it is not valid."
      },
      {
        "date": "2022-07-21T19:49:00.000Z",
        "voteCount": 1,
        "content": "Answer-A,C"
      },
      {
        "date": "2022-05-22T01:38:00.000Z",
        "voteCount": 1,
        "content": "Answer is A &amp; C"
      },
      {
        "date": "2022-04-30T15:53:00.000Z",
        "voteCount": 1,
        "content": "Answer-A,C"
      },
      {
        "date": "2021-11-20T07:04:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2021-11-06T22:53:00.000Z",
        "voteCount": 3,
        "content": "A, C is correct but why Redshift with HSM is asked in 2020? Redshift only works with HSM Classic and new customer can't create HSM classic anymore."
      },
      {
        "date": "2021-11-06T06:52:00.000Z",
        "voteCount": 1,
        "content": "Answer: A,C (Similar question is there in Jon Bonso's practice exam)."
      },
      {
        "date": "2021-11-02T07:27:00.000Z",
        "voteCount": 1,
        "content": "B = wrong, to use HSM you have to create new clusters. D = wrong, key rotation is not done by HSM, but Redshift. E = wrong, nonsense. This is a textbook question.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html#migrating-to-an-encrypted-cluster"
      },
      {
        "date": "2021-10-28T15:27:00.000Z",
        "voteCount": 1,
        "content": "the answers are A and C."
      },
      {
        "date": "2021-10-26T06:13:00.000Z",
        "voteCount": 3,
        "content": "Definitely A and C. First answer is from the link provided by testtaker3434, and 2nd answer from the following link https://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html"
      },
      {
        "date": "2021-10-22T17:06:00.000Z",
        "voteCount": 1,
        "content": "A, C is the right answer"
      },
      {
        "date": "2021-10-21T14:56:00.000Z",
        "voteCount": 1,
        "content": "A and C are correct"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/28717-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is planning to do a proof of concept for a machine learning (ML) project using Amazon SageMaker with a subset of existing on-premises data hosted in the company's 3 TB data warehouse. For part of the project, AWS Direct Connect is established and tested. To prepare the data for ML, data analysts are performing data curation. The data analysts want to perform multiple step, including mapping, dropping null fields, resolving choice, and splitting fields. The company needs the fastest solution to curate the data for this project.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest data into Amazon S3 using AWS DataSync and use Apache Spark scrips to curate the data in an Amazon EMR cluster. Store the curated data in Amazon S3 for ML processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate custom ETL jobs on-premises to curate the data. Use AWS DMS to ingest data into Amazon S3 for ML processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest data into Amazon S3 using AWS DMS. Use AWS Glue to perform data curation and store the data in Amazon S3 for ML processing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a full backup of the data store and ship the backup files using AWS Snowball. Upload Snowball data into Amazon S3 and schedule data curation jobs using AWS Batch to prepare the data for ML."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T00:08:00.000Z",
        "voteCount": 28,
        "content": "C is correct, s3 is a valid target for DMS https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"
      },
      {
        "date": "2021-09-29T02:00:00.000Z",
        "voteCount": 2,
        "content": "I guess it should be A. DMS can can not do the data preprocessing and Spark is the best option on the large datasets"
      },
      {
        "date": "2021-11-03T14:16:00.000Z",
        "voteCount": 2,
        "content": "C is correct as AWS Glue uses Spark engine"
      },
      {
        "date": "2021-09-22T16:58:00.000Z",
        "voteCount": 6,
        "content": "C. DMS supports S3 as a target."
      },
      {
        "date": "2023-11-23T20:20:00.000Z",
        "voteCount": 1,
        "content": "C:  Option A, using AWS DataSync and Apache Spark scripts, involves maintaining an on-premises EMR cluster, which adds complexity and management overhead. Option B, creating custom ETL jobs on-premises, requires significant development effort and may not be as efficient as using AWS Glue. Option D, using AWS Snowball for data transfer and AWS Batch for data curation, is less efficient and more time-consuming compared to the direct ingestion and curation approach."
      },
      {
        "date": "2023-11-20T11:48:00.000Z",
        "voteCount": 1,
        "content": "C is correct using glue would be faster than using EMR"
      },
      {
        "date": "2023-11-08T01:45:00.000Z",
        "voteCount": 2,
        "content": "This is the differentiator. DMS can read a database source. DataSync cannot. The question says \"hosted in the company's 3 TB data warehouse.\". DataSync can read NFS, SMB, HDFS, S3.  \nhttps://docs.aws.amazon.com/datasync/latest/userguide/how-datasync-transfer-works.html#onprem-aws"
      },
      {
        "date": "2023-11-08T01:41:00.000Z",
        "voteCount": 1,
        "content": "DataSync can indeed pull a subset of data. https://docs.aws.amazon.com/datasync/latest/userguide/filtering.html"
      },
      {
        "date": "2023-11-08T01:37:00.000Z",
        "voteCount": 1,
        "content": "The question mentions \"subset\" of data. Can DataSync do that? DMS can."
      },
      {
        "date": "2023-10-14T08:36:00.000Z",
        "voteCount": 1,
        "content": "A. I don't understand that all people agree on C. DMS means database migration service and here they mention data warehouse and not database, so this is not a DMS compatible source: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html.\nA is the valid option because with DataSync you can migrate your DATA to the S3 and then we can process it with EMR (more efficient than Glue)"
      },
      {
        "date": "2023-09-25T18:11:00.000Z",
        "voteCount": 1,
        "content": "C. \nBecause, 1. Datasync is used for file migration, DMS for Data. 2. GLUE ETL required to transform data after migration."
      },
      {
        "date": "2023-08-01T09:50:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-05-01T07:12:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-11-04T05:32:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is C as DMS can be used for data migration to S3. AWS Glue can be used for preprocessing and data curation.\n\nOption A is wrong as DataSync is usually for storage migration and using Spark might be as operationally efficient as Glue.\n\nOption B is wrong as using on-premises custom ETL jobs might not be time-efficient.\n\nOption D is wrong as the data migration using Snowball will take time."
      },
      {
        "date": "2022-09-24T22:29:00.000Z",
        "voteCount": 1,
        "content": "Glue is the answer, as all the mentioned data operations are readily available with Glue."
      },
      {
        "date": "2022-08-06T14:03:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-11-30T08:21:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer, use DMS to ingest the data from on-prem data warehouse to S3 and use Glue DataBrew to data curation."
      },
      {
        "date": "2021-11-07T00:49:00.000Z",
        "voteCount": 1,
        "content": "Answer C. Ingest data into Amazon S3 using AWS DMS. Use AWS Glue to perform data curation and store the data in Amazon 3 for ML processing."
      },
      {
        "date": "2021-11-05T08:25:00.000Z",
        "voteCount": 3,
        "content": "A = wrong, DataSync is for storage migration not data warehouse. B = wrong, ETL job on-premise is not fast. D = wrong, too slow."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/27815-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A US-based sneaker retail company launched its global website. All the transaction data is stored in Amazon RDS and curated historic transaction data is stored in Amazon Redshift in the us-east-1 Region. The business intelligence (BI) team wants to enhance the user experience by providing a dashboard for sneaker trends.<br>The BI team decides to use Amazon QuickSight to render the website dashboards. During development, a team in Japan provisioned Amazon QuickSight in ap- northeast-1. The team is having difficulty connecting Amazon QuickSight from ap-northeast-1 to Amazon Redshift in us-east-1.<br>Which solution will solve this issue and meet the requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Amazon Redshift console, choose to configure cross-Region snapshots and set the destination Region as ap-northeast-1. Restore the Amazon Redshift Cluster from the snapshot and connect to Amazon QuickSight launched in ap-northeast-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC endpoint from the Amazon QuickSight VPC to the Amazon Redshift VPC so Amazon QuickSight can access data from Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Redshift endpoint connection string with Region information in the string and use this connection string in Amazon QuickSight to connect to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in ap-northeast-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T21:33:00.000Z",
        "voteCount": 44,
        "content": "D any thoughts?\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\nNot B: https://docs.aws.amazon.com/quicksight/latest/user/working-with-aws-vpc.html"
      },
      {
        "date": "2021-09-24T18:18:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2021-10-24T13:22:00.000Z",
        "voteCount": 4,
        "content": "D is the right answer!"
      },
      {
        "date": "2021-12-20T23:05:00.000Z",
        "voteCount": 3,
        "content": "Agree\nRemember cross region ingestion is also supported ( https://aws.amazon.com/about-aws/whats-new/2014/06/29/amazon-redshift-announces-cross-region-ingestion-and-improved-query-functionality/)\nAs of 23 Nov 21, there's a preview capability to have cross region data sharing as well https://aws.amazon.com/about-aws/whats-new/2014/06/29/amazon-redshift-announces-cross-region-ingestion-and-improved-query-functionality/"
      },
      {
        "date": "2021-12-20T23:10:00.000Z",
        "voteCount": 2,
        "content": "Changing my answer to B\nhttps://aws.amazon.com/blogs/big-data/amazon-quicksight-deployment-models-for-cross-account-and-cross-region-access-to-amazon-redshift-and-amazon-rds/"
      },
      {
        "date": "2022-03-23T14:50:00.000Z",
        "voteCount": 1,
        "content": "\"Set up an Amazon Redshift-managed VPC endpoint between the Amazon Redshift cluster VPC and QuickSight VPC. For instructions, see Connecting to Amazon Redshift using an interface VPC endpoint.\"\nhttps://aws.amazon.com/blogs/big-data/amazon-quicksight-deployment-models-for-cross-account-and-cross-region-access-to-amazon-redshift-and-amazon-rds/\n\n\nOption B says something very different"
      },
      {
        "date": "2022-03-23T14:36:00.000Z",
        "voteCount": 2,
        "content": "Quicksight has no vpc. What is mentioned in the post is connection from quicksight to a vpc using vpcid subnet id and security grp id. Answer=D"
      },
      {
        "date": "2021-09-24T11:14:00.000Z",
        "voteCount": 11,
        "content": "D. As mentioned in the link shared by Priyanka, B is not the answer. \"QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.\""
      },
      {
        "date": "2023-12-04T23:17:00.000Z",
        "voteCount": 1,
        "content": "Hi guys, there's a new amazing IA feature in the AWS documentation called Amazon Q,  if you type this question there, the answer is or \"Create an Amazon Redshift cluster in the ap-northeast-1 region and copy the required data from us-east-1 Redshift\" or \"they can configure cross-region connectivity between QuickSight and Redshift. This involves setting up a VPC endpoint for Redshift in ap-northeast-1 region that connects to the us-east-1 Redshift cluster.\", so chosing the option B as it's cheaper than the A."
      },
      {
        "date": "2023-09-25T18:25:00.000Z",
        "voteCount": 1,
        "content": "D.\nNOT A - \"cross-Region snapshots ..\" - expensive\nNOT B - \"VPC endpoint..\" - cross region connectivity not possible from Quicksight\nNOT C - \"Redshift endpoint connection string ..\" - needs address to be added through Security Group to allow connetivity"
      },
      {
        "date": "2023-08-25T01:03:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-08-01T10:03:00.000Z",
        "voteCount": 1,
        "content": "Going with D"
      },
      {
        "date": "2023-05-21T10:55:00.000Z",
        "voteCount": 1,
        "content": "D is the correct option"
      },
      {
        "date": "2023-05-01T07:13:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-31T02:33:00.000Z",
        "voteCount": 1,
        "content": "Definitely D. This scenario is explored in Stephane Maarek's and Abhishek Singh's Udemy practice exam set."
      },
      {
        "date": "2023-03-21T14:31:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-03-06T03:02:00.000Z",
        "voteCount": 3,
        "content": "D: https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\n\"If you activated Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.\n\nAn Amazon QuickSight user or administrator who uses Amazon QuickSight in multiple AWS Regions is treated as a single user. In other words, even if you are using Amazon QuickSight in every AWS Region, both your Amazon QuickSight account and your users are global\""
      },
      {
        "date": "2023-02-16T03:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is D!"
      },
      {
        "date": "2022-11-04T05:36:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D as the Redshift security group should be updated to allow inbound access from QuickSight IP addresses.\n\nOption B is wrong as QuickSight does not support cross-region access within the VPC.\n\nOption A is wrong as it leads to data duplication and is not the most efficient solution.\n\n\nOption C is wrong as changing the endpoint would not allow connectivity. The access needs to be enabled using security groups."
      },
      {
        "date": "2022-10-25T10:22:00.000Z",
        "voteCount": 1,
        "content": "D per AWS DA Course on Udemy!"
      },
      {
        "date": "2022-10-25T03:14:00.000Z",
        "voteCount": 1,
        "content": "Ans is D\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html"
      },
      {
        "date": "2022-10-22T22:18:00.000Z",
        "voteCount": 1,
        "content": "B is not correct as this: https://docs.aws.amazon.com/quicksight/latest/user/vpc-finding-setup-information.html\nMake sure also that you use Amazon QuickSight in the same AWS Region with the VPC.\nYou can't use QuickSight in one AWS Region and expect to connect to a VPC in a different AWS Region."
      },
      {
        "date": "2022-10-07T01:39:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/quicksight/latest/user/vpc-creating-a-connection-in-quicksight.html \nAWS Region \u2013 The AWS Region where you plan to create a connection to your data source.\nVPC ID \u2013 The ID of the VPC that contains the data, the subnets, and the security groups that you plan to use.\nSubnet ID \u2013 The ID of the subnet that the QuickSight network interface is using.\nSecurity group ID \u2013 The ID of the security group.\nQuicksight can access using VPC ID and region so B is correct"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/28727-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An airline has .csv-formatted data stored in Amazon S3 with an AWS Glue Data Catalog. Data analysts want to join this data with call center data stored in<br>Amazon Redshift as part of a dally batch process. The Amazon Redshift cluster is already under a heavy load. The solution must be managed, serverless, well- functioning, and minimize the load on the existing Amazon Redshift cluster. The solution should also require minimal effort and development activity.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnload the call center data from Amazon Redshift to Amazon S3 using an AWS Lambda function. Perform the join with AWS Glue ETL scripts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the call center data from Amazon Redshift using a Python shell in AWS Glue. Perform the join with AWS Glue ETL scripts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external table using Amazon Redshift Spectrum for the call center data and perform the join with Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the call center data from Amazon Redshift to Amazon EMR using Apache Sqoop. Perform the join with Apache Hive."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T23:29:00.000Z",
        "voteCount": 39,
        "content": "I would go for C, Spectrum is serverless as well. Ques also asks for minimum development effort. For option A, you need to develop Lambda and Glue code."
      },
      {
        "date": "2021-09-24T05:37:00.000Z",
        "voteCount": 7,
        "content": "Agree C. However, one thing I am still confused about - how can Spectrum create external table for the call centre data whereas it doesn't get stored on S3?"
      },
      {
        "date": "2021-09-24T17:12:00.000Z",
        "voteCount": 1,
        "content": "with a Create External Table as Select... I suppose"
      },
      {
        "date": "2021-10-30T03:15:00.000Z",
        "voteCount": 1,
        "content": "Wrong \"Create External Table as Select\" is Redshift command not Spectrum. Spectrum is used to query external as Read Only."
      },
      {
        "date": "2021-10-02T20:23:00.000Z",
        "voteCount": 3,
        "content": "If you are confused, check https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html"
      },
      {
        "date": "2021-10-19T23:29:00.000Z",
        "voteCount": 5,
        "content": "I think Jh2501 is not challenging Spectrum capability to create external tables, but how/why to create an external table on data which is stored in Redshift and not in S3. So, if there is not a typo in the question, C is a doubtful answer, and A could be the right one."
      },
      {
        "date": "2022-10-20T21:38:00.000Z",
        "voteCount": 1,
        "content": "To define an external table in Amazon Redshift, use the CREATE EXTERNAL TABLE command"
      },
      {
        "date": "2023-01-04T22:57:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/ko/premiumsupport/knowledge-center/redshift-spectrum-external-table/"
      },
      {
        "date": "2021-10-13T10:16:00.000Z",
        "voteCount": 19,
        "content": "C doesn't make sense. Call center data is already stored in Redshift. What would be the purpose of creating an external table for the call center data? Also C suggests to perform the join with Redshift which is already under a heavy load."
      },
      {
        "date": "2021-10-22T19:38:00.000Z",
        "voteCount": 28,
        "content": "1. Redshift Spectrum is a compute layer that sits between S3 and Redshift, so it will not add more load to Redshift \n\n2. What the case is saying is that because Redshift is already under heavy load, we shouldn't load the .CSV data from S3 into redshift, so an external table would be better in Redshift Spectrum \n\n3. The best use case for Redshift Spectrum, as described in the question, is to JOIN data in Redshift with another external data source, which in this case is S3, without having the need to bring everything into Redshift. \n\nC is the undeniable correct answer here"
      },
      {
        "date": "2024-03-07T17:29:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer. I do agree that table creation is part of RS and external table created to access the NON RS data sources (e.g. S3). External tables allow you to query data in S3 using the same SELECT syntax as with other Amazon Redshift tables.  Here, the question says RS is already overloaded, hence we should not load the data in to RS. RS spectrum will join the S3 data along with RS data which is also serverless and minimal development efforts"
      },
      {
        "date": "2023-11-20T11:51:00.000Z",
        "voteCount": 2,
        "content": "C is correct as it is the one with minimal effort as no data is moved."
      },
      {
        "date": "2023-11-06T06:03:00.000Z",
        "voteCount": 1,
        "content": "I would pick B:\n\nA: Lambda is limited to 15 minutes of execution time, might not be enough to unload.\nC: The call center data is already in Redshift, the missing data is the airport data.\n\nBest possible option seems to be B: Unload redshift data and develop something to merge/join data, so redshift doesn't want to run the queries and merge."
      },
      {
        "date": "2023-08-25T00:40:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A\nReason why C is incorrect is as it mentions Creating an external table using Amazon Redshift Spectrum for the call center data ( which is already in Redshift) and performing the join with Amazon Redshift (not sure how to join on redshift as the data is already in Redshift) so i guess this incorrect until it is  a type regarding the call center data. So my Option will be A"
      },
      {
        "date": "2023-08-01T10:07:00.000Z",
        "voteCount": 1,
        "content": "going with C"
      },
      {
        "date": "2023-05-10T08:02:00.000Z",
        "voteCount": 2,
        "content": "I would pick B:\n\nA: Lambda is limited to 15 minutes of execution time, might not be enough to unload. \nC: The call center data is already in Redshift, the missing data is the airport data.\n\nBest possible option seems to be B: Unload redshift data and develop something to merge/join data, so redshift doesn't want to run the queries and merge."
      },
      {
        "date": "2023-05-07T01:54:00.000Z",
        "voteCount": 1,
        "content": "C is wrong. To use the CREATE External Table command the data has TO BE IN S3. \"To define an external table in Amazon Redshift, use the CREATE EXTERNAL TABLE command. The external table statement defines the table columns, the format of your data files, and the location of your data in Amazon S3.\" The solution also requires to put as little burden on redshift as possible. Lambda and Glue are serverless and by choosing solution 1 we offload the burden from Redshift completely. Solution A must be correct."
      },
      {
        "date": "2023-05-06T13:18:00.000Z",
        "voteCount": 2,
        "content": "I will say none of the answers are exactly correct without additional information such as call centre data structure and volume. D is not correct as its not serverless. C is not correct - you cant create external table when data is in redshift and also it will put load on redshift. A aij ok only if data is small else lambda will timeout  Again B can be a problem if data is large as it will put load on redshift."
      },
      {
        "date": "2023-05-01T07:14:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-04T09:46:00.000Z",
        "voteCount": 1,
        "content": "\"Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster.\"\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\n\nC: Only caveat being that external table is not created in redshift-spectrum\n\nA: UNLOAD is faster as oppose to Lambda, however, it also burdens redshift."
      },
      {
        "date": "2023-02-16T03:51:00.000Z",
        "voteCount": 1,
        "content": "Use Redshift Spectrum for it!"
      },
      {
        "date": "2022-12-10T10:21:00.000Z",
        "voteCount": 2,
        "content": "I think that there is a typo here, instead of call center data it should be airline data. I had the same question in another paid question dump and the answer was the same as C), but will try to see if there is the same typo in there."
      },
      {
        "date": "2022-11-16T00:34:00.000Z",
        "voteCount": 1,
        "content": "B seems to be a valid approach, taking into consideration that the load shouldn't be on RedShift, The Glue can export the data from RedShift and run (Spark serverless). \nThis blog explains it (ignore the daatbrew which is just a UI above the architecture):\nhttps://aws.amazon.com/blogs/big-data/data-preparation-using-amazon-redshift-with-aws-glue-databrew/"
      },
      {
        "date": "2022-11-04T08:17:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer \n\nA is wrong  Although this is a possible solution, it requires a lot of development overhead to build Glue ETL scripts for joining the Redshift and S3 data. A better solution here is to use Amazon Redshift Spectrum"
      },
      {
        "date": "2022-10-24T14:02:00.000Z",
        "voteCount": 1,
        "content": "C looks simple enough."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/28772-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analyst is using Amazon QuickSight for data visualization across multiple datasets generated by applications. Each application stores files within a separate Amazon S3 bucket. AWS Glue Data Catalog is used as a central catalog across all application data in Amazon S3. A new application stores its data within a separate S3 bucket. After updating the catalog to include the new application data source, the data analyst created a new Amazon QuickSight data source from an Amazon Athena table, but the import into SPICE failed.<br>How should the data analyst resolve the issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the permissions for the AWS Glue Data Catalog from within the Amazon QuickSight console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the permissions for the new S3 bucket from within the Amazon QuickSight console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the permissions for the AWS Glue Data Catalog from within the AWS Glue console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the permissions for the new S3 bucket from within the S3 console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T15:14:00.000Z",
        "voteCount": 29,
        "content": "B. \nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html"
      },
      {
        "date": "2021-12-20T19:00:00.000Z",
        "voteCount": 1,
        "content": "For other issues please read - https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena.html"
      },
      {
        "date": "2023-10-14T08:44:00.000Z",
        "voteCount": 1,
        "content": "B. https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-S3.html"
      },
      {
        "date": "2023-08-25T00:52:00.000Z",
        "voteCount": 1,
        "content": "Quick sight already has access to AWS Glue Data Catalog So A and C are not valid.\nThe issue is Quick sight does not have access to new S3 bucket, so we need to Edit the permissions for the new S3 bucket from within the S3 console to Give access to Quick sight\nSo Option D"
      },
      {
        "date": "2023-08-25T00:56:00.000Z",
        "voteCount": 2,
        "content": "So its B . https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-S3.html"
      },
      {
        "date": "2023-08-01T10:10:00.000Z",
        "voteCount": 1,
        "content": "its a b"
      },
      {
        "date": "2023-05-01T07:17:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-09-24T22:35:00.000Z",
        "voteCount": 2,
        "content": "As new S3 bucket is added. The permission to this bucket needs to be added from QuickSight console."
      },
      {
        "date": "2022-07-20T23:22:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-07-14T22:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-athena.html"
      },
      {
        "date": "2022-06-04T12:57:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-04-30T14:16:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      },
      {
        "date": "2021-11-19T14:19:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2021-11-05T14:11:00.000Z",
        "voteCount": 1,
        "content": "The new Quicksight dashboard doesn't have S3 access. From within the S3 console, edit the new bucket permissions.\nhttps://docs.aws.amazon.com/ja_jp/quicksight/latest/user/create-a-data-set-athena.html"
      },
      {
        "date": "2021-11-01T16:50:00.000Z",
        "voteCount": 2,
        "content": "Agree with B. D is possible if the bucket policy has a Deny rule with Quicksight service role but it is rarely. https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-deny-policy-allow-bucket/"
      },
      {
        "date": "2021-10-24T15:40:00.000Z",
        "voteCount": 2,
        "content": "Answer B. A and C = wrong, Glue is updated without error so no problem there, also Glue permission is not set in QuickSight, QuickSight connects to Athena, not via Glue. D = wrong, QuickSight will create the rules for you.\n\nNote: Athena is a very strange service as it transparently uses user\u2019s access to S3 buckets, instead of relying on service roles like most other AWS services. So, to be able to use Athena, the user itself will need to have S3 access, there is no service role creation for Athena.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html"
      },
      {
        "date": "2021-10-23T04:40:00.000Z",
        "voteCount": 1,
        "content": "B as per the below\nIf you need use Amazon QuickSight with Amazon Athena or Amazon Athena Federated Query, you first need to authorize connections to Athena and the associated buckets in Amazon Simple Storage Service (Amazon S3). \nhttps://docs.aws.amazon.com/quicksight/latest/user/athena.html"
      },
      {
        "date": "2021-10-11T07:03:00.000Z",
        "voteCount": 2,
        "content": "B - \nAthena is able to access the data source so its not S3 problem. On the quick sight end the new bucket should be added \nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-connect-S3.html"
      },
      {
        "date": "2021-10-08T19:51:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/27845-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A team of data scientists plans to analyze market trend data for their company's new investment strategy. The trend data comes from five different data sources in large volumes. The team wants to utilize Amazon Kinesis to support their use case. The team uses SQL-like queries to analyze trends and wants to send notifications based on certain significant patterns in the trends. Additionally, the data scientists want to save the data to Amazon S3 for archival and historical re- processing, and use AWS managed services wherever possible. The team wants to implement the lowest-cost solution.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish data to one Kinesis data stream. Deploy a custom application using the Kinesis Client Library (KCL) for analyzing trends, and send notifications using Amazon SNS. Configure Kinesis Data Firehose on the Kinesis data stream to persist data to an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish data to one Kinesis data stream. Deploy Kinesis Data Analytic to the stream for analyzing trends, and configure an AWS Lambda function as an output to send notifications using Amazon SNS. Configure Kinesis Data Firehose on the Kinesis data stream to persist data to an S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish data to two Kinesis data streams. Deploy Kinesis Data Analytics to the first stream for analyzing trends, and configure an AWS Lambda function as an output to send notifications using Amazon SNS. Configure Kinesis Data Firehose on the second Kinesis data stream to persist data to an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish data to two Kinesis data streams. Deploy a custom application using the Kinesis Client Library (KCL) to the first stream for analyzing trends, and send notifications using Amazon SNS. Configure Kinesis Data Firehose on the second Kinesis data stream to persist data to an S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T17:59:00.000Z",
        "voteCount": 50,
        "content": "B any thoughts ?\nMultiple applications can consume from a single Kinesis Stream\nKinesis Analytics  for sql like queries for analysis\nKinesis firhose can directly transfer the data into S3 from the same data stream"
      },
      {
        "date": "2022-11-04T10:25:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B as a single Kinesis Data Streams can be configured for data ingestion. The stream can be consumed by Kinesis Data Firehose to store the data in S3 for archival. The stream can also be consumed by kinesis Data Analytics for analysis and use Lambda and SNS for notifications.\n\nOption A is wrong as KCL solution is not ideal for executing SQL-like queries to analyze trends.\n\nOptions C &amp; D are wrong as two Kinesis Data Streams is not needed."
      },
      {
        "date": "2023-08-25T01:17:00.000Z",
        "voteCount": 1,
        "content": "Option B\nMultiple applications can consume from a single Kinesis Stream\nKinesis Analytics for sql like queries for analysis &amp; the Output can be sent to lambda to send SNs Trigger\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html\nKinesis firehose can directly transfer the data into S3 from the same data stream"
      },
      {
        "date": "2023-08-01T12:21:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-05-01T07:19:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-02-16T03:54:00.000Z",
        "voteCount": 1,
        "content": "I would also suggest B"
      },
      {
        "date": "2022-11-28T18:17:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer-B"
      },
      {
        "date": "2022-11-24T15:01:00.000Z",
        "voteCount": 2,
        "content": "I think it's C. We need Two D Data Stream. One to consume RAW data and another one to send Agregated data after Kinesis Data Analytics.\nhttps://aws.amazon.com/blogs/big-data/building-a-real-time-notification-system-with-amazon-kinesis-data-streams-for-amazon-dynamodb-and-amazon-kinesis-data-analytics-for-apache-flink/"
      },
      {
        "date": "2022-10-25T04:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. Using Kinesis analytics, we can analyse the trends using sql like queries. Same data stream would be a source to Firehose to put the data in s3."
      },
      {
        "date": "2022-09-24T22:38:00.000Z",
        "voteCount": 1,
        "content": "KDA is best applied in this scenario. Also we do not need multiple Kinesis streams to ingest from different data sources. Any custom application will add additional development and testing overhead."
      },
      {
        "date": "2022-07-26T19:37:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-05-21T05:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-03-17T22:34:00.000Z",
        "voteCount": 2,
        "content": "Kinesis Analytics for sql like queries for analysis"
      },
      {
        "date": "2021-11-23T13:21:00.000Z",
        "voteCount": 3,
        "content": "The answer is B."
      },
      {
        "date": "2021-11-03T16:32:00.000Z",
        "voteCount": 2,
        "content": "B is preferable because:\nKinesis firhose can directly transfer the data into S3 from the same data stream in addition to cost effectiveness specified"
      },
      {
        "date": "2021-11-03T10:37:00.000Z",
        "voteCount": 1,
        "content": "KCL is self-service, so A and D are out. If you want multiple consumers for the same stream, just make sure you have enough shards to deal with the read-write throughput. A stream is different from a queue in that one can traverse back and forth in a stream, where in a queue one can only process one by one."
      },
      {
        "date": "2021-11-02T13:18:00.000Z",
        "voteCount": 1,
        "content": "B seems okay"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/28718-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company currently uses Amazon Athena to query its global datasets. The regional data is stored in Amazon S3 in the us-east-1 and us-west-2 Regions. The data is not encrypted. To simplify the query process and manage it centrally, the company wants to use Athena in us-west-2 to query data from Amazon S3 in both<br>Regions. The solution should be as low-cost as possible.<br>What should the company do to achieve this goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to migrate the AWS Glue Data Catalog from us-east-1 to us-west-2. Run Athena queries in us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the AWS Glue crawler in us-west-2 to catalog datasets in all Regions. Once the data is crawled, run Athena queries in us-west-2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable cross-Region replication for the S3 buckets in us-east-1 to replicate data in us-west-2. Once the data is replicated in us-west-2, run the AWS Glue crawler there to update the AWS Glue Data Catalog in us-west-2 and run Athena queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate AWS Glue resource policies to provide us-east-1 AWS Glue Data Catalog access to us-west-2. Once the catalog in us-west-2 has access to the catalog in us-east-1, run Athena queries in us-west-2."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T01:25:00.000Z",
        "voteCount": 30,
        "content": "B.\nAWS DMS is not for this purpose, so A dropped. C would be costly since it literally replicates all data. There\u2019s no \u201cresource policies\u201d in AWS Glue, so D dropped."
      },
      {
        "date": "2021-11-05T18:29:00.000Z",
        "voteCount": 2,
        "content": "I agree with you that D is wrong but my ideas is you shouldn't based on a property that is not available for the service. Instead, think in-depth about what is the answer actually suggest. https://docs.aws.amazon.com/glue/latest/dg/glue-resource-policies.html\nHere, the answer wants AWS Glue to use Data Catalog from different region which is not supported."
      },
      {
        "date": "2022-05-21T20:49:00.000Z",
        "voteCount": 2,
        "content": "glue crawler will simply generate the metadata on top of s3 files. But the Athena running in another region will still not have access to the first region files. Also, even glue crawler might not have permission to crawl in another region s3 files. Hence replication is the only option."
      },
      {
        "date": "2022-05-27T19:49:00.000Z",
        "voteCount": 5,
        "content": "No, glue crawler is not restricted to a region and can catalogue data in other regions. And then Athena can use the catalogue and generate results. I have seen this happening in my project"
      },
      {
        "date": "2022-10-17T04:16:00.000Z",
        "voteCount": 3,
        "content": "There is 'resource policies':\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-policy-examples-resource-policies.html"
      },
      {
        "date": "2022-11-04T10:32:00.000Z",
        "voteCount": 14,
        "content": "B is correct as AWS Glue can crawl data in different AWS Regions. When you define an Amazon S3 data store to crawl, you can choose whether to crawl a path in your account or another account.\n\nThe output of the crawler is one or more metadata tables defined in the AWS Glue Data Catalog. A table is created for one or more files found in your data store. If all the Amazon S3 files in a folder have the same schema, the crawler creates one table. Also, if the Amaazon S3 object is partitioned, only one metadata table is created.\n\n\nA is wrong  because you can't use AWS DMS with AWS Glue Data Catalog.\n\nC is incorrect because replicating the data in S3 means that your storage costs will also double.\n\n\nD is wrong  because a resource-based policy is primarily used to provide IAM users and roles granular access to metadata definitions of databases, tables, connections, and user-defined functions, and not the actual S3 data."
      },
      {
        "date": "2023-12-21T14:18:00.000Z",
        "voteCount": 1,
        "content": "A: DMS is not required to migrate data from one region to another. It can even be used to migrate data from an S3 bucket to another bucket in another account, but there are better and cheaper ways to do this (considering the volume of data, of course).\n\nB: It is the correct alternative. Glue crawlers can catalog data that is in different regions. It's simple to set up and not expensive.\n\nC: Cross-region works for data replication, but it will be duplicated unnecessarily.\n\nD: This type of permissions is best suited for LakeFormation and would not help catalog data that is in different regions."
      },
      {
        "date": "2023-08-25T01:54:00.000Z",
        "voteCount": 1,
        "content": "Option D\nhttps://aws.amazon.com/blogs/big-data/configure-cross-region-table-access-with-the-aws-glue-catalog-and-aws-lake-formation/"
      },
      {
        "date": "2023-08-01T12:26:00.000Z",
        "voteCount": 1,
        "content": "going w B"
      },
      {
        "date": "2023-05-07T02:39:00.000Z",
        "voteCount": 1,
        "content": "B. Source: https://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html. You can choose to crawl a path in your account or in another account. Crawlers use an AWS Identity and Access Management (IAM) role for permission to access your data stores. The role you pass to the crawler must have permission to access Amazon S3 paths and Amazon DynamoDB tables that are crawled. Another source: https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html. Athena can query cross-region Athena supports the ability to query Amazon S3 data in an AWS Region that is different from the Region in which you are using Athena. Querying across Regions can be an option when moving the data is not practical or permissible, or if you want to query data across multiple regions. Even if Athena is not available in a particular Region, data from that Region can be queried from another Region in which Athena is available."
      },
      {
        "date": "2023-05-06T13:38:00.000Z",
        "voteCount": 2,
        "content": "B is correct for context of this question but will be a bad implementation in real life. D can be good pattern but with help of Lakeformation."
      },
      {
        "date": "2023-05-01T07:20:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-02-09T07:25:00.000Z",
        "voteCount": 1,
        "content": "the data is not encrypted so moving data is not \"practical or permissible\"?"
      },
      {
        "date": "2023-02-08T10:25:00.000Z",
        "voteCount": 1,
        "content": "D should be..."
      },
      {
        "date": "2023-02-04T03:28:00.000Z",
        "voteCount": 1,
        "content": "A, B, D simply wouldn't work because of lacking connection to the data source. The only thing that I am not sure is about the 'lowest cost'. It can be option B if the wording implies that the connectivity exits \nhttps://aws.amazon.com/blogs/big-data/create-cross-account-and-cross-region-aws-glue-connections/"
      },
      {
        "date": "2023-01-23T11:39:00.000Z",
        "voteCount": 1,
        "content": "D.\n\nSee: https://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html"
      },
      {
        "date": "2023-01-05T20:22:00.000Z",
        "voteCount": 2,
        "content": "That's why D is wrong?  Each AWS account owns a single catalog in an AWS Region whose catalog ID is the same as the AWS account ID\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-resource-policies.html"
      },
      {
        "date": "2022-10-23T10:55:00.000Z",
        "voteCount": 1,
        "content": "Both B and D will work. The answer is B because option D is a bit more expensive."
      },
      {
        "date": "2022-10-11T00:02:00.000Z",
        "voteCount": 2,
        "content": "Must be D"
      },
      {
        "date": "2022-10-09T19:47:00.000Z",
        "voteCount": 1,
        "content": "B is correct\nD is wrong because there is no resource policies but only trust policy."
      },
      {
        "date": "2022-10-12T07:48:00.000Z",
        "voteCount": 2,
        "content": "It has https://docs.aws.amazon.com/glue/latest/dg/glue-resource-policies.html"
      },
      {
        "date": "2022-09-24T22:40:00.000Z",
        "voteCount": 1,
        "content": "The lowest cost option is B. All other options are involved with greater cost, as data migration between regions costs more."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/27800-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large company receives files from external parties in Amazon EC2 throughout the day. At the end of the day, the files are combined into a single file, compressed into a gzip file, and uploaded to Amazon S3. The total size of all the files is close to 100 GB daily. Once the files are uploaded to Amazon S3, an<br>AWS Batch program executes a COPY command to load the files into an Amazon Redshift cluster.<br>Which program modification will accelerate the COPY process?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the individual files to Amazon S3 and run the COPY command as soon as the files become available.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the number of files so they are equal to a multiple of the number of slices in the Amazon Redshift cluster. Gzip and upload the files to Amazon S3. Run the COPY command on the files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the number of files so they are equal to a multiple of the number of compute nodes in the Amazon Redshift cluster. Gzip and upload the files to Amazon S3. Run the COPY command on the files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply sharding by breaking up the files so the distkey columns with the same values go to the same file. Gzip and upload the sharded files to Amazon S3. Run the COPY command on the files."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T21:54:00.000Z",
        "voteCount": 23,
        "content": "B. Split your data into files so that the number of files is a multiple of the number of slices in your cluster. That way Amazon Redshift can divide the data evenly among the slices."
      },
      {
        "date": "2021-11-07T00:40:00.000Z",
        "voteCount": 7,
        "content": "B : \nThis is a textbook question. Sequential loading vs. parallel loading.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_splitting-data-files.html"
      },
      {
        "date": "2023-12-21T14:26:00.000Z",
        "voteCount": 1,
        "content": "files -&gt; EC2 -&gt; merge files at the end of the day single file compressed -&gt; s3 (100GB daily)\n\nA: The copy command for a large file (100GB) is slow and not effective as redshift will try to distribute the processing across the cluster and only after this division will the copy be carried out.\n\nB: Files must be large enough to run on only one slice of the node. With this pre-processing done, the master node does not need to worry about \"allocating memory\" to copy this file. If each slice processes a file, the transfer speed will be optimal.\n\nC: It's a smart option, but not the most effective. The number of slices is directly related to the number of nodes, but if the division is made thinking only about the number of nodes, it is possible to make the mistake of executing the COPY command for files that are too large.\n\nD: The dist style must be done after loading the data."
      },
      {
        "date": "2023-08-25T02:05:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-08-01T12:28:00.000Z",
        "voteCount": 1,
        "content": "I think B"
      },
      {
        "date": "2023-05-01T07:21:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-11-14T04:52:00.000Z",
        "voteCount": 1,
        "content": "Agree I passed the test two times in a row."
      },
      {
        "date": "2022-12-20T01:54:00.000Z",
        "voteCount": 1,
        "content": "Why can't I use Option D?"
      },
      {
        "date": "2022-12-16T08:39:00.000Z",
        "voteCount": 1,
        "content": "b\nhttps://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html\nGranting access to Data Catalog resources across accounts enables your extract, transform, and load (ETL) jobs to query and join data from different accounts."
      },
      {
        "date": "2022-11-04T10:35:00.000Z",
        "voteCount": 7,
        "content": "B is correct as the COPY command loads the data in parallel from multiple files, dividing the workload among the nodes in your cluster. When you load all the data from a single large file, Amazon Redshift is forced to perform a serialized load, which is much slower. Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression. The number of files should be a multiple of the number of slices in your cluster."
      },
      {
        "date": "2022-09-24T22:43:00.000Z",
        "voteCount": 1,
        "content": "GZIP cannot be split. So first split the files and then gzip the slices. Also, it is recommended to have number of files equal to a number which is multiple of total slices in Redshift cluster, so that copy command can engage all worked nodes parallelly and evenly distribute the load."
      },
      {
        "date": "2022-09-12T15:32:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-09-12T15:32:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-07-27T23:40:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-11-06T22:32:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-11-03T03:20:00.000Z",
        "voteCount": 1,
        "content": "B is correct for me"
      },
      {
        "date": "2021-10-27T01:53:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2021-10-08T20:30:00.000Z",
        "voteCount": 2,
        "content": "Option B, By using a single full file forces copy to do a serial load. Splitting the files in multiple of number of slices in cluster and compressing them is ideal for better performance of copy"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/28720-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large ride-sharing company has thousands of drivers globally serving millions of unique customers every day. The company has decided to migrate an existing data mart to Amazon Redshift. The existing schema includes the following tables.<br>\u2711 A trips fact table for information on completed rides.<br>\u2711 A drivers dimension table for driver profiles.<br>\u2711 A customers fact table holding customer profile information.<br>The company analyzes trip details by date and destination to examine profitability by region. The drivers data rarely changes. The customers data frequently changes.<br>What table design provides optimal query performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DISTSTYLE KEY (destination) for the trips table and sort by date. Use DISTSTYLE ALL for the drivers and customers tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DISTSTYLE EVEN for the trips table and sort by date. Use DISTSTYLE ALL for the drivers table. Use DISTSTYLE EVEN for the customers table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DISTSTYLE KEY (destination) for the trips table and sort by date. Use DISTSTYLE ALL for the drivers table. Use DISTSTYLE EVEN for the customers table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DISTSTYLE EVEN for the drivers table and sort by date. Use DISTSTYLE ALL for both fact tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T15:05:00.000Z",
        "voteCount": 38,
        "content": "C.\nDrivers\u2019 data -&gt; ALL, Customer\u2019s data -&gt; EVEN, Trips table -&gt; KEY (destination) &amp; sort by date\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html\nhttps://slideshare.net/AmazonWebServices/deep-dive-on-amazon-redshift-80877515"
      },
      {
        "date": "2021-09-29T13:27:00.000Z",
        "voteCount": 2,
        "content": "Not sure how the 2 links gave you this answer! Not saying your suggestion is wrong"
      },
      {
        "date": "2021-10-07T11:51:00.000Z",
        "voteCount": 12,
        "content": "IMO, it should be B.. Reasons:  Distributing the data on destination might cause a data skew which we don't want. If there is no clear dist key for a fact, it's better to dist it evenly."
      },
      {
        "date": "2023-08-01T12:31:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2023-07-24T00:02:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer!"
      },
      {
        "date": "2023-05-01T07:22:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-11-04T10:37:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C as as the trip would be queries on destination and date, the trips table needs a DISTSTYLE KEY (destination) for the trips table and sort by date. As the drivers data rarely changes DISTSTYLE ALL can be applied for the drivers table, which will maintain a copy per node. Also as customers data changes frequently,"
      },
      {
        "date": "2022-09-24T22:46:00.000Z",
        "voteCount": 8,
        "content": "Use All Distribution for rarely changing tables, as they are copied to all slices. Use Even distribution to frequently changing and large tables, as Redshift engine can randomly distribute them to different data slices. Use diststyle key for the tables where you know a join key."
      },
      {
        "date": "2022-08-06T14:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-05-21T05:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-05-10T03:04:00.000Z",
        "voteCount": 2,
        "content": "C should be right.\nThe data will be analized by destination. And the requirement doesn't mention that it need to join trip and customer/driver table. So, using DISTSTYLE KEY (destination) for the trips should improve the performance of trip data.\nFor A, there are millons of customers, so using ALL for that will cause too much copy data."
      },
      {
        "date": "2022-01-08T02:37:00.000Z",
        "voteCount": 3,
        "content": "I think answer is B. Destination is not as unique a key and can cause skew with key partition. So even for the trips seems right. Drivers is updated rarely, so suitable for All. Cust is updated often so Even is good for it too."
      },
      {
        "date": "2021-11-05T08:30:00.000Z",
        "voteCount": 4,
        "content": "The answer is B. We don't know which key (with high cardinality) to use for fact table for even distribution so we should chose even diststyle and use fields that should be used to access data as sort key.. Rare updated dimension table good candidate for diststyle all, and another one frequently updated should have even diststyle."
      },
      {
        "date": "2021-10-25T05:14:00.000Z",
        "voteCount": 4,
        "content": "FACT table in DW is central table and will be queried the most. Because we query trips by destination and date therefore the higher cardinality is destination -&gt; use Destination as key. Drivers is dimension table and data is small so can be ALL to speedup the join. Customer data frequently changes and we are not sure which columns should be joined so EVEN is safe."
      },
      {
        "date": "2021-10-24T20:36:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2021-10-24T12:08:00.000Z",
        "voteCount": 3,
        "content": "Ans C..This is a textbook question. However, if there is an answer where both customer and driver tables are EVEN, I would go for it. ALL does not quite give benefits over EVEN.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"
      },
      {
        "date": "2021-10-23T01:39:00.000Z",
        "voteCount": 1,
        "content": "C seems okay"
      },
      {
        "date": "2021-10-20T19:44:00.000Z",
        "voteCount": 2,
        "content": "C in my opnion is the best choice, because the Trip table is a fact table so it will join with the other tables, like customer and driver tables. The table customer is frequently updated, so in this case ALL is not recommended."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/28785-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "Three teams of data analysts use Apache Hive on an Amazon EMR cluster with the EMR File System (EMRFS) to query data stored within each teams Amazon<br>S3 bucket. The EMR cluster has Kerberos enabled and is configured to authenticate users from the corporate Active Directory. The data is highly sensitive, so access must be limited to the members of each team.<br>Which steps will satisfy the security requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the EMR cluster Amazon EC2 instances, create a service role that grants no access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the additional IAM roles to the cluster's EMR role for the EC2 trust policy. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the EMR cluster Amazon EC2 instances, create a service role that grants no access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the service role for the EMR cluster EC2 instances to the trust policies for the additional IAM roles. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the EMR cluster Amazon EC2 instances, create a service role that grants full access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the service role for the EMR cluster EC2 instances to the trust polices for the additional IAM roles. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the EMR cluster Amazon EC2 instances, create a service role that grants full access to Amazon S3. Create three additional IAM roles, each granting access to each team's specific bucket. Add the service role for the EMR cluster EC2 instances to the trust polices for the base IAM roles. Create a security configuration mapping for the additional IAM roles to Active Directory user groups for each team."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-09T19:24:00.000Z",
        "voteCount": 30,
        "content": "No doubt its B. If you will have full access on Ec2 instance role and no role match then it will fall back to the default role [When a cluster application makes a request to Amazon S3 through EMRFS, EMRFS evaluates role mappings in the top-down order that they appear in the security configuration. If a request made through EMRFS doesn\u2019t match any identifier, EMRFS falls back to using the service role for cluster EC2 instances.] Also this is tested fully and its more secure then any other options."
      },
      {
        "date": "2021-11-05T00:16:00.000Z",
        "voteCount": 11,
        "content": "Ans B :\nThis is a textbook question. Basically you:\n\ncreate a new EMR service role, removing default permission from original service role which is too permissive with s3:*\ncreate some new roles to allow access to respective s3 buckets\nEMRFS by default will assume EMR service role, which means it gets all access to S3, but can be configured to assume an additional role created by user\nTo be able to do that, user-created roles needs to trust EMR service role (because EMRFS will assume that role first)\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-iam-roles.html"
      },
      {
        "date": "2024-03-25T21:48:00.000Z",
        "voteCount": 1,
        "content": "Bling choose A \nfor the below explanation \nOption A is correct because it ensures that each team only has access to its own S3 bucket. By creating a service role that grants no access to Amazon S3 for the EMR cluster EC2 instances, you prevent unauthorized access. Then, by creating additional IAM roles that grant access to each team\u2019s specific bucket and adding these roles to the EMR role for the EC2 trust policy, you ensure that each team can access only its own data. Finally, by creating a security configuration mapping for the additional IAM roles to Active Directory user groups for each team, you ensure that only the members of each team can access their own data.\nOther options are not the best solutions for this scenario. For example, Options B, C, and D involve adding the service role for the EMR cluster EC2 instances to the trust policies for the additional IAM roles or the base IAM roles, which could potentially allow unauthorized access to the S3 buckets."
      },
      {
        "date": "2023-08-01T12:35:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-05-01T07:23:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-11-04T10:43:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B as the EMR service role should be provided with no access and the mapping defined for security configuration for using IAM roles mapped to groups.\n\nOption A is wrong as the service role for the EMR cluster EC2 instances should be updated to the trust policies for the additional IAM roles.\n\nOptions C &amp; D are wrong as the EMR service role should have no access."
      },
      {
        "date": "2022-10-09T20:00:00.000Z",
        "voteCount": 1,
        "content": "B\nB is right, the service role need assume the additional roles, which means add it to the trust policy of the additional roles.\nA is the opposite."
      },
      {
        "date": "2022-07-31T16:43:00.000Z",
        "voteCount": 1,
        "content": "isnt b and c the same?"
      },
      {
        "date": "2022-08-03T23:25:00.000Z",
        "voteCount": 2,
        "content": "B - create a service role that grants no access to Amazon S3.\nC- create a service role that grants FULL access to Amazon S3\nB is the right choice"
      },
      {
        "date": "2022-07-25T21:17:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-05-21T04:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-11-20T16:29:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2021-11-06T05:30:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2021-11-03T11:30:00.000Z",
        "voteCount": 1,
        "content": "When a cluster application makes a request to Amazon S3 through EMRFS, EMRFS evaluates role mappings in the top-down order that they appear in the security configuration. If a request made through EMRFS doesn\u2019t match any identifier, EMRFS falls back to using the service role for cluster EC2 instances. For this reason, we recommend that the policies attached to this role limit permissions to Amazon S3. For more information, see Service Role for Cluster EC2 Instances (EC2 Instance Profile)."
      },
      {
        "date": "2021-10-30T10:28:00.000Z",
        "voteCount": 4,
        "content": "C. We need additional IAM roles.\n\nA, B. If EC2 service role has no access to Amazon S3, no one on this EC2 can access S3 at any level. Besides, S3 deny unauthorized access by default.\nD. We need additional IAM roles."
      },
      {
        "date": "2021-10-28T01:45:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-26T21:26:00.000Z",
        "voteCount": 1,
        "content": "Would go with A.\nBy default, no privilege given in the \"default\" instance profile.\nPrivileges are given through dedicated roles and policies for each domain.\nThen allow these roles to be assumed by the EMR Service Role.\nEMR will then use the appropriate IAM roles based on to the role mapping definition."
      },
      {
        "date": "2021-10-23T03:37:00.000Z",
        "voteCount": 4,
        "content": "B is correct as of https://aws.amazon.com/de/blogs/big-data/build-a-multi-tenant-amazon-emr-cluster-with-kerberos-microsoft-active-directory-integration-and-emrfs-authorization/"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/27803-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is planning to create a data lake in Amazon S3. The company wants to create tiered storage based on access patterns and cost objectives. The solution must include support for JDBC connections from legacy clients, metadata management that allows federation for access control, and batch-based ETL using PySpark and Scala. Operational management should be limited.<br>Which combination of components can meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue Data Catalog for metadata management\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EMR with Apache Spark for ETL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue for Scala-based ETL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EMR with Apache Hive for JDBC clients",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Athena for querying data in Amazon S3 using JDBC drivers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EMR with Apache Hive, using an Amazon RDS with MySQL-compatible backed metastore"
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "ABE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ABC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T00:05:00.000Z",
        "voteCount": 48,
        "content": "I will go with A,C,E . Glue can do both pyspark and scala based ETL. Glue for Metadata and JDBC drivers to connect Athena from outside of AWS. \nServer less . so, Operational management is limited"
      },
      {
        "date": "2021-10-04T15:03:00.000Z",
        "voteCount": 4,
        "content": "ya i thought so too, ACE for me"
      },
      {
        "date": "2021-10-12T18:24:00.000Z",
        "voteCount": 6,
        "content": "Each word has meaning, So I will go with ABD, A metadata management that allows federation for access control, B- batch-based ETL using PySpark, D-JDBC connections from legacy clients. Not-C- because it mentioned only scala but questions mentioned scala operation is limited, E- you need JDBC to connect clinet not the Athena"
      },
      {
        "date": "2021-10-15T04:32:00.000Z",
        "voteCount": 1,
        "content": "Correct. ABD is right"
      },
      {
        "date": "2021-10-21T12:12:00.000Z",
        "voteCount": 1,
        "content": "I agree with this. Correct is ABD."
      },
      {
        "date": "2022-12-03T23:37:00.000Z",
        "voteCount": 1,
        "content": "EMR=Operationd overhead. ETL does it all and it is a managed service. ACE is better answer"
      },
      {
        "date": "2022-12-03T23:42:00.000Z",
        "voteCount": 1,
        "content": "I mean AWS Glue (not ETL) is a serverless service and you don't need to provision it."
      },
      {
        "date": "2022-11-06T23:13:00.000Z",
        "voteCount": 1,
        "content": "if we select B,D (EMR-spark,Hive-jdbc),does it not make more sense to use Emr-Hive-datastore(F),instead of glue-catalog(A),limiting operational management.\n- making BDF more appropriate."
      },
      {
        "date": "2024-03-25T21:57:00.000Z",
        "voteCount": 1,
        "content": "Bing\nOption A is correct because AWS Glue Data Catalog provides a unified metadata repository across a variety of data sources and data formats, and it integrates with Amazon S3, Amazon RDS, Amazon Athena, Amazon Redshift, and others.\nOption B is correct because Amazon EMR with Apache Spark supports PySpark and Scala for batch-based ETL processing.\nOption E is correct because Amazon Athena supports SQL queries and can be integrated with JDBC drivers, allowing legacy clients to execute queries."
      },
      {
        "date": "2024-02-21T02:16:00.000Z",
        "voteCount": 1,
        "content": "I will go with A. AWS Glue Data Catalog, B.Amazon EMR with Apache Spark, E. Amazon Athena aligns well with the company's requirements for a data lake architecture, offering a balance of performance, cost-efficiency, and ease of management.\n\nWhile C is also a viable option for ETL processes, it's more aligned with serverless ETL jobs and might not be as flexible for Scala as Amazon EMR with Apache Spark. D and F could provide JDBC connectivity and metadata management but are more operationally intensive and less integrated with S3 tiered storage strategies compared to using Athena with the Glue Data Catalog."
      },
      {
        "date": "2023-08-12T20:50:00.000Z",
        "voteCount": 1,
        "content": "Why are we saying C ? C just says \"Scala\" ETL, even though Glue supports both pyspark and scala and AWS managed, the option specifically mentions \"Scala based\". Requirement is for both Scala and Pyspark that directly points to EMR. answer should be ABE.. about operational management, it says \"limited\", and EMR can qualify with it. using glue there is 'no' operational overhead."
      },
      {
        "date": "2023-08-01T12:37:00.000Z",
        "voteCount": 1,
        "content": "ACE it"
      },
      {
        "date": "2023-05-01T07:24:00.000Z",
        "voteCount": 3,
        "content": "ACE: I passed the test"
      },
      {
        "date": "2022-11-04T10:44:00.000Z",
        "voteCount": 5,
        "content": "Correct answers are A, C &amp; E\n\nOption A as Glue Data Catalog provides metadata management with the federation for access control.\n\nOption C as AWS Glue supports both serverless PySpark and Scala-based ETL with the least operational overhead.\n\nOption E as Athena can be used for querying S3 data. Athena can be connected using JDBC drivers from the external legacy clients.\n\nOptions B, D &amp; E is wrong as using EMR and RDS would increase the operational management and cost."
      },
      {
        "date": "2022-09-24T22:52:00.000Z",
        "voteCount": 2,
        "content": "As operation management should be less, so all EMR related options are invalid, as EMR needs management of underlying EC2 instances"
      },
      {
        "date": "2022-09-05T21:22:00.000Z",
        "voteCount": 2,
        "content": "A. *Less* operational overhead compared to F (selected)\nB. High operational overhead, when compared to \"C\" AWS Glue based Scala\nC. *Less* operational overhead compared to \"B\" EMR PySpark (selected)\nD. Higher operational overhead when compared to \"E\" Athena. https://docs.aws.amazon.com/emr/latest/ReleaseGuide/HiveJDBCDriver.html\nE. *Less* operational overhead when compared to \"D\" EMR Hive JDBC (selected)\nF. Higher operational overhead when compared to \"A\" Glue metadata"
      },
      {
        "date": "2022-08-06T14:06:00.000Z",
        "voteCount": 2,
        "content": "I will go with A,C,E"
      },
      {
        "date": "2022-06-24T01:15:00.000Z",
        "voteCount": 1,
        "content": "D and F are wrong because the question never mentions Hive. E is not right, since Athena don't need JDBC to query S3. C is right because AWS Glue can be used for Scala-based ETL. \nA is right because Glue can connect on-premises DB through JDBC. https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/. B is right because Apache Spark can support PySpark."
      },
      {
        "date": "2022-05-21T05:30:00.000Z",
        "voteCount": 3,
        "content": "My Answer is A, C &amp; E"
      },
      {
        "date": "2022-02-13T00:20:00.000Z",
        "voteCount": 1,
        "content": "D, F are clearly wrong.\nD : JDBC connection from older clients is required, NOT from Athena.\nF : It is redundant to use RDS as a metastore. And there is no requirement for MySQL-compatible backed metastore.\nI'm torn between B and C. B is predominant in terms of cost constraints, but I am doubtful."
      },
      {
        "date": "2021-11-04T16:40:00.000Z",
        "voteCount": 2,
        "content": "Answer: A,C,E \nEMR has operational overhead."
      },
      {
        "date": "2021-11-03T17:10:00.000Z",
        "voteCount": 3,
        "content": "Answer: A,C,E \nEMR has operational overhead."
      },
      {
        "date": "2021-11-03T15:44:00.000Z",
        "voteCount": 4,
        "content": "Ans - ACE\nNote: This is a free score question. Anything EMR comparing to serverless Glue / Athena is operational overhead. Also remember Glue can do PySpark and Scala, and Athena can do JDBC."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/73913-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to optimize the cost of its data and analytics platform. The company is ingesting a number of .csv and JSON files in Amazon S3 from various data sources. Incoming data is expected to be 50 GB each day. The company is using Amazon Athena to query the raw data in Amazon S3 directly. Most queries aggregate data from the past 12 months, and data that is older than 5 years is infrequently queried. The typical query scans about 500 MB of data and is expected to return results in less than 1 minute. The raw data must be retained indefinitely for compliance requirements.<br>Which solution meets the company's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue ETL job to compress, partition, and convert the data into a columnar data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the processed data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after object creation. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after object creation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue ETL job to partition and convert the data into a row-based data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after object creation. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after object creation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue ETL job to compress, partition, and convert the data into a columnar data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the processed data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after the object was last accessed. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after the last date the object was accessed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue ETL job to partition and convert the data into a row-based data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after the object was last accessed. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after the last date the object was accessed."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T04:56:00.000Z",
        "voteCount": 11,
        "content": "Agree with answer A as C&amp;D was eliminated due to the last accessed rather than created for Lifecycle policy.\nBy compressing you save cost and converting to columnar data, performance is increased."
      },
      {
        "date": "2022-11-04T10:47:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is A as columnar data format store data efficiently by employing column-wise compression and enables split and parallel processing. Storing processed data in S3 in SA-IA and moving raw data in Glacier would help reduce costs.\n\nOption B &amp; D is wrong as it is recommended to use columnar data format for processing.\n\nOptions C is wrong as lifecycle rules are based on Object creation data and not last date when the object was accessed."
      },
      {
        "date": "2023-10-28T14:37:00.000Z",
        "voteCount": 1,
        "content": "columnar and based on object creation time"
      },
      {
        "date": "2023-08-01T12:40:00.000Z",
        "voteCount": 1,
        "content": "A make sense"
      },
      {
        "date": "2023-05-01T07:25:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-09-24T22:55:00.000Z",
        "voteCount": 1,
        "content": "It should be based on object creation, not based on object access"
      },
      {
        "date": "2022-07-20T23:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-07-14T22:14:00.000Z",
        "voteCount": 2,
        "content": "should be 5 years after object creation to Infrequent for processed data\nand 7 days after object creation to glacier for raw data\n\nThere is no point of counting days from \"Last accessed\""
      },
      {
        "date": "2022-07-04T02:41:00.000Z",
        "voteCount": 1,
        "content": "Columnar data is a way of optimizing (eleminate B, D). And the lifecycle policy should be assigned after object creation (eleminate C). Ans is A"
      },
      {
        "date": "2022-05-22T01:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-04-20T08:09:00.000Z",
        "voteCount": 2,
        "content": "ans should be A"
      },
      {
        "date": "2022-04-21T04:55:00.000Z",
        "voteCount": 1,
        "content": "Agree with answer A as C&amp;D was eliminated due to the last accessed rather than created for Lifecycle policy.\nBy compressing you save cost and converting to columnar data, performance is increased."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/73985-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An energy company collects voltage data in real time from sensors that are attached to buildings. The company wants to receive notifications when a sequence of two voltage drops is detected within 10 minutes of a sudden voltage increase at the same building. All notifications must be delivered as quickly as possible. The system must be highly available. The company needs a solution that will automatically scale when this monitoring feature is implemented in other cities. The notification system is subscribed to an Amazon Simple Notification Service (Amazon SNS) topic for remediation.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Managed Streaming for Apache Kafka cluster to ingest the data. Use an Apache Spark Streaming with Apache Kafka consumer API in an automatically scaled Amazon EMR cluster to process the incoming data. Use the Spark Streaming application to detect the known event sequence and send the SNS message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a REST-based web service by using Amazon API Gateway in front of an AWS Lambda function. Create an Amazon RDS for PostgreSQL database with sufficient Provisioned IOPS to meet current demand. Configure the Lambda function to store incoming events in the RDS for PostgreSQL database, query the latest data to detect the known event sequence, and send the SNS message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream to capture the incoming sensor data. Use an AWS Lambda transformation function to detect the known event sequence and send the SNS message.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream to capture the incoming sensor data. Create another stream for notifications. Set up AWS Application Auto Scaling on both streams. Create an Amazon Kinesis Data Analytics for Java application to detect the known event sequence, and add a message to the message stream Configure an AWS Lambda function to poll the message stream and publish to the SNS topic."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T00:02:00.000Z",
        "voteCount": 28,
        "content": "Answer = D"
      },
      {
        "date": "2022-06-23T15:58:00.000Z",
        "voteCount": 16,
        "content": "It's C. Your confusing the \"immediately\" and ignoring that prior to that it's an event that happens as a sequence within 10 minutes, so firehose is a viable option.\n\nAWS auto scaling does not support Amazon Kinesis so that rules out D\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/integrated-services-list.html"
      },
      {
        "date": "2023-08-08T06:56:00.000Z",
        "voteCount": 2,
        "content": "It seems AWS does support application auto-scaling for Kinesis Data streams since Nov 2018. https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/"
      },
      {
        "date": "2023-09-19T08:08:00.000Z",
        "voteCount": 1,
        "content": "for Kinesis Data Stream we can use on demand option instead of provision that will able to handle"
      },
      {
        "date": "2023-03-06T07:34:00.000Z",
        "voteCount": 1,
        "content": "I think C is correct: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-windows"
      },
      {
        "date": "2023-03-06T07:36:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/"
      },
      {
        "date": "2023-09-11T23:05:00.000Z",
        "voteCount": 3,
        "content": "The two links seems to be evidence of why lambda is not the right choice for the question. It is stated in the two links you provided that lambda cannot deal with stateful computing except for tumbling window in the streams , and pattern recognition as the question asked for is obvious example of  stateful computing that tumbling  window is not applicable."
      },
      {
        "date": "2022-07-11T06:54:00.000Z",
        "voteCount": 6,
        "content": "no, within the 10 minutes, the user wants to be notified immediately when the second event happens"
      },
      {
        "date": "2024-03-25T21:57:00.000Z",
        "voteCount": 2,
        "content": "Bing \nOption D is correct because it provides a highly available, scalable, and real-time solution. Amazon Kinesis Data Streams can capture and process large streams of data records in real time. AWS Application Auto Scaling can automatically adjust capacity to maintain steady, predictable performance at the lowest possible cost. Amazon Kinesis Data Analytics can process and analyze streaming data using standard SQL, and AWS Lambda can run your code in response to events and automatically manage the compute resources for you.\nOther options are not the best solutions for this scenario. For example, Options A and B involve using technologies that may not provide the real-time processing required for this use case. Option C does not provide a mechanism for detecting the known event sequence in real time."
      },
      {
        "date": "2024-02-21T02:40:00.000Z",
        "voteCount": 4,
        "content": "I will go with D as KDS is designed to handle massive streams of real-time data and can scale automatically to match the volume of data input and KDA processes of streaming data using SQL and analyse the incoming data stream for patterns, such as the sequence of voltage drops within 10 minutes of a voltage increase. KDA can scale based on demand, supporting the expansion to other cities. AWS Lambda to poll a notification stream and then publish to the Amazon SNS topic for immediate action upon detecting the specified event.\nOption A involves managing an Apache Kafka cluster and an EMR cluster, which adds complexity and operational overhead. B uses a REST-based web service and RDS, might not scale as seamlessly and could introduce latency in detecting and notifying about the events. C serverless, does not offer the same level of real-time processing and pattern detection capabilities needed for this specific use case as KDA."
      },
      {
        "date": "2024-01-10T16:01:00.000Z",
        "voteCount": 2,
        "content": "kinesis data stream don't support aws auto-scaling, but like everything that happens in the cloud, there exists a way to scale your kds based on lambda and aws auto-scaling... then the D question is not completely wrong...\n\nemr is not highly available, then the A question is killed in this fact...\n\nthis problem announces real-time data streaming, but we discard kdf... not because kdf, but because lambda function... in near-real-time data processing using kdf and lambda with a consumer, just the records streamed in 1MB or 60s are processed by lambda...\n\nb makes no sense...\n\nthis is a poor question and they should be removed from this dump..."
      },
      {
        "date": "2023-12-26T01:00:00.000Z",
        "voteCount": 2,
        "content": "The answer is D. It has a requirement of \" All notifications must be delivered as quickly as possible.\". That requirement rules out Firehose as it can have a delay for buffering and Kinesis data stream is real time."
      },
      {
        "date": "2023-10-14T13:37:00.000Z",
        "voteCount": 2,
        "content": "C. Option D mention \"Set up AWS Application Auto Scaling on both streams\" and this is not possible. Here is not mention that you enable on demand capacity... no, it mentions \"AWS Application Auto Scaling\". And what about this part? \"Create another stream for notifications.\" It is not needed!\nOption C is the correct one"
      },
      {
        "date": "2023-09-01T02:26:00.000Z",
        "voteCount": 3,
        "content": "Vote C.\nA: No, as EMR is not HA. Single region only.\nB: No, RDS is not ideal for streaming.\nC: Yes. All systems are fully managed.\nD: No. Although there is a solution in 2018 (https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/) but it can\u2019t scale more 10 times per 24-hrs nor scale double current shard."
      },
      {
        "date": "2023-08-12T21:12:00.000Z",
        "voteCount": 1,
        "content": "C can't be right, it asks for \"real-time\", Firehouse collects before it pushes the data out which makes it \"near real-time\". Answer should be D"
      },
      {
        "date": "2023-08-01T12:48:00.000Z",
        "voteCount": 2,
        "content": "C, because of non-HA EMR"
      },
      {
        "date": "2023-06-21T04:10:00.000Z",
        "voteCount": 2,
        "content": "EMR is an incorrect answer because it is not high-available"
      },
      {
        "date": "2023-06-18T01:43:00.000Z",
        "voteCount": 2,
        "content": "EMR clusters dy default is not highly available."
      },
      {
        "date": "2023-05-22T23:45:00.000Z",
        "voteCount": 2,
        "content": "Very good question. Both A and C are correct. But A can be ruled out because it uses EMR and out of box its not a highly available service, which is a critical requirement here."
      },
      {
        "date": "2023-05-01T07:26:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-04-27T09:54:00.000Z",
        "voteCount": 2,
        "content": "Answer = C\nAccording to https://aws.amazon.com/kinesis/data-firehose/faqs/\nIt is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration."
      },
      {
        "date": "2023-03-21T14:48:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-02-21T23:53:00.000Z",
        "voteCount": 3,
        "content": "Its A By Elimination. \n\nB is not ideal for streams. \nC. Is not immediate. We need real time thing. \nD. Way too complex. Not sure if we would need additional Stream, Application auto scalling will scale Lambda. Where did Java come from? I believe there is a hard limit for KDS too. \n\nMSK wins on the scalability front. Spark streaming Application can call SNS SDK. Unbelievable, but it is A."
      },
      {
        "date": "2024-01-10T15:51:00.000Z",
        "voteCount": 1,
        "content": "but emr is not high available"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/74075-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A media company has a streaming playback application. The company needs to collect and analyze data to provide near-real-time feedback on playback issues within 30 seconds. The company requires a consumer application to identify playback issues, such as decreased quality during a specified time frame. The data will be streamed in JSON format. The schema can change over time.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Amazon Kinesis Data Firehose with delivery to Amazon S3. Configure an S3 event to invoke an AWS Lambda function to process and analyze the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Amazon Managed Streaming for Apache Kafka. Configure Amazon Kinesis Data Analytics for SQL Application as the consumer application to process and analyze the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Amazon Kinesis Data Firehose with delivery to Amazon S3. Configure Amazon S3 to initiate an event for AWS Lambda to process and analyze the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Amazon Kinesis Data Streams. Configure an Amazon Kinesis Data Analytics for Apache Flink application as the consumer application to process and analyze the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T20:08:00.000Z",
        "voteCount": 12,
        "content": "https://aws.amazon.com/kinesis/data-analytics/features/?pg=ln&amp;sec=hs"
      },
      {
        "date": "2024-02-21T03:35:00.000Z",
        "voteCount": 1,
        "content": "Options A and C, which involve Kinesis Data Firehose with delivery to Amazon S3 and subsequent processing by AWS Lambda, are not optimized for near-real-time feedback due to the inherent latency in delivering data to S3 and then processing it. Option B, involving Amazon Managed Streaming for Apache Kafka and Kinesis Data Analytics for SQL, could also be a viable solution for real-time analytics, but the specific choice of Apache Flink in Option D is more directly aligned with the company's need for complex event processing and near-real-time analysis capabilities."
      },
      {
        "date": "2023-10-14T13:41:00.000Z",
        "voteCount": 1,
        "content": "D. Amazon Managed Service for Apache Flink"
      },
      {
        "date": "2023-09-19T08:30:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C cannot be option as it uses firehose, which has min record buffered time as 60 second / 1 minute and here we have to do processing in 30 second. So we left with B and D"
      },
      {
        "date": "2023-09-19T08:36:00.000Z",
        "voteCount": 1,
        "content": "D seems correct as KDA using flink for processing (sql style)"
      },
      {
        "date": "2023-07-24T00:41:00.000Z",
        "voteCount": 1,
        "content": "D. AWS KDA doesn't take input from Kafka, only if KDA is provisioned using Flink, then Kafka can be a source."
      },
      {
        "date": "2023-05-01T07:27:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-11-14T12:54:00.000Z",
        "voteCount": 5,
        "content": "Was the \"test\" about typing the same comment in every single discussion to show people how you realy wasting your time with stupid actions on the internet?"
      },
      {
        "date": "2023-03-12T08:52:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most suitable solution as it allows the company to stream the data in real-time to Amazon Managed Streaming for Apache Kafka. Then, the data can be processed and analyzed using Amazon Kinesis Data Analytics for SQL Application, which can handle data in JSON format with dynamic schema changes. This solution allows the company to identify playback issues in near-real-time within 30 seconds.\n\nOption A is not optimal because it requires an S3 event to trigger the Lambda function, which may introduce some latency. Option C is similar to option A, but with an additional step of writing the data to S3, which may not be necessary. Option D uses Apache Flink, which may be overkill for this use case and can be more complex to set up compared to the SQL-based Kinesis Data Analytics application in option B."
      },
      {
        "date": "2023-03-12T08:53:00.000Z",
        "voteCount": 2,
        "content": "But since the question never ask for Cost optimisation, I will go with D"
      },
      {
        "date": "2023-01-07T12:03:00.000Z",
        "voteCount": 1,
        "content": "According to https://aws.amazon.com/kinesis/data-analytics/faqs/\nSome keywords are \"json formatted data\", \"schema update\". These are what Kinesis Data Analytics SQL applications do. And Kinesis Data Analytics can also get data from MSK.  Also MSK is a valid data source for KDA SQL Application based on below. https://aws.amazon.com/kinesis/data-analytics/?nc=sn&amp;loc=1"
      },
      {
        "date": "2023-01-15T05:47:00.000Z",
        "voteCount": 1,
        "content": "The preferred service may be the Kinesis Video stream, which is intended for playback. The problem with D is the message size limit in KDS (1MB), while media can be larger.\n\nhttps://aws.amazon.com/kinesis/video-streams/?nc=sn&amp;loc=0&amp;amazon-kinesis-video-streams-resources-blog.sort-by=item.additionalFields.createdDate&amp;amazon-kinesis-video-streams-resources-blog.sort-order=desc#:~:text=Amazon%20Kinesis%20Video%20Streams%20makes%20it%20easy%20to%20securely%20stream%20video%20from%20connected%20devices%20to%20AWS%20for%20analytics%2C%20machine%20learning%20(ML)%2C%20playback%2C%20and%20other%20processing"
      },
      {
        "date": "2022-10-09T21:12:00.000Z",
        "voteCount": 4,
        "content": "D \nFirehose can be ruled out for it has 60 sec data latency.\nKDA for SQL cannot support MSK as source."
      },
      {
        "date": "2022-09-24T23:00:00.000Z",
        "voteCount": 2,
        "content": "As the allowed time offset is of 30 seconds, we can eliminate options with fireshose. Kafka is third party, and not a preferred answer."
      },
      {
        "date": "2022-10-18T02:56:00.000Z",
        "voteCount": 2,
        "content": "This is not the reason for not selecting B, since 'Amazon Managed Streaming for Apache Kafka(MSK)' is a AWS service not third party."
      },
      {
        "date": "2022-07-01T15:34:00.000Z",
        "voteCount": 4,
        "content": "Ans is D.\nOption A &amp; C - Firehose and its minimum buffer time is 60 sec\nOption B - Will work, But JSON schema changes over time. Kinesis Data Analytics requires manual intervention to update column mapping if input changes."
      },
      {
        "date": "2022-05-22T01:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-05-21T03:03:00.000Z",
        "voteCount": 1,
        "content": "How does Managed Kafka and Kinesis Analytics even interact with each other? Correct ans is D."
      },
      {
        "date": "2022-05-11T06:32:00.000Z",
        "voteCount": 2,
        "content": "Changed to D.\nKDA for SQL can not set MSK as source. KDF can not process in 30 seconds."
      },
      {
        "date": "2022-05-05T01:18:00.000Z",
        "voteCount": 2,
        "content": "Vote for B.\nAccording to https://aws.amazon.com/kinesis/data-analytics/faqs/\nSome keywords are \"json formatted data\", \"schema update\". These are what Kinesis Data Analytics SQL applications do. And Kinesis Data Analytics can also get data from MSK."
      },
      {
        "date": "2022-05-11T06:31:00.000Z",
        "voteCount": 3,
        "content": "Change to D, KDA for SQL can not set MSK as source. KDA for Flink can."
      },
      {
        "date": "2022-12-04T00:25:00.000Z",
        "voteCount": 1,
        "content": "Also, data is in JSON format, and the schema can change. SQL is useless in this case"
      },
      {
        "date": "2022-04-30T14:07:00.000Z",
        "voteCount": 1,
        "content": "D- looks good"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/29144-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An ecommerce company stores customer purchase data in Amazon RDS. The company wants a solution to store and analyze historical data. The most recent 6 months of data will be queried frequently for analytics workloads. This data is several terabytes large. Once a month, historical data for the last 5 years must be accessible and will be joined with the more recent data. The company wants to optimize performance and cost.<br>Which storage solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the RDS database to store the most recent 6 months of data. Copy the historical data into Amazon S3. Create an AWS Glue Data Catalog of the data in Amazon S3 and Amazon RDS. Run historical queries using Amazon Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an ETL tool to incrementally load the most recent 6 months of data into an Amazon Redshift cluster. Run more frequent queries against this cluster. Create a read replica of the RDS database to run queries on the historical data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrementally copy data from Amazon RDS to Amazon S3. Create an AWS Glue Data Catalog of the data in Amazon S3. Use Amazon Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrementally copy data from Amazon RDS to Amazon S3. Load and store the most recent 6 months of data in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all historical data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T13:47:00.000Z",
        "voteCount": 20,
        "content": "D seems correct"
      },
      {
        "date": "2021-10-23T08:03:00.000Z",
        "voteCount": 17,
        "content": "Ans D\nNote: A and B are immediately out because RDS is not for analysis. C and D both work, but D is balanced between performance and cost. C may cost less (depending on data compression, frequency of queries) but query to recent data will be slower."
      },
      {
        "date": "2021-10-24T14:39:00.000Z",
        "voteCount": 6,
        "content": "Correct, answer is D.\nRedshift is suitable for running complex analytical queries. Athena is suitable for small ad-hoc queries."
      },
      {
        "date": "2024-01-10T17:02:00.000Z",
        "voteCount": 1,
        "content": "when we say \"analytics queries\", \"analytics workloads\" or \"complex analysis\", in 80% of the cases we call redshift... if we sum a short period of analysis (6 months in this case) redshift is a better option... rds will continue as a relational database,  don't run analytics queries"
      },
      {
        "date": "2023-05-01T07:28:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-05-22T08:05:00.000Z",
        "voteCount": 5,
        "content": "I see you have been posting only I passed the test :)"
      },
      {
        "date": "2023-06-27T22:24:00.000Z",
        "voteCount": 2,
        "content": "hahah!!"
      },
      {
        "date": "2023-03-12T09:09:00.000Z",
        "voteCount": 2,
        "content": "Option D is the most suitable solution for this scenario.\n\nExplanation:\n\nIncrementally copy data from Amazon RDS to Amazon S3: This allows for storing of historical data in a cost-effective manner while allowing frequent querying of the more recent data in RDS.\nLoad and store the most recent 6 months of data in Amazon Redshift: This provides a performant solution for frequent queries of the most recent data.\nConfigure an Amazon Redshift Spectrum table to connect to all historical data: This enables joining of historical data with the more recent data in Redshift, providing the required analysis capability.\nOption A does not address the requirement to optimize performance for querying the most recent data. Option B involves creating a read replica of RDS, which may not be efficient for frequently queried data. Option C also does not provide a solution for frequent querying of the most recent data."
      },
      {
        "date": "2023-02-25T19:19:00.000Z",
        "voteCount": 1,
        "content": "Option D suggests copying data from RDS to S3 incrementally, storing the most recent 6 months of data in Amazon Redshift, and configuring an Amazon Redshift Spectrum table to connect to all historical data. This approach allows the company to optimize cost and performance as Redshift is a cost-effective data warehousing solution that can handle large volumes of data. Additionally, using Redshift Spectrum enables the company to query both the recent and historical data sets together in real-time.\n\nOption A suggests creating a read replica of the RDS database to store the most recent 6 months of data and copying the historical data into Amazon S3. This approach does not allow for real-time querying of the historical data and may result in increased query latency."
      },
      {
        "date": "2023-02-12T15:53:00.000Z",
        "voteCount": 1,
        "content": "A. By moving the data to S3 and Glue Catalog that carries both RDS and S3 schema will enable them to use the same schema for queries. Remember the requirement says \"low cost\". Redshift is out of the picture."
      },
      {
        "date": "2023-03-11T05:47:00.000Z",
        "voteCount": 1,
        "content": "I don't think read replicas for certain months can be created. Read replicas will replicate entire db. Unlikely A is the answer"
      },
      {
        "date": "2023-02-03T07:58:00.000Z",
        "voteCount": 1,
        "content": "D for the win"
      },
      {
        "date": "2022-11-05T07:14:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer as loading and querying recent 6 months of data via Redshift gives better performance and old data can be queried via Redshift spectrum\nC is wrong though it's possible to query the entire data in S3 using Athena, however, it will not be able to match the high performance offered by Redshift to query the last six months of data. So this option is not the best fit for the given use case.\n\nOptions A &amp; B are wrong as RDS is not an ideal solution to store and query historical data. Also, 6 months data may be several terabytes large."
      },
      {
        "date": "2022-10-25T19:02:00.000Z",
        "voteCount": 1,
        "content": "D seems correct"
      },
      {
        "date": "2022-07-21T19:57:00.000Z",
        "voteCount": 1,
        "content": "Answer-D"
      },
      {
        "date": "2022-04-30T16:08:00.000Z",
        "voteCount": 1,
        "content": "Answer-D"
      },
      {
        "date": "2022-03-27T09:20:00.000Z",
        "voteCount": 1,
        "content": "D seems correct"
      },
      {
        "date": "2022-03-20T04:14:00.000Z",
        "voteCount": 1,
        "content": "D is correct..."
      },
      {
        "date": "2022-03-06T14:44:00.000Z",
        "voteCount": 1,
        "content": "effective way to query across S3 and RDS is using redshift spectrum"
      },
      {
        "date": "2021-12-22T11:29:00.000Z",
        "voteCount": 1,
        "content": "Historical Data points to Redshift Spectrum. Hence D"
      },
      {
        "date": "2021-11-20T07:16:00.000Z",
        "voteCount": 1,
        "content": "answer is D."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/28685-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company leverages Amazon Athena for ad-hoc queries against data stored in Amazon S3. The company wants to implement additional controls to separate query execution and query history among users, teams, or applications running in the same AWS account to comply with internal security policies.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket for each given use case, create an S3 bucket policy that grants permissions to appropriate individual IAM users. and apply the S3 bucket policy to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Athena workgroup for each given use case, apply tags to the workgroup, and create an IAM policy using the tags to apply appropriate permissions to the workgroup.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role for each given use case, assign appropriate permissions to the role for the given use case, and add the role to associate the role with Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue Data Catalog resource policy for each given use case that grants permissions to appropriate individual IAM users, and apply the resource policy to the specific tables used by Athena."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T06:31:00.000Z",
        "voteCount": 29,
        "content": "B any thoughts?\nhttps://docs.aws.amazon.com/athena/latest/ug/user-created-workgroups.html"
      },
      {
        "date": "2021-09-22T08:25:00.000Z",
        "voteCount": 10,
        "content": "I think it's B. based on this link:\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/"
      },
      {
        "date": "2024-02-21T03:53:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B - creating Athena workgroups for each use case, tagging those workgroups, and applying IAM policies based on those tags is the most effective way to meet the company's security and compliance requirements.\n\nOption A focuses on S3 bucket policies, which do not directly address the separation of query execution within Athena.\nOption C involves creating IAM roles for use cases but does not inherently separate query execution and history within Athena itself.\nOption D pertains to AWS Glue Data Catalog resource policies, which, while important for controlling access to data, do not directly manage the separation of Athena query execution and history."
      },
      {
        "date": "2023-05-01T07:29:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-12T09:11:00.000Z",
        "voteCount": 3,
        "content": "The solution that meets the requirements to separate query execution and query history among users, teams, or applications running in the same AWS account is to create an Athena workgroup for each given use case, apply tags to the workgroup, and create an IAM policy using the tags to apply appropriate permissions to the workgroup. This allows the company to control access to specific workgroups and apply different permissions to different groups. Option B is therefore the correct answer.\n\nOption A is not a suitable solution as creating S3 buckets for each use case would not effectively control access to Athena queries and history.\n\nOption C is not a suitable solution as creating an IAM role for each use case would not allow for granular control over permissions and would not effectively separate query execution and history.\n\nOption D is not a suitable solution as creating an AWS Glue Data Catalog resource policy would not effectively separate query execution and history within Athena."
      },
      {
        "date": "2023-03-11T05:50:00.000Z",
        "voteCount": 4,
        "content": "Does this site deliberately provide wrong answer choice?"
      },
      {
        "date": "2022-11-05T07:16:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B as Athena Workgroup can help comply with the internal security policies by separating execution for users, teams, applications and applying access control and auditing."
      },
      {
        "date": "2022-08-02T10:03:00.000Z",
        "voteCount": 1,
        "content": "without a doubt, the best way to guarantee isolation and better control over history, cost and the lowest possible level of access is to create workgroups for athena. Letter B without a doubt!"
      },
      {
        "date": "2022-07-29T22:00:00.000Z",
        "voteCount": 1,
        "content": "Athena group"
      },
      {
        "date": "2022-07-26T08:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-07-20T11:37:00.000Z",
        "voteCount": 1,
        "content": "Athena Workgroups"
      },
      {
        "date": "2022-06-05T13:53:00.000Z",
        "voteCount": 1,
        "content": "B \nbased on that link:\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/"
      },
      {
        "date": "2022-05-22T01:59:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B"
      },
      {
        "date": "2022-04-12T06:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-03-29T23:29:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-03-26T10:09:00.000Z",
        "voteCount": 1,
        "content": "I think B is the correct answer"
      },
      {
        "date": "2022-02-26T01:05:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/28696-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to use an automatic machine learning (ML) Random Cut Forest (RCF) algorithm to visualize complex real-world scenarios, such as detecting seasonality and trends, excluding outers, and imputing missing values.<br>The team working on this project is non-technical and is looking for an out-of-the-box solution that will require the LEAST amount of management overhead.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue ML transform to create a forecast and then use Amazon QuickSight to visualize the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight to visualize the data and then use ML-powered forecasting to forecast the key business metrics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a pre-build ML AMI from the AWS Marketplace to create forecasts and then use Amazon QuickSight to visualize the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse calculated fields to create a new forecast and then use Amazon QuickSight to visualize the data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T00:01:00.000Z",
        "voteCount": 26,
        "content": "It is B based on this link: \nhttps://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html"
      },
      {
        "date": "2021-11-15T00:21:00.000Z",
        "voteCount": 1,
        "content": "Agreed:  more reference \nhttps://docs.aws.amazon.com/quicksight/latest/user/how-does-rcf-generate-forecasts.html"
      },
      {
        "date": "2023-08-14T13:50:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C.\n\nHere is the explanation:\n\n    Use a pre-build ML AMI from the AWS Marketplace to create forecasts and then use Amazon QuickSight to visualize the data. This is the most out-of-the-box solution and will require the least amount of management overhead.\n    AWS Glue ML transforms are a great way to automate ML tasks, but they require some technical expertise to set up and use.\n    Amazon QuickSight is a great visualization tool, but it does not have built-in ML capabilities.\n    Calculated fields are a way to create new fields in a data set, but they cannot be used to create forecasts."
      },
      {
        "date": "2023-08-03T04:03:00.000Z",
        "voteCount": 1,
        "content": "Quicksight supports RCF and can connect easily to multiple data sources (S3, JDBC ..etc)"
      },
      {
        "date": "2023-05-22T08:33:00.000Z",
        "voteCount": 1,
        "content": "Quicksight uses a built-in version of RCF"
      },
      {
        "date": "2023-05-01T07:30:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2024-05-12T20:51:00.000Z",
        "voteCount": 1,
        "content": "Dude is a legend now"
      },
      {
        "date": "2023-12-09T10:02:00.000Z",
        "voteCount": 1,
        "content": "It's very nice to know!! No one noticed"
      },
      {
        "date": "2023-04-27T10:00:00.000Z",
        "voteCount": 2,
        "content": "this scenario is shocking to see because the team is all non-technical ...."
      },
      {
        "date": "2023-03-12T09:16:00.000Z",
        "voteCount": 1,
        "content": "Both options B and C involve using Amazon QuickSight to visualize the data. However, option C involves using a pre-built machine learning Amazon Machine Image (AMI) from the AWS Marketplace to create forecasts, which may require more technical expertise to set up and manage than option B, which simply involves using ML-powered forecasting within Amazon QuickSight. Therefore, option B may be more suitable for a non-technical team looking for an out-of-the-box solution with minimal management overhead."
      },
      {
        "date": "2023-02-25T19:30:00.000Z",
        "voteCount": 1,
        "content": "Amazon QuickSight uses a built-in version of the Random Cut Forest (RCF) algorithm. \n\nB is correct"
      },
      {
        "date": "2023-02-25T19:28:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best solution for this scenario. The company wants an out-of-the-box solution that requires the least amount of management overhead, and using a pre-built ML AMI from the AWS Marketplace to create forecasts and then using Amazon QuickSight to visualize the data is the most straightforward approach. The pre-built ML AMI will provide the Random Cut Forest algorithm for the team to use, and Amazon QuickSight provides an easy-to-use interface for data visualization. This solution will require minimal technical expertise and management overhead from the non-technical team.\n\nOption B is not the best solution as using ML-powered forecasting in Amazon QuickSight does not provide the Random Cut Forest algorithm that the company wants to use."
      },
      {
        "date": "2023-03-12T09:16:00.000Z",
        "voteCount": 1,
        "content": "Both options B and C involve using Amazon QuickSight to visualize the data. However, option C involves using a pre-built machine learning Amazon Machine Image (AMI) from the AWS Marketplace to create forecasts, which may require more technical expertise to set up and manage than option B, which simply involves using ML-powered forecasting within Amazon QuickSight. Therefore, option B may be more suitable for a non-technical team looking for an out-of-the-box solution with minimal management overhead."
      },
      {
        "date": "2022-12-11T03:52:00.000Z",
        "voteCount": 2,
        "content": "B, Quicksigth has many ML tools."
      },
      {
        "date": "2022-11-05T07:17:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B as QuickSight ML provides an out-of-the-box ML Random Cut Forest (RCF) algorithm to help visualize complex real-world scenarios, such as detecting seasonality and trends, excluding outliers, and imputing missing values.\n\nOptions A, C &amp; D are wrong as they do not come with the least operational overhead."
      },
      {
        "date": "2022-10-31T01:14:00.000Z",
        "voteCount": 3,
        "content": "B is correct.\nAmazon QuickSight uses a built-in version of the Random Cut Forest (RCF) algorithm. The following sections explain what that means and how it is used in Amazon QuickSight.\n\nFirst, let's look at some of the terminology involved:\n\n    Anomaly \u2013 Something that is characterized by its difference from the majority of the other things in the same sample. Also known as an outlier, an exception, a deviation, and so on.\n\n    Data point \u2013 A discrete unit\u2014or simply put, a row\u2014in a dataset. However, a row can have multiple data points if you use a measure over different dimensions.\n\n    Decision Tree \u2013 A way of visualizing the decision process of the algorithm that evaluates patterns in the data.\n\n    Forecast \u2013 A prediction of future behavior based on current and past behavior.\n\n    Model \u2013 A mathematical representation of the algorithm or what the algorithm learns.\n\n    Seasonality \u2013 The repeating patterns of behavior that occur cyclically in time series data.\n\n    Time series \u2013 An ordered set of date or time data in one field or column."
      },
      {
        "date": "2022-10-25T23:27:00.000Z",
        "voteCount": 2,
        "content": "B. Amazon QuickSight enables nontechnical users to confidently forecast their key business metrics. The built-in ML Random Cut Forest algorithm automatically handles complex real-world scenarios such as detecting seasonality and trends, excluding outliers, and imputing missing values. You can interact with the data with point-and-click simplicity."
      },
      {
        "date": "2022-08-11T17:44:00.000Z",
        "voteCount": 1,
        "content": "it is B\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/concept-of-ml-algorithms.html"
      },
      {
        "date": "2022-07-21T19:15:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-05-22T01:40:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B"
      },
      {
        "date": "2022-05-06T01:10:00.000Z",
        "voteCount": 1,
        "content": "Aggree with B.\nBut one problem is, the question mentions about \"inputing missing data\". Can quicksight handle missing data for ML related visualization?"
      },
      {
        "date": "2022-10-18T03:43:00.000Z",
        "voteCount": 1,
        "content": "Yes of course. \"The built-in ML Random Cut Forest algorithm automatically handles complex real-world scenarios such as detecting seasonality and trends, excluding outliers, and imputing missing values. You can interact with the data with point-and-click simplicity.\"\n\nLink: https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/28683-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company's data analytics team recently created multiple product sales analysis dashboards for the average selling price per product using Amazon<br>QuickSight. The dashboards were created from .csv files uploaded to Amazon S3. The team is now planning to share the dashboards with the respective external product owners by creating individual users in Amazon QuickSight. For compliance and governance reasons, restricting access is a key requirement. The product owners should view only their respective product analysis in the dashboard reports.<br>Which approach should the data analytics team take to allow product owners to view only their products in the dashboard?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate the data by product and use S3 bucket policies for authorization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate the data by product and use IAM policies for authorization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manifest file with row-level security.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate dataset rules with row-level security.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T00:53:00.000Z",
        "voteCount": 37,
        "content": "D any thoughts?\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2021-12-21T00:03:00.000Z",
        "voteCount": 3,
        "content": "Agree\nQuick tip - row level security only available in Enterprise Edition"
      },
      {
        "date": "2024-01-10T17:08:00.000Z",
        "voteCount": 2,
        "content": "agree, this question appearedin my test"
      },
      {
        "date": "2021-11-02T14:47:00.000Z",
        "voteCount": 7,
        "content": "Ans D\nThis is a textbook question.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2024-02-21T04:13:00.000Z",
        "voteCount": 1,
        "content": "Options A and B involve managing access at the S3 level, which does not directly apply to controlling visibility within QuickSight dashboards. \nOption C, creating a manifest file, is part of how you might structure data for QuickSight, but it does not directly address row-level security within QuickSight itself.\n\nOption D is correct - creating dataset rules with row-level security within Amazon QuickSight is the most effective and governance-compliant method to ensure that product owners have access only to their respective product analysis in the dashboard reports."
      },
      {
        "date": "2023-05-01T07:32:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2024-02-13T00:25:00.000Z",
        "voteCount": 1,
        "content": "agree with no doubt"
      },
      {
        "date": "2023-04-27T10:02:00.000Z",
        "voteCount": 1,
        "content": "D without any doubt. \nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2023-03-12T09:19:00.000Z",
        "voteCount": 4,
        "content": "D. Create dataset rules with row-level security would be the best approach for this use case. Row-level security (RLS) allows you to define filters at the row level, which can be used to control access to specific data based on user attributes or permissions. This would enable the data analytics team to ensure that each product owner can only see the data for their respective products, as the filters would be applied to the dashboard data before it is displayed to the user. Option A and B would not provide the necessary granularity to restrict access to specific data based on the product owners, and option C would not be applicable in this case as it is used for securing data in Amazon Redshift clusters."
      },
      {
        "date": "2023-04-12T07:00:00.000Z",
        "voteCount": 1,
        "content": "Hey man thanks for you answers across threads really helped me a lot, could you let us know if you have taken the exam or any tips &amp; tricks"
      },
      {
        "date": "2022-11-28T21:45:00.000Z",
        "voteCount": 1,
        "content": "i think D"
      },
      {
        "date": "2022-11-05T07:21:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D as dataset rules with row-level security can be used to restrict the data the product owners would see, which is based on the product.\n\nOptions A &amp; B are wrong as they would not provide fine-grained access control and would need extra effort.\n\n\nOption C is wrong as the row-level security rules need to be defined in the dataset and not in the manifest file."
      },
      {
        "date": "2022-10-31T01:17:00.000Z",
        "voteCount": 3,
        "content": "D is correct\nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. When you share a dataset with RLS with dataset owners, they can still see all the data. When you share it with readers, however, they can only see the data restricted by the permission dataset rules. By adding row-level security, you can further control their access."
      },
      {
        "date": "2022-10-25T23:41:00.000Z",
        "voteCount": 2,
        "content": "In the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. When you share a dataset with RLS with dataset owners, they can still see all the data. When you share it with readers, however, they can only see the data restricted by the permission dataset rules. By adding row-level security, you can further control their access."
      },
      {
        "date": "2022-09-24T23:04:00.000Z",
        "voteCount": 1,
        "content": "It should be done through dataset rules"
      },
      {
        "date": "2022-07-21T19:57:00.000Z",
        "voteCount": 2,
        "content": "Answer-D"
      },
      {
        "date": "2022-09-13T10:57:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2022-06-28T02:33:00.000Z",
        "voteCount": 1,
        "content": "For sure it is right answer"
      },
      {
        "date": "2022-05-22T01:37:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D"
      },
      {
        "date": "2022-05-21T19:21:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer. Note that there are various dashboards for different products. And access needs to be provisioned to the product owners for respective products. Hence, ideal way is to create an IAM user for each product owner and use that user to access quickSight dashboard."
      },
      {
        "date": "2022-04-30T16:09:00.000Z",
        "voteCount": 2,
        "content": "Answer-D"
      },
      {
        "date": "2022-04-29T01:21:00.000Z",
        "voteCount": 2,
        "content": "as mentions by others https://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/28708-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has developed an Apache Hive script to batch process data stared in Amazon S3. The script needs to run once every day and store the output in<br>Amazon S3. The company tested the script, and it completes within 30 minutes on a small local three-node cluster.<br>Which solution is the MOST cost-effective for scheduling and executing the script?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to spin up an Amazon EMR cluster with a Hive execution step. Set KeepJobFlowAliveWhenNoSteps to false and disable the termination protection flag. Use Amazon CloudWatch Events to schedule the Lambda function to run daily.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Management Console to spin up an Amazon EMR cluster with Python Hue. Hive, and Apache Oozie. Set the termination protection flag to true and use Spot Instances for the core nodes of the cluster. Configure an Oozie workflow in the cluster to invoke the Hive script daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue job with the Hive script to perform the batch operation. Configure the job to run once a day using a time-based schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda layers and load the Hive runtime to AWS Lambda and copy the Hive script. Schedule the Lambda function to run daily by creating a workflow using AWS Step Functions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-01T01:18:00.000Z",
        "voteCount": 44,
        "content": "For me it is A. Not B because we are not supposed to run core nodes in spot instances, just task nodes and it is more expensive because to schedule with oozie, our cluster have to be up all the time.  It is not C because glue cannot run hive script, and it is not c because lambda cannot run hive scripts also. https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html"
      },
      {
        "date": "2021-10-02T10:09:00.000Z",
        "voteCount": 2,
        "content": "Agree with A"
      },
      {
        "date": "2021-10-03T03:42:00.000Z",
        "voteCount": 2,
        "content": "Perfect Explanation ; I wanted to write something but your text covers everything."
      },
      {
        "date": "2021-11-03T21:08:00.000Z",
        "voteCount": 9,
        "content": "https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html\nWith KeepJobFlowAliveWhenNoSteps parameter is set to False, the cluster will be shutdown once the steps are completed, thus the cost effective requirement is met"
      },
      {
        "date": "2021-10-30T16:44:00.000Z",
        "voteCount": 8,
        "content": "+ A is the correct answer. \n- B : Spot Instances are not a good option to run a 30-min-script\n- C:  Glue cannot run Hive scripts\n- D: Lambda can run for 15 minutes maximum. Not enough time to run that script."
      },
      {
        "date": "2023-04-03T17:55:00.000Z",
        "voteCount": 4,
        "content": "C: Glue cannot run Hive scripts---&gt; Clue can run hive scripts. But the problem is that C keep all the Glue setting and does not terminate it.\nA By default, an Amazon EMR cluster will be terminated automatically when all steps have completed and there are no pending steps or other applications running on the cluster."
      },
      {
        "date": "2024-03-25T22:10:00.000Z",
        "voteCount": 1,
        "content": "Bing\nOption C is correct because AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. You can create a job in AWS Glue that incorporates your Hive script, and you can schedule this job to run once a day. This approach does not require the provisioning or management of servers, making it a cost-effective solution.\nOther options involve using Amazon EMR or AWS Lambda, which could incur higher costs due to the need for server provisioning and potential for idle resources."
      },
      {
        "date": "2023-10-14T13:55:00.000Z",
        "voteCount": 1,
        "content": "A. As carol1522 explains in her comment"
      },
      {
        "date": "2023-10-08T11:40:00.000Z",
        "voteCount": 1,
        "content": "A satisfies all the requirements"
      },
      {
        "date": "2023-09-23T06:39:00.000Z",
        "voteCount": 1,
        "content": "\"Could anyone who chose \"A\" as the correct answer please explain how to make a Lambda function run for 30 minutes?\""
      },
      {
        "date": "2023-10-14T13:54:00.000Z",
        "voteCount": 2,
        "content": "The lambda function only create and initialise the EMR..."
      },
      {
        "date": "2023-06-25T01:44:00.000Z",
        "voteCount": 3,
        "content": "Since AWS Glue can run Hive script. So C will be cheaper than A."
      },
      {
        "date": "2023-06-06T21:27:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-05-01T07:33:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-09-23T06:41:00.000Z",
        "voteCount": 1,
        "content": "You passed, OK but this question was wrong in your test I think, how can you make lambda run for 30 minutes?"
      },
      {
        "date": "2023-09-27T01:19:00.000Z",
        "voteCount": 1,
        "content": "I guess that the A means Lambda function would just spin up the EMR Cluster, when Cluster has started, the Lambda function would stop. Then the Hive script run on EMR Cluster, and terminated when script running done."
      },
      {
        "date": "2023-02-17T12:32:00.000Z",
        "voteCount": 4,
        "content": "aws glue can run hive scripts - https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html"
      },
      {
        "date": "2023-08-07T12:06:00.000Z",
        "voteCount": 3,
        "content": "I think you misunderstand this blog. hive can use the catalog generated by glue, but glue running hive script.\nSo, C is still wrong, A is the correct answer"
      },
      {
        "date": "2022-11-05T07:25:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is A as the EMR cluster can be used to execute the Hive scripts. KeepJobFlowAliveWhenNoSteps set to false and disabling the termination protection flag would help destroy the cluster once no running jobs. CloudWatch Events with Lambda can be used to trigger the scheduled activity.\n\nOption B is wrong as Oozie requires the EMR cluster always running, else the job cannot be scheduled and executed. Using Spot instances for core nodes is not recommended.\n\nOption C is wrong as Glue does not support running Hive scripts.\n\nOption D is wrong as Lambda would not be able to meet the 30 minutes job runtime requirement."
      },
      {
        "date": "2022-09-24T23:05:00.000Z",
        "voteCount": 2,
        "content": "This one is a classic scenario of Transient cluster. So A is the answer here."
      },
      {
        "date": "2022-07-27T23:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-11-07T04:12:00.000Z",
        "voteCount": 3,
        "content": "I will go with A."
      },
      {
        "date": "2021-11-06T19:55:00.000Z",
        "voteCount": 4,
        "content": "Ans A\nB = wrong, termination flag should be off, spot instances not good for core nodes. C = Glue runs on Spark, cannot run Hive scripts. D = wrong, Lambda maximum running time 15 minutes."
      },
      {
        "date": "2021-11-03T00:24:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2021-10-31T14:16:00.000Z",
        "voteCount": 2,
        "content": "A based on its similarity to this article https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-process-sample-data.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/28797-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to improve the data load time of a sales data dashboard. Data has been collected as .csv files and stored within an Amazon S3 bucket that is partitioned by date. The data is then loaded to an Amazon Redshift data warehouse for frequent analysis. The data volume is up to 500 GB per day.<br>Which solution will improve the data loading performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress .csv files and use an INSERT statement to ingest data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit large .csv files, then use a COPY command to load data into Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to ingest data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the .csv files in an unsorted key order and vacuum the table in Amazon Redshift."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T15:16:00.000Z",
        "voteCount": 44,
        "content": "B for sure.\nThe COPY command loads the data in parallel from multiple files, dividing the workload among the nodes in your cluster. When you load all the data from a single large file, Amazon Redshift is forced to perform a serialized load, which is much slower. Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression. The number of files should be a multiple of the number of slices in your cluster"
      },
      {
        "date": "2021-11-07T08:28:00.000Z",
        "voteCount": 11,
        "content": "Ans B\nA = wrong, compression means download file from S3 then compress, time-consuming, also you use COPY for files not INSERT. C = wrong, will not improve performance. D = wrong, vacuum frees up storage. This is a question about parallel loading."
      },
      {
        "date": "2023-07-23T11:43:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-05-01T07:34:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-12T09:27:00.000Z",
        "voteCount": 3,
        "content": "Option B is the most appropriate solution for improving data loading performance. Splitting large .csv files and using a COPY command can parallelize the load process and reduce the data load time. The data partitioning by date can help further optimize the load process by reducing the data scanned for each load. Compressing the .csv files may help reduce the storage cost, but it may not improve the data load time. Using an INSERT statement to ingest data into Amazon Redshift can be slow and does not take advantage of Redshift's parallel processing capability. Amazon Kinesis Data Firehose can be used to ingest streaming data in real-time, but may not be the best choice for large batch loads. Loading the .csv files in an unsorted key order and vacuuming the table can help optimize the table for query performance but may not improve the data loading performance."
      },
      {
        "date": "2022-11-05T07:27:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B as splitting the large file into multiple files can help improve the data loading performance using the COPY command.\n\nOption A is wrong as the COPY command would provide the best benefit.\n\nOption C is wrong as Kinesis Data Firehose cannot move data from S3 to Redshift. Kinesis Data Firehose delivers your data to your S3 bucket first and then issues an Amazon Redshift COPY command to load the data into your Amazon Redshift cluster. So it doesn't still improve the load performance.\n\nOption D is wrong as vacuuming will free up space but does not improve the load performance."
      },
      {
        "date": "2022-09-24T23:07:00.000Z",
        "voteCount": 1,
        "content": "S3 to Redshift upload can be done through copy command. To utilize parallelism, large files are recommended to split into small chunk of files."
      },
      {
        "date": "2022-07-23T06:54:00.000Z",
        "voteCount": 2,
        "content": "Don't know why B? here is uncompressed csv file, so no need to split file (Redshift will do automatically?\nIn contrast, when you load delimited data from a large, uncompressed file, Amazon Redshift makes use of multiple slices. These slices work in parallel, automatically. This provides fast load performance.\nIn https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-use-multiple-files.html"
      },
      {
        "date": "2022-11-27T00:37:00.000Z",
        "voteCount": 1,
        "content": "Agreed. It seems like no correct answer among the choices."
      },
      {
        "date": "2022-07-19T23:29:00.000Z",
        "voteCount": 1,
        "content": "Answer = B"
      },
      {
        "date": "2022-05-22T01:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-03-17T17:57:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-11-30T07:07:00.000Z",
        "voteCount": 1,
        "content": "Ans:B split large files will help loading in performance. Having one large file will load in serialized manner which lowers performance"
      },
      {
        "date": "2021-10-11T11:41:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-11T11:34:00.000Z",
        "voteCount": 2,
        "content": "B is correct for me"
      },
      {
        "date": "2021-10-05T09:06:00.000Z",
        "voteCount": 2,
        "content": "B for sure."
      },
      {
        "date": "2021-10-01T11:44:00.000Z",
        "voteCount": 1,
        "content": "It's already in S3 so answer is B 100%"
      },
      {
        "date": "2021-09-28T15:59:00.000Z",
        "voteCount": 3,
        "content": "Answer is B, A compress is a good practice but then copy command not insert, Kinesis Firehose will not improve performance, the vacuum will help freeing space not improve perf too."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/29408-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has a data warehouse in Amazon Redshift that is approximately 500 TB in size. New data is imported every few hours and read-only queries are run throughout the day and evening. There is a particularly heavy load with no writes for several hours each morning on business days. During those hours, some queries are queued and take a long time to execute. The company needs to optimize query execution and avoid any downtime.<br>What is the MOST cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable concurrency scaling in the workload management (WLM) queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more nodes using the AWS Management Console during peak hours. Set the distribution style to ALL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse elastic resize to quickly add nodes during peak times. Remove the nodes when they are not needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a snapshot, restore, and resize operation. Switch to the new target cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T03:58:00.000Z",
        "voteCount": 28,
        "content": "Answer A is correct- https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html"
      },
      {
        "date": "2021-11-27T08:08:00.000Z",
        "voteCount": 10,
        "content": "Answer: A\nWLM Concurrency scaling feature automatically adds additional capacity for both read and write queries and charged only for the duration the queries are actively running. Hence, it is the cost-effective approach. https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"
      },
      {
        "date": "2024-01-11T03:46:00.000Z",
        "voteCount": 2,
        "content": "Easy question, A any througs?\n\nThrough workload management, you can prioritize queries and define execution patterns across your cluster...\n\nresizing, snapshot, or other solutions in these answers is a bit expansive and ineffective."
      },
      {
        "date": "2023-05-01T07:35:00.000Z",
        "voteCount": 4,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-12T09:29:00.000Z",
        "voteCount": 1,
        "content": "A. Enable concurrency scaling in the workload management (WLM) queue is the most cost-effective solution.\n\nEnabling concurrency scaling in the workload management (WLM) queue allows Amazon Redshift to add more cluster capacity to handle the increased query load during peak hours. This is done automatically and can be configured based on the number of users or the number of queries. Concurrency scaling can be turned off during off-peak hours to save costs. This is a more cost-effective solution compared to adding more nodes or using elastic resize, which can be more expensive and take longer to configure. Snapshot, restore, and resize operations can also be time-consuming and may result in downtime."
      },
      {
        "date": "2022-11-05T07:30:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A as Redshift Concurrency Scaling can help scale the cluster to support virtually unlimited concurrent users and queries.\nOptions B, C &amp; D are wrong as they scale the cluster by adding resources that would not be cost-effective."
      },
      {
        "date": "2022-09-24T23:09:00.000Z",
        "voteCount": 1,
        "content": "Wherever query is stuck for other long-running queries, we can use WLM."
      },
      {
        "date": "2022-07-19T23:35:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-04-22T11:26:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      },
      {
        "date": "2021-11-17T13:50:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-11-07T05:02:00.000Z",
        "voteCount": 3,
        "content": "Answer A is correct. B, C, D = wrong, because they all talk about resizing or scaling which will not be cost-effective."
      },
      {
        "date": "2021-11-04T10:31:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2021-10-20T19:25:00.000Z",
        "voteCount": 2,
        "content": "A is correct for me"
      },
      {
        "date": "2021-10-16T07:48:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html\nAnswer is A 100%"
      },
      {
        "date": "2021-09-26T14:28:00.000Z",
        "voteCount": 4,
        "content": "A for sure.\nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/29115-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company analyzes its data in an Amazon Redshift data warehouse, which currently has a cluster of three dense storage nodes. Due to a recent business acquisition, the company needs to load an additional 4 TB of user data into Amazon Redshift. The engineering team will combine all the user data and apply complex calculations that require I/O intensive resources. The company needs to adjust the cluster's capacity to support the change in analytical and storage requirements.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResize the cluster using elastic resize with dense compute nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResize the cluster using classic resize with dense compute nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResize the cluster using elastic resize with dense storage nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResize the cluster using classic resize with dense storage nodes."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T19:49:00.000Z",
        "voteCount": 34,
        "content": "Vote A.\n\"currently has a cluster of three dense storage nodes.\" means it is not single-node cluster, both resizing work, but classic resize take 2 hours\u20132 days or longer, depending on your data's size.\nDense Compute (DC) allow creation of very high performance data warehouses using fast CPUs, large amounts of RAM and solid-state disks (SSDs)"
      },
      {
        "date": "2021-09-26T15:46:00.000Z",
        "voteCount": 3,
        "content": "A &amp; C should be avoided. Elastic resize is for temporary size adjustment. When you do the  Elastic resize, the \"SLICE\" count \"will not\" change and not good for long term data load and computation."
      },
      {
        "date": "2021-10-18T08:29:00.000Z",
        "voteCount": 5,
        "content": "I don't think this is correct . When you elastic upsize you'll get more slices but when you elastic downsize you won't loose any slices. (I tested it myself)"
      },
      {
        "date": "2021-09-23T20:54:00.000Z",
        "voteCount": 26,
        "content": "The answer is C.\n\nby https://aws.amazon.com/redshift/features/?nc1=h_ls\naccoranding to the question \"...combine all the user data and apply complex calculations that require I/O intensive resources\",\n\ndrop A, B sinice \"Dense Compute (DC) the best choice for less than 500GB of data\".\n\nDS2 (Dense Storage) nodes enable you to create large data warehouses using hard disk drives (HDDs).Most customers who run on DS2 clusters can migrate their workloads to RA3 clusters and get up to 2x performance and more storage for the same cost as DS2.\n\nby https://aws.amazon.com/redshift/pricing/\n\"What to expect\" section\nOnce you make your selection, you may wish to use elastic resize to easily adjust the amount of provisioned compute capacity within minutes for steady-state processing."
      },
      {
        "date": "2021-09-27T02:17:00.000Z",
        "voteCount": 8,
        "content": "how DS would manage complex calculations and IO intensive aspects?"
      },
      {
        "date": "2022-05-27T04:44:00.000Z",
        "voteCount": 2,
        "content": "is 500 GB limit for each node? Since we are doing resize, we are adding new nodes as well. So we should take care of compute i think. Hence A."
      },
      {
        "date": "2024-04-03T06:11:00.000Z",
        "voteCount": 1,
        "content": "classic resize does not let you change node type ( note that original config for infra is nodetype = DS2 ) meaning option B is out . If you only want to increase the number of nodes of same type , then you can do that with classic resize ( offcource it will create a new cluster ) so in my opinion C is useless as i can use classic storage itself . Between A and D i vote for A since the use case talks about compute more than storage"
      },
      {
        "date": "2024-04-03T06:22:00.000Z",
        "voteCount": 1,
        "content": "correction - read the use case again and the company already has data in the warehouse so one thing for sure that classic resize will not retain system tables and data . So option B and D are out . Between A and C , i still vote for A since as per redshift documentation dc2 is a good choice for data &lt; 10 TB . From the use case we don't know what's existing size of the warehouse but we are 4 TB to X . We don't know X so dc2 seems logical"
      },
      {
        "date": "2024-03-29T22:57:00.000Z",
        "voteCount": 1,
        "content": "definitively C"
      },
      {
        "date": "2024-03-26T00:18:00.000Z",
        "voteCount": 1,
        "content": "Bing answer B\nB. Resize the cluster using classic resize with dense compute nodes.\n\nExplanation:\n\nOption B is correct because classic resize allows you to change both the node type and number of nodes. In this case, switching to dense compute nodes would provide the I/O intensive resources needed for the complex calculations. The classic resize operation also redistributes the data and reclaims the space, which would be beneficial given the additional 4 TB of user data.\nOther options are not the best solutions for this scenario. For example, Options A and C involve using elastic resize, which allows you to quickly add or remove nodes, but it doesn\u2019t allow you to change the node type. Option D involves using classic resize with dense storage nodes, but this might not provide the I/O intensive resources needed for the complex calculations."
      },
      {
        "date": "2024-02-21T04:43:00.000Z",
        "voteCount": 2,
        "content": "It's important to note that while classic resize (Option B and D) allows for a change in node types (from DS to DC or vice versa), it involves a longer downtime as it copies data from the old cluster to a new one.\nElastic resize is faster than classic resize. it allows you to quickly add or remove nodes to the cluster while keeping the cluster online. This minimizes downtime and can be completed in minutes, as opposed to classic resize, which can take several hours or more, depending on the size of the dataset.\nDense compute (DC) nodes are optimized for performance-intensive workloads. They offer faster CPUs, increased I/O performance, and higher storage throughput compared to dense storage (DS) nodes. Given the engineering team's need to apply complex calculations that require I/O intensive resources, switching to dense compute nodes will provide the necessary computational power and I/O performance."
      },
      {
        "date": "2024-01-19T07:03:00.000Z",
        "voteCount": 1,
        "content": "Vote A.\n\nThe questions specifies I/O as being important which goes right to compute nodes. The reccomendation from AWS is to start with Elastic Resize first. The DC node types have enough to store this amount of data and the number of resized nodes won't exceed the limitations of an elastic resize; primarily the either 2x increase or decrease max of a cluster. If you need to 4x or 8x you node size, then you need to use classic.\n\nThis was also a (similar) question in Jon Bosno's practice tests in tutorialdojos"
      },
      {
        "date": "2024-01-11T03:54:00.000Z",
        "voteCount": 1,
        "content": "You need to store 4TB of user data in Redshift (data that changes little) so the size of these datasets would vary little...\n\nThey will also execute extremely complex queries...\n\nThey need to adjust to support these requirements. The first point implies continuing with dense storage nodes, but they also need compute nodes. Given that they have storage nodes and need more computation, I would change the node type, increasing this quantity... Then I would opt for the classic resize.\n\nC agreed?"
      },
      {
        "date": "2023-11-10T17:30:00.000Z",
        "voteCount": 1,
        "content": "I cant understand this question what to ask me?\nvery abstract question.\nso I choose C . ChatCPT answerd hahaha."
      },
      {
        "date": "2023-11-10T03:36:00.000Z",
        "voteCount": 1,
        "content": "Elastic as faster and dense storage due to size limit6"
      },
      {
        "date": "2023-11-06T08:50:00.000Z",
        "voteCount": 2,
        "content": "I believe option C is the most suitable choice. The reason is that dc2 nodes have a relatively limited SSD capacity of 160GB, while your data size is 4TB. Therefore, you'll need to opt for a dense storage node type to handle the increased storage requirements effectively."
      },
      {
        "date": "2023-09-06T12:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct: Elastic resize across node type automates the steps of taking a snapshot, creating a new cluster, deleting the old cluster, and renaming the new cluster into a simple, quick, and familiar operation. Elastic resize operation can be run at any time or can be scheduled to run at a future time. Customers can quickly upgrade their existing DS2 or DC2 node type-based cluster to the new RA3 node type with elastic resize."
      },
      {
        "date": "2023-09-01T17:04:00.000Z",
        "voteCount": 3,
        "content": "Vote for B. Following is the experience I tried on Sept 1st 2023\n- Redshift currently allows me to create a dc2 or ra3 cluster. Although it shows the ds2 option on Resize option, the console will display \u201cNumberOfNodesQuotaExceeded You do not have access to node type ds2.xlarge. Choose another node type\u201d even the quota in the service is enough. In short, I don\u2019t think the candidate can simulate this at home unless he/she currently works for some big companies with existing ds2 Redshift clusters."
      },
      {
        "date": "2023-09-01T17:04:00.000Z",
        "voteCount": 1,
        "content": "- The way how Redshift increases its computation power and storage is either adding more nodes (elastic) with uniform node type and storage, or snapshot whole cluster then upgrade (classic). The snapshot way can change node type.\n- Since AWS deprecated dense storage(ds2) cluster creation since 2021-08-01 and deprecated ds2 node type since 2021-12-31, no dense storage node can be added, which means we should drop C and D.\n- Since converting from ds to dc, there is no way to use elastic, so we drop A.\n- Can dc2 handle extra 4TB without exceeding its maximum number of nodes? Yes. dc2.l max nodes 32, total capacity 5.12TB; dc2.8xl max nodes 128, total capacity 326TB"
      },
      {
        "date": "2023-09-01T17:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#working-with-clusters-overview"
      },
      {
        "date": "2023-09-01T12:56:00.000Z",
        "voteCount": 2,
        "content": "The Answer is C:\nBecause... Dense storage nodes provide a balance between \"storage capacity and computational power\", making them suitable for analytical workloads that involve heavy I/O operations."
      },
      {
        "date": "2023-08-03T04:26:00.000Z",
        "voteCount": 2,
        "content": "Elsatic resize allows to change node types."
      },
      {
        "date": "2023-07-17T06:57:00.000Z",
        "voteCount": 3,
        "content": "Classic resize is for change the node types, we already got storage and we need compute"
      },
      {
        "date": "2023-07-14T12:28:00.000Z",
        "voteCount": 2,
        "content": "Classic Resize can allow changing node type and no of nodes, + there is no time constraints added in the question."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/28811-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company stores its sales and marketing data that includes personally identifiable information (PII) in Amazon S3. The company allows its analysts to launch their own Amazon EMR cluster and run analytics reports with the data. To meet compliance requirements, the company must ensure the data is not publicly accessible throughout this process. A data engineer has secured Amazon S3 but must ensure the individual EMR clusters created by the analysts are not exposed to the public internet.<br>Which solution should the data engineer to meet this compliance requirement with LEAST amount of effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EMR security configuration and ensure the security configuration is associated with the EMR clusters when they are created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the security group of the EMR clusters regularly to ensure it does not allow inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the block public access setting for Amazon EMR at the account level before any EMR cluster is created.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS WAF to block public internet access to the EMR clusters across the board."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T22:14:00.000Z",
        "voteCount": 25,
        "content": "C??\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html"
      },
      {
        "date": "2021-09-22T08:41:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2021-09-23T07:56:00.000Z",
        "voteCount": 1,
        "content": "the cluster is already created, and you can not recreate it because\n is much effort"
      },
      {
        "date": "2021-09-25T04:48:00.000Z",
        "voteCount": 2,
        "content": "my bad. I read again and is c"
      },
      {
        "date": "2024-02-11T02:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html"
      },
      {
        "date": "2024-01-11T04:07:00.000Z",
        "voteCount": 2,
        "content": "--- workflow ---\n\ndata with PII -&gt; s3\n\nanalyst 1 -&gt; EMR 1\nanalyst 2 -&gt; EMR 2\n...\nanalyst n -&gt; EMR n\n\n(In fact, what company allows its analysts to create an individual EMR for each person?! o.O)\n\n--- objective ---\n\nensure the EMR is not accessible by public internet\n\n--- way to make this with the least effort and least cost ---\n\nblock all account emr public access\n\n--- have another way to make this? ---\n\nyes, if a data analyst specialist designs a AMI for all EMR clusters and schedules a daily job to create an EMR for all analysts... buuuuuuuuuut, have a lot of effort rsrsrs"
      },
      {
        "date": "2023-11-10T02:57:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-emr-introduces-block-public-access-configuration-to-secure-emr-clusters-from-unintentional-network-exposure/"
      },
      {
        "date": "2023-05-01T07:40:00.000Z",
        "voteCount": 4,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-02-08T12:12:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-11-05T07:46:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is C as the EMR clusters can be configured with a block public access setting which is applied to all regions within an account.\n\nAmazon EMR block public access prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception. Port 22 is an exception by default. You can configure exceptions to allow public access on a port or range of ports. Block public access does not take effect in private subnets.\n\n\nA is wrong as security configurations can be used to configure data encryption, Kerberos authentication, and Amazon S3 authorization for EMRFS.\n\n\nB is wrong Although this approach is possible, it entails a management overhead of regularly updating the security groups of the EMR cluster.\n\nOption D is wrong as WAF does not work with EMR clusters."
      },
      {
        "date": "2022-10-29T16:30:00.000Z",
        "voteCount": 1,
        "content": "the company must ensure the data is not publicly accessible throughout this process.  How to ensure SG not be modified during the whole process if you choose C?"
      },
      {
        "date": "2022-10-29T16:26:00.000Z",
        "voteCount": 1,
        "content": "B ---- as  Block public access does not block IAM principals with appropriate permissions from updating security group configurations to allow public access on running clusters.... https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html\nI would suggest customer to use config to trigger auto mitigation if any port is opened to public access."
      },
      {
        "date": "2022-09-24T23:14:00.000Z",
        "voteCount": 1,
        "content": "\"with LEAST amount of effort\" - this is the key statement here."
      },
      {
        "date": "2022-08-06T13:54:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-06-26T07:15:00.000Z",
        "voteCount": 2,
        "content": "B is obviously wrong. AWS Exams would never allow a compliance solution to manually check if the settings are correct every now and then. \nC is better"
      },
      {
        "date": "2022-05-21T05:25:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C"
      },
      {
        "date": "2022-03-27T05:40:00.000Z",
        "voteCount": 1,
        "content": "Option C does not make sense since this is already enabled by default. \nOption B is better. I think the best solution is to use a custom config rule with SSM remediation\n\nhttps://asecure.cloud/a/cfgrule_c_emr_security_groups_restricted/"
      },
      {
        "date": "2022-03-27T05:42:00.000Z",
        "voteCount": 1,
        "content": "Also it does not prevent authorised persons from overriding the default EMR block public access settings when the cluster is running.\n\"Block public access is only applicable during cluster creation. Block public access does not block IAM principals with appropriate permissions from updating security group configurations to allow public access on running clusters.\"\nRef: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html"
      },
      {
        "date": "2021-11-04T05:49:00.000Z",
        "voteCount": 3,
        "content": "Ans C\nThis is a textbook question.\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html"
      },
      {
        "date": "2021-11-04T01:46:00.000Z",
        "voteCount": 4,
        "content": "Ans C\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html"
      },
      {
        "date": "2021-10-22T08:57:00.000Z",
        "voteCount": 4,
        "content": "I think C is Default. The question is what we need to do to ensure that, and we have to make sure the ports are not open as public.. Do you think the correct answer is B?"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/29070-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A financial company uses Amazon S3 as its data lake and has set up a data warehouse using a multi-node Amazon Redshift cluster. The data files in the data lake are organized in folders based on the data source of each data file. All the data files are loaded to one table in the Amazon Redshift cluster using a separate<br>COPY command for each data file location. With this approach, loading all the data files into Amazon Redshift takes a long time to complete. Users want a faster solution with little or no increase in cost while maintaining the segregation of the data files in the S3 data lake.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to copy all the data files into one folder and issue a COPY command to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all the data files in parallel to Amazon Aurora, and run an AWS Glue job to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue job to copy all the data files into one folder and issue a COPY command to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manifest file that contains the data file locations and issue a COPY command to load the data into Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T19:02:00.000Z",
        "voteCount": 25,
        "content": "D? https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html"
      },
      {
        "date": "2022-11-05T07:49:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is D as a manifest file can be used to load the data. Also, its recommended to have a single COPY command instead of multiple concurrent COPY commands for performance.\n\nUse the COPY command to load a table in parallel from data files on Amazon S3. You can specify the files to be loaded by using an Amazon S3 object prefix or by using a manifest file.\n\nAmazon Redshift can automatically load in parallel from multiple compressed data files.\n\nHowever, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined.\n\nOptions A, B &amp; C are wrong as they add unnecessary work and cost."
      },
      {
        "date": "2023-05-12T20:17:00.000Z",
        "voteCount": 1,
        "content": "Can you share a link that gives more insight into \"However, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined\"?"
      },
      {
        "date": "2024-02-13T00:43:00.000Z",
        "voteCount": 1,
        "content": "Ans D, single COPY command for performance and manifest file for loading data"
      },
      {
        "date": "2024-01-11T04:44:00.000Z",
        "voteCount": 1,
        "content": "datalake s3 -&gt; dw redshift\n\nproblems copying files using copy-by-prefix\n\nneed a solution without increasing costs\n\nA) emr is expansive\nb) aurora needs effort configuration and glue needs development effort\nc) glue job needs a development effort and copying all files to the same prefix will create a problem... which file goes to which table?\nd) manifest file is the best option because you can specify exactly the prefix/key to your copy command"
      },
      {
        "date": "2023-06-29T13:08:00.000Z",
        "voteCount": 1,
        "content": "D: manifest file is valid option"
      },
      {
        "date": "2023-05-01T07:42:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-09-24T23:16:00.000Z",
        "voteCount": 1,
        "content": "You can use a single copy command with manifest file, containing different S3 locations. This will speed up the COPY process."
      },
      {
        "date": "2022-07-27T23:34:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-05-21T05:13:00.000Z",
        "voteCount": 1,
        "content": "My Answer is D"
      },
      {
        "date": "2022-04-22T11:33:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2021-11-04T05:11:00.000Z",
        "voteCount": 6,
        "content": "Ans D\nA = wrong, no segregation, increased cost. B = wrong, no segregation, unnecessary work, increased cost. C = wrong, no segregation, increased cost. This is a question on how COPY command work. In general you should use only one COPY command because Redshift will load data in parallel, if you use many COPYs Redshift will have to load data in sequential manner.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_COPY_command_examples.html#copy-command-examples-manifest"
      },
      {
        "date": "2021-11-03T08:01:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-22T18:56:00.000Z",
        "voteCount": 2,
        "content": "My answer is D"
      },
      {
        "date": "2021-10-22T05:16:00.000Z",
        "voteCount": 2,
        "content": "D is right answer."
      },
      {
        "date": "2021-10-14T10:51:00.000Z",
        "voteCount": 7,
        "content": "From the link:https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\"You can use a manifest to ensure that the COPY command loads all of the required files, and only the required files, for a data load\"\nSo answer is D"
      },
      {
        "date": "2021-10-05T09:08:00.000Z",
        "voteCount": 3,
        "content": "Using manifest file is the right choice. So option D."
      },
      {
        "date": "2021-10-03T15:18:00.000Z",
        "voteCount": 4,
        "content": "Yes D is the right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/30835-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company's marketing team has asked for help in identifying a high performing long-term storage service for their data based on the following requirements:<br>\u2711 The data size is approximately 32 TB uncompressed.<br>\u2711 There is a low volume of single-row inserts each day.<br>\u2711 There is a high volume of aggregation queries each day.<br>\u2711 Multiple complex joins are performed.<br>\u2711 The queries typically involve a small subset of the columns in a table.<br>Which storage service will provide the MOST performant solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora MySQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Redshift\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Neptune",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Elasticsearch"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T12:25:00.000Z",
        "voteCount": 28,
        "content": "Redhift for sure."
      },
      {
        "date": "2021-09-24T01:32:00.000Z",
        "voteCount": 9,
        "content": "The simplest question in the exam."
      },
      {
        "date": "2024-02-13T00:45:00.000Z",
        "voteCount": 1,
        "content": "Amazon Redshift meets all the requirements."
      },
      {
        "date": "2023-05-01T07:43:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-11-05T07:50:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B as Redshift as it can be used for OLAP processing and meets all the requirements.\n\nAmazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes using AWS-designed hardware and machine learning to deliver the best price-performance at any scale.\nOption A is wrong as Amazon Aurora MySQL is ideal for OLTP solutions and not OLAP.\n\nOption C s wrong as Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications.\n\nOption D is wrong as Amazon Elasticsearch would not allow complex queries."
      },
      {
        "date": "2022-09-24T23:17:00.000Z",
        "voteCount": 1,
        "content": "The scenario here is indicating for a columnar data storage. So the answer will be Amazon Redshift."
      },
      {
        "date": "2022-08-06T14:45:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-04-22T11:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-12-18T22:18:00.000Z",
        "voteCount": 1,
        "content": "why not D ? elastic search, search in subset of column works better in ES..."
      },
      {
        "date": "2021-11-03T14:15:00.000Z",
        "voteCount": 1,
        "content": "Ans  B"
      },
      {
        "date": "2021-10-18T18:04:00.000Z",
        "voteCount": 4,
        "content": "Ans B\nA = wrong, Aurora for OLTP not OLAP. C = wrong, graph database not relevant. D = wrong, complex joins in ES are expensive."
      },
      {
        "date": "2021-10-05T05:17:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer"
      },
      {
        "date": "2021-09-29T18:55:00.000Z",
        "voteCount": 3,
        "content": "My answer is B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/74126-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A technology company is creating a dashboard that will visualize and analyze time-sensitive data. The data will come in through Amazon Kinesis Data Firehose with the butter interval set to 60 seconds. The dashboard must support near-real-time data.<br>Which visualization solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Amazon OpenSearch Service (Amazon Elasticsearch Service) as the endpoint for Kinesis Data Firehose. Set up an OpenSearch Dashboards (Kibana) using the data in Amazon OpenSearch Service (Amazon ES) with the desired analyses and visualizations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Amazon S3 as the endpoint for Kinesis Data Firehose. Read data into an Amazon SageMaker Jupyter notebook and carry out the desired analyses and visualizations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Amazon Redshift as the endpoint for Kinesis Data Firehose. Connect Amazon QuickSight with SPICE to Amazon Redshift to create the desired analyses and visualizations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Amazon S3 as the endpoint for Kinesis Data Firehose. Use AWS Glue to catalog the data and Amazon Athena to query it. Connect Amazon QuickSight with SPICE to Athena to create the desired analyses and visualizations."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 27,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-22T06:03:00.000Z",
        "voteCount": 15,
        "content": "near real-time dashboards -&gt; operational =&gt; OpenSearch"
      },
      {
        "date": "2022-11-05T07:52:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is A as Kinesis Data Firehose can ingest data to ElasticSearch and with Kibana it can provide near-real-time visualization.\n\nOption B is wrong as SageMaker Jupyter notebook does not provide near-realt0tie visualization, but it is more for data exploration.\n\nOption C is wrong as data with SPICE is cached and needs to be refreshed, so it does not provide near-real-time data.\n\nOption D is wrong as using SPICE and Glue does not provide near-real-time data."
      },
      {
        "date": "2023-01-15T09:14:00.000Z",
        "voteCount": 2,
        "content": "The minimum refresh frequency of QucikSight is hourly (E. edition)\nhttps://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html"
      },
      {
        "date": "2023-05-01T07:44:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-02-02T09:15:00.000Z",
        "voteCount": 1,
        "content": "Real time dashboard is Kibana, so option A."
      },
      {
        "date": "2022-10-11T03:18:00.000Z",
        "voteCount": 1,
        "content": "A \ntextbox question"
      },
      {
        "date": "2022-09-24T23:19:00.000Z",
        "voteCount": 1,
        "content": "\"The dashboard must support near-real-time data\" and \"time-sensitive\" are the keys here. OpenSearch and Kibana can only meet these requirements."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/27702-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A financial company uses Apache Hive on Amazon EMR for ad-hoc queries. Users are complaining of sluggish performance.<br>A data analyst notes the following:<br>\u2711 Approximately 90% of queries are submitted 1 hour after the market opens.<br>Hadoop Distributed File System (HDFS) utilization never exceeds 10%.<br><img src=\"/assets/media/exam-media/04144/0002200002.png\" class=\"in-exam-image\"><br>Which solution would help address the performance issues?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate instance fleet configurations for core and task nodes. Create an automatic scaling policy to scale out the instance groups based on the Amazon CloudWatch CapacityRemainingGB metric. Create an automatic scaling policy to scale in the instance fleet based on the CloudWatch CapacityRemainingGB metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate instance fleet configurations for core and task nodes. Create an automatic scaling policy to scale out the instance groups based on the Amazon CloudWatch YARNMemoryAvailablePercentage metric. Create an automatic scaling policy to scale in the instance fleet based on the CloudWatch YARNMemoryAvailablePercentage metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate instance group configurations for core and task nodes. Create an automatic scaling policy to scale out the instance groups based on the Amazon CloudWatch CapacityRemainingGB metric. Create an automatic scaling policy to scale in the instance groups based on the CloudWatch CapacityRemainingGB metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate instance group configurations for core and task nodes. Create an automatic scaling policy to scale out the instance groups based on the Amazon CloudWatch YARNMemoryAvailablePercentage metric. Create an automatic scaling policy to scale in the instance groups based on the CloudWatch YARNMemoryAvailablePercentage metric.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-03T12:32:00.000Z",
        "voteCount": 23,
        "content": "Ans D\nA and B = wrong, instance fleet does not support auto scaling. C = wrong, HDFS utilization never exceeds 10% no scaling will never happen."
      },
      {
        "date": "2021-12-20T22:47:00.000Z",
        "voteCount": 3,
        "content": "The following are two commonly used metrics for automatic scaling: \nYarnMemoryAvailablePercentage: This is the percentage of remaining memory that's available for YARN. \nContainerPendingRatio: This is the ratio of pending containers to allocated containers. You can use this metric to scale a cluster based on container-allocation behavior for varied loads. This is useful for performance tuning."
      },
      {
        "date": "2021-12-20T22:46:00.000Z",
        "voteCount": 2,
        "content": "Agree\nFor further reference see \nhttps://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-in-amazon-emr/"
      },
      {
        "date": "2021-10-31T10:21:00.000Z",
        "voteCount": 7,
        "content": "D should be the right answer. Considering the following links: the first link is possible to see that the right metric to this requirement is YARNMemoryAvailablePercentage, because the HDFS never is over 10%. The second link explain that if you will use auto scaling so you should use instance group.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html"
      },
      {
        "date": "2023-11-10T03:11:00.000Z",
        "voteCount": 1,
        "content": "\"Managed scaling is available for clusters composed of either instance groups or instance fleets.\"\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-scaling.html#:~:text=Managed%20scaling%20is%20available%20for%20clusters%20composed%20of%20either%20instance%20groups%20or%20instance%20fleets."
      },
      {
        "date": "2023-05-01T07:45:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-02-22T09:46:00.000Z",
        "voteCount": 1,
        "content": "YARNMemoryAvailablePercentage-&gt; is for CPU intensive workload, \nCapacityRemainingGB-&gt; for Capacity intensive workload, \nInstance fleet is ruled out. Hence, D"
      },
      {
        "date": "2022-11-05T07:59:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D as instance group configurations for core and task nodes can be used to scale as per the YARNMemoryAvailablePercentage metric.\n\noptions A &amp; B are incorrect because an Instance Fleet doesn\u2019t have an automatic scaling policy. Only an Instance Group has this feature.\n\nOption C is incorrect as the CapacityRemainingGB metric is just the amount of remaining HDFS disk capacity and this does not exceed 10% for each run. The cluster will not scale-in or scale-out if you choose this metric."
      },
      {
        "date": "2022-11-05T07:59:00.000Z",
        "voteCount": 2,
        "content": "CloudWatch metrics that you can use for automatic scaling in Amazon EMR, The following are two commonly used metrics for automatic scaling:\n\nYarnMemoryAvailablePercentage: This is the percentage of remaining memory that's available for YARN.\n\nContainerPendingRatio: This is the ratio of pending containers to allocated containers. You can use this metric to scale a cluster based on container-allocation behavior for varied loads. This is useful for performance tuning.\n\n\nFor the given use case, the correct solution should support automatic scaling. You can set up automatic scaling in Amazon EMR for an instance group, adding and removing instances automatically based on the value of an Amazon CloudWatch metric that you specify. The metric YARNMemoryAvailablePercentage represents the percentage of remaining memory available to YARN (YARNMemoryAvailablePercentage = MemoryAvailableMB / MemoryTotalMB). This value is useful for scaling cluster resources based on YARN memory usage."
      },
      {
        "date": "2022-09-24T23:20:00.000Z",
        "voteCount": 1,
        "content": "Instance Fleet cannot take part in Auto-Scaling.  CapacityRemainingGB is not the parameter to refer as \"(HDFS) utilization never exceeds 10%\". So the answer is D."
      },
      {
        "date": "2022-07-25T20:58:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-06-28T01:56:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D like others have said. However, I think it would be even better to use Instance fleets and EMR Managed auto scaling, but this is not an option here."
      },
      {
        "date": "2022-05-21T04:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-04-30T16:11:00.000Z",
        "voteCount": 1,
        "content": "Answer-D"
      },
      {
        "date": "2022-03-20T04:38:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      },
      {
        "date": "2021-11-20T07:44:00.000Z",
        "voteCount": 1,
        "content": "Option D is the right choice."
      },
      {
        "date": "2021-11-05T00:47:00.000Z",
        "voteCount": 1,
        "content": "Ans     D"
      },
      {
        "date": "2021-10-22T08:50:00.000Z",
        "voteCount": 4,
        "content": "Answer is D over B. Because Spot instance fleet support \"managed\" auto scaling and managed auto scaling can't use Cloud watch metric like  YARNMemoryAvailablePercentage. \nManaged auto scaling scaled depends load on the cluster."
      },
      {
        "date": "2021-10-21T15:27:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-20T18:11:00.000Z",
        "voteCount": 3,
        "content": "D IS Correct for my"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/28681-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A media company has been performing analytics on log data generated by its applications. There has been a recent increase in the number of concurrent analytics jobs running, and the overall performance of existing jobs is decreasing as the number of new jobs is increasing. The partitioned data is stored in<br>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) and the analytic processing is performed on Amazon EMR clusters using the EMR File System<br>(EMRFS) with consistent view enabled. A data analyst has determined that it is taking longer for the EMR task nodes to list objects in Amazon S3.<br>Which action would MOST likely increase the performance of accessing log data in Amazon S3?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a hash function to create a random string and add that to the beginning of the object prefixes when storing the log data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a lifecycle policy to change the S3 storage class to S3 Standard for the log data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the read capacity units (RCUs) for the shared Amazon DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedeploy the EMR clusters that are running slowly to a different Availability Zone."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T14:55:00.000Z",
        "voteCount": 28,
        "content": "Option C.\nEMRFS consistent view tracks consistency using a DynamoDB table to track objects in Amazon S3 that have been synced with or created by EMRF. So increasing RCU for the shared DynamoDB table will help here."
      },
      {
        "date": "2021-09-19T18:29:00.000Z",
        "voteCount": 13,
        "content": "C any thoughts??\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html"
      },
      {
        "date": "2021-09-22T02:07:00.000Z",
        "voteCount": 1,
        "content": "Agreed https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Optimizing.html"
      },
      {
        "date": "2021-11-02T04:59:00.000Z",
        "voteCount": 3,
        "content": "Your link is about DynamoDB as a data source and therefore unrelated to the question. The correct page is\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html"
      },
      {
        "date": "2021-09-23T21:07:00.000Z",
        "voteCount": 2,
        "content": "Since the question specifically says \"Amazon EMR clusters using the EMR File System\n(EMRFS) with consistent view enabled\" C makes most sense."
      },
      {
        "date": "2023-07-01T00:13:00.000Z",
        "voteCount": 2,
        "content": "Popular option C is actually wrong. Increasing RCU of dynamodb wont increase S3 list performance. Looks like there are too many tiny objects and its hitting S3 API limitations. A sounds more logical."
      },
      {
        "date": "2023-05-01T07:47:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-04-21T23:22:00.000Z",
        "voteCount": 2,
        "content": "Answer is C.\nWhen consistent view is enabled, a dynamo db table is created and s3 object metadata is stored in this table. Whenever s3 listing is done, it reads data from dynamodb table."
      },
      {
        "date": "2023-02-22T10:00:00.000Z",
        "voteCount": 1,
        "content": "A is right. C is not right since DDB is not in picture."
      },
      {
        "date": "2023-02-17T09:44:00.000Z",
        "voteCount": 1,
        "content": "How is DynamoDB related to this question pls ? EMR Task node not able to lsit S3 directories and if a hash function to create a random string and add that to the beginning of the object prefixes when storing the log data in Amazon S3. This approach is known as \"sharding\" and can be an effective way to reduce the number of S3 requests required to retrieve log data. Therefore, option A is the most likely action to increase the performance of accessing log data in Amazon S3."
      },
      {
        "date": "2022-11-28T20:21:00.000Z",
        "voteCount": 6,
        "content": "The answer is A. There is no limit to create prefix inside a bucket. To scale the read/write capacity from/to S3 bucket, the recommended approach is to have additional prefixes inside the bucket to enhance the read/write capacity. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket.\n\nFor example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket.\n\nYou can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second."
      },
      {
        "date": "2022-11-05T10:30:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is C as the current setup uses EMR and EMRFS with Consistent View enabled which is supported by DynamoDB for metadata. Increasing the DynamoDB RCUs should help increase performance.\n\nEMRFS consistent view tracks consistency using a DynamoDB table to track objects in Amazon S3 that have been synced with or created by EMRFS. The metadata is used to track all operations (read, write, update, and copy), and no actual content is stored in it. This metadata is used to validate whether the objects or metadata received from Amazon S3 matches what is expected. This confirmation gives EMRFS the ability to check list consistency and read-after-write consistency for new objects EMRFS writes to Amazon S3 or objects synced with EMRFS. Multiple clusters can share the same metadata."
      },
      {
        "date": "2022-11-05T10:30:00.000Z",
        "voteCount": 1,
        "content": "Option A is wrong as for S3 list operation it's recommended to store metadata externally. S3 performance has been significantly improved and optimized as well.\n\nOption C is wrong as the S3 standard would not help increase the performance issue. It only increases availability and durability.\n\nOption D is wrong as it would not increase the S3 querying performance issue."
      },
      {
        "date": "2022-12-03T03:58:00.000Z",
        "voteCount": 1,
        "content": "Is there a typo in your comment?"
      },
      {
        "date": "2022-09-29T16:31:00.000Z",
        "voteCount": 3,
        "content": "------------------------------------------\nAnswer : A\nConfirmed by paid dumps"
      },
      {
        "date": "2022-10-18T07:41:00.000Z",
        "voteCount": 1,
        "content": "but what is the reason of not C?"
      },
      {
        "date": "2022-09-28T12:14:00.000Z",
        "voteCount": 3,
        "content": "my answer is A. https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/"
      },
      {
        "date": "2022-09-17T07:13:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. There is no limit to create prefix inside a bucket. To scale the read/write capacity from/to S3 bucket, the recommended approach is to have additional prefixes inside the bucket to enhance the read/write capacity. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket.\n\nFor example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket.\n\nYou can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second."
      },
      {
        "date": "2022-09-08T08:50:00.000Z",
        "voteCount": 1,
        "content": "This looks like the correct answer"
      },
      {
        "date": "2022-08-06T14:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is C."
      },
      {
        "date": "2022-05-21T05:40:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C"
      },
      {
        "date": "2021-11-06T16:59:00.000Z",
        "voteCount": 2,
        "content": "Contrary to most suggestions here. The correct answer is D. As the S3 is in one availability zone, it is likely that the entire cluster is deployed in a different zone. So redeploying the cluster to a different AZ can solve the problem. And Question clearly mentions \"most likely\". IMO definitely D"
      },
      {
        "date": "2022-03-29T00:58:00.000Z",
        "voteCount": 1,
        "content": "I dont think this is the case here. The problem is that listing the s3 bucket items is taking longer because of the increase in new jobs. If emr was in different AZ, you may not be able to list bucket items"
      },
      {
        "date": "2021-11-05T04:26:00.000Z",
        "voteCount": 2,
        "content": "I Think A. Applications running on Amazon S3 today will enjoy this performance improvement with no changes, and customers building new applications on S3 do not have to make any application customizations to achieve this performance. Amazon S3\u2019s support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. Performance scales per prefix, so you can use as many prefixes as you need in parallel to achieve the required throughput. There are no limits to the number of prefixes."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/28711-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has developed several AWS Glue jobs to validate and transform its data from Amazon S3 and load it into Amazon RDS for MySQL in batches once every day. The ETL jobs read the S3 data using a DynamicFrame. Currently, the ETL developers are experiencing challenges in processing only the incremental data on every run, as the AWS Glue job processes all the S3 input data on each run.<br>Which approach would allow the developers to solve the issue with minimal coding effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the ETL jobs read the data from Amazon S3 using a DataFrame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job bookmarks on the AWS Glue jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate custom logic on the ETL jobs to track the processed S3 objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the ETL jobs delete the processed objects or data from Amazon S3 after each run."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T10:10:00.000Z",
        "voteCount": 21,
        "content": "It is B"
      },
      {
        "date": "2021-10-20T13:50:00.000Z",
        "voteCount": 8,
        "content": "Ans B\nThis is a textbook question.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"
      },
      {
        "date": "2023-05-01T07:48:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-12T19:15:00.000Z",
        "voteCount": 5,
        "content": "The correct approach to solve the issue with minimal coding effort would be to enable job bookmarks on the AWS Glue jobs.\n\nEnabling job bookmarks on the AWS Glue jobs would allow the ETL job to keep track of the last processed record in the data source. This way, on the next run, the job will only process the new or updated data that was added to the source since the last successful run, thus processing only the incremental data.\n\nUsing DataFrame instead of DynamicFrame or custom logic to track processed S3 objects could require significant coding effort and may not be the most efficient approach. Deleting processed objects or data from Amazon S3 after each run may not be ideal since it may result in loss of valuable historical data.\n\nTherefore, enabling job bookmarks is the most appropriate approach to solve the issue with minimal coding effort."
      },
      {
        "date": "2022-11-05T10:28:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B as AWS Glue can be used to export the data incrementally using job bookmarks with coding required.\n\nAWS Glue tracks data that has already been processed during a previous run of an ETL job by persisting state information from the job run. This persisted state information is called a job bookmark. Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data. With job bookmarks, you can process new data when rerunning on a scheduled interval. A job bookmark is composed of the states for various elements of jobs, such as sources, transformations, and targets. For example, your ETL job might read new partitions in an Amazon S3 file. AWS Glue tracks which partitions the job has processed successfully to prevent duplicate processing and duplicate data in the job's target data store.\n\n\nJob bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources."
      },
      {
        "date": "2022-09-24T23:25:00.000Z",
        "voteCount": 1,
        "content": "For incremental data, Job bookmark is the built-in feature for Glue."
      },
      {
        "date": "2022-09-24T23:24:00.000Z",
        "voteCount": 1,
        "content": "For incremental data, Job bookmark is the built-in option to choose for Glue."
      },
      {
        "date": "2022-07-25T21:47:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-05-21T04:58:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-11-18T03:23:00.000Z",
        "voteCount": 4,
        "content": "its strongly B as book mark only take new data it stops processing preprocessed data"
      },
      {
        "date": "2021-10-27T00:20:00.000Z",
        "voteCount": 1,
        "content": "The answer is B, the hint is in the wording 'only in incremental data'."
      },
      {
        "date": "2021-10-22T01:44:00.000Z",
        "voteCount": 2,
        "content": "Ans    B"
      },
      {
        "date": "2021-10-14T20:18:00.000Z",
        "voteCount": 1,
        "content": "although B is the obvious answer the part of the question that says minimal coding effort suggests it might be D."
      },
      {
        "date": "2023-01-19T05:34:00.000Z",
        "voteCount": 1,
        "content": "There is no code change effort you just need to enabled job bookmark. \nRemoving processed data from S3 is the worst option as you are simply loosing the data from your datalake"
      },
      {
        "date": "2021-10-07T19:15:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-04T15:02:00.000Z",
        "voteCount": 1,
        "content": "My answer is B"
      },
      {
        "date": "2021-09-29T13:11:00.000Z",
        "voteCount": 2,
        "content": "Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data so answer is B 100%"
      },
      {
        "date": "2021-09-28T12:01:00.000Z",
        "voteCount": 1,
        "content": "Job Bookmarks should do the trick. So option B."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/29479-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A mortgage company has a microservice for accepting payments. This microservice uses the Amazon DynamoDB encryption client with AWS KMS managed keys to encrypt the sensitive data before writing the data to DynamoDB. The finance team should be able to load this data into Amazon Redshift and aggregate the values within the sensitive fields. The Amazon Redshift cluster is shared with other data analysts from different business units.<br>Which steps should a data analyst take to accomplish this task efficiently and securely?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to process the DynamoDB stream. Decrypt the sensitive data using the same KMS key. Save the output to a restricted S3 bucket for the finance team. Create a finance table in Amazon Redshift that is accessible to the finance team only. Use the COPY command to load the data from Amazon S3 to the finance table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to process the DynamoDB stream. Save the output to a restricted S3 bucket for the finance team. Create a finance table in Amazon Redshift that is accessible to the finance team only. Use the COPY command with the IAM role that has access to the KMS key to load the data from S3 to the finance table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EMR cluster with an EMR_EC2_DefaultRole role that has access to the KMS key. Create Apache Hive tables that reference the data stored in DynamoDB and the finance table in Amazon Redshift. In Hive, select the data from DynamoDB and then insert the output to the finance table in Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EMR cluster. Create Apache Hive tables that reference the data stored in DynamoDB. Insert the output to the restricted Amazon S3 bucket for the finance team. Use the COPY command with the IAM role that has access to the KMS key to load the data from Amazon S3 to the finance table in Amazon Redshift."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T06:07:00.000Z",
        "voteCount": 53,
        "content": "Answer is B \u2013 \nC and D are cancelled because \u2013 EMR is not needed to process DynamoDB streams. Lambda function would be good enough.\n\nOption A is wrong because it suggests decrypting the data and storing in S3 which is not good since it contains sensitive fields.\nOption B is correct because Redshift will only decrypt the data while reading it."
      },
      {
        "date": "2021-10-02T02:07:00.000Z",
        "voteCount": 3,
        "content": "But why do we need to create DynamoDB streams. Streams is mentioned only in answer. Also it will be only for new data. What about the data which is already present. One of the requirement is Finane team should be able to aggregate data on sensitive field. But if they do not hae all the data in Redshift then how will aggregation provide correct result?"
      },
      {
        "date": "2023-12-26T02:37:00.000Z",
        "voteCount": 1,
        "content": "B is wrong because the data is encrypted before loading into DynamoDB which implies that it is client side encryption and Redshift doesn't support the client side encryption - \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"
      },
      {
        "date": "2021-10-09T14:55:00.000Z",
        "voteCount": 13,
        "content": "Correct: A \nC and D can be eliminated because this is a shared Redshift cluster, so you need to create a table accessible only to the finance team. B is wrong as the application uses DynamoDB client-side encryption (not S3 client-side encryption), which means it will not automatically decrypt by AWS, and needs manual decryption before sending to S3 and then COPY\u2019d into Redshift. Even if you want to use COPY ENCRYPTED to copy client-side encrypted S3 files, you need to specify credentials not IAM roles.\nREPORT THIS AD\nREPORT THIS AD\nHowever, DynamoDB stream only does new data, so existing data won\u2019t be processed, this is not a perfect answer.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"
      },
      {
        "date": "2022-06-19T01:03:00.000Z",
        "voteCount": 1,
        "content": "I think this is the best explanation."
      },
      {
        "date": "2022-07-14T08:16:00.000Z",
        "voteCount": 2,
        "content": "In A the issue is\nthough S3 is restricted the data stored still is unencrypted before loading to redshift"
      },
      {
        "date": "2022-10-18T19:28:00.000Z",
        "voteCount": 3,
        "content": "Actually it will automatically decrypt by AWS, and no need to do manual decryption.\n\"After you create and configure the required components, the DynamoDB Encryption Client transparently encrypts and signs your table items when you add them to a table, and verifies and decrypts them when you retrieve them.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"
      },
      {
        "date": "2022-12-07T01:26:00.000Z",
        "voteCount": 1,
        "content": "on the link you shared above, The COPY command doesn't support the following types of Amazon S3 encryption:\nClient-side encryption using an AWS KMS key\n\nif client side encruption does not support than decrypting will not work as well"
      },
      {
        "date": "2024-04-12T15:13:00.000Z",
        "voteCount": 1,
        "content": "&lt;a href=\u201chttps://striveenterprise.com/digital-marketing-tampa-fl/\u201d&gt;Digital marketing&lt;/a&gt; isn't just about selling a product; it's about crafting an immersive brand experience. It's the fusion of compelling storytelling, cutting-edge technology, and a deep understanding of consumer psychology."
      },
      {
        "date": "2024-02-28T05:58:00.000Z",
        "voteCount": 1,
        "content": "When you load an encrypted file from Amazon S3 to Redshift, the encryption involved is neither purely client-side nor server-side using AWS KMS. It's a hybrid approach \n\nDecryption during Load:\n\nWhen you use the COPY command in Redshift to load the data, Redshift retrieves the data key from KMS using its IAM role or credentials.\nThis retrieval can be considered a server-side operation from Redshift's perspective."
      },
      {
        "date": "2024-02-21T08:20:00.000Z",
        "voteCount": 2,
        "content": "Lambda function processes the DynamoDB stream, the sensitive data encrypted with KMS keys can be decrypted securely using the same KMS key. Storing the decrypted data in a restricted S3 bucket accessible only to the finance team ensures that sensitive information is not exposed to unauthorised users. Creating a dedicated finance table in Redshift is accessible only to the finance team ensures that the aggregated sensitive data remains confidential and is not accessible by others. The COPY command to load data from the restricted S3 bucket into the finance table in Redshift is efficient. \nB- loading encrypted data directly into Redshift and decrypting it during the COPY process is not directly supported as part of the COPY command.\nC, D - involve using EMR and Apache Hive, which could add complexity and operational overhead to the data processing workflow and decryption needs to occur before the data can be processed by EMR."
      },
      {
        "date": "2024-03-08T13:31:00.000Z",
        "voteCount": 1,
        "content": "COPY command does decrypt - https://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"
      },
      {
        "date": "2024-02-16T20:45:00.000Z",
        "voteCount": 1,
        "content": "Will go for B"
      },
      {
        "date": "2023-12-26T02:41:00.000Z",
        "voteCount": 1,
        "content": "B is wrong because the data is encrypted before loading into DynamoDB which implies that it is client side encryption and Redshift doesn't support the client side encryption -\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"
      },
      {
        "date": "2024-03-08T13:33:00.000Z",
        "voteCount": 1,
        "content": "the key is this i feel - \"AWS KMS managed keys to encrypt the sensitive data\""
      },
      {
        "date": "2023-10-14T21:55:00.000Z",
        "voteCount": 1,
        "content": "B. A and B are similar but B is more secure option"
      },
      {
        "date": "2023-09-24T05:51:00.000Z",
        "voteCount": 1,
        "content": "B is incorrect,  this option omits the step of decrypting the data before saving, I think A is the correct option."
      },
      {
        "date": "2023-06-24T08:36:00.000Z",
        "voteCount": 3,
        "content": "The COPY command doesn't support the following types of Amazon S3 encryption: Answer A\nServer-side encryption with customer-provided keys (SSE-C)\nClient-side encryption using an AWS KMS key\nClient-side encryption using a customer-provided asymmetric root key"
      },
      {
        "date": "2023-06-07T04:09:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2023-05-01T07:49:00.000Z",
        "voteCount": 3,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-21T05:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is B as lambda can analyze the data in dynamo db and is save to restricted bucket which cannot be accessed without desired permission. Also to copy from s3 bucket to redshift it requires IAM role."
      },
      {
        "date": "2023-03-24T07:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html"
      },
      {
        "date": "2023-02-17T08:56:00.000Z",
        "voteCount": 3,
        "content": "Option B is not the best solution because it does not address the need to decrypt the sensitive data before loading it into Amazon Redshift. The finance team needs to be able to aggregate the values within the sensitive fields, which would not be possible if the data is not decrypted before loading it into Redshift. Option A solves this problem by creating a Lambda function that processes the DynamoDB stream and decrypts the sensitive data using the same KMS key used for encryption before loading it into Redshift. The data is also saved to a restricted S3 bucket to ensure that only the finance team has access to it."
      },
      {
        "date": "2023-01-07T14:42:00.000Z",
        "voteCount": 4,
        "content": "the decrypting is automatically done by AWS as the data is moved from Dynamo to S3. Before it's written to S3 it's reencyrpted using SSE. Writing to S3 and leaving it decrypting (as Option A suggests) would not be a secure move.  Hence B should be the right answer."
      },
      {
        "date": "2022-11-28T20:52:00.000Z",
        "voteCount": 2,
        "content": "Ans- B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/28736-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is building a data lake and needs to ingest data from a relational database that has time-series data. The company wants to use managed services to accomplish this. The process needs to be scheduled daily and bring incremental data only from the source into Amazon S3.<br>What is the MOST cost-effective approach to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to connect to the data source using JDBC Drivers. Ingest incremental records only using job bookmarks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to connect to the data source using JDBC Drivers. Store the last updated key in an Amazon DynamoDB table and ingest the data using the updated key as a filter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to connect to the data source using JDBC Drivers and ingest the entire dataset. Use appropriate Apache Spark libraries to compare the dataset, and find the delta.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to connect to the data source using JDBC Drivers and ingest the full data. Use AWS DataSync to ensure the delta only is written into Amazon S3."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T21:40:00.000Z",
        "voteCount": 27,
        "content": "Seems  answer is A"
      },
      {
        "date": "2021-09-23T22:50:00.000Z",
        "voteCount": 9,
        "content": "my answer is A"
      },
      {
        "date": "2023-11-28T16:38:00.000Z",
        "voteCount": 1,
        "content": "It's B. Glue job bookmark option is for S3 not for database."
      },
      {
        "date": "2023-08-23T15:00:00.000Z",
        "voteCount": 1,
        "content": "Of course A"
      },
      {
        "date": "2023-06-19T12:13:00.000Z",
        "voteCount": 1,
        "content": "Undoubtedly it's A."
      },
      {
        "date": "2023-05-01T07:51:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-21T05:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nSince Glue has the Job bookmark option to save the info regarding the last load, this can be used when starting next load so that duplicates are not inserted."
      },
      {
        "date": "2022-12-11T15:27:00.000Z",
        "voteCount": 2,
        "content": "I'm sure it's A. Follow documnet explain about JDBC source.\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"
      },
      {
        "date": "2022-11-05T10:46:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A as AWS Glue can be used to export the data from the relational database incrementally using job bookmarks in a cost-effective way."
      },
      {
        "date": "2022-09-25T01:09:00.000Z",
        "voteCount": 1,
        "content": "Incremental Data and Managed Service, these are the two key words here. So AWS Glue with Job Bookmark will do the trick."
      },
      {
        "date": "2022-07-27T23:42:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-05-21T05:21:00.000Z",
        "voteCount": 1,
        "content": "My Answer is A"
      },
      {
        "date": "2022-03-19T22:10:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2022-03-02T11:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. why to use other services when Glue itself can perform the desired functions as per the question"
      },
      {
        "date": "2021-11-04T13:13:00.000Z",
        "voteCount": 2,
        "content": "Ans A\nThis is a textbook question."
      },
      {
        "date": "2021-10-26T14:46:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2021-10-25T06:16:00.000Z",
        "voteCount": 2,
        "content": "A.\nJob bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/29149-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An Amazon Redshift database contains sensitive user data. Logging is necessary to meet compliance requirements. The logs must contain database authentication attempts, connections, and disconnections. The logs must also contain each query run against the database and record which database user ran each query.<br>Which steps will create the required logs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon Redshift Enhanced VPC Routing. Enable VPC Flow Logs to monitor traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow access to the Amazon Redshift database using AWS IAM only. Log access using AWS CloudTrail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable audit logging for Amazon Redshift using the AWS Management Console or the AWS CLI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable and download audit reports from AWS Artifact."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T12:50:00.000Z",
        "voteCount": 20,
        "content": "Its C; Enhanced VPC Routing enforce COPY/UNLOAD to use VPC"
      },
      {
        "date": "2021-09-28T01:26:00.000Z",
        "voteCount": 8,
        "content": "Agreed\n\nAmazon Redshift logs information in the following log files:\n\u2022\tConnection log \u2014 logs authentication attempts, and connections and disconnections. \n\u2022\tUser log \u2014 logs information about changes to database user definitions. \n\u2022\tUser activity log \u2014 logs each query before it is run on the database. \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html"
      },
      {
        "date": "2021-10-18T00:09:00.000Z",
        "voteCount": 2,
        "content": "I am inclined to C. But does anyone know how come B is not right? CloudTrail is supposed to provide the requested service."
      },
      {
        "date": "2021-10-29T06:00:00.000Z",
        "voteCount": 5,
        "content": "https://stackify.com/aws-redshift-monitoring-the-complete-guide/\nmay help with the difference between cloudtrail and db audit logging"
      },
      {
        "date": "2021-12-20T23:50:00.000Z",
        "voteCount": 2,
        "content": "Agree\nFurther Audit logs can be analysed using Redshift Spectrum\nhttps://aws.amazon.com/blogs/big-data/analyze-database-audit-logs-for-security-and-compliance-using-amazon-redshift-spectrum/"
      },
      {
        "date": "2021-11-05T20:41:00.000Z",
        "voteCount": 8,
        "content": "Ans C\nA = wrong, enhanced VPC routing means data in/out within VPC. B = wrong, CloudTrail do not log data events, only configuration events. D = wrong, nonsense. This is a textbook question.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html"
      },
      {
        "date": "2023-05-01T07:52:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-04-21T05:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\nA-User should connect to VPN first to access Redshift in VPC, in question there is no details regarding VPN\nB. Only users with AWS access will be able to connect to redshift\nD. Artifacts is not a solution.\n\nAlso cloudtrail will log only access to the service and not what happened inside the service"
      },
      {
        "date": "2022-11-05T10:48:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C as Redshift Audit Logging can provide the required information.\n\nAudit logging is not enabled by default in Amazon Redshift. When you enable logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged.The connection log, user log, and user activity log are enabled together by using the AWS Management Console, the Amazon Redshift API Reference, or the AWS Command Line Interface (AWS CLI).\n\nAmazon Redshift logs information in the following log files:\n\nConnection log \u2014 logs authentication attempts, and connections and disconnections.\nUser log \u2014 logs information about changes to database user definitions.\nUser activity log \u2014 logs each query before it is run on the database."
      },
      {
        "date": "2022-11-05T10:49:00.000Z",
        "voteCount": 1,
        "content": "Option A is wrong as Redshift Enhanced VPC Routing supports the use of standard VPC features such as VPC Endpoints, security groups, network ACLs, managed NAT and internet gateways, enabling you to tightly manage the flow of data between your Amazon Redshift cluster and all of your data sources.\nOption D is wrong as AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS\u2019 security and compliance reports and select online agreements."
      },
      {
        "date": "2022-11-05T10:49:00.000Z",
        "voteCount": 1,
        "content": "Option B is wrong as Amazon Redshift is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon Redshift. CloudTrail captures all API calls for Amazon Redshift as events. These include calls from the Amazon Redshift console and from code calls to the Amazon Redshift API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon Redshift. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history. Using the information collected by CloudTrail, you can determine certain details. These include the request that was made to Amazon Redshift, the IP address it was made from, who made it, when it was made, and other information."
      },
      {
        "date": "2022-09-25T01:11:00.000Z",
        "voteCount": 1,
        "content": "It can be done by enabling Audit Logging of Redshift."
      },
      {
        "date": "2022-07-25T21:15:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-05-06T18:11:00.000Z",
        "voteCount": 1,
        "content": "This is what Redshift audit log do."
      },
      {
        "date": "2022-04-30T16:31:00.000Z",
        "voteCount": 1,
        "content": "Answer - A , Enhanced Routing"
      },
      {
        "date": "2022-04-29T07:10:00.000Z",
        "voteCount": 1,
        "content": "as discussed by others"
      },
      {
        "date": "2021-11-20T09:14:00.000Z",
        "voteCount": 1,
        "content": "Ans is C"
      },
      {
        "date": "2021-11-02T05:50:00.000Z",
        "voteCount": 3,
        "content": "Correct C\nAmazon Redshift logs information in the following log files:\n- Connection log \u2014 logs authentication attempts, and connections and disconnections.\n- User log \u2014 logs information about changes to database user definitions.\n- User activity log \u2014 logs each query before it is run on the database.\nThe connection log, user log, and user activity log are enabled together by using the AWS Management Console, the Amazon Redshift API Reference, or the AWS Command Line Interface (AWS CLI).\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html"
      },
      {
        "date": "2021-10-27T05:02:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer"
      },
      {
        "date": "2021-10-25T22:48:00.000Z",
        "voteCount": 1,
        "content": "C for sure: https://aws.amazon.com/premiumsupport/knowledge-center/logs-redshift-database-cluster/"
      },
      {
        "date": "2021-10-25T16:07:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-21T10:31:00.000Z",
        "voteCount": 1,
        "content": "Link provided confirms C as the answer"
      },
      {
        "date": "2021-10-18T03:14:00.000Z",
        "voteCount": 2,
        "content": "The connection log, user log, and user activity log are enabled together by using the AWS Management Console, the Amazon Redshift API Reference, or the AWS Command Line Interface (AWS CLI). Answer is C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/28680-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company that monitors weather conditions from remote construction sites is setting up a solution to collect temperature data from the following two weather stations.<br>\u2711 Station A, which has 10 sensors<br>\u2711 Station B, which has five sensors<br>These weather stations were placed by onsite subject-matter experts.<br>Each sensor has a unique ID. The data collected from each sensor will be collected using Amazon Kinesis Data Streams.<br>Based on the total incoming and outgoing data throughput, a single Amazon Kinesis data stream with two shards is created. Two partition keys are created based on the station names. During testing, there is a bottleneck on data coming from Station A, but not from Station B. Upon review, it is confirmed that the total stream throughput is still less than the allocated Kinesis Data Streams throughput.<br>How can this bottleneck be resolved without increasing the overall cost and complexity of the solution, while retaining the data collection quality requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in Kinesis Data Streams to increase the level of parallelism.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate Kinesis data stream for Station A with two shards, and stream Station A sensor data to the new stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the partition key to use the sensor ID instead of the station name.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of sensors in Station A from 10 to 5 sensors."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T11:50:00.000Z",
        "voteCount": 34,
        "content": "C?\nA and B increase the cost"
      },
      {
        "date": "2021-10-08T16:05:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2021-12-20T22:36:00.000Z",
        "voteCount": 2,
        "content": "Agreed\nFor further reading see - \nhttps://aws.amazon.com/blogs/big-data/under-the-hood-scaling-your-kinesis-data-streams/"
      },
      {
        "date": "2021-11-01T02:03:00.000Z",
        "voteCount": 11,
        "content": "C is 100% correct answer."
      },
      {
        "date": "2023-05-01T07:53:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-04-21T06:03:00.000Z",
        "voteCount": 4,
        "content": "Answer is C\n\nA. No need to Increase the number of shards as in question its mentioned the throughput is less\nB. More cost\nC. Modifying the partition key to use the sensor ID instead of the station name is the correct answer. As of now all data from Station A which has more sensors is going to one shard and all data from Station B to another shard which has less sensors. By changing partition key to sensor ID will help to divide the data base on sensors to shard.\nD. Change in infra which is not required"
      },
      {
        "date": "2023-03-02T18:33:00.000Z",
        "voteCount": 1,
        "content": "Option B does involve creating a separate Kinesis data stream for Station A, which could be seen as increasing the complexity of the solution compared to modifying the partition key. However, in this scenario, the bottleneck is on data coming from Station A, and creating a separate stream with dedicated shards for that station can help to increase parallelism and improve throughput without increasing the overall cost of the solution.\n\nOn the other hand, modifying the partition key to use the sensor ID instead of the station name could result in uneven shard distribution and hot partitions if the distribution of sensors across stations is uneven. This could lead to degraded performance and require additional scaling in the future, which could increase complexity and cost over time.\n\nSo, while both options have their pros and cons, creating a separate Kinesis data stream for Station A with dedicated shards can be a more effective and scalable solution for improving throughput in this scenario."
      },
      {
        "date": "2022-11-05T10:51:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C as currently the partition keys are based on station names and with two shards, Station A shard is overloaded with 10 sensors, and Station B shard with 5 sensors. Changing the partition key from station names to sensor id would distribute the data equally across shards without increasing the overall cost and complexity of the solution.\n\nOption A is wrong as increasing shards would increase the cost.\n\nOption B is wrong as adding Kinesis Data Stream would increase the cost.\n\nOption D is wrong as reducing the number of sensors would reduce the data collection quality."
      },
      {
        "date": "2022-11-05T10:51:00.000Z",
        "voteCount": 2,
        "content": "The partition key determines to which shard the record is written. The partition key is a Unicode string with a maximum length of 256 bytes. Kinesis runs the partition key value that you provide in the request through an MD5 hash function. The resulting value maps your record to a specific shard within the stream, and Kinesis writes the record to that shard. Partition keys dictate how to distribute data across the stream and use shards.\n\nCertain use cases require you to partition data based on specific criteria for efficient processing by the consuming applications. As an example, if you use player ID pk1234 as the hash key, all scores related to that player route to shard1. The consuming application can use the fact that data stored in shard1 has an affinity with the player ID and can efficiently calculate the leaderboard. An increase in traffic related to players mapped to shard1 can lead to a hot shard. Kinesis Data Streams allows you to handle such scenarios by splitting or merging shards without disrupting your streaming pipeline."
      },
      {
        "date": "2022-11-05T10:52:00.000Z",
        "voteCount": 2,
        "content": "If your use cases do not require data stored in a shard to have high affinity, you can achieve high overall throughput by using a random partition key to distribute data. Random partition keys help distribute the incoming data records evenly across all the shards in the stream and reduce the likelihood of one or more shards getting hit with a disproportionate number of records. You can use a universally unique identifier (UUID) as a partition key to achieve this uniform distribution of records across shards. This strategy can increase the latency of record processing if the consumer application has to aggregate data from multiple shards."
      },
      {
        "date": "2022-10-27T00:44:00.000Z",
        "voteCount": 1,
        "content": "Ans is C.\nA and B will increase the overall cost. D - reducing the sensors is not the good option.\nC - by modifying the partition key to sensor id , input data will be evenly distributed across both the shards by avoiding the hot-sharding in the first shard"
      },
      {
        "date": "2022-09-25T01:14:00.000Z",
        "voteCount": 2,
        "content": "It gives you the answer here - \n1. Station A is facing problem. This has 10 sensor ID and obviously more data.\n2. total stream throughput is still less than the allocated Kinesis Data Streams throughput.\n\nSo we are not utilizing full stream's capability. So workloads are not evenly distributed amongst Shards."
      },
      {
        "date": "2022-07-21T20:00:00.000Z",
        "voteCount": 1,
        "content": "Answer-C"
      },
      {
        "date": "2022-05-21T19:35:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer. Increasing shards won't help there as partitioning is based on station name and there are only two stations."
      },
      {
        "date": "2022-05-21T04:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2022-04-30T16:10:00.000Z",
        "voteCount": 1,
        "content": "Answer-C"
      },
      {
        "date": "2022-04-29T01:31:00.000Z",
        "voteCount": 1,
        "content": "C- sensor id as partition key allows equal distribution of data between the two shards"
      },
      {
        "date": "2021-11-20T07:39:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2021-11-02T12:36:00.000Z",
        "voteCount": 4,
        "content": "C is the right answer"
      },
      {
        "date": "2021-11-01T20:27:00.000Z",
        "voteCount": 4,
        "content": "C is correct!"
      },
      {
        "date": "2021-11-01T00:54:00.000Z",
        "voteCount": 1,
        "content": "D is obviously wrong\nFrom link: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html\n\"Splitting increases the number of shards in your stream and therefore increases the data capacity of the stream. Because you are charged on a per-shard basis, splitting increases the cost of your stream\"\nSo answer is C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/27703-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "Once a month, a company receives a 100 MB .csv file compressed with gzip. The file contains 50,000 property listing records and is stored in Amazon S3 Glacier.<br>The company needs its data analyst to query a subset of the data for a specific vendor.<br>What is the most cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Amazon S3 and query it with Amazon S3 Select.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the data from Amazon S3 Glacier directly with Amazon Glacier Select.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data to Amazon S3 and query it with Amazon Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data to Amazon S3 and query it with Amazon Redshift Spectrum."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-07T07:02:00.000Z",
        "voteCount": 39,
        "content": "Since we are talking about compressed file, Amazon Glacier Select cannot be used. So we need to transfer data to S3 and then use S3 select. So option A is the right choice."
      },
      {
        "date": "2023-01-12T20:24:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/selecting-content-from-objects.html"
      },
      {
        "date": "2021-12-21T00:52:00.000Z",
        "voteCount": 6,
        "content": "Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV)."
      },
      {
        "date": "2022-03-23T04:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html"
      },
      {
        "date": "2021-10-05T12:13:00.000Z",
        "voteCount": 18,
        "content": "my answer is A. (You may need to use Athena if data is in multiple files but this question - data is in a single compressed file)"
      },
      {
        "date": "2021-12-11T20:32:00.000Z",
        "voteCount": 2,
        "content": "Athena cant query Glacier data, so A cant be right."
      },
      {
        "date": "2022-01-31T07:37:00.000Z",
        "voteCount": 3,
        "content": "A doesn't mention Athena...."
      },
      {
        "date": "2022-02-27T07:11:00.000Z",
        "voteCount": 1,
        "content": "please read \"The file is hosted in Amazon S3 Glacier...\""
      },
      {
        "date": "2021-10-05T22:12:00.000Z",
        "voteCount": 2,
        "content": "Yes! I agree. Thank you."
      },
      {
        "date": "2024-02-21T09:01:00.000Z",
        "voteCount": 1,
        "content": "Glacier Select allows to run queries directly on data stored in S3 Glacier without needing to restore and move the data to an active storage class in S3. This feature is designed for scenarios exactly like this, where you need to retrieve only a small subset of data from a large archive stored in Glacier and it is more cost-effective. You are charged for the queries you run and the data retrieved, which, for a small subset of a 100 MB file, could be minimal. This avoids the costs associated with moving the data to Amazon S3 and storing it there for querying. \nA - restoring the file to S3 and then querying it with S3 Select incurs extra costs and processing time.\nC and D - Athena or Redshift Spectrum are powerful for analyzing large datasets, they introduce unnecessary complexity and costs for the given task considering the relatively small size of the dataset."
      },
      {
        "date": "2023-12-12T02:25:00.000Z",
        "voteCount": 2,
        "content": "It's B, you can use Amazon Glacier Select to query archived data in Amazon Glacier.\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/amazon-glacier-select-makes-big-data-analytics-of-archive-data-possible/?nc1=h_ls"
      },
      {
        "date": "2024-01-11T05:52:00.000Z",
        "voteCount": 1,
        "content": "accepted, this is an old question that does not reflect current standards of the"
      },
      {
        "date": "2023-09-20T00:41:00.000Z",
        "voteCount": 1,
        "content": "It is tricky question as now a day \"Glacier-Select\" support compressed g-zip csv. but the problem is file loaded a month ago and deep archival has limitation of one month to retrieve else we need to pay expedited retrieval fees"
      },
      {
        "date": "2023-09-20T00:29:00.000Z",
        "voteCount": 1,
        "content": "https://github.com/awsdocs/amazon-glacier-developer-guide/blob/master/doc_source/glacier-select.md"
      },
      {
        "date": "2023-08-04T15:53:00.000Z",
        "voteCount": 5,
        "content": "Looks like not up-to-date question. Links to AWS Docs posted in this discussion before are redirected now and don't mention Glacier Select (I don't count Blog and unofficial resources). It's only S3 Select now and it has no limit for Amazon S3 Glacier Instant Retrieval storage class. \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html\nThere is no just \"S3 Glacier\" now. \nThis limitation \"The archived objects that are being queried by the select request must be formatted as uncompressed comma-separated values (CSV) files\" is googled now only in Restore Object command in SDK and CLI which I guess is used to restore from Glacier.\nIf come back to the past, I would go with A, since I believe guys saw this limitation for Glacier Select to query from uncompressed files in the links before."
      },
      {
        "date": "2023-08-09T10:50:00.000Z",
        "voteCount": 2,
        "content": "Amazon S3 objects that are stored in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes are not immediately accessible. To access an object in these storage classes, you must restore a temporary copy of the object to its S3 bucket for a specified duration (number of days). https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html"
      },
      {
        "date": "2023-07-18T02:11:00.000Z",
        "voteCount": 7,
        "content": "The Amazon S3 Glacier Select works on objects as it supports a subset of SQL with a format like CSV, JSON, or Apache Parquet format. Objects compressed with GZIP or BZIP2 (for CSV and JSON objects only), and server-side encrypted objects can also be retrieved.\nRef : https://www.scaler.com/topics/aws/s3-glacier-select/"
      },
      {
        "date": "2023-05-01T07:54:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-22T03:04:00.000Z",
        "voteCount": 1,
        "content": "Option A:\nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/"
      },
      {
        "date": "2023-04-22T00:08:00.000Z",
        "voteCount": 3,
        "content": "I have searched a lot and couldn't find document stating glacier will not support compressed file. For me Glacier select is first choice and then s3 select considering cost."
      },
      {
        "date": "2023-03-25T12:24:00.000Z",
        "voteCount": 3,
        "content": "Answer: B.\nNowhere have I read that Glacier Select cannot query compressed CSV files."
      },
      {
        "date": "2022-11-05T10:54:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A as AWS S3 Select enables querying S3 data on selected fields. As S3 Glacier Select does not support uncompressed data, it needs to be restored to S3.\n\nWith Amazon S3 Select, you can use simple structured query language (SQL) statements to filter the contents of an Amazon S3 object and retrieve just the subset of data that you need. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency to retrieve this data.\n\nAmazon S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only), and server-side encrypted objects. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited.\n\nOption B is wrong as Archive objects that are queried by S3 Glacier Select must be formatted as uncompressed comma-separated values (CSV).\n\nOptions C &amp; D are wrong as Athena and Redshift would add additional cost."
      },
      {
        "date": "2023-10-20T00:36:00.000Z",
        "voteCount": 1,
        "content": "https://www.scaler.com/topics/aws/s3-glacier-select/\nbut in this document they have mentioned s3 glacier select support compressed gzip format"
      },
      {
        "date": "2022-10-29T23:48:00.000Z",
        "voteCount": 1,
        "content": "A and B seems to be wrong according to the below statement: Amazon S3 Select scan range requests support Parquet, CSV (without quoted delimiters), and JSON objects (in LINES mode only). CSV and JSON objects must be uncompressed. For line-based CSV and JSON objects, when a scan range is specified as part of the Amazon S3 Select request, all records that start within the scan range are processed. For Parquet objects, all of the row groups that start within the scan range requested are processed.\n\nD is costly and hence only feasible ans I see is C."
      },
      {
        "date": "2022-10-29T23:45:00.000Z",
        "voteCount": 1,
        "content": "Amazon S3 Select scan range requests support Parquet, CSV (without quoted delimiters), and JSON objects (in LINES mode only). CSV and JSON objects must be uncompressed. For line-based CSV and JSON objects, when a scan range is specified as part of the Amazon S3 Select request, all records that start within the scan range are processed. For Parquet objects, all of the row groups that start within the scan range requested are processed."
      },
      {
        "date": "2022-10-24T06:55:00.000Z",
        "voteCount": 1,
        "content": "GZIP or BZIP2 - CSV and JSON files can be compressed using GZIP or BZIP2. GZIP and BZIP2 are the only compression formats that Amazon S3 Select supports for CSV and JSON files. Amazon S3 Select supports columnar compression for Parquet using GZIP or Snappy. Amazon S3 Select does not support whole-object compression for Parquet objects."
      },
      {
        "date": "2022-09-25T01:16:00.000Z",
        "voteCount": 1,
        "content": "Both Athena and Redshift are viable but way more costly than the S3 select option. Glacier Select cannot query on zipped data."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/28066-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company is building its data warehouse solution using Amazon Redshift. As a part of that effort, the company is loading hundreds of files into the fact table created in its Amazon Redshift cluster. The company wants the solution to achieve the highest throughput and optimally use cluster resources when loading data into the company's fact table.<br>How should the company meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse multiple COPY commands to load the data into the Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3DistCp to load multiple files into the Hadoop Distributed File System (HDFS) and use an HDFS connector to ingest the data into the Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse LOAD commands equal to the number of Amazon Redshift cluster nodes and load the data in parallel into each node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a single COPY command to load the data into the Amazon Redshift cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T14:30:00.000Z",
        "voteCount": 35,
        "content": "D.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2022-11-05T10:55:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is D as using a single COPY command would load the data in parallel.\n\nAmazon Redshift can automatically load in parallel from multiple compressed data files.\n\n\nHowever, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined.\n\nOption A is wrong as multiple COPY commands would force Redshift to perform a serialized load.\n\nOption B is wrong as using EMR just makes the solution complicated.\n\nOption C is wrong as there is no LOAD command with Redshift."
      },
      {
        "date": "2023-10-14T22:31:00.000Z",
        "voteCount": 1,
        "content": "D. https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2023-05-01T07:55:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-04-22T00:15:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\nCopy Command is used to load data to redshift. Already single copy command load data in parallel."
      },
      {
        "date": "2022-09-25T01:17:00.000Z",
        "voteCount": 1,
        "content": "Single copy command is the correct answer."
      },
      {
        "date": "2022-08-09T04:50:00.000Z",
        "voteCount": 2,
        "content": "The copy command is by default parallelized and effcient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commans like in SQL."
      },
      {
        "date": "2022-08-09T04:49:00.000Z",
        "voteCount": 2,
        "content": "The copy command is by default parallelized and effcient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commans like in SQL."
      },
      {
        "date": "2022-07-19T22:12:00.000Z",
        "voteCount": 1,
        "content": "The copy command is by default parallelized and efficient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commas like in SQL. No point of creating expensive solution as given in B. \nThe answer should be D"
      },
      {
        "date": "2022-06-19T01:12:00.000Z",
        "voteCount": 1,
        "content": "The copy command is by default parallelized and effcient. It uses to load data from sources other than RedShift. If it is RedShift then use INSERT INTO or CREATE TABLE AS commans like in SQL. No point of creating expensive solution as given in B. The answer should be D"
      },
      {
        "date": "2022-05-22T01:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-05-18T04:50:00.000Z",
        "voteCount": 1,
        "content": "It's D. The only requirement is that all the files should lie under a common directory and in the redshift copy command you need to pass the path till directory so that it will consider all the files inside it."
      },
      {
        "date": "2022-04-27T04:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2022-02-13T03:28:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer"
      },
      {
        "date": "2022-01-24T08:11:00.000Z",
        "voteCount": 1,
        "content": "its D  https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2021-11-14T15:03:00.000Z",
        "voteCount": 1,
        "content": "Option D\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2021-11-05T18:54:00.000Z",
        "voteCount": 1,
        "content": "Ans    D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/74000-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analyst is designing a solution to interactively query datasets with SQL using a JDBC connection. Users will join data stored in Amazon S3 in Apache ORC format with data stored in Amazon OpenSearch Service (Amazon Elasticsearch Service) and Amazon Aurora MySQL.<br>Which solution will provide the MOST up-to-date results?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue jobs to ETL data from Amazon ES and Aurora MySQL to Amazon S3. Query the data with Amazon Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DMS to stream data from Amazon ES and Aurora MySQL to Amazon Redshift. Query the data with Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery all the datasets in place with Apache Spark SQL running on an AWS Glue developer endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery all the datasets in place with Apache Presto running on Amazon EMR.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-05T10:58:00.000Z",
        "voteCount": 17,
        "content": "Correct answer is D as Presto is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources.\n\nOption A is wrong as Glue is not ideal for interactive queries but more for batch ETL jobs.\n\nOption B is wrong as it would not provide the up-to-date results as the data needs to copied over to Redshift for querying. Also, it does not cover S3 which would need Redshift Spectrum.\n\nOption C is wrong as Spark SQL does not allow the capability to query multiple data sources. Also, Glue Developer Endpoints help test Glue ETL jobs."
      },
      {
        "date": "2022-11-05T10:59:00.000Z",
        "voteCount": 6,
        "content": "Presto is an open-source distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. Presto can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3.\n\nPresto uses a custom query execution engine with operators designed to support SQL semantics. Different from Hive/MapReduce, Presto executes queries in memory, pipelined across the network between stages, thus avoiding unnecessary I/O. The pipelined execution model runs multiple stages in parallel and streams data from one stage to the next as it becomes available.\n\nPresto supports the ANSI SQL standard, which makes it easy for data analysts and developers to query both structured and unstructured data at scale. Currently, Presto supports a wide variety of SQL functionality, including complex queries, aggregations, joins, and window functions."
      },
      {
        "date": "2022-04-21T03:53:00.000Z",
        "voteCount": 8,
        "content": "Answer = D (use presto)"
      },
      {
        "date": "2023-05-01T07:56:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-04-22T00:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\nA-not upto date data as its glue job\nB- Up-to-date data as its DMS but DMS to ES integeration not possible\nC-Not interactive"
      },
      {
        "date": "2022-09-25T01:18:00.000Z",
        "voteCount": 1,
        "content": "Disparate data sources are present and OpenSearch data cannot be ingested via DMS."
      },
      {
        "date": "2022-08-06T14:44:00.000Z",
        "voteCount": 1,
        "content": "Answer = D"
      },
      {
        "date": "2022-05-21T05:45:00.000Z",
        "voteCount": 2,
        "content": "Answer should be D"
      },
      {
        "date": "2022-05-21T05:43:00.000Z",
        "voteCount": 2,
        "content": "Answer should be D"
      },
      {
        "date": "2022-05-09T22:22:00.000Z",
        "voteCount": 3,
        "content": "For A, I didn't find document about exporting data from open search to S3.\nB: DMS doesn't support to export from ES.\nC: to use spark SQL to query from ES, we also need a third-party connector, so C is not complete. \nD: should work."
      },
      {
        "date": "2022-04-26T12:05:00.000Z",
        "voteCount": 1,
        "content": "I say it should be B"
      },
      {
        "date": "2022-04-23T16:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-04-22T13:12:00.000Z",
        "voteCount": 1,
        "content": "IMO the answer should be B since its highlighted that it must be most up to-date results and DMS supports change data capture."
      },
      {
        "date": "2022-04-22T05:49:00.000Z",
        "voteCount": 2,
        "content": "Most up-to-date -&gt; in-place queries if possible, so Presto. Athena solution implies moving data from RDS to S3 so adds a potential delay (S3 is not real-time)"
      },
      {
        "date": "2022-04-21T07:59:00.000Z",
        "voteCount": 1,
        "content": "JDBC connection is a key. so Athena is the answer"
      },
      {
        "date": "2022-11-23T10:21:00.000Z",
        "voteCount": 1,
        "content": "You can use JDBC on the Presto running on EMR: \nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/presto-adding-db-connectors.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/28633-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company developed a new elections reporting website that uses Amazon Kinesis Data Firehose to deliver full logs from AWS WAF to an Amazon S3 bucket.<br>The company is now seeking a low-cost option to perform this infrequent data analysis with visualizations of logs in a way that requires minimal development effort.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue crawler to create and update a table in the Glue data catalog from the logs. Use Athena to perform ad-hoc analyses and use Amazon QuickSight to develop data visualizations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second Kinesis Data Firehose delivery stream to deliver the log files to Amazon OpenSearch Service (Amazon Elasticsearch Service). Use Amazon ES to perform text-based searches of the logs for ad-hoc analyses and use OpenSearch Dashboards (Kibana) for data visualizations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to convert the logs into .csv format. Then add the function to the Kinesis Data Firehose transformation configuration. Use Amazon Redshift to perform ad-hoc analyses of the logs using SQL queries and use Amazon QuickSight to develop data visualizations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EMR cluster and use Amazon S3 as the data source. Create an Apache Spark job to perform ad-hoc analyses and use Amazon QuickSight to develop data visualizations."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T20:19:00.000Z",
        "voteCount": 48,
        "content": "A ? Any thoughts \nhttps://aws.amazon.com/blogs/big-data/analyzing-aws-waf-logs-with-amazon-es-amazon-athena-and-amazon-quicksight/"
      },
      {
        "date": "2021-11-17T20:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/waf-logs.html"
      },
      {
        "date": "2022-11-21T02:12:00.000Z",
        "voteCount": 1,
        "content": "from the link: If your use case requires the analysis of data in real time, then Amazon OpenSearch Service is more suitable for your needs. If you prefer a serverless approach that doesn\u2019t require capacity planning or cluster management, then the solution with AWS Glue, Athena, and Amazon QuickSight is more suitable."
      },
      {
        "date": "2021-10-16T03:05:00.000Z",
        "voteCount": 16,
        "content": "\"infrequently\" is a typical keyword for athena. Same as \"ad-hoc\" ."
      },
      {
        "date": "2023-10-18T04:14:00.000Z",
        "voteCount": 1,
        "content": "Why not B? the question doesn't restrict cost parameter and this will be with least development effort"
      },
      {
        "date": "2023-05-01T07:59:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-22T00:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nB. ES cost is high\nC. redshift cost is High\nD.EMR is less costly than Glue but nothing mentioned about queries, just job\n\nD.EMR is less costly than Glue. But development effort is more considering cluster creation"
      },
      {
        "date": "2023-02-22T10:33:00.000Z",
        "voteCount": 2,
        "content": "Why not B. Not a single line of code required. It just requires a streaming config in ES and configuration of Kibana. It is B for me."
      },
      {
        "date": "2023-04-22T00:26:00.000Z",
        "voteCount": 1,
        "content": "Cost of ES is high"
      },
      {
        "date": "2022-11-05T11:16:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A. Using Glue crawler over S3 data can be used to create a data catalog, that can be queried using Athena and visualized using QuickSight. This solution does not require additional resources, data duplication and uses serverless managed services.\nOption B is wrong as Elasticsearch cluster would not provide a cost-effective solution.\n\nOption C is wrong as Redshift cluster with a Lambda job would not provide a cost-effective solution, and would also need development effort.\n\n\nOption D is wrong as EMR cluster with a Spark job would not provide a cost-effective solution, and would also need development effort."
      },
      {
        "date": "2023-04-22T00:27:00.000Z",
        "voteCount": 2,
        "content": "EMR is less costly than Glue. But development effort is more considering cluster creation"
      },
      {
        "date": "2022-09-25T01:20:00.000Z",
        "voteCount": 2,
        "content": "\"low-cost option to perform this infrequent data analysis with visualizations of logs in a way that requires minimal development effort\" - This is the key. As the solution is required for infrequent analysis, so OpenSearch will be costlier solution than a combination of Athena and QuickSight."
      },
      {
        "date": "2022-07-20T22:51:00.000Z",
        "voteCount": 1,
        "content": "A is the answer as its cost effective."
      },
      {
        "date": "2022-06-27T12:02:00.000Z",
        "voteCount": 1,
        "content": "Low cost and rare"
      },
      {
        "date": "2022-05-22T01:52:00.000Z",
        "voteCount": 1,
        "content": "Either A or B"
      },
      {
        "date": "2022-05-05T07:35:00.000Z",
        "voteCount": 1,
        "content": "Per this https://aws.amazon.com/blogs/big-data/analyzing-aws-waf-logs-with-amazon-es-amazon-athena-and-amazon-quicksight/ A should be answer"
      },
      {
        "date": "2021-12-14T11:35:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html\nIntegrates easily with Glue and Quicksight"
      },
      {
        "date": "2021-11-17T14:10:00.000Z",
        "voteCount": 2,
        "content": "A is prefered"
      },
      {
        "date": "2021-11-09T22:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A if the ask is low-cost (asked in this question)\nB - this should be the answer if the ask is to have the best solution for log analysis (this is not cost efficient)\nC and D can be ruled out for obvious reasons of including so many services when the requirement can be met without them"
      },
      {
        "date": "2021-10-31T14:05:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2021-10-29T08:10:00.000Z",
        "voteCount": 2,
        "content": "A is the answer as its cost effective. Athena and Quicksight Standard are very cost effective compared to Elasticsearch Cluster which is expensive."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/28627-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large company has a central data lake to run analytics across different departments. Each department uses a separate AWS account and stores its data in an<br>Amazon S3 bucket in that account. Each AWS account uses the AWS Glue Data Catalog as its data catalog. There are different data lake access requirements based on roles. Associate analysts should only have read access to their departmental data. Senior data analysts can have access in multiple departments including theirs, but for a subset of columns only.<br>Which solution achieves these required access patterns to minimize costs and administrative tasks?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsolidate all AWS accounts into one account. Create different S3 buckets for each department and move all the data from every account to the central data lake account. Migrate the individual data catalogs into a central data catalog and apply fine-grained permissions to give to each user the required access to tables and databases in AWS Glue and Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the account structure and the individual AWS Glue catalogs on each account. Add a central data lake account and use AWS Glue to catalog data from various accounts. Configure cross-account access for AWS Glue crawlers to scan the data in each departmental S3 bucket to identify the schema and populate the catalog. Add the senior data analysts into the central account and apply highly detailed access controls in the Data Catalog and Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an individual AWS account for the central data lake. Use AWS Lake Formation to catalog the cross-account locations. On each individual S3 bucket, modify the bucket policy to grant S3 permissions to the Lake Formation service-linked role. Use Lake Formation permissions to add fine-grained access controls to allow senior analysts to view specific tables and columns.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an individual AWS account for the central data lake and configure a central S3 bucket. Use an AWS Lake Formation blueprint to move the data from the various buckets into the central S3 bucket. On each individual bucket, modify the bucket policy to grant S3 permissions to the Lake Formation service-linked role. Use Lake Formation permissions to add fine-grained access controls for both associate and senior analysts to view specific tables and columns."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-29T19:44:00.000Z",
        "voteCount": 15,
        "content": "Answer is C. I have implemented federated data lake."
      },
      {
        "date": "2022-11-05T18:01:00.000Z",
        "voteCount": 10,
        "content": "Correct answer is C as AWS Data Lake Formation can help provide a centralized place for maintaining data catalog to various locations, without moving the data. Also, AWS Lake Formation permissions can help provide a central access control location.\n\nOption A is wrong as consolidating accounts would increase administrative tasks.\n\nOption B is wrong as although it might work, it is more simpler to use AWS Lake Formation for access control.\n\nOption D is wrong as moving all the data to central S3 would duplicate the storage cost and increase administrative tasks."
      },
      {
        "date": "2024-01-11T11:37:00.000Z",
        "voteCount": 1,
        "content": "accounts consolidation has a big administrative effort... then A is discarded...\n\nB works but doesn't have permission requirements for an analyst role... then B is discarded...\n\nif we talk about fine-grained access control and the strong power of data catalog, lake formation always is the better option... not expansive and easy to use...\n\na central bucket is a big administrative effort and increases storage costs due to data storage duplication... then D is discarded...\n\nC any througs?"
      },
      {
        "date": "2023-05-01T08:00:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-04-27T13:38:00.000Z",
        "voteCount": 1,
        "content": "Seems to be Data Lake Formation\nSimplified data lake setup: Streamlines creation and configuration of a centralized data lake.\nFine-grained access control: Enables table and column-level permissions for users and groups.\nData cataloging and discovery: Facilitates a searchable, centralized data catalog using AWS Glue.\nData transformation: Supports ETL jobs to clean, enrich, and prepare data for analysis.\nIntegration with AWS services: Seamlessly connects with various AWS analytics and processing tools.\nSecurity and compliance: Ensures data encryption, monitors access, and provides audit logs."
      },
      {
        "date": "2023-04-22T00:52:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\nA-Move all data means cost and lot of effort\nB-It works but Lakeformation is easy\nC-Answer\nD- Move all data means cost and lot of effort"
      },
      {
        "date": "2023-03-19T06:06:00.000Z",
        "voteCount": 1,
        "content": "Option C is incorrect because it requires setting up an individual AWS account for the central datalake. This would be an unnecessary expense. It also requires using AWS Lake Formation to catalog the cross-account locations. This would be a time-consuming and expensive process.\n\nAWS Lake Formation is a service that makes it easy to set up a secure data lake in days. It simplifies and automates many of the complex manual steps required to create a data lake, including collecting, cleaning, and cataloging data. You can use AWS Lake Formation to create a central data catalog that is accessible to all departments. You can use Lake Formation permissions to add fine-grained access controls to allow senior analysts to view specific tables and columns.\n\nHowever, setting up an individual AWS account for the central datalake would be an unnecessary expense. It would also require additional administrative overhead to manage the different accounts."
      },
      {
        "date": "2023-03-06T18:09:00.000Z",
        "voteCount": 1,
        "content": "Option B, keeps the account structure and the individual AWS Glue catalogs on each account, but still allows for a centralized catalog using AWS Glue. It uses cross-account access for AWS Glue crawlers to scan the data in each departmental S3 bucket and identify the schema, which populates the central catalog. The senior data analysts can be added to the central account with highly detailed access controls in the Data Catalog and Amazon S3. This approach is more scalable and cost-effective in cases where there are many departments or AWS accounts involved.\n\nOption C is a valid solution to the problem described, but it may not be the most cost-effective and efficient one. Setting up an individual AWS account for the central data lake and using AWS Lake Formation to catalog the cross-account locations with fine-grained access controls for senior analysts is a good approach, but it may involve additional administrative tasks and costs. Additionally, modifying the bucket policy for each individual S3 bucket may be cumbersome and error-prone."
      },
      {
        "date": "2022-09-25T01:28:00.000Z",
        "voteCount": 1,
        "content": "Lake Formation for such fine grained access. Also, no need to use Lake Formation BluePrint as data source is S3."
      },
      {
        "date": "2022-08-22T18:36:00.000Z",
        "voteCount": 1,
        "content": "should access to the subset of columns means fine-grained access control. It can be implemented by Lake Formation, not individual S3 buckets.\nSo the answer is C."
      },
      {
        "date": "2022-07-25T21:36:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-05-21T04:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2022-05-06T20:24:00.000Z",
        "voteCount": 1,
        "content": "Use AWS Lake Formation for cross account catalog and permission."
      },
      {
        "date": "2021-11-23T16:28:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C."
      },
      {
        "date": "2021-11-06T22:13:00.000Z",
        "voteCount": 1,
        "content": "When did B say..to move the data.... isn't catalog of data and moving the data two different things?"
      },
      {
        "date": "2021-11-04T22:58:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-10-30T16:17:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/27705-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to improve user satisfaction for its smart home system by adding more features to its recommendation engine. Each sensor asynchronously pushes its nested JSON data into Amazon Kinesis Data Streams using the Kinesis Producer Library (KPL) in Java. Statistics from a set of failed sensors showed that, when a sensor is malfunctioning, its recorded data is not always sent to the cloud.<br>The company needs a solution that offers near-real-time analytics on the data from the most updated sensors.<br>Which solution enables the company to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the RecordMaxBufferedTime property of the KPL to \"\u05d2\u02c6\u20191\" to disable the buffering on the sensor side. Use Kinesis Data Analytics to enrich the data based on a company-developed anomaly detection SQL script. Push the enriched data to a fleet of Kinesis data streams and enable the data transformation feature to flatten the JSON file. Instantiate a dense storage Amazon Redshift cluster and use it as the destination for the Kinesis Data Firehose delivery stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the sensors code to use the PutRecord/PutRecords call from the Kinesis Data Streams API with the AWS SDK for Java. Use Kinesis Data Analytics to enrich the data based on a company-developed anomaly detection SQL script. Direct the output of KDA application to a Kinesis Data Firehose delivery stream, enable the data transformation feature to flatten the JSON file, and set the Kinesis Data Firehose destination to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the RecordMaxBufferedTime property of the KPL to \"0\" to disable the buffering on the sensor side. Connect for each stream a dedicated Kinesis Data Firehose delivery stream and enable the data transformation feature to flatten the JSON file before sending it to an Amazon S3 bucket. Load the S3 data into an Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the sensors code to use the PutRecord/PutRecords call from the Kinesis Data Streams API with the AWS SDK for Java. Use AWS Glue to fetch and process data from the stream using the Kinesis Client Library (KCL). Instantiate an Amazon Elasticsearch Service cluster and use AWS Lambda to directly push data into it."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T05:58:00.000Z",
        "voteCount": 21,
        "content": "B?\nhttps://aws.amazon.com/blogs/big-data/perform-near-real-time-analytics-on-streaming-data-with-amazon-kinesis-and-amazon-elasticsearch-service/"
      },
      {
        "date": "2024-01-11T11:41:00.000Z",
        "voteCount": 1,
        "content": "agreed"
      },
      {
        "date": "2021-09-26T18:32:00.000Z",
        "voteCount": 13,
        "content": "When Not to Use the KPL  :\n\nThe KPL can incur an additional processing delay of up to RecordMaxBufferedTime within the library (user-configurable). Larger values of RecordMaxBufferedTime results in higher packing efficiencies and better performance. Applications that cannot tolerate this additional delay may need to use the AWS SDK directly. For more information about using the AWS SDK with Kinesis Data Streams, see Developing Producers Using the Amazon Kinesis Data Streams API with the AWS SDK for Java. For more information about RecordMaxBufferedTime and other user-configurable properties of the KPL, see Configuring the Kinesis Producer Library. \n\nhttps://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html"
      },
      {
        "date": "2021-09-30T21:55:00.000Z",
        "voteCount": 14,
        "content": "I agree the answer is B."
      },
      {
        "date": "2023-05-01T08:02:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-11-06T03:46:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is B. Using PutRecord/PutRecords would send the data synchronously to Kinesis Data Streams. Kinesis Data Analytics can be detect anomalies and the data can be pushed to Kinesis Data Firehose with transformation to Elasticsearch for analysis.\nOption A is wrong as Kinesis data streams does not provide data transformation feature.\n\nOption C is wrong as copying data to S3 and loading to Redshift would not make it near-real time.\n\nOption D is wrong as using Glue is ideal for batch jobs and not for near-real time analytics."
      },
      {
        "date": "2022-08-02T03:47:00.000Z",
        "voteCount": 1,
        "content": "Low latency retrival can be achieved with DynamoDB, Redis and OpenSearch. I guess that would be enough to select the answer."
      },
      {
        "date": "2022-08-02T03:48:00.000Z",
        "voteCount": 1,
        "content": "D can be eleminated coz, KCL can't read from SDK producer"
      },
      {
        "date": "2022-07-21T19:03:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2022-06-11T01:23:00.000Z",
        "voteCount": 1,
        "content": "near realtime should be opensearch"
      },
      {
        "date": "2022-05-22T01:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-04-30T15:14:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2022-03-07T09:25:00.000Z",
        "voteCount": 2,
        "content": "B - near-realtime analytics keyword, only ES can provide that from the set of options"
      },
      {
        "date": "2021-11-19T20:52:00.000Z",
        "voteCount": 1,
        "content": "B it is"
      },
      {
        "date": "2021-11-06T23:40:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2021-11-02T10:00:00.000Z",
        "voteCount": 1,
        "content": "Looks like Answer is B.. \n\nhttps://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html\n\nWhen Not to Use the KPL\nThe KPL can incur an additional processing delay of up to RecordMaxBufferedTime within the library (user-configurable). Larger values of RecordMaxBufferedTime results in higher packing efficiencies and better performance. Applications that cannot tolerate this additional delay may need to use the AWS SDK directly."
      },
      {
        "date": "2021-10-29T18:00:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2021-10-29T09:55:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-25T22:46:00.000Z",
        "voteCount": 1,
        "content": "The only thing about B is why transform to CSV since putting to Elastic Search ?"
      },
      {
        "date": "2021-10-19T17:14:00.000Z",
        "voteCount": 2,
        "content": "Answer B\nA and C dropped because RedShift is not meant for \"near-real-time\" analysis. Also, it would require some kind on Visualization on top of it to do the analysis. \nDropped D because KDS in itself cannot transform data."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/29208-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A global company has different sub-organizations, and each sub-organization sells its products and services in various countries. The company's senior leadership wants to quickly identify which sub-organization is the strongest performer in each country. All sales data is stored in Amazon S3 in Parquet format.<br>Which approach can provide the visuals that senior leadership requested with the least amount of effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight with Amazon Athena as the data source. Use heat maps as the visual type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight with Amazon S3 as the data source. Use heat maps as the visual type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight with Amazon Athena as the data source. Use pivot tables as the visual type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight with Amazon S3 as the data source. Use pivot tables as the visual type."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T18:33:00.000Z",
        "voteCount": 34,
        "content": "It is A. QuickSight does not support S3 files with parquet format, Athena does it. For visualization is better a graph than a pivot table.  \nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html\nhttps://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html"
      },
      {
        "date": "2021-10-09T19:01:00.000Z",
        "voteCount": 1,
        "content": "100% agree with A"
      },
      {
        "date": "2021-10-10T12:04:00.000Z",
        "voteCount": 2,
        "content": "Why not B. Why cant S3 be the datasource for Quicksight"
      },
      {
        "date": "2021-10-13T01:44:00.000Z",
        "voteCount": 3,
        "content": "Sorry just noticed that you\u2019ve mentioned Quicksight doesn\u2019t support files in S3 in parquet format"
      },
      {
        "date": "2021-10-14T03:05:00.000Z",
        "voteCount": 2,
        "content": "I agree. It should be B\nhttps://aws.amazon.com/quicksight/\nhttps://d1.awsstatic.com/r2018/h/QuickSight%20Q/Provide%20interactive%20dashboards_QuickSight.7efb01bbe9a6b6592a821bccff04f463647afd82.png"
      },
      {
        "date": "2021-10-13T04:45:00.000Z",
        "voteCount": 2,
        "content": "You can use any of the following relational data stores as data sources for Amazon QuickSight:\n\nAmazon Athena\n\nAmazon Aurora\n\nAmazon Redshift\n\nAmazon Redshift Spectrum\n\nAmazon S3\n\nAmazon S3 Analytics"
      },
      {
        "date": "2021-10-17T08:35:00.000Z",
        "voteCount": 1,
        "content": "Agree with you. QuickSight supports S3. the data is already on S3. I don't see need for Anthena in this scenario."
      },
      {
        "date": "2021-10-24T13:25:00.000Z",
        "voteCount": 9,
        "content": "No. https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\nParquet files not supported by Quicksight, must use Athena."
      },
      {
        "date": "2021-12-21T00:38:00.000Z",
        "voteCount": 4,
        "content": "I am inclined towards C\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to further analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns.\nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html"
      },
      {
        "date": "2021-09-25T00:28:00.000Z",
        "voteCount": 10,
        "content": "Agree wih C. Thoughts?"
      },
      {
        "date": "2021-10-04T23:30:00.000Z",
        "voteCount": 2,
        "content": "Totally agree :-)"
      },
      {
        "date": "2023-10-14T22:54:00.000Z",
        "voteCount": 1,
        "content": "A or C? For me both are valid\n- QuickSight does not support S3 files with parquet format. \n- AWS says here https://docs.aws.amazon.com/quicksight/latest/user/pivot-table.html \"Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to analyze data on the visual.\""
      },
      {
        "date": "2023-08-28T04:36:00.000Z",
        "voteCount": 1,
        "content": "Ans Is C . Athena as the data is in Parquet format and Pivot table as it was to identify sub-organization is the strongest performer in each country whereas heat map will provide only correlation"
      },
      {
        "date": "2023-05-01T08:03:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-04-22T01:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nSince Parquet is mentioned then Athena.\nVisuals means heat map, Pivot table is just a table with Data"
      },
      {
        "date": "2023-04-22T01:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nSince Parquet is mentioned then Athena.\nVisuals means heat map, Pivot table is just a table with Data"
      },
      {
        "date": "2023-03-19T06:28:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. Here\u2019s why:\n\nA is correct because Amazon QuickSight can be used with Amazon Athena as the data source to visualize sales data stored in Amazon S3 in Parquet format. Heat maps can be used as the visual type to quickly identify which sub-organization is the strongest performer in each country. Heat maps are a great way to visualize data that is organized in a grid format, such as sales data.\n\nC is incorrect because pivot tables are not the best visual type to use for this scenario. Pivot tables are great for summarizing and analyzing large amounts of data, but they are not the best way to visualize data."
      },
      {
        "date": "2022-11-06T03:52:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is A as Quicksight can use Athena as data source to query the data in S3. Heat Maps would provide the required visual representation.\n\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to analyze data on the visual.\n\nOption C is wrong as Heat Map would provide the required visual and would be preferred over Pivot tables as there is not need to analyze the data.\n\nOptions B &amp; D are wrong as Quicksight supports S3 as a data source, however it does not work directly with parquet file format."
      },
      {
        "date": "2022-09-25T01:32:00.000Z",
        "voteCount": 1,
        "content": "Quickly identification is possible by HeatMap color coding feature. Pivot table is unnecessary here. Athena is required as data is stored in Parquet format."
      },
      {
        "date": "2022-09-23T18:27:00.000Z",
        "voteCount": 1,
        "content": "c, heat maps are for correlation ("
      },
      {
        "date": "2022-08-24T08:00:00.000Z",
        "voteCount": 1,
        "content": "heat maps are for correlation (P.S.: heat maps do not use a world map) -&gt; Pivot tables is correct here.\n\nquick-sight cannot directly query parquet files."
      },
      {
        "date": "2022-07-25T21:20:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-06-25T12:42:00.000Z",
        "voteCount": 1,
        "content": "Ans is B.\nHeat map allows one simple chart to show the pattern for the whole thing easily.\n\nQuickSight can use S3 as data source.\nCreating a dataset using Amazon S3 files - Amazon QuickSight\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-s3.html"
      },
      {
        "date": "2022-05-27T19:47:00.000Z",
        "voteCount": 3,
        "content": "Ans is C. Heat map is to show co-relation and identify outliers. Here the requirement is very straightforward which is to identify best performer in each region. Hence Pivot table."
      },
      {
        "date": "2022-05-22T01:29:00.000Z",
        "voteCount": 1,
        "content": "My Answer is A"
      },
      {
        "date": "2022-03-16T23:44:00.000Z",
        "voteCount": 3,
        "content": "swiftly determine which sub-organization is the best performance in each country\n\nC: is the answer , because (in each country)you should use pivot table"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/74121-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has 1 million scanned documents stored as image files in Amazon S3. The documents contain typewritten application forms with information including the applicant first name, applicant last name, application date, application type, and application text. The company has developed a machine learning algorithm to extract the metadata values from the scanned documents. The company wants to allow internal data analysts to analyze and find applications using the applicant name, application date, or application text. The original images should also be downloadable. Cost control is secondary to query performance.<br>Which solution organizes the images and metadata to drive insights while meeting the requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each image, use object tags to add the metadata. Use Amazon S3 Select to retrieve the files based on the applicant name and application date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIndex the metadata and the Amazon S3 location of the image file in Amazon OpenSearch Service (Amazon Elasticsearch Service). Allow the data analysts to use OpenSearch Dashboards (Kibana) to submit queries to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the metadata and the Amazon S3 location of the image file in an Amazon Redshift table. Allow the data analysts to run ad-hoc queries on the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the metadata and the Amazon S3 location of the image files in an Apache Parquet file in Amazon S3, and define a table in the AWS Glue Data Catalog. Allow data analysts to use Amazon Athena to submit custom queries."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-22T05:31:00.000Z",
        "voteCount": 12,
        "content": "OpenSearch to scan all text"
      },
      {
        "date": "2023-11-28T17:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nStore the metadata in Redshift. Metadata is extracted using company provided ML program."
      },
      {
        "date": "2023-05-27T00:21:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. Keywords to look for in question - \"Performance\" and \"search using text\". D can be correct only if there is no text based search requirement."
      },
      {
        "date": "2023-05-01T08:04:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-19T06:41:00.000Z",
        "voteCount": 3,
        "content": "Option A is incorrect because object tags are not searchable and cannot be used to query the data. S3 Select can be used to retrieve the files based on the applicant name and application date, but object tags cannot be used to store metadata.\nOption B is correct because Amazon OpenSearch Service (Amazon Elasticsearch Service) can be used to index the metadata and the Amazon S3 location of the image file. Data analysts can use OpenSearch Dashboards (Kibana) to submit queries to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.\nOption C is incorrect because Amazon Redshift is not designed for storing large binary objects such as images. It is a data warehousing solution that is optimized for querying structured data.\nOption D is incorrect because Apache Parquet files are not optimized for querying unstructured data such as images. Amazon Athena can be used to submit custom queries, but it is not optimized for querying large binary objects."
      },
      {
        "date": "2023-03-06T19:00:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer\n\nin Option B there is no direct method provided in this option to download the image file(s) associated with the search results."
      },
      {
        "date": "2022-11-06T04:10:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B as the metadata can be indexed with the S3 file location in ElasticSearch to provide a quick search and allow the users to download the file as well.\n\nhttps://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/\n\nOption A is wrong as using S3 Select would impact query performance.\n\nOption C is wrong as it would have a huge cost impact without improving query performance much.\n\nOption D is wrong as using Athena would impact query performance."
      },
      {
        "date": "2022-09-26T11:22:00.000Z",
        "voteCount": 2,
        "content": "Answer A: Validated using Jon bosco paid dumps"
      },
      {
        "date": "2022-10-19T05:58:00.000Z",
        "voteCount": 1,
        "content": "But why A?"
      },
      {
        "date": "2022-09-25T01:34:00.000Z",
        "voteCount": 2,
        "content": "Cost control is secondary to query performance - This is the key here. Though D also can do the work, but it will be slower than option B."
      },
      {
        "date": "2022-08-15T18:06:00.000Z",
        "voteCount": 3,
        "content": "Parquet format improves performance.  None of the other options talk about performance improvement."
      },
      {
        "date": "2022-08-24T17:45:00.000Z",
        "voteCount": 2,
        "content": "and 'Cost control is secondary to query performance.'"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/28829-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A mobile gaming company wants to capture data from its gaming app and make the data available for analysis immediately. The data record size will be approximately 20 KB. The company is concerned about achieving optimal throughput from each device. Additionally, the company wants to develop a data stream processing application with dedicated throughput for each consumer.<br>Which solution would achieve this goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature while consuming the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the app call the PutRecordBatch API to send data to Amazon Kinesis Data Firehose. Submit a support case to enable dedicated throughput on the account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the app use Amazon Kinesis Producer Library (KPL) to send data to Kinesis Data Firehose. Use the enhanced fan-out feature while consuming the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Host the stream-processing application on Amazon EC2 with Auto Scaling."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T16:29:00.000Z",
        "voteCount": 30,
        "content": "A\nDedicated throughput equals enhanced fan-out. So BCD dropped.\nhttps://docs.aws.amazon.com/streams/latest/dev/enhanced-consumers.html"
      },
      {
        "date": "2021-10-22T00:50:00.000Z",
        "voteCount": 2,
        "content": "A seems correct"
      },
      {
        "date": "2021-10-12T07:35:00.000Z",
        "voteCount": 6,
        "content": "A: Developing Custom Consumers with Dedicated Throughput (Enhanced Fan-Out)"
      },
      {
        "date": "2023-05-01T08:06:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-11-06T04:42:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A as Kinesis Data Streams with Enhanced Fanout provides dedicated throughput for each consumer.\n\nIn Amazon Kinesis Data Streams, you can build consumers that use a feature called enhanced fan-out. This feature enables consumers to receive records from a stream with throughput of up to 2 MB of data per second per shard. This throughput is dedicated, which means that consumers that use enhanced fan-out don't have to contend with other consumers that are receiving data from the stream. Kinesis Data Streams pushes data records from the stream to consumers that use enhanced fan-out. Therefore, these consumers don't need to poll for data.\n\n\nOption B is wrong as there is no option to open support case to enable dedicated throughput.\n\n\nOption C is wrong as Kinesis Data Firehose does not support enhanced fan-out feature.\n\n\nD is incorrect. An Auto Scaling group of EC2 instances will not provide dedicated throughput for the consumers. You have to enable the enhanced fan-out feature in Amazon Kinesis Data Streams."
      },
      {
        "date": "2022-09-25T01:36:00.000Z",
        "voteCount": 1,
        "content": "\"dedicated throughput for each consumer\" - this is the key statement here."
      },
      {
        "date": "2022-07-27T23:36:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-05-21T05:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-11-06T09:11:00.000Z",
        "voteCount": 4,
        "content": "Have the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature while consuming the data. ==&gt; PutRecords aggregate while sending data to Kinesis Data Streams to increase producer throghuput and Enhanced fan-out increase consumer through put\nB. Have the app call the PutRecordBatch API to send data to Amazon Kinesis Data Firehose. Submit a support case to enable dedicated throughput on the account. ==&gt; Question is about stream processing using producer and consumer. \nC. Have the app use Amazon Kinesis Producer Library (KPL) to send data to Kinesis Data Firehose. Use the enhanced fan-out feature while consuming the data. ==&gt; Enhanced fanout feature is not part of Firehose\nD. Have the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Host the stream-processing application on Amazon EC2 with Auto Scaling. ==&gt; We don't need EC@"
      },
      {
        "date": "2021-11-06T07:34:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2021-11-03T21:23:00.000Z",
        "voteCount": 1,
        "content": "A.\nWhatever how you consume your data (EC2 in ASG or not), the stream must have Enhanced fan out."
      },
      {
        "date": "2021-10-28T14:29:00.000Z",
        "voteCount": 1,
        "content": "A for sure."
      },
      {
        "date": "2021-10-21T08:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\n\nThere is no limit for the enhanced fan out . Check this link..\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\n\nQ: Is there a limit on the number of consumers using enhanced fan-out on a given stream?\n\nThere is a default limit of 20 consumers using enhanced fan-out per data stream. If you need more than 20, please submit a limit increase request though AWS support. Keep in mind that you can have more than 20 total consumers reading from a stream by having 20 consumers using enhanced fan-out and other consumers not using enhanced fan-out at the same time."
      },
      {
        "date": "2021-10-20T05:22:00.000Z",
        "voteCount": 1,
        "content": "How A is even possible ! enhanced fan out has limitation of 20 consumers.\n\nI will go with D"
      },
      {
        "date": "2021-10-17T20:15:00.000Z",
        "voteCount": 1,
        "content": "Enhanced fan out has a limit of 20 consumer per stream (not shard, stream). In a case of thousand or maybe millions devices I don't think is a valid solution.\nI'll probably go with D here."
      },
      {
        "date": "2021-11-03T04:40:00.000Z",
        "voteCount": 3,
        "content": "I think the gaming devices are producers not consumers in the given use case."
      },
      {
        "date": "2022-10-19T06:22:00.000Z",
        "voteCount": 1,
        "content": "\"A consumer is an application that processes all data from a Kinesis data stream. \""
      },
      {
        "date": "2021-10-14T23:23:00.000Z",
        "voteCount": 2,
        "content": "Option A is the right choice."
      },
      {
        "date": "2021-10-10T01:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/29549-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A marketing company wants to improve its reporting and business intelligence capabilities. During the planning phase, the company interviewed the relevant stakeholders and discovered that:<br>\u2711 The operations team reports are run hourly for the current month's data.<br>\u2711 The sales team wants to use multiple Amazon QuickSight dashboards to show a rolling view of the last 30 days based on several categories. The sales team also wants to view the data as soon as it reaches the reporting backend.<br>\u2711 The finance team's reports are run daily for last month's data and once a month for the last 24 months of data.<br>Currently, there is 400 TB of data in the system with an expected additional 100 TB added every month. The company is looking for a solution that is as cost- effective as possible.<br>Which solution meets the company's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the last 24 months of data in Amazon Redshift. Configure Amazon QuickSight with Amazon Redshift as the data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the last 2 months of data in Amazon Redshift and the rest of the months in Amazon S3. Set up an external schema and table for Amazon Redshift Spectrum. Configure Amazon QuickSight with Amazon Redshift as the data source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the last 24 months of data in Amazon S3 and query it using Amazon Redshift Spectrum. Configure Amazon QuickSight with Amazon Redshift Spectrum as the data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the last 2 months of data in Amazon Redshift and the rest of the months in Amazon S3. Use a long-running Amazon EMR with Apache Spark cluster to query the data as needed. Configure Amazon QuickSight with Amazon EMR as the data source."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T23:25:00.000Z",
        "voteCount": 41,
        "content": "For me is B.  Redshift offers better performance for querying and analyzing latest 2 months of data and in combination with Spectrum for non-frequent queries on 24 months of data."
      },
      {
        "date": "2021-10-05T01:22:00.000Z",
        "voteCount": 5,
        "content": "they didnt require performance, the required cost-effectiveness !! 2 months of data means 200TB... a redshift cluster of 200TB is not cheap !!!"
      },
      {
        "date": "2022-08-24T21:46:00.000Z",
        "voteCount": 2,
        "content": "it's mentioned 'The sales team also wants to view the data as soon as it reaches the reporting backend', so I think there require perfomance"
      },
      {
        "date": "2021-10-12T19:20:00.000Z",
        "voteCount": 15,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/redshift-spectrum-query-charges/\n\n\"Load the data in S3 and use Redshift Spectrum if the data is infrequently accessed.\"\nIn this case, the operations data is accessed hourly; this is not infrequent.  I think even with the statement of cost effective, the answer is B. IF all the monthly data of 100TB is scanned hourly as part of those reports (and there's nothing in the question to say it isn't), then the cost becomes 100TB * $5 * ~720 hours in a month, which is $360,000 per month! The storage costs for 200TB of Redshift data is $5000 a month."
      },
      {
        "date": "2023-05-01T08:07:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-14T08:26:00.000Z",
        "voteCount": 1,
        "content": "Option B seems to be the best solution for this scenario. It suggests storing the last 2 months of data in Amazon Redshift and the rest of the months in Amazon S3. Setting up an external schema and table for Amazon Redshift Spectrum will allow for querying data stored in Amazon S3. Additionally, configuring Amazon QuickSight with Amazon Redshift as the data source will allow for creating reports and dashboards for the data.\n\nThis solution is cost-effective because it uses Amazon S3 to store the majority of the data, which is cheaper than storing it all in Amazon Redshift. Also, it leverages Amazon Redshift Spectrum, which allows for querying data in Amazon S3 using a standard SQL interface without needing to move the data into Amazon Redshift. Finally, storing only two months of data in Amazon Redshift will minimize storage costs in Redshift while still allowing for fast query performance for the most recent data."
      },
      {
        "date": "2022-11-06T07:46:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B as the base requirements are cost and performance, keeping data in Redshift for 2 months would allow data analysis for current and previous month. Holding data for 24 months in S3 would provide a cost-effective option.\n\nOption A is wrong as holding 24 months data in Redshift is not cost-effective.\n\nOption C is wrong as storing 24 months of data in S3 would not provide performance.\n\nOption D is wrong as using a long-running EMR cluster is not cost-effective."
      },
      {
        "date": "2022-09-25T01:38:00.000Z",
        "voteCount": 1,
        "content": "\"as cost- effective as possible\" - this is the key statement here. So we need fast retrieval and query performance on last 2 months data, and infrequent querying capability for last 24 months of data. So B is the correct answer."
      },
      {
        "date": "2022-08-02T03:07:00.000Z",
        "voteCount": 1,
        "content": "I would choose \"B\", although I had doubts to choose \"C\". The main reason for the switch is that, its not a very good use case of using Redshift Spectrum without using Redshift for any part of the job, I don't know if its possible. Ideally Redshift suppose to query hot data and Redshift Spectrum supposed to extend the querying capability to exabytes of data"
      },
      {
        "date": "2022-07-21T19:01:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2022-05-22T01:41:00.000Z",
        "voteCount": 1,
        "content": "B should be Correct"
      },
      {
        "date": "2022-04-30T15:07:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2022-03-01T20:12:00.000Z",
        "voteCount": 1,
        "content": "Agree B is the best and most cost effective option"
      },
      {
        "date": "2022-01-27T20:14:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-21T13:44:00.000Z",
        "voteCount": 1,
        "content": "Definitely, B is the right one. The cost was already being cut in half. EMR long-running instance is not cheap."
      },
      {
        "date": "2021-11-21T07:27:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-11-15T13:24:00.000Z",
        "voteCount": 3,
        "content": "RD Spectrum can query exabytes of unstructured data in S3 without loading. It even supports gzip and snappy.  So option C is correct."
      },
      {
        "date": "2021-10-29T21:46:00.000Z",
        "voteCount": 3,
        "content": "I think answer B is correct because of the hourly scanning costs.\nHowever, I think we don't have enough information:\n - what are the usage patterns? is only a small subset of columns required? -&gt; less scanning\n - what compression factor is possible? Spectrum seems to support compressed data, whereas data in Redshift seems to be uncompressed\n[Compression factor] * [column selection] can easily decrease the amount of data that needs to be scanned by a factor of 100x, possibly even 1000x or more.\n\nFinally, the phrasing \"The sales team also wants to view the data as soon as it reaches the reporting backend.\" could go either way if you ask me - Spectrum doesn't introduce a lag because data is loaded lazily, however it leads to slower queries compared to Redshift."
      },
      {
        "date": "2021-10-25T01:02:00.000Z",
        "voteCount": 1,
        "content": "B. One more thing that make C wrong is Spectrum only runs within a Redshift Cluster. Therefore you are both charged by the cluster and the data scanned."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/27709-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A media company wants to perform machine learning and analytics on the data residing in its Amazon S3 data lake. There are two data transformation requirements that will enable the consumers within the company to create reports:<br>\u2711 Daily transformations of 300 GB of data with different file formats landing in Amazon S3 at a scheduled time.<br>\u2711 One-time transformations of terabytes of archived data residing in the S3 data lake.<br>Which combination of solutions cost-effectively meets the company's requirements for transforming the data? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use AWS Glue crawlers to scan and identify the schema.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use Amazon Athena to scan and identify the schema.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use Amazon Redshift to perform transformations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use AWS Glue workflows with AWS Glue jobs to perform transformations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor archived data, use Amazon EMR to perform data transformations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor archived data, use Amazon SageMaker to perform data transformations."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T00:29:00.000Z",
        "voteCount": 46,
        "content": "To me, ADE.\nNot B. Athena will use Glue (option A)\nNot C. Its an antipattern to use Redshift to do transformations.\nNot F. Would pick EMR instead of Sagemaker to do one time transformations"
      },
      {
        "date": "2021-09-24T11:58:00.000Z",
        "voteCount": 3,
        "content": "Agreed"
      },
      {
        "date": "2021-09-22T04:02:00.000Z",
        "voteCount": 9,
        "content": "My answer is ADE."
      },
      {
        "date": "2023-05-01T08:08:00.000Z",
        "voteCount": 1,
        "content": "ADE: I passed the test"
      },
      {
        "date": "2022-11-06T08:45:00.000Z",
        "voteCount": 6,
        "content": "Correct answers are A, D &amp; E\n\nOptions A &amp; D using Glue Crawler and Glue Workflows would provide ETL for daily transactions.\n\nOption E as EMR can help perform data transformation for archived data.\n\nOption B is wrong as Athena does not identify the schema but uses Glue Catalog.\n\nOption C is wrong as Redshift would need to be persistent and does not provide a cost-effective solution as compared to Glue.\n\nOption F is wrong as Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. It does not provide ETL capability on large data."
      },
      {
        "date": "2022-11-27T03:53:00.000Z",
        "voteCount": 1,
        "content": "What about SageMaker Data Wrangler?"
      },
      {
        "date": "2022-09-25T01:41:00.000Z",
        "voteCount": 1,
        "content": "cost-effectively solution is required. So, A, D and E."
      },
      {
        "date": "2022-08-06T14:44:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: ADE"
      },
      {
        "date": "2022-07-23T08:01:00.000Z",
        "voteCount": 2,
        "content": "A: For schema and new partition of data for Incremental load\nD: Incremental transformation\nE: Historical data migration using EMR"
      },
      {
        "date": "2022-06-25T02:45:00.000Z",
        "voteCount": 2,
        "content": "sorry, for the 41 upvotes.  Ans A can't be it.  Athena doesn't scan and identify schema.  Athena use the Glue Data Catalog, which is generated by Glue Crawler.\n\nMy answer: A,D,E."
      },
      {
        "date": "2021-11-24T13:36:00.000Z",
        "voteCount": 1,
        "content": "ADE is ans"
      },
      {
        "date": "2021-11-01T07:49:00.000Z",
        "voteCount": 5,
        "content": "A, D, E is the right answer"
      },
      {
        "date": "2021-10-31T03:05:00.000Z",
        "voteCount": 3,
        "content": "Yep ADE also.\nI guess SageMaker will use the data more than 1 time for learning processes"
      },
      {
        "date": "2021-10-19T06:45:00.000Z",
        "voteCount": 5,
        "content": "100% ADE"
      },
      {
        "date": "2021-10-16T13:53:00.000Z",
        "voteCount": 5,
        "content": "Notice that the answers given are paired so if you were to break it down:\nIdentify schema --&gt; Glue\nTransformations --&gt;  Glue Jobs\nArchived TBs worth of data --&gt; EMR\nSo is ADE"
      },
      {
        "date": "2021-10-06T17:45:00.000Z",
        "voteCount": 5,
        "content": "A, D and E."
      },
      {
        "date": "2021-09-25T09:14:00.000Z",
        "voteCount": 1,
        "content": "can glue handle 300GB data every day? It seems too much for glue."
      },
      {
        "date": "2022-06-25T02:47:00.000Z",
        "voteCount": 1,
        "content": "glue can.  you can properly size the glue cluster for the glue job with one simple dial."
      },
      {
        "date": "2021-10-11T12:47:00.000Z",
        "voteCount": 8,
        "content": "Absolutely! Glue can handle same amount of data as EMR because in the end Glue is a simplified EMR cluster with Spark, HDFS, YARN and the Glue dependencies but have the advantage of being serverless. Configuring the appropriate amount and type of DPUs you can handle 300GB of data"
      },
      {
        "date": "2021-10-29T01:04:00.000Z",
        "voteCount": 1,
        "content": "Thanks for describing the origion of EMR"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/28830-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A hospital uses wearable medical sensor devices to collect data from patients. The hospital is architecting a near-real-time solution that can ingest the data securely at scale. The solution should also be able to remove the patient's protected health information (PHI) from the streaming data and store the data in durable storage.<br>Which solution meets these requirements with the least operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data using Amazon Kinesis Data Streams, which invokes an AWS Lambda function using Kinesis Client Library (KCL) to remove all PHI. Write the data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data using Amazon Kinesis Data Firehose to write the data to Amazon S3. Have Amazon S3 trigger an AWS Lambda function that parses the sensor data to remove all PHI in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data using Amazon Kinesis Data Streams to write the data to Amazon S3. Have the data stream launch an AWS Lambda function that parses the sensor data and removes all PHI in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data using Amazon Kinesis Data Firehose to write the data to Amazon S3. Implement a transformation AWS Lambda function that parses the sensor data to remove all PHI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T01:55:00.000Z",
        "voteCount": 37,
        "content": "D;  transformation AWS Lambda function applied with stream data; before loading to s3"
      },
      {
        "date": "2021-09-25T14:32:00.000Z",
        "voteCount": 2,
        "content": "Changed to D"
      },
      {
        "date": "2021-09-26T15:00:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2022-05-24T06:28:00.000Z",
        "voteCount": 2,
        "content": "where does it say 'before' in option D ?"
      },
      {
        "date": "2022-01-28T00:06:00.000Z",
        "voteCount": 1,
        "content": "Why is it not B?"
      },
      {
        "date": "2022-03-06T19:36:00.000Z",
        "voteCount": 5,
        "content": "Because B removes the PHI *after* it is stored into S3. The question asks that PHI is removed from \"streaming data\", and it is also better practice to remove sensitive info before reaching storage"
      },
      {
        "date": "2023-05-01T08:09:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-11-27T03:22:00.000Z",
        "voteCount": 1,
        "content": "D as IOT rules can send info to Firehose using an action https://docs.aws.amazon.com/firehose/latest/dev/writing-with-iot.html"
      },
      {
        "date": "2022-11-07T05:54:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D as Kinesis Data Firehose can be used for data ingestion and storage to S3 with Lambda function for data filtering and transformation. This solution involves the least operational overhead.\n\nOption A is wrong as Kinesis Data Streams and KCLs involve operational overhead as Data Streams need to be provisioned and maintained.\n\nOption B is wrong as the solution removes the PHI after the data is stored.\n\nOption C is wrong as Kinesis Data Streams does not integrate with S3 directly and involves the operational overhead as Data Streams need to be provisioned and maintained."
      },
      {
        "date": "2022-10-29T06:58:00.000Z",
        "voteCount": 2,
        "content": "Ans is D. \nSolution need least operational overhead, so kinesis data stream is out. option B is also out bcoz it is removing PHI data after putting the data in S3. \nSo Option D is correct. Firehose will do the transformation via lambda to filter out the PHI data from stream and store the non-PHI in S3."
      },
      {
        "date": "2022-09-25T01:43:00.000Z",
        "voteCount": 1,
        "content": "AWS Kinesis Data Firehose is required as destination is S3. Also, Lambda function should be called as a transformation from Firehose before sending data to S3."
      },
      {
        "date": "2022-07-26T19:29:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-05-21T05:02:00.000Z",
        "voteCount": 2,
        "content": "My Answer is D"
      },
      {
        "date": "2021-11-05T06:17:00.000Z",
        "voteCount": 2,
        "content": "Answer D.\nIts true that the answer wordings are bad and confusing.\nKinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream."
      },
      {
        "date": "2021-11-03T19:35:00.000Z",
        "voteCount": 2,
        "content": "ANSWER D:\nEXPLAINATION:With the keyword 'near-real-time',option A and C are filtered out as KDS is real time streaming .NOw between option B &amp; D ,We already have transformation lambda attached to 'Firehose' by default  to do the necessary transformation ."
      },
      {
        "date": "2021-11-02T18:43:00.000Z",
        "voteCount": 1,
        "content": "Correct D\nKDF uses transformation (Lambda) before writing to S3\nIncorrect A\nImplementing Lambda to process and write data to S3 with streams is crazy. You should use KDF as KDS consumer. Shuld be least operational overhead.\nIncorrect B, C - security of data"
      },
      {
        "date": "2021-11-02T02:38:00.000Z",
        "voteCount": 1,
        "content": "Going with D, Even the reference in the answer also pointing the same (Reference:\nhttps://aws.amazon.com/blogs/big-data/persist-streaming-data-to-amazon-s3-using-amazon-kinesis-firehose-and-aws-lambda/)"
      },
      {
        "date": "2021-10-30T15:42:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-27T02:19:00.000Z",
        "voteCount": 1,
        "content": "Badly worded answer for D, but that's the correct answer here."
      },
      {
        "date": "2021-10-24T09:39:00.000Z",
        "voteCount": 1,
        "content": "Answer : D\n\nRefer : https://aws.amazon.com/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda/#:~:text=Introducing%20Firehose%20Data%20Transformations&amp;text=When%20you%20enable%20Firehose%20data,then%20delivered%20to%20the%20destination"
      },
      {
        "date": "2021-10-13T07:36:00.000Z",
        "voteCount": 2,
        "content": "I think D means that the data is cleaned already before writing to s3, but the formulation is bad."
      },
      {
        "date": "2021-10-21T10:16:00.000Z",
        "voteCount": 1,
        "content": "Agree, formulation is confusing.  I guess it means lambda transformation is applied before wrting to S3, so D is right."
      },
      {
        "date": "2021-10-07T20:51:00.000Z",
        "voteCount": 3,
        "content": "Answer is D.\nIt's near real-time so no need to use KDS\nA - task can be handled much easier (less complicated) way by D then A.\nB - PHI data is written to S3 before removal which is not acceptable.\nC - KDS cannot write data to S3."
      },
      {
        "date": "2021-10-08T05:04:00.000Z",
        "voteCount": 1,
        "content": "Agree, D"
      },
      {
        "date": "2021-10-15T12:21:00.000Z",
        "voteCount": 1,
        "content": "Hi Sanjay\n have you completed data analytic professional exam recently"
      },
      {
        "date": "2021-10-16T23:59:00.000Z",
        "voteCount": 2,
        "content": "disagree, I'll go A since firehose natually integrated with lambda."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/28897-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is migrating its existing on-premises ETL jobs to Amazon EMR. The code consists of a series of jobs written in Java. The company needs to reduce overhead for the system administrators without changing the underlying code. Due to the sensitivity of the data, compliance requires that the company use root device volume encryption on all nodes in the cluster. Corporate standards require that environments be provisioned though AWS CloudFormation when possible.<br>Which solution satisfies these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall open-source Hadoop on Amazon EC2 instances with encrypted root device volumes. Configure the cluster in the CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a CloudFormation template to launch an EMR cluster. In the configuration section of the cluster, define a bootstrap action to enable TLS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom AMI with encrypted root device volumes. Configure Amazon EMR to use the custom AMI using the CustomAmild property in the CloudFormation template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a CloudFormation template to launch an EMR cluster. In the configuration section of the cluster, define a bootstrap action to encrypt the root device volume of every node."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T15:50:00.000Z",
        "voteCount": 30,
        "content": "I think the answer is C\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticmapreduce-cluster.html"
      },
      {
        "date": "2021-09-30T20:07:00.000Z",
        "voteCount": 2,
        "content": "Agree with c"
      },
      {
        "date": "2021-10-28T05:28:00.000Z",
        "voteCount": 2,
        "content": "me too"
      },
      {
        "date": "2021-09-30T21:01:00.000Z",
        "voteCount": 3,
        "content": "Agreed"
      },
      {
        "date": "2021-12-20T21:03:00.000Z",
        "voteCount": 4,
        "content": "Agree C.\nIf you are using an Amazon EMR version earlier than 5.24.0, an encrypted EBS root device volume is supported only when using a custom AMI.\nFor  Amazon EMR version 5.24.0 and later, you can use a security configuration option to encrypt EBS root device and storage volumes when you specify AWS KMS as your key provider. \nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-localdisk"
      },
      {
        "date": "2021-11-07T01:17:00.000Z",
        "voteCount": 7,
        "content": "Agree with C. D is a trap, it is Security Configuration section not bootstrap action in Configuration section."
      },
      {
        "date": "2023-05-01T08:11:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-11-28T23:21:00.000Z",
        "voteCount": 1,
        "content": "I will go for D.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-localdisk\n\nAccording to the link, we can use EBS encryption from a security configuration and it says \"We recommend using EBS encryption\"."
      },
      {
        "date": "2022-11-07T05:56:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as CloudFormation can be used to launch an EMR cluster with custom AMI with encrypted root device volumes.\n\nOption A is wrong as open source Hadoop would not be provisioned using CloudFormation.\n\nOption B is wrong as TLS does not provide data at rest encryption.\n\nOption D is wrong as bootstrap actions cannot be used to encrypt root device volume."
      },
      {
        "date": "2022-12-27T09:16:00.000Z",
        "voteCount": 5,
        "content": "bro, can you tell if we just do first 80 questions , can we pass?"
      },
      {
        "date": "2022-09-25T01:45:00.000Z",
        "voteCount": 2,
        "content": "\"without changing the underlying code\" and \"CloudFomation Template\" are the keys here. So CustomAMIID for including a Custom AMI with encrypted root volume will work."
      },
      {
        "date": "2022-07-21T19:04:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer."
      },
      {
        "date": "2022-04-30T15:16:00.000Z",
        "voteCount": 1,
        "content": "Answer - C"
      },
      {
        "date": "2021-12-20T21:01:00.000Z",
        "voteCount": 2,
        "content": "If you are using an Amazon EMR version earlier than 5.24.0, an encrypted EBS root device volume is supported only when using a custom AMI. For more information, see Creating a custom AMI with an encrypted Amazon EBS root device volume in the Amazon EMR Management Guide\nBeginning with Amazon EMR version 5.24.0, you can use a security configuration option to encrypt EBS root device and storage volumes when you specify AWS KMS as your key provider. For more information, see Local disk encryption."
      },
      {
        "date": "2021-11-19T20:59:00.000Z",
        "voteCount": 1,
        "content": "Agree with c"
      },
      {
        "date": "2021-11-05T06:57:00.000Z",
        "voteCount": 3,
        "content": "C is the right answer"
      },
      {
        "date": "2021-11-03T15:28:00.000Z",
        "voteCount": 2,
        "content": "C sounds right but where in CF can you define a CustomAmild? Its imageID and that's it. An AMI is an AMI. For D to work, you would have to use a 3rd party software, but it would work"
      },
      {
        "date": "2021-11-04T18:23:00.000Z",
        "voteCount": 1,
        "content": "Scratch that, you can do CustomAmiID in an EMR cluster..... C is indeed the answer."
      },
      {
        "date": "2021-10-27T09:36:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-10-22T20:38:00.000Z",
        "voteCount": 1,
        "content": "Sensing answer should be C."
      },
      {
        "date": "2021-10-13T17:22:00.000Z",
        "voteCount": 2,
        "content": "Its C, you cant use bootstrap action to encrypt the root volume, you need to pass it using security configurations."
      },
      {
        "date": "2021-10-09T07:04:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-root-volume-property/\nAnswer is C"
      },
      {
        "date": "2021-10-07T09:28:00.000Z",
        "voteCount": 1,
        "content": "Confused between C and D."
      },
      {
        "date": "2021-10-08T17:43:00.000Z",
        "voteCount": 2,
        "content": "I think the bootstrap config is only for installing additional softwares https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-elasticmapreduce-cluster-bootstrapactionconfig.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/29562-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A transportation company uses IoT sensors attached to trucks to collect vehicle data for its global delivery fleet. The company currently sends the sensor data in small .csv files to Amazon S3. The files are then loaded into a 10-node Amazon Redshift cluster with two slices per node and queried using both Amazon Athena and Amazon Redshift. The company wants to optimize the files to reduce the cost of querying and also improve the speed of data loading into the Amazon<br>Redshift cluster.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to convert all the files from .csv to a single large Apache Parquet file. COPY the file into Amazon Redshift and query the file with Athena from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to convert each .csv file to Apache Avro. COPY the files into Amazon Redshift and query the file with Athena from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to convert the files from .csv to a single large Apache ORC file. COPY the file into Amazon Redshift and query the file with Athena from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to convert the files from .csv to Apache Parquet to create 20 Parquet files. COPY the files into Amazon Redshift and query the files with Athena from Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-11T16:02:00.000Z",
        "voteCount": 27,
        "content": "D, is the good answer. In fact each nodes have 2 slices so ideally we can parrelize the copy process by sending a multiple of 20."
      },
      {
        "date": "2021-10-28T17:09:00.000Z",
        "voteCount": 3,
        "content": "D for sure"
      },
      {
        "date": "2021-10-12T16:29:00.000Z",
        "voteCount": 8,
        "content": "Trick question. Since we have 10 nodes with 2 slices each, ideally a multiple of 20 files should help in the parallelize the Copy process. So D is the right answer."
      },
      {
        "date": "2023-11-17T04:11:00.000Z",
        "voteCount": 1,
        "content": "And why it is a trick question? IMO it's an obv hint."
      },
      {
        "date": "2023-05-01T08:12:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-11-07T05:58:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D as AWS Glue can be used to combine the .csv files to 20. parquet files. This would allow even processing across 2 slices. Also, multiple files help rapid loading of data to Redshift.\n\nOptions A &amp; C are wrong as single large file is not efficient as it would use only single slice.\n\n\nOption B is wrong as using Glue would be more cost effective as compared to EMR."
      },
      {
        "date": "2023-05-06T17:56:00.000Z",
        "voteCount": 1,
        "content": "I agree with your response - however EMR is more cost effective than Glue.. Glue is serverless while EMR is just a managed service."
      },
      {
        "date": "2022-10-29T07:41:00.000Z",
        "voteCount": 1,
        "content": "Option D is perfect solution to achieve both the requirements - to reduce the cost of querying (when we query on parquet files, lower the amount of data would be scanned which in turn reduce the cost ) and also improve the speed of data loading into the Amazon\nRedshift cluster(Split large files wherever possible to a number equal to a multiple of total number of slices. So here 20*n would be the correct splitting of the large data file.)."
      },
      {
        "date": "2022-09-25T01:48:00.000Z",
        "voteCount": 3,
        "content": "Best practices to Copy data from S3 to Redshift - \n1) Use Columnar data format. Which is Parquet/ORC.\n2) Split large files wherever possible to a number equal to a multiple of total number of slices. So here 20*n would be the correct splitting of the large data file."
      },
      {
        "date": "2022-08-20T15:28:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      },
      {
        "date": "2022-07-27T23:10:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-06-22T03:46:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer. 20 files = one per slice. If you use COPY on a dataset all the files will be divided over the available nodes/slices."
      },
      {
        "date": "2022-05-15T09:36:00.000Z",
        "voteCount": 1,
        "content": "Per the link https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-use-multiple-files.html , the answer shall be D, for the compressed files multiple copy commands for its slices adhere parallel load whereas for the delimited file a single copy command works better. Here in all the options the file compression is taking place , so option D seems the best choice here."
      },
      {
        "date": "2021-12-17T10:28:00.000Z",
        "voteCount": 1,
        "content": "Not sure D is correct or not but only reason i choose D since A and C are almost same and B won't work"
      },
      {
        "date": "2021-11-24T13:52:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2021-11-06T14:33:00.000Z",
        "voteCount": 1,
        "content": "D is the answer, but I doubt the total time will be shorter.The load will be quicker, shure, but it\u2019s not as if Spark reads CSV files quicker in any way, so all that you get is overhead. The Athena queries will run faster on fewer files, though, and if that wasthe focus this question would have made sense."
      },
      {
        "date": "2021-11-01T11:55:00.000Z",
        "voteCount": 1,
        "content": "Is Athena query cheaper than the existing redshift query?"
      },
      {
        "date": "2021-10-31T20:53:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-25T09:10:00.000Z",
        "voteCount": 2,
        "content": "D is correct answer."
      },
      {
        "date": "2021-10-20T19:11:00.000Z",
        "voteCount": 1,
        "content": "is D ok? In understand you should avoid multiple concurrency copy commands\n\"We strongly recommend using the COPY command to load large amounts of data. Using individual INSERT statements to populate a table might be prohibitively slow. Alternatively, if your data already exists in other Amazon Redshift database tables, use INSERT INTO ... SELECT or CREATE TABLE AS to improve performance. For information, see INSERT or CREATE TABLE AS\".\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Loading_tables_with_the_COPY_command.html"
      },
      {
        "date": "2022-03-26T04:44:00.000Z",
        "voteCount": 2,
        "content": "Single COPY command loads multiple files into Redshift in parallel"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/73879-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online retail company with millions of users around the globe wants to improve its ecommerce analytics capabilities. Currently, clickstream data is uploaded directly to Amazon S3 as compressed files. Several times each day, an application running on Amazon EC2 processes the data and makes search options and reports available for visualization by editors and marketers. The company wants to make website clicks and aggregated data available to editors and marketers in minutes to enable them to connect with users more effectively.<br>Which options will help meet these requirements in the MOST efficient way? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to upload compressed and batched clickstream records to Amazon OpenSearch Service (Amazon Elasticsearch Service).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload clickstream records to Amazon S3 as compressed files. Then use AWS Lambda to send data to Amazon OpenSearch Service (Amazon Elasticsearch Service) from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon OpenSearch Service (Amazon Elasticsearch Service) deployed on Amazon EC2 to aggregate, filter, and process the data. Refresh content performance dashboards in near-real time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse OpenSearch Dashboards (Kibana) to aggregate, filter, and visualize the data stored in Amazon OpenSearch Service (Amazon Elasticsearch Service). Refresh content performance dashboards in near-real time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload clickstream records from Amazon S3 to Amazon Kinesis Data Streams and use a Kinesis Data Streams consumer to send records to Amazon OpenSearch Service (Amazon Elasticsearch Service)."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T07:28:00.000Z",
        "voteCount": 10,
        "content": "OpenSearch can ingest from KDF and results not in real-time but in minutes, as such, KDF can still be used."
      },
      {
        "date": "2023-07-24T03:25:00.000Z",
        "voteCount": 1,
        "content": "AD for sure."
      },
      {
        "date": "2023-05-01T08:13:00.000Z",
        "voteCount": 3,
        "content": "AD: I passed the test"
      },
      {
        "date": "2022-11-07T06:00:00.000Z",
        "voteCount": 4,
        "content": "Correct answers are A &amp; D as Kinesis Data Firehose can be used for data ingestion with its micro batching to push the data directly to Elastic Search. Kibana can be used for visualization.\n\nOptions B, C &amp; E are wrong as they are not the MOST efficient way."
      },
      {
        "date": "2022-10-29T07:58:00.000Z",
        "voteCount": 3,
        "content": "KDF can ingest that data directly to Opensearch ( Still KDF has 60 sec buffer time, they want results in minutes. so this is fine). Choose opensearch dashboards for visualization when it comes to time-sensitive.\n\nhere, some people choosing B over A. My explanation why we can ignore B is - they want result in minutes not real-time. So KDF is sufficient and will replace the whole process of streaming the data to s3 and then loading the data to OpenSearch via lambda."
      },
      {
        "date": "2022-09-25T01:51:00.000Z",
        "voteCount": 2,
        "content": "\"in minutes\" - this is the key here. Always choose OpenSearch over Athena+Quicksight, in case of time sensitivity."
      },
      {
        "date": "2022-08-15T06:26:00.000Z",
        "voteCount": 2,
        "content": "B and D.  \n(Reason for B as opposed to A:  Firehose has minimum 1 min delay. Lambda will be \"instantly\".  Question )"
      },
      {
        "date": "2022-07-25T21:16:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: AD"
      },
      {
        "date": "2022-06-05T15:09:00.000Z",
        "voteCount": 1,
        "content": "AD IS THE ANSWER"
      },
      {
        "date": "2022-05-22T01:33:00.000Z",
        "voteCount": 3,
        "content": "Answer should be A &amp; D"
      },
      {
        "date": "2022-05-21T20:36:00.000Z",
        "voteCount": 2,
        "content": "B and D. AWS Lambda can be triggered for s3 events generated while copying to s3. And then it loads to OpenSearch in near real time. Then use Kibana for visualization and search requirement."
      },
      {
        "date": "2022-08-15T06:26:00.000Z",
        "voteCount": 2,
        "content": "B and D.\n(Reason for B as opposed to A: Firehose has minimum 1 min delay. Lambda will be \"instantly\". Question )"
      },
      {
        "date": "2022-05-21T04:49:00.000Z",
        "voteCount": 1,
        "content": "My Answer is D &amp; E"
      },
      {
        "date": "2022-05-22T01:33:00.000Z",
        "voteCount": 1,
        "content": "No A &amp; D I meant"
      },
      {
        "date": "2022-04-30T16:35:00.000Z",
        "voteCount": 1,
        "content": "Answer - A,D"
      },
      {
        "date": "2022-04-20T10:04:00.000Z",
        "voteCount": 1,
        "content": "A, D most efficient and direct to destination. B instead of A feasible but uses S3. But qn also says data stored in S3 - assume for the best approach to A, D"
      },
      {
        "date": "2022-04-20T04:25:00.000Z",
        "voteCount": 2,
        "content": "A, D - firehose direct to ES; Kibana built in"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/28622-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is streaming its high-volume billing data (100 MBps) to Amazon Kinesis Data Streams. A data analyst partitioned the data on account_id to ensure that all records belonging to an account go to the same Kinesis shard and order is maintained. While building a custom consumer using the Kinesis Java SDK, the data analyst notices that, sometimes, the messages arrive out of order for account_id. Upon further investigation, the data analyst discovers the messages that are out of order seem to be arriving from different shards for the same account_id and are seen when a stream resize runs.<br>What is an explanation for this behavior and what is the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are multiple shards in a stream and order needs to be maintained in the shard. The data analyst needs to make sure there is only a single shard in the stream and no stream resize runs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe hash key generation process for the records is not working correctly. The data analyst should generate an explicit hash key on the producer side so the records are directed to the appropriate shard accurately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe records are not being received by Kinesis Data Streams in order. The producer should use the PutRecords API call instead of the PutRecord API call with the SequenceNumberForOrdering parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe consumer is not processing the parent shard completely before processing the child shards after a stream resize. The data analyst should process the parent shard completely first before processing the child shards.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-13T12:33:00.000Z",
        "voteCount": 80,
        "content": "D:https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-after-resharding.html\n the parent shards that remain after the reshard could still contain data that you haven't read yet that was added to the stream before the reshard. If you read data from the child shards before having read all data from the parent shards, you could read data for a particular hash key out of the order given by the data records' sequence numbers. Therefore, assuming that the order of the data is important, you should, after a reshard, always continue to read data from the parent shards until it is exhausted. Only then should you begin reading data from the child shards."
      },
      {
        "date": "2023-05-01T08:15:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-11-07T06:02:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D as Kinesis Data Streams resharding causes the existing data to be in parent shard and new data is routed to child shards. The issue can occur if the consumers starts processing the child shard without completely processing the parent shard."
      },
      {
        "date": "2022-05-21T05:30:00.000Z",
        "voteCount": 2,
        "content": "Answer is D for sure"
      },
      {
        "date": "2021-11-24T14:02:00.000Z",
        "voteCount": 1,
        "content": "D is ans"
      },
      {
        "date": "2021-11-03T07:07:00.000Z",
        "voteCount": 1,
        "content": "Answer D."
      },
      {
        "date": "2021-10-27T20:02:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-26T21:06:00.000Z",
        "voteCount": 2,
        "content": "Answer should be D."
      },
      {
        "date": "2021-10-19T01:28:00.000Z",
        "voteCount": 3,
        "content": "Nicely explained by Priyanka_01."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/74030-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A media analytics company consumes a stream of social media posts. The posts are sent to an Amazon Kinesis data stream partitioned on user_id. An AWS<br>Lambda function retrieves the records and validates the content before loading the posts into an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. The validation process needs to receive the posts for a given user in the order they were received by the Kinesis data stream.<br>During peak hours, the social media posts take more than an hour to appear in the Amazon OpenSearch Service (Amazon ES) cluster. A data analytics specialist must implement a solution that reduces this latency with the least possible operational overhead.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the validation process from Lambda to AWS Glue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Lambda consumers from standard data stream iterators to an HTTP/2 stream consumer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in the Kinesis data stream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the posts stream to Amazon Managed Streaming for Apache Kafka instead of the Kinesis data stream."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-07-02T04:22:00.000Z",
        "voteCount": 12,
        "content": "Increasing the number of shards seems to be a good idea since Lambda can process 1 batch of data from each Kinesis shard with 1 lambda invocation. This means that if you have 100 shards you can have 100 concurrent lambda invocations. If you increase the number of shards you can increase the parallelism and you could be quicker to process the data. This is assuming that the Lambda ParallelizationFactor is set to 1.\nSwitching to AWS Glue could increase the speed of the data processing (since Glue can use Spark, which can be way faster than a Lambda function when processing a lot of data) but this would increase the operational overhead."
      },
      {
        "date": "2024-02-22T00:31:00.000Z",
        "voteCount": 1,
        "content": "For those wondering why not go for 'A', and go for 'C' instead: in glue worker types such as G.1X, G.2X, etc must be selected which increases overhead. Hence, the one with least overhead is 'C' by using the concept of parallelism."
      },
      {
        "date": "2024-01-11T13:25:00.000Z",
        "voteCount": 2,
        "content": "A: is wrong because glue has concurrency limit and spark is poor option to small files;\nB: if lambda is a unique consumer, dont have necessity to upgrade these for enhanced-fan-out;\nC: this is a tipically bottleneck problem... to solve this just insert more shards;\nD: is wrong because switch SaaS have A LOT OF operational overhead"
      },
      {
        "date": "2023-08-10T06:46:00.000Z",
        "voteCount": 2,
        "content": "I want to contribute in this Question and telling you why B isn't correct and C yes.\nOption B is useless because the question never tell that there is another consumer, so lambda is leveraging the shard throughput (there is no need to set enhanced fan-out consumer).\nIncrementing shard will work, because in AWS there's 1 lambda function invocation per kinesis shard. At the same time, per shard you can increase lambda functions concurrency with the ParallelizationFactor set to a name between 1 (default value) to 10."
      },
      {
        "date": "2023-08-03T19:32:00.000Z",
        "voteCount": 2,
        "content": "Could be C or B depending on multiple factors.\nTo increase the prformance of KDS you have 3 options : \n- More shards.\n- Parallelization factors (specific to Lambda)\n- HTTP/2\n- Enhanced Fan-out."
      },
      {
        "date": "2023-07-10T10:48:00.000Z",
        "voteCount": 3,
        "content": "B seems to be correct. As ordering has to be maintained, Lambda function will be handy only if the consumer is KCL because it has that inbuilt sorting logic for parent and child shard."
      },
      {
        "date": "2023-05-27T12:12:00.000Z",
        "voteCount": 1,
        "content": "C can never be right - increasing shards cannot assure ordering and thats the catch here. B seems close."
      },
      {
        "date": "2023-08-03T19:28:00.000Z",
        "voteCount": 3,
        "content": "If you partition by user_id, it does guarantee order, since only the same user_id go to the same shard."
      },
      {
        "date": "2023-08-03T19:30:00.000Z",
        "voteCount": 3,
        "content": "The stream is partitioned by user_id, increasing the number of shards won't impact the ordering of records for a specific user because all posts from a particular user would go to the same shard."
      },
      {
        "date": "2023-05-01T08:15:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-25T15:45:00.000Z",
        "voteCount": 2,
        "content": "Answer  B\nbased on below link\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/aws-lambda-supports-kinesis-data-streams-enhanced-fan-out-and-http2/"
      },
      {
        "date": "2023-02-17T06:25:00.000Z",
        "voteCount": 3,
        "content": "option B- Migrating the Lambda consumers to an HTTP/2 stream consumer can significantly reduce processing latency and improve the overall performance of the system. This is because HTTP/2 stream consumers allow Lambda to retrieve records from the stream more efficiently, which can help to reduce processing latency and improve the overall performance of the system.\n\nMigrating the Lambda consumers to an HTTP/2 stream consumer can significantly reduce processing latency and improve the overall performance of the system. This is because HTTP/2 stream consumers allow Lambda to retrieve records from the stream more efficiently, which can help to reduce processing latency and improve the overall performance of the system.\n\nMigrating to an HTTP/2 stream consumer requires minimal operational overhead, as it only involves updating the Lambda function code to use the new consumer type. This can be done easily using the AWS SDK for Lambda, and does not require any major changes to the existing architecture.\n\nTherefore, option B is the best solution for reducing the latency with the least possible operational overhead."
      },
      {
        "date": "2023-04-21T16:53:00.000Z",
        "voteCount": 2,
        "content": "Increasing shards is easier than enhanced fan-out and cheaper too."
      },
      {
        "date": "2023-01-16T03:37:00.000Z",
        "voteCount": 1,
        "content": "C is a temporary solution, as there is no idea of the expected number of shards you need to increase. There is no simple auto-scaling in Kinesis, so there will be an operational overhead to continuously monitor the system and increase the number of shards. In addition, the partitioning-sharding is according to user_id - how this can be solved?\n\nB - the enhanced fanout approach is good for it, as described here:\n\"The enhanced capacity enables you to achieve higher outbound throughput without provisioning more streams or shards in the same stream.\"\nhttps://aws.amazon.com/blogs/compute/increasing-real-time-stream-processing-performance-with-amazon-kinesis-data-streams-enhanced-fan-out-and-aws-lambda/"
      },
      {
        "date": "2022-09-25T01:55:00.000Z",
        "voteCount": 1,
        "content": "\"least possible operational overhead\" - This is the key here. As the solution demands to reduce latency, this will be the easiest way to do so. Notice, that cost factor is not mentioned in the question."
      },
      {
        "date": "2022-07-20T23:03:00.000Z",
        "voteCount": 1,
        "content": "Increasing the number of shards looks ok."
      },
      {
        "date": "2022-06-30T16:27:00.000Z",
        "voteCount": 4,
        "content": "I go with B for these two reasons.\n1. Messages should be received in same order for user. Scaling out the shards during peak hours and scaling in after peak hours may change the message order. C is not the correct one.\n2. HTTP/2 is enhanced fan out consumer which will reduce the latency from 200ms to 70ms. 65% latency reduction"
      },
      {
        "date": "2022-05-21T00:43:00.000Z",
        "voteCount": 2,
        "content": "C, but you must ensure that you use Partition Keys (In this case the User) to ensure the requested ordering per User. HTTP/2 would also decrease latency but needs more effort"
      },
      {
        "date": "2022-04-30T13:48:00.000Z",
        "voteCount": 2,
        "content": "C - Increase Shards"
      },
      {
        "date": "2022-04-28T04:42:00.000Z",
        "voteCount": 3,
        "content": "I think B \nstandard consumer latency = 200ms\nHttp/2 latency =  70ms"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/28554-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company launched a service that produces millions of messages every day and uses Amazon Kinesis Data Streams as the streaming service.<br>The company uses the Kinesis SDK to write data to Kinesis Data Streams. A few months after launch, a data analyst found that write performance is significantly reduced. The data analyst investigated the metrics and determined that Kinesis is throttling the write requests. The data analyst wants to address this issue without significant changes to the architecture.<br>Which actions should the data analyst take to resolve this issue? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Kinesis Data Streams retention period to reduce throttling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the Kinesis API-based data ingestion mechanism with Kinesis Agent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in the stream using the UpdateShardCount API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose partition keys in a way that results in a uniform record distribution across shards.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustomize the application code to include retry logic to improve performance."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T16:19:00.000Z",
        "voteCount": 33,
        "content": "CD. If wrong partition keys are distributed well, then retrying would still hit the hot shards.\nhttps://aws.amazon.com/blogs/big-data/under-the-hood-scaling-your-kinesis-data-streams/"
      },
      {
        "date": "2021-10-01T23:38:00.000Z",
        "voteCount": 2,
        "content": "agreed."
      },
      {
        "date": "2021-10-04T07:35:00.000Z",
        "voteCount": 1,
        "content": "D i agree with because of this other link https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-stream-throttling-errors/  \"use a random partition key to ingest your records. If the operations already use a random partition key, then adjust the key to correct the distribution.\"  Why would you want to increase the number of shards, is that in the link?"
      },
      {
        "date": "2021-10-02T15:47:00.000Z",
        "voteCount": 7,
        "content": "C and D for me."
      },
      {
        "date": "2023-08-03T19:42:00.000Z",
        "voteCount": 4,
        "content": "More shards and better partitioning is always the answer for write performance issues."
      },
      {
        "date": "2023-05-01T08:16:00.000Z",
        "voteCount": 1,
        "content": "CD: I passed the test"
      },
      {
        "date": "2023-01-07T13:37:00.000Z",
        "voteCount": 2,
        "content": "Why no B? Kinesis should have better performance than KPL"
      },
      {
        "date": "2022-11-07T06:05:00.000Z",
        "voteCount": 3,
        "content": "Correct answers are C &amp; D as the common causes are hitting shard limits and. increasing shard count and choosing an appropriate partition key would help solve the issue."
      },
      {
        "date": "2022-10-30T01:16:00.000Z",
        "voteCount": 3,
        "content": "throttling writes mean there is no sufficient shards. so we need to increase the shard counts. Introducing partition key helps in uniform distribution of writes across the shards which in turn we can avoid hot shards"
      },
      {
        "date": "2022-09-25T01:57:00.000Z",
        "voteCount": 1,
        "content": "None of the other options makes sense."
      },
      {
        "date": "2022-07-25T21:40:00.000Z",
        "voteCount": 1,
        "content": "answer is CD"
      },
      {
        "date": "2022-05-21T04:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is C &amp; D"
      },
      {
        "date": "2021-11-24T14:08:00.000Z",
        "voteCount": 1,
        "content": "C and D"
      },
      {
        "date": "2021-11-05T17:52:00.000Z",
        "voteCount": 1,
        "content": "C,D is the answer."
      },
      {
        "date": "2021-11-04T00:50:00.000Z",
        "voteCount": 1,
        "content": "Considering the link bellow the option E isn't a valid option. \"If there is a retry mechanic in the producer, failed records are tried again. This can also cause a delay in processing.\"\nhttps://aws.amazon.com/pt/premiumsupport/knowledge-center/kinesis-data-stream-throttling/"
      },
      {
        "date": "2021-10-30T18:56:00.000Z",
        "voteCount": 3,
        "content": "C,D is the right answer"
      },
      {
        "date": "2021-10-27T07:46:00.000Z",
        "voteCount": 5,
        "content": "When there are failed records that aren't able to enter the Kinesis data stream, the stream throttles. If there is a retry mechanic in the producer, failed records are tried again. This can also cause a delay in processing.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-stream-throttling/\n\nSo an existing retry logic can also result in throttling. I think C&amp;D will make more sense as they are meant for performance increase. Retry logic is more for handling errors and not handling performance."
      },
      {
        "date": "2021-10-26T18:46:00.000Z",
        "voteCount": 6,
        "content": "C and D for me.\n\"A few months after launch, a data analyst found that write performance is significantly reduced\" means the load is increasing constantly and will not decrease, and it's not a temporary spike. So we are facing a lack of shard or bad partition key is burning some shards over time"
      },
      {
        "date": "2021-10-16T18:40:00.000Z",
        "voteCount": 1,
        "content": "It's C &amp; E for me. D is not an option because changing the partition key affects the architecture."
      },
      {
        "date": "2022-05-06T20:57:00.000Z",
        "voteCount": 2,
        "content": "\"changing the partition key\" just change the code to use different key during putting item. No architecture changed."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/29252-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A smart home automation company must efficiently ingest and process messages from various connected devices and sensors. The majority of these messages are comprised of a large number of small files. These messages are ingested using Amazon Kinesis Data Streams and sent to Amazon S3 using a Kinesis data stream consumer application. The Amazon S3 message data is then passed through a processing pipeline built on Amazon EMR running scheduled PySpark jobs.<br>The data platform team manages data processing and is concerned about the efficiency and cost of downstream data processing. They want to continue to use<br>PySpark.<br>Which solution improves the efficiency of the data processing jobs and is well architected?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the sensor and devices data directly to a Kinesis Data Firehose delivery stream to send the data to Amazon S3 with Apache Parquet record format conversion enabled. Use Amazon EMR running PySpark to process the data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Lambda function with a Python runtime environment. Process individual Kinesis data stream messages from the connected devices and sensors using Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon Redshift cluster. Copy the collected data from Amazon S3 to Amazon Redshift and move the data processing jobs from Amazon EMR to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Glue Python jobs to merge the small data files in Amazon S3 into larger files and transform them to Apache Parquet format. Migrate the downstream PySpark jobs from Amazon EMR to AWS Glue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-01T21:15:00.000Z",
        "voteCount": 28,
        "content": "D seems the good choice because is the only answer dealing with small files but the doubt is... Glue is only batch! but there is a new article of 04/20 that says glue is now also supporting streaming process. So if we consider this article D is right. But we can?\nhttps://aws.amazon.com/it/about-aws/whats-new/2020/04/aws-glue-now-supports-serverless-streaming-etl/"
      },
      {
        "date": "2021-10-15T22:20:00.000Z",
        "voteCount": 2,
        "content": "In question, it is not mentioned that they definitely need a real-time solution. And Glue can be used for batch processing of stream data."
      },
      {
        "date": "2021-11-06T01:02:00.000Z",
        "voteCount": 1,
        "content": "they already use Firehouse that does data batching, so no problem with D"
      },
      {
        "date": "2022-01-28T00:14:00.000Z",
        "voteCount": 2,
        "content": "Since question is searching for an answer that reflects operational efficiency, setting up individual glue jobs is definitely more time consuming so the answer is A"
      },
      {
        "date": "2022-01-28T00:15:00.000Z",
        "voteCount": 2,
        "content": "Plus, AWS Kinesis Data Firehose has in-house data transformation so you don\u2019t need to add operational overhead by utilizing AWS Lambda"
      },
      {
        "date": "2023-06-22T11:55:00.000Z",
        "voteCount": 2,
        "content": "No, the question asks for a solution that replaces ETL with Pyspark in EMR cluster indeed, so AWS GLUE ETL jobs would be a good choice here.\nUndoubtedly, D is the correct option, but option A is not as bad as seems, but maybe it's not as cheap as D."
      },
      {
        "date": "2021-09-25T22:28:00.000Z",
        "voteCount": 23,
        "content": "Anas: A\n\nhttps://aws.amazon.com/blogs/big-data/optimizing-downstream-data-processing-with-amazon-kinesis-data-firehose-and-amazon-emr-running-apache-spark/"
      },
      {
        "date": "2021-09-27T06:22:00.000Z",
        "voteCount": 1,
        "content": "Should this not be D? Where are we handling the small files in A?"
      },
      {
        "date": "2021-10-08T06:16:00.000Z",
        "voteCount": 1,
        "content": "You are changing them to parquet on the fly with Firehose."
      },
      {
        "date": "2022-11-06T12:39:00.000Z",
        "voteCount": 2,
        "content": "Firehose will batch in buffer time and reduce number of files."
      },
      {
        "date": "2021-10-02T11:22:00.000Z",
        "voteCount": 8,
        "content": "Remember \"They want to continue to use PySpark.\"\nD is migrating PySpark into Glue"
      },
      {
        "date": "2021-10-12T07:00:00.000Z",
        "voteCount": 2,
        "content": "This part of the question is what many ppl here have missed. You\u2019re right. It\u2019s A"
      },
      {
        "date": "2022-10-24T09:32:00.000Z",
        "voteCount": 2,
        "content": "There is no problem in using pyspark with Glue."
      },
      {
        "date": "2024-02-21T15:09:00.000Z",
        "voteCount": 2,
        "content": "Converting data into Apache Parquet format before storing it in S3 optimizes the data for analytical processing. KDF able to automatically batch, compress, and convert incoming streaming data into Parquet format. It reduces the overhead with processing a large number of small files without the need for additional processing or intermediate steps. And it allows the team to continue using PySpark on Amazon EMR for data processing.\nB - AWS Lambda to process individual messages could introduce operational overhead and not efficiently handle the conversion of a large number of small files.\nC - moving data processing to Redshift would require changes to the existing PySpark-based processing pipeline and not most cost-effective solution.\n D - merging small files into larger ones using Glue addresses the efficiency concern, it suggests migrating PySpark jobs from EMR to AWS Glue could involve refactoring of the existing jobs."
      },
      {
        "date": "2024-01-11T13:37:00.000Z",
        "voteCount": 1,
        "content": "emr is very expansive and spark doesn't work well with a large number of small files... the best option is to merge small files into large files and use job glue to decrease the cost of downstream processing... D is a perfect answer"
      },
      {
        "date": "2023-12-25T16:29:00.000Z",
        "voteCount": 1,
        "content": "take a look at this sentence \"...the solution needs to be well-architected\"... that is, cost-efficient, secure, highly available and operational-efficient... aws emr are not highly available and need a lot of operational resources... then disagree emr... to continue pyspark job, aws glue are the best option"
      },
      {
        "date": "2023-08-03T19:49:00.000Z",
        "voteCount": 2,
        "content": "D solves the issue with small files and replaces the EMR batch job with a Glue one, which is cheaper.\nIf a Transient EMR cluster was in the A proposition, it would be acceptable."
      },
      {
        "date": "2023-05-27T12:28:00.000Z",
        "voteCount": 1,
        "content": "Both A and D correct technically and very difficult to figure out the cost effective solution without more context. Other answers assuming EMR is a long running and is expensive but thats not mentioned here. D has upper hand considering all will be serverless."
      },
      {
        "date": "2023-05-01T08:18:00.000Z",
        "voteCount": 3,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-24T07:01:00.000Z",
        "voteCount": 1,
        "content": "\"cost of downstream data processing\" so migrate it"
      },
      {
        "date": "2023-01-07T13:47:00.000Z",
        "voteCount": 4,
        "content": "This question is testing if you know Glue can run PySpark job"
      },
      {
        "date": "2023-01-07T08:28:00.000Z",
        "voteCount": 1,
        "content": "D for sue"
      },
      {
        "date": "2023-01-02T02:04:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-12-12T08:50:00.000Z",
        "voteCount": 1,
        "content": "glue cheaper than emr"
      },
      {
        "date": "2022-11-29T19:29:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D :- the option that says: Replace the Amazon EMR with AWS Glue. Program an AWS Glue ETL script in Python to merge the small sensor data into larger files and convert them to Apache Parquet format.\n\nThe option that says: Deploy a Kinesis Data Firehose delivery stream to collect and convert sensor data to Apache Parquet format. Deliver the transformed data into an Amazon S3 bucket. Process the data from the bucket using a PySpark Job running on an Amazon EMR cluster is incorrect. Although this option is valid, there is no significant cost reduction since Amazon EMR is still running. AWS Glue can provide lower costs while providing the same function. In addition, it is better to merge the smaller files to a large file, than just compressing them using the Apache Parquet format to improve ingestion performance."
      },
      {
        "date": "2022-11-27T04:46:00.000Z",
        "voteCount": 1,
        "content": "The question is about the data processing and not the full pipeline including ingestion so D is the most efficient from processing perspective"
      },
      {
        "date": "2022-11-26T23:20:00.000Z",
        "voteCount": 1,
        "content": "Glue can run PySpark"
      },
      {
        "date": "2022-11-13T03:13:00.000Z",
        "voteCount": 1,
        "content": "Lambda can run pyspark and is cost effective and serverless meaning well architectured."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/28556-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large financial company is running its ETL process. Part of this process is to move data from Amazon S3 into an Amazon Redshift cluster. The company wants to use the most cost-efficient method to load the dataset into Amazon Redshift.<br>Which combination of steps would meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the COPY command with the manifest file to load data into Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3DistCp to load files into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse temporary staging tables during the loading process.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the UNLOAD command to upload data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift Spectrum to query files from Amazon S3."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T07:38:00.000Z",
        "voteCount": 31,
        "content": "A &amp; C\nCopy command and loading into temp staging tables"
      },
      {
        "date": "2021-09-26T10:17:00.000Z",
        "voteCount": 14,
        "content": "A and c, because the goal is move data from s3 to redshift, and in the E we are not moving."
      },
      {
        "date": "2023-05-27T12:30:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C. But If you are going to appear exam in near future  - redshift auto copy is now a new no-ETL feature and may replace these options."
      },
      {
        "date": "2023-05-01T10:02:00.000Z",
        "voteCount": 1,
        "content": "AC: I passed the test"
      },
      {
        "date": "2022-11-07T06:51:00.000Z",
        "voteCount": 8,
        "content": "Correct answers are A &amp; C.\n\nOption B is wrong as S3DistCp is used to copy data between S3 and HDFS.\n\nOption D is wrong as UNLOAD helps unloading the data from Redshift to S3.\n\nOption E is wrong as Redshift Spectrum does not load the data into Redshift, but the requirement is to load."
      },
      {
        "date": "2022-11-07T06:51:00.000Z",
        "voteCount": 4,
        "content": "Option A as the COPY command loads data in parallel from Amazon S3, Amazon EMR, Amazon DynamoDB, or multiple data sources on remote hosts. COPY loads large amounts of data much more efficiently than using INSERT statements, and stores the data more effectively as well. Amazon S3 provides eventual consistency for some operations. Thus, it's possible that new data won't be available immediately after the upload, which can result in an incomplete data load or loading stale data. You can manage data consistency by using a manifest file to load data\n\nOption C as you can efficiently update and insert new data by loading your data into a staging table first. Amazon Redshift doesn't support a single merge statement (update or insert, also known as an upsert) to insert and update data from a single data source. However, you can effectively perform a merge operation. To do so, load your data into a staging table and then join the staging table with your target table for an UPDATE statement and an INSERT statement."
      },
      {
        "date": "2022-08-24T02:41:00.000Z",
        "voteCount": 1,
        "content": "B is not correct because its used with EMR. D is not correct because UNLOAD is used to put data from Redshift to S3. C seems to be involve lot of work, but E does not allow to move data to Redshift but the organization requires that and A is anyway correct. So I would go with A nd C"
      },
      {
        "date": "2022-07-20T23:34:00.000Z",
        "voteCount": 1,
        "content": "A, C are correct"
      },
      {
        "date": "2022-05-22T01:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is A &amp; C"
      },
      {
        "date": "2022-04-30T14:32:00.000Z",
        "voteCount": 1,
        "content": "Answer - A,C"
      },
      {
        "date": "2021-11-19T19:48:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2021-11-05T17:53:00.000Z",
        "voteCount": 1,
        "content": "A, C are correct"
      },
      {
        "date": "2021-10-31T10:42:00.000Z",
        "voteCount": 2,
        "content": "A,C is the right answer"
      },
      {
        "date": "2021-10-30T19:30:00.000Z",
        "voteCount": 10,
        "content": "https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\nPoint number 1 and 2. Option A and  C must be the answer"
      },
      {
        "date": "2022-06-16T00:59:00.000Z",
        "voteCount": 1,
        "content": "Point 5 is also important to note in the article mentioned by Subho_in.\nAlso look at this why to use Staging tables: https://docs.aws.amazon.com/redshift/latest/dg/merge-create-staging-table.html"
      },
      {
        "date": "2021-10-16T04:21:00.000Z",
        "voteCount": 1,
        "content": "I disagree with C. Question is about Loading data. Staging tables is about Transformation. It's A and E for me."
      },
      {
        "date": "2022-08-17T02:31:00.000Z",
        "voteCount": 1,
        "content": "\"The organization want to load the dataset onto Amazon Redshift\". answer E is not moving any data not does help with it"
      },
      {
        "date": "2021-10-15T23:39:00.000Z",
        "voteCount": 2,
        "content": "It's asking a \"combination of steps\", so they are A and C.."
      },
      {
        "date": "2021-10-12T11:30:00.000Z",
        "voteCount": 2,
        "content": "A and C"
      },
      {
        "date": "2021-10-10T06:10:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C for sure; the rest are clearly wrong"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/28542-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A university intends to use Amazon Kinesis Data Firehose to collect JSON-formatted batches of water quality readings in Amazon S3. The readings are from 50 sensors scattered across a local lake. Students will query the stored data using Amazon Athena to observe changes in a captured metric over time, such as water temperature or acidity. Interest has grown in the study, prompting the university to reconsider how data will be stored.<br>Which data format and partitioning choices will MOST significantly reduce costs? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Apache Avro format using Snappy compression.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by year, month, and day.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Apache ORC format using no compression.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Apache Parquet format using Snappy compression.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by sensor, year, month, and day."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T23:40:00.000Z",
        "voteCount": 44,
        "content": "D :can save from 30% to 90% on your per-query costs and get better performance by compressing, partitioning, and converting your data into columnar formats.\nB: For partition"
      },
      {
        "date": "2021-10-27T12:14:00.000Z",
        "voteCount": 30,
        "content": "B and D are the right answers.\n\nSome background: Snappy compresses the data to help with I/O, it roughly does the same level of compression  for both parquet and AVRO. AVRO stores the data in row format and does not compresses the data. However, Parquet is a columnar store (without any additional compression algorithm like snappy applied), it natively compresses the data by 2X to 5X on average.  \n\nA) Since Parquet does a better job in compression, this option is incorrect\nB) This is correct since data is partitioned with keys (year, month, day) with medium cardinality. \nC) Even though ORC and Parquet are both columnar storage formats and both supported by Athena, Since no compression is used in this option, we can safely ignore this. \nD) Parquet with Snappy is a better choice than ORC with no compression, so this is correct. \nE) Adding sensor(ID) to the partition creates high cardinality on the partitions and may lead to multiple small files under each partition which will slow down performance. So, B is a better option as you can keep all 50 sensor data in a single file for a day."
      },
      {
        "date": "2023-06-30T13:28:00.000Z",
        "voteCount": 1,
        "content": "I found this at this link:\n\nColumns that are used as filters are good candidates for partitioning.\nPartitioning has a cost. As the number of partitions in your table increases, the higher the overhead of retrieving and processing the partition metadata, and the smaller your files. Partitioning too finely can wipe out the initial benefit.\n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\n\nSo I'd also go with B!"
      },
      {
        "date": "2024-03-21T15:26:00.000Z",
        "voteCount": 1,
        "content": "In the question it is mentioned that \"Athena to observe changes in a captured metric over time, such as water temperature or acidity.\"\n\nNo signs of using a specific sensor and then observe metrics.\nSo if we introduce sensor in partition and not filter it in the query you are introducing an additional partition to search.\nThe request of the question it makes sense that we use B"
      },
      {
        "date": "2023-08-03T19:55:00.000Z",
        "voteCount": 1,
        "content": "BD : Makes the most sense"
      },
      {
        "date": "2023-05-01T10:05:00.000Z",
        "voteCount": 1,
        "content": "DE: I passed the test"
      },
      {
        "date": "2023-12-25T16:41:00.000Z",
        "voteCount": 1,
        "content": "i'm also passed the test, but the sensor id increases the cardinality of the dataset... then the best option is to partition the data by year, month, and day, compress and convert JSON to a colunar file, in this case, parquet file."
      },
      {
        "date": "2023-03-25T16:38:00.000Z",
        "voteCount": 2,
        "content": "Partitioning by sensor, year, month, and day (option E) would likely increase costs as compared to partitioning by only year, month, and day (option B) because it would create a larger number of smaller partitions. Each partition would contain data from a single sensor for a given date range, resulting in more small files that would need to be scanned by Athena for each query.\n\nSo B is better answer than E"
      },
      {
        "date": "2023-02-13T07:46:00.000Z",
        "voteCount": 1,
        "content": "partition by sensor and then by year/month/day make sense, parquet with snappy gives best compressions"
      },
      {
        "date": "2023-02-03T23:09:00.000Z",
        "voteCount": 4,
        "content": "B partition strategy better than E. \nD for sure."
      },
      {
        "date": "2022-12-03T02:59:00.000Z",
        "voteCount": 2,
        "content": "D is an obvious choice. E has the highest potential to save costs also for queries that filter the sensor and the task is to find the solution with the most cost savings"
      },
      {
        "date": "2022-12-24T05:14:00.000Z",
        "voteCount": 2,
        "content": "Adding sensor(ID) to the partition creates high cardinality on the partitions and may lead to multiple small files under each partition which will slow down performance. But the question mentions about saving costs and not performance."
      },
      {
        "date": "2022-11-07T06:54:00.000Z",
        "voteCount": 3,
        "content": "Correct answers are B &amp; D\n\nOption B as the data can be partitions by year, month, and day as it needs to be analyzed using captured metrics over time and not specific to any sensor.\n\nOption D as columnar data format helps to improve query performance.\n\nOptions A &amp; C are wrong as Avro and ORC without compression would not provide query performance similar to parquet with compression.\n\nOption E is wrong as the data needs to be analyzed as per the metrics and not specific to a particular sensor."
      },
      {
        "date": "2022-11-09T08:10:00.000Z",
        "voteCount": 1,
        "content": "(A. Store the data in Apache Avro format using Snappy compression) Option A includes compression but Parquet with Snappy Compression is better option because Avro stores data in row format per @jay1ram2. Correct me if I am wrong."
      },
      {
        "date": "2022-10-30T17:57:00.000Z",
        "voteCount": 1,
        "content": "B and D. They are will query by time, not sensor id."
      },
      {
        "date": "2022-10-16T00:45:00.000Z",
        "voteCount": 1,
        "content": "If possible, avoid having a large number of small files \u2013 Amazon S3 has a limit of 5500 requests per second. Athena queries share the same limit.\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html"
      },
      {
        "date": "2022-09-25T02:04:00.000Z",
        "voteCount": 1,
        "content": "Because more and optimal number of partitions can be done through the option E. Snappy compression with Parquet format, allows easy integration and maximum storage saving."
      },
      {
        "date": "2022-08-17T05:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is B &amp; D"
      },
      {
        "date": "2022-07-20T23:30:00.000Z",
        "voteCount": 2,
        "content": "B and D are the right answers."
      },
      {
        "date": "2022-12-03T02:57:00.000Z",
        "voteCount": 1,
        "content": "E has the highest potential to save costs also for queries that filter the sensor and the task is to find the solution with the most cost savings."
      },
      {
        "date": "2022-07-14T22:45:00.000Z",
        "voteCount": 2,
        "content": "The metrics of temperature and acidity may be varying between different locations of the lake, students may want to see if any issues at a particular location level based on metrics. So its advisable to partition by Sensor"
      },
      {
        "date": "2022-07-02T12:37:00.000Z",
        "voteCount": 1,
        "content": "I vote A &amp; E. Vote for A because \"gather JSON-formatted batches of water quality values in Amazon S3\" is the requirement. We can't compress the Json format file using Parquet or ORC."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/amazon/view/29734-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A healthcare company uses AWS data and analytics tools to collect, ingest, and store electronic health record (EHR) data about its patients. The raw EHR data is stored in Amazon S3 in JSON format partitioned by hour, day, and year and is updated every hour. The company wants to maintain the data catalog and metadata in an AWS Glue Data Catalog to be able to access the data using Amazon Athena or Amazon Redshift Spectrum for analytics.<br>When defining tables in the Data Catalog, the company has the following requirements:<br>\u2711 Choose the catalog table name and do not rely on the catalog table naming algorithm.<br>\u2711 Keep the table updated with new partitions loaded in the respective S3 bucket prefixes.<br>Which solution meets these requirements with minimal effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an AWS Glue crawler that connects to one or more data stores, determines the data structures, and writes tables in the Data Catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue console to manually create a table in the Data Catalog and schedule an AWS Lambda function to update the table partitions hourly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue API CreateTable operation to create a table in the Data Catalog. Create an AWS Glue crawler and specify the table as the source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Apache Hive catalog in Amazon EMR with the table schema definition in Amazon S3, and update the table partition with a scheduled job. Migrate the Hive catalog to the Data Catalog."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T23:40:00.000Z",
        "voteCount": 31,
        "content": "C.\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html\nIn this section :\nUpdating Manually Created Data Catalog Tables Using Crawlers\n\"The following are other reasons why you might want to manually create catalog tables and specify catalog tables as the crawler source:\n\n    You want to choose the catalog table name and not rely on the catalog table naming algorithm.\n\""
      },
      {
        "date": "2021-09-30T12:24:00.000Z",
        "voteCount": 1,
        "content": "I agree is C. B is wrong because it takes more effort than B using Lambda, instead the Glue Crawler is full automated to update and find new partitions"
      },
      {
        "date": "2021-10-06T09:11:00.000Z",
        "voteCount": 1,
        "content": "I change my answer to C"
      },
      {
        "date": "2023-03-09T10:34:00.000Z",
        "voteCount": 1,
        "content": "However there is no info on scheduling the crawler in C. any thoughts?"
      },
      {
        "date": "2024-02-21T15:40:00.000Z",
        "voteCount": 1,
        "content": "I think Option A is correct. AWS Glue crawlers automatically scan data in S3, recognize the format and schema, and create metadata tables in the AWS Glue Data Catalog. This eliminates manual schema definition, table creation and meets new partitions with minimal effort. We can specify the database in which the tables are created and control the naming of the tables through the crawler's configuration settings rather than relying on an automated naming algorithm. As new data is added to S3 in hourly, daily, and yearly partitions, running the crawler at regular intervals ensures that new partitions are discovered and added to the respective Data Catalog tables automatically.\nOption C uses the AWS Glue API to create a table, which is more manual than allowing a crawler to discover and manage tables and does not automatically address the ongoing discovery of new partitions."
      },
      {
        "date": "2023-05-01T10:11:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-11-07T06:59:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C as the AWS Glue API CreateTable operation can be used to create a table in the Data Catalog and the AWS Glue crawler can be created and the table as the source can be specified. This meets the requirement of catalog table names and keeping the table updated with new data.\n\nB is incorrect. Although creating a new catalog table is right, the use of a Lambda function to update the table partitions entails a lot of development work. Remember that it is explicitly stated in the scenario that the solution should be implemented with the least configuration overhead.\nA is incorrect because you must create a new catalog table if you do not want to rely on the catalog table naming algorithm provided by AWS Glue.\nD is incorrect because this solution entails a lot of effort. A better and easier solution is to just create a new table in AWS Glue Data Catalog and set up an AWS Glue crawler."
      },
      {
        "date": "2022-10-12T22:07:00.000Z",
        "voteCount": 2,
        "content": "The following are other reasons why you might want to manually create catalog tables and specify catalog tables as the crawler source:\n\nYou want to choose the catalog table name and not rely on the catalog table naming algorithm.\n\nYou want to prevent new tables from being created in the case where files with a format that could disrupt partition detection are mistakenly saved in the data source path."
      },
      {
        "date": "2022-08-06T14:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-05-21T05:33:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C"
      },
      {
        "date": "2022-04-09T11:25:00.000Z",
        "voteCount": 1,
        "content": "You need to use the API to be able to provide a custom name"
      },
      {
        "date": "2021-12-21T22:05:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/tables-described.html#update-manual-tables"
      },
      {
        "date": "2021-11-24T14:22:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C."
      },
      {
        "date": "2021-11-02T16:59:00.000Z",
        "voteCount": 1,
        "content": "C as per the linked provided not B \nread \"Updating Manually Created Data Catalog Tables Using Crawlers\"\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html"
      },
      {
        "date": "2021-10-30T19:32:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer"
      },
      {
        "date": "2021-10-27T12:59:00.000Z",
        "voteCount": 1,
        "content": "I think there might be a miss typo in C, the table needs to be defined as target and not source"
      },
      {
        "date": "2021-10-30T09:18:00.000Z",
        "voteCount": 1,
        "content": "It also mentions this \nThe following are other reasons why you might want to manually create catalog tables and specify catalog tables as the crawler source:\n\n-You want to choose the catalog table name and not rely on the catalog table naming algorithm"
      },
      {
        "date": "2021-10-29T14:52:00.000Z",
        "voteCount": 2,
        "content": "No typo I think..\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html\nUpdating Manually Created Data Catalog Tables Using Crawlers:\nTo do this, when you define a crawler, instead of specifying one or more data stores as the source of a crawl, you specify one or more existing Data Catalog tables. The crawler then crawls the data stores specified by the catalog tables. In this case, no new tables are created; instead, your manually created tables are updated."
      },
      {
        "date": "2021-10-25T06:58:00.000Z",
        "voteCount": 2,
        "content": "C for me."
      },
      {
        "date": "2021-10-22T02:23:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C."
      },
      {
        "date": "2021-10-16T21:52:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2021-10-14T16:51:00.000Z",
        "voteCount": 3,
        "content": "Answer is C."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/amazon/view/29824-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large university has adopted a strategic goal of increasing diversity among enrolled students. The data analytics team is creating a dashboard with data visualizations to enable stakeholders to view historical trends. All access must be authenticated using Microsoft Active Directory. All data in transit and at rest must be encrypted.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon QuickSight Standard edition configured to perform identity federation using SAML 2.0. and the default encryption settings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 and the default encryption settings.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-14T15:50:00.000Z",
        "voteCount": 31,
        "content": "Answer is B\nAuthentication: https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp \nEncryption: https://docs.aws.amazon.com/quicksight/latest/user/data-encryption-at-rest.html\n\"All keys associated with Amazon QuickSight are managed by AWS.\" in https://docs.aws.amazon.com/quicksight/latest/user/key-management.html - No way to use customer-provided keys in QuickSight"
      },
      {
        "date": "2021-11-03T16:37:00.000Z",
        "voteCount": 5,
        "content": "\"Single Sign On with SAML or OpenID Connect\" is available in both Standard and Enterprise edition, which means both A and B are correct. Since this is not a multiple choice question, then both A and B are out. Active Directory is only available in Enterprise edition, so the answer is D"
      },
      {
        "date": "2022-07-06T07:54:00.000Z",
        "voteCount": 7,
        "content": "A and B aren't both correct, because encryption at rest is only available with Entreprise edition\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-encryption.html\nSo, answer is B"
      },
      {
        "date": "2023-07-01T10:32:00.000Z",
        "voteCount": 4,
        "content": "I don't know how much has changed after 20 months since you posted this. But\nAccording to: \nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\nIf you have an existing directory that you want to use for Amazon QuickSight, you can use Active Directory Connector.\n\nSince it's only available in enterprise, then the only option here to choose is D.\n\nAlso https://docs.aws.amazon.com/quicksight/latest/user/key-management.html gives a full tutorial on how to use a customer provided key as the encryption key"
      },
      {
        "date": "2021-10-08T10:26:00.000Z",
        "voteCount": 11,
        "content": "All Keys are managed by QuickSight enterprise edition and hence D can not be the answer. I would go with B"
      },
      {
        "date": "2024-02-27T01:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. \n\nAn AWS Organization is required to use IAM Identity Center with AD Connector. https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_getting_started.html, https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\n\nSince the univerasity does not has an AWS Organization and did not mention that they need one. Option B would b a more feasible option"
      },
      {
        "date": "2024-02-24T15:05:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT chose answer D."
      },
      {
        "date": "2024-02-21T15:46:00.000Z",
        "voteCount": 3,
        "content": "Option D mentioned using AD Connector and configuring Amazon QuickSight to use customer-provided keys imported into AWS Key Management Service (KMS). While using customer-provided keys in AWS KMS for encryption offers additional control over encryption keys, the question does not specify a requirement that necessitates this level of key management. Additionally, the AD Connector is not a feature of Amazon QuickSight; instead, the Enterprise edition supports direct AD integration.\nTherefore, option B is the correct solution, as it leverages the capabilities of Amazon QuickSight Enterprise edition to meet the university\u2019s requirements for Active Directory authentication and data encryption."
      },
      {
        "date": "2023-12-10T09:26:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2023-11-09T19:14:00.000Z",
        "voteCount": 1,
        "content": "Probably B is correct when the problem was created in the past.\nbut now D looks like also correct.(AD connecter looks like very easy) \nEncryption method is not defined in the sentence.\nso B is more correct.It is less effort.ummm"
      },
      {
        "date": "2023-08-30T21:27:00.000Z",
        "voteCount": 1,
        "content": "Ans  :D"
      },
      {
        "date": "2023-08-03T20:03:00.000Z",
        "voteCount": 1,
        "content": "B : SAML 2.0 works with AD, \nEnterprise Edition offers encryption at rest.\nAll data is encrypted in transit by default https://docs.aws.amazon.com/quicksight/latest/user/data-encryption-in-transit.html"
      },
      {
        "date": "2023-05-01T10:12:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-09-12T12:06:00.000Z",
        "voteCount": 5,
        "content": "Sure you pass the test. But this one you got it wrong. The answer is \"D\""
      },
      {
        "date": "2023-04-30T17:53:00.000Z",
        "voteCount": 1,
        "content": "QuickSight enables you to encrypt your SPICE datasets using the keys you have stored in AWS Key Management Service. This provides you with the tools to audit access to data and satisfy regulatory security requirements. If you need to do so, you have the option to immediately lock down access to your data by revoking access to AWS KMS keys."
      },
      {
        "date": "2023-04-13T04:40:00.000Z",
        "voteCount": 2,
        "content": "D seems to be right. Reason: it is stated that user authentication must be via Microsoft Active Directory. This rules out options A, B and C. A and B mention SAML, C mentions an AD Connector but this is only supported in the Enterprise Edition. Source: https://docs.aws.amazon.com/quicksight/latest/user/directory-integration.html and https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html. And yes it is possible to use CMKs with aws managed KMS, even for SPICE data not just meta data. Source: https://docs.aws.amazon.com/quicksight/latest/user/key-management.html - Using customer-managed keys from AWS KMS with SPICE datasets in Amazon QuickSight"
      },
      {
        "date": "2023-03-25T17:07:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html"
      },
      {
        "date": "2023-03-19T09:36:00.000Z",
        "voteCount": 2,
        "content": "To create customer-managed keys (CMKs), you use AWS Key Management Service (AWS KMS) in the same AWS account and AWS Region as the Amazon QuickSight SPICE dataset. A QuickSight administrator can then use a CMK to encrypt SPICE datasets and control access.\nhttps://docs.aws.amazon.com/quicksight/latest/user/key-management.html"
      },
      {
        "date": "2023-03-19T09:38:00.000Z",
        "voteCount": 1,
        "content": "Key statement - \"All data in transit and at rest must be encrypted.\" A and B are out"
      },
      {
        "date": "2023-02-17T06:07:00.000Z",
        "voteCount": 1,
        "content": "QuickSight enables you to encrypt your SPICE datasets using the keys you have stored in AWS Key Management Service. This provides you with the tools to audit access to data and satisfy regulatory security requirements. If you need to do so, you have the option to immediately lock down access to your data by revoking access to AWS KMS keys. All data access to encrypted datasets in QuickSight SPICE is logged in AWS CloudTrail. Administrators or auditors can trace data access in CloudTrail to identify when and where data was accessed.\n\nTo create customer-managed keys (CMKs), you use AWS Key Management Service (AWS KMS) in the same AWS account and AWS Region as the Amazon QuickSight SPICE dataset. A QuickSight administrator can then use a CMK to encrypt SPICE datasets and control access."
      },
      {
        "date": "2023-02-06T07:10:00.000Z",
        "voteCount": 3,
        "content": "Answer should be D. This allow the use of customer managed keys: https://docs.aws.amazon.com/quicksight/latest/user/key-management.html\nAnd easy enabling of AD"
      },
      {
        "date": "2023-01-07T10:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/amazon/view/29732-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An airline has been collecting metrics on flight activities for analytics. A recently completed proof of concept demonstrates how the company provides insights to data analysts to improve on-time departures. The proof of concept used objects in Amazon S3, which contained the metrics in .csv format, and used Amazon<br>Athena for querying the data. As the amount of data increases, the data analyst wants to optimize the storage solution to improve query performance.<br>Which options should the data analyst use to improve performance as the data lake grows? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a randomized string to the beginning of the keys in S3 to get more throughput across partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 bucket in the same account as Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the objects to reduce the data transfer I/O.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 bucket in the same Region as Athena.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprocess the .csv data to JSON to reduce I/O by fetching only the document keys needed by the query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprocess the .csv data to Apache Parquet to reduce I/O by fetching only the data blocks needed for predicates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDF",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T16:16:00.000Z",
        "voteCount": 26,
        "content": "For me is CDF"
      },
      {
        "date": "2021-10-04T14:04:00.000Z",
        "voteCount": 4,
        "content": "Parquet file is by default compressed which is convered under F. The answer should be A,D,F"
      },
      {
        "date": "2021-10-11T05:02:00.000Z",
        "voteCount": 14,
        "content": "A is not best practice any more. [Quoted]previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals. You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes. [Unquoted]"
      },
      {
        "date": "2022-09-06T23:31:00.000Z",
        "voteCount": 3,
        "content": "@Woong Thanks for quoting the excerpt. This makes option \"A\" incorrect.\nSharing the link to this statement for anyone who wish to verify\nhttps://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf\n\"This guidance supersedes any previous guidance on optimizing performance for Amazon S3. For\nexample, previously Amazon S3 performance guidelines recommended randomizing prefix naming\nwith hashed characters to optimize performance for frequent data retrievals. You no longer have to\nrandomize prefix naming for performance, and can use sequential date-based naming for your prefixes\""
      },
      {
        "date": "2021-10-13T02:35:00.000Z",
        "voteCount": 3,
        "content": "That's absolutely right, hence should be C,D,F"
      },
      {
        "date": "2023-11-18T02:08:00.000Z",
        "voteCount": 1,
        "content": "The comment about random strings is also in this useful link:\nhttps://docs.aws.amazon.com/whitepapers/latest/s3-optimizing-performance-best-practices/introduction.html"
      },
      {
        "date": "2023-05-27T12:51:00.000Z",
        "voteCount": 1,
        "content": "C and F are not doubt easy answers. But I believe A and D both are correct. People quoting randomize prefix not required - are ignoring \"sequential date-based naming\" which is also not mentioned in question and Athen can run longer as S3 list operations will take time without a well distributed prefix."
      },
      {
        "date": "2023-05-01T10:13:00.000Z",
        "voteCount": 3,
        "content": "CDF: I passed the test"
      },
      {
        "date": "2022-11-29T20:06:00.000Z",
        "voteCount": 1,
        "content": "A,C,F\nSome data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. In this scenario, millions of data points are stored on Amazon S3 and it is recommended to create a random string and add that to the beginning of the object prefixes to increase the read performance for S3 objects."
      },
      {
        "date": "2022-11-07T07:07:00.000Z",
        "voteCount": 8,
        "content": "Correct answers are C, D &amp; F\n\nOptions C &amp; F as using compression and columnar data format helps improve query performance and optimize storage\nOption D as using Athena and S3 within the same region would help with query performance and cost.\n\nOption A is wrong as S3 scales automatically now and is not bounded by the restriction.\n\nOption B is wrong as using the same account does not help in optimizing the cost of query performance.\n\nOption E is wrong as using JSON is the same as using CSV files and does help in n optimizing the cost or query performance."
      },
      {
        "date": "2022-07-26T19:34:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: CDF"
      },
      {
        "date": "2022-06-22T16:22:00.000Z",
        "voteCount": 1,
        "content": "Ans A. can't be correct at all.\nAdding randomized string will make partition size too small to reap the benefits.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2022-05-22T01:32:00.000Z",
        "voteCount": 1,
        "content": "C,D,F fulfills are needs"
      },
      {
        "date": "2022-05-21T05:04:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C, D &amp; F"
      },
      {
        "date": "2021-11-24T14:25:00.000Z",
        "voteCount": 4,
        "content": "For me is CDF"
      },
      {
        "date": "2021-11-07T00:29:00.000Z",
        "voteCount": 1,
        "content": "As a best practice, S3 and Athena should be in the same region and account and columnar based is appropriate for the performance. My answer would be BDF"
      },
      {
        "date": "2021-11-04T16:43:00.000Z",
        "voteCount": 3,
        "content": "CDF.\nParquet compresses by default, yes, but there is also an \"uncompressed\" option. So C is not redundant."
      },
      {
        "date": "2021-11-02T14:32:00.000Z",
        "voteCount": 1,
        "content": "I think CDF"
      },
      {
        "date": "2021-10-31T09:13:00.000Z",
        "voteCount": 1,
        "content": "I goes with C,D,F."
      },
      {
        "date": "2021-10-29T02:01:00.000Z",
        "voteCount": 1,
        "content": "C -&gt; is WRONG in my opinion\n1. How do you want to compress Apache Parquet that is already compressed by default? We selected Parquet as file format in F.\n\"For Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable.\"\n2. We still need prefixes but we don't have to randomize them\nYou can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.\nBUT\nYou no longer have to randomize prefix naming for performance and can use sequential date-based naming for your prefixes.\n3. You can not reduce data transfer I/O. I/O represents an entity that sends/receives data, therefore, you can only reduce parameters of I/O e.g. data transfer bandwidth, speed, no of operations (e.g. IOPS) etc."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/amazon/view/29733-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company uses the Amazon Kinesis SDK to write data to Kinesis Data Streams. Compliance requirements state that the data must be encrypted at rest using a key that can be rotated. The company wants to meet this encryption requirement with minimal coding effort.<br>How can these requirements be met?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a customer master key (CMK) in AWS KMS. Assign the CMK an alias. Use the AWS Encryption SDK, providing it with the key alias to encrypt and decrypt the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a customer master key (CMK) in AWS KMS. Assign the CMK an alias. Enable server-side encryption on the Kinesis data stream using the CMK alias as the KMS master key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a customer master key (CMK) in AWS KMS. Create an AWS Lambda function to encrypt and decrypt the data. Set the KMS key ID in the function's environment variables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable server-side encryption on the Kinesis data stream using the default KMS key for Kinesis Data Streams."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-07T18:05:00.000Z",
        "voteCount": 20,
        "content": "B.\nhttps://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html"
      },
      {
        "date": "2022-11-07T07:09:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is B as Kinesis Data Streams supports data at rest encryption using Server-Side encryption. Data is encrypted before persisting and decrypted before being read by the consumers and requires no changes to producers and consumers.\n\nOptions A &amp; C are wrong as it would require coding effort.\n\nOption D is wrong as the default key cannot be rotated."
      },
      {
        "date": "2023-05-27T12:57:00.000Z",
        "voteCount": 1,
        "content": "B is correct. D is wrong - AWS Managed keys are rotated but as per AWS not as per customer. Here customer want rotation capability and they might want to do it number of times in a year."
      },
      {
        "date": "2023-05-01T10:14:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-07-25T21:31:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-06-21T06:26:00.000Z",
        "voteCount": 1,
        "content": "It should be B. B describes the method to encrypt your data at rest inside your Kinesis Data Streams. \nOption D is the only valid alternative given the constraints, but there is no valid method to rotate this key yourself. So I would argue this key is not rotatable."
      },
      {
        "date": "2022-05-06T20:20:00.000Z",
        "voteCount": 2,
        "content": "Vote for B.\nI think \"rotatable key\" means you can rotate manually, it should be CMK, not AWS managed key.\nD said \"using the default KMS key\", it is saying to use AWS managed key. So it's not right."
      },
      {
        "date": "2022-05-05T05:15:00.000Z",
        "voteCount": 2,
        "content": "I'd say D:\nA and C are out because they involve more coding.\nB would work, but key rotation of CMK is disabled by default and the answer did not say to enable it (but mentions creation and alias, so that was likely left out on purpose)\nD works, is the simplest and the key is rotated by default (no every year, used to be every 3 years)\nParagraph \"Customer managaged keys\" \n in https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-aws-owned-keys"
      },
      {
        "date": "2022-05-06T20:18:00.000Z",
        "voteCount": 2,
        "content": "I think \"rotatable key\" means you can rotate manually, it should be CMK, not AWS managed key."
      },
      {
        "date": "2022-04-30T17:16:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2021-11-21T07:13:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-11-03T22:15:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-30T16:48:00.000Z",
        "voteCount": 3,
        "content": "The Answer is B. You cannot rotate \"AWS Managed CMK\" i.e. Default keys. It is automatically rotated every 3 years. \n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html"
      },
      {
        "date": "2021-10-26T11:18:00.000Z",
        "voteCount": 2,
        "content": "\"key that can be rotated\" it is talking about CMK. B must be the answer."
      },
      {
        "date": "2021-10-25T21:06:00.000Z",
        "voteCount": 3,
        "content": "Just checked the Kinesis console. You can select either the default or the CMK for encryption at rest. Since rotation can be set for the CMK, B is the answer."
      },
      {
        "date": "2021-10-25T05:50:00.000Z",
        "voteCount": 2,
        "content": "D. \n\nthe FAQ's  https://aws.amazon.com/kinesis/data-streams/faqs/#kinesis-encryption \nquestion : \n\"What is server-side encryption\"\nIt mentions \"Server-side encryption for Kinesis Data Streams automatically encrypts data using a user specified AWS KMS master key (CMK) \""
      },
      {
        "date": "2023-03-30T11:58:00.000Z",
        "voteCount": 1,
        "content": "No, there is no such thing as default key for KDS. It's B"
      },
      {
        "date": "2021-10-22T02:19:00.000Z",
        "voteCount": 3,
        "content": "B.\nit's written here: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\nYou cannot rotate key that you did not create"
      },
      {
        "date": "2021-10-12T08:30:00.000Z",
        "voteCount": 1,
        "content": "It should be B. \nHere is the link,\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/amazon/view/30049-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to enrich application logs in near-real-time and use the enriched dataset for further analysis. The application is running on Amazon EC2 instances across multiple Availability Zones and storing its logs using Amazon CloudWatch Logs. The enrichment source is stored in an Amazon DynamoDB table.<br>Which solution meets the requirements for the event collection and enrichment?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a CloudWatch Logs subscription to send the data to Amazon Kinesis Data Firehose. Use AWS Lambda to transform the data in the Kinesis Data Firehose delivery stream and enrich it with the data in the DynamoDB table. Configure Amazon S3 as the Kinesis Data Firehose delivery destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the raw logs to Amazon S3 on an hourly basis using the AWS CLI. Use AWS Glue crawlers to catalog the logs. Set up an AWS Glue connection for the DynamoDB table and set up an AWS Glue ETL job to enrich the data. Store the enriched data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to write the logs locally and use Amazon Kinesis Agent to send the data to Amazon Kinesis Data Streams. Configure a Kinesis Data Analytics SQL application with the Kinesis data stream as the source. Join the SQL application input stream with DynamoDB records, and then store the enriched output stream in Amazon S3 using Amazon Kinesis Data Firehose.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the raw logs to Amazon S3 on an hourly basis using the AWS CLI. Use Apache Spark SQL on Amazon EMR to read the logs from Amazon S3 and enrich the records with the data from DynamoDB. Store the enriched data in Amazon S3."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T06:52:00.000Z",
        "voteCount": 38,
        "content": "The answer is A - Since they are already using CloudWatch Logs, it makes sense to send the CW logs to KFH which will invoke lambda and send data to S3 for further analysis."
      },
      {
        "date": "2022-01-06T23:05:00.000Z",
        "voteCount": 3,
        "content": "KDA can refrer s3 only for data enrichment."
      },
      {
        "date": "2022-11-07T07:11:00.000Z",
        "voteCount": 10,
        "content": "Correct answer is A as CloudWatch logs can be integrated with Kinesis Data Firehose using subscription filters, the data can be enriched using Lambda doing a lookup on the DynamoDB tables and data storage to S3.\n\nOptions B &amp; D are wrong as exporting the logs would not provide near-real-time data handling.\n\nOption C is wrong as using Kinesis Data Stream for data collection with agents would increase overhead, also Kinesis Data Analytics does not support DynamoDB as a reference data source for enrichment."
      },
      {
        "date": "2023-05-01T10:15:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-07-25T21:27:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-06-28T07:35:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample. B, D are obviously wrong. DynamoDB is not for joining."
      },
      {
        "date": "2022-12-13T13:06:00.000Z",
        "voteCount": 1,
        "content": "Agree!"
      },
      {
        "date": "2022-05-22T01:28:00.000Z",
        "voteCount": 1,
        "content": "My Answer is A"
      },
      {
        "date": "2022-03-20T06:01:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2022-01-14T04:16:00.000Z",
        "voteCount": 2,
        "content": "The answer is C - Since CW logs is written in gzip format to Kinesis Firehose. In the Lambda function the uncompress command should be applied first. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample"
      },
      {
        "date": "2022-06-21T04:50:00.000Z",
        "voteCount": 1,
        "content": "You can uncompress inside the Lambda without any issue. So answer A works."
      },
      {
        "date": "2021-12-04T11:31:00.000Z",
        "voteCount": 1,
        "content": "How KDF refer data stored in DynamoDB ? i don't think it is possible"
      },
      {
        "date": "2022-06-21T04:51:00.000Z",
        "voteCount": 1,
        "content": "Lambda does the enrichment and so the Lambda needs to read the DynamoDB table. This is perfectly fine."
      },
      {
        "date": "2021-11-21T07:06:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2021-11-04T01:09:00.000Z",
        "voteCount": 1,
        "content": "Why not D ?\nhttps://aws.amazon.com/es/blogs/big-data/analyze-your-data-on-amazon-dynamodb-with-apache-spark/"
      },
      {
        "date": "2021-11-05T16:53:00.000Z",
        "voteCount": 4,
        "content": "because hourly is not near realtime"
      },
      {
        "date": "2021-10-30T15:49:00.000Z",
        "voteCount": 4,
        "content": "A is the right answer"
      },
      {
        "date": "2021-10-29T05:15:00.000Z",
        "voteCount": 2,
        "content": "The thing that confuses me about A is the combination of Firehose and Lambda, is it even possible to add a Lambda processing step in Firehose?"
      },
      {
        "date": "2021-10-29T21:35:00.000Z",
        "voteCount": 3,
        "content": "Never mind, it is indeed possible!"
      },
      {
        "date": "2021-10-21T12:46:00.000Z",
        "voteCount": 1,
        "content": "Here Key word ' multiple Availability Zones' is crucial. We need to export the log from differnet zones."
      },
      {
        "date": "2021-10-11T02:42:00.000Z",
        "voteCount": 3,
        "content": "C - is possible  too -&gt; KDS-&gt;  KDA join Dynamo -&gt; KDF-&gt;s3\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html\nhttps://aws.amazon.com/blogs/big-data/joining-and-enriching-streaming-data-on-amazon-kinesis/\n\nboth solution can be near real time . \nA - only because CW logs is used ?? Tricky Question"
      },
      {
        "date": "2021-10-15T01:20:00.000Z",
        "voteCount": 1,
        "content": "K Agent -&gt;KDS-&gt; KDA join Dynamo -&gt; KDF-&gt;s3"
      },
      {
        "date": "2021-10-17T07:53:00.000Z",
        "voteCount": 2,
        "content": "Ignore C &lt;&lt;&lt; KDA join Dynamo not possible . ( reference data on S3 would have worked ) \nA works well &lt;&lt;&lt;&lt;"
      },
      {
        "date": "2021-09-29T14:34:00.000Z",
        "voteCount": 1,
        "content": "Only A provides near-real-time. Rest would have much longer delay."
      },
      {
        "date": "2021-09-29T11:33:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/amazon/view/30053-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company uses Amazon Redshift as its data warehouse. A new table has columns that contain sensitive data. The data in the table will eventually be referenced by several existing queries that run many times a day.<br>A data analyst needs to load 100 billion rows of data into the new table. Before doing so, the data analyst must ensure that only members of the auditing group can read the columns containing sensitive data.<br>How can the data analyst meet these requirements with the lowest maintenance overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all the data into the new table and grant the auditing group permission to read from the table. Load all the data except for the columns containing sensitive data into a second table. Grant the appropriate users read-only permissions to the second table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all the data into the new table and grant the auditing group permission to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns to the appropriate users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all the data into the new table and grant all users read-only permissions to non-sensitive columns. Attach an IAM policy to the auditing group with explicit ALLOW access to the sensitive data columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all the data into the new table and grant the auditing group permission to read from the table. Create a view of the new table that contains all the columns, except for those considered sensitive, and grant the appropriate users read-only permissions to the table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-08T11:35:00.000Z",
        "voteCount": 22,
        "content": "It's B.\nhttps://aws.amazon.com/jp/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/"
      },
      {
        "date": "2021-12-03T07:34:00.000Z",
        "voteCount": 2,
        "content": "B it is\nFor row level access I guess only option to create views, any thoughts?"
      },
      {
        "date": "2021-12-20T20:43:00.000Z",
        "voteCount": 2,
        "content": "grant select(cust_name, cust_phone) on cust_profile to user1;"
      },
      {
        "date": "2023-03-09T11:21:00.000Z",
        "voteCount": 1,
        "content": "B talks about providing access at a user level rather than a group. It this not an operational overhead?"
      },
      {
        "date": "2022-11-07T07:13:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is B as Redshift supports column-level access control, which works best with the table-level access control without having to implement views.\n\nOption A is wrong as it increases maintenance overhead.\n\nOption C is wrong as IAM policy does not help provide column-level access control.\n\n\nOption D is wrong as using Redshift column-level access control is better than views."
      },
      {
        "date": "2023-05-01T10:16:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-05-02T02:10:00.000Z",
        "voteCount": 1,
        "content": "got how many questions from the dump?"
      },
      {
        "date": "2023-03-19T10:19:00.000Z",
        "voteCount": 1,
        "content": "GRANT can be used to assume an IAM role as well which covers options C as well.\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT-usage-notes.html#r_GRANT-usage-notes-assumerole"
      },
      {
        "date": "2023-03-19T10:29:00.000Z",
        "voteCount": 2,
        "content": "\"The data in the table will eventually be referenced by several existing queries that run many times a day.\"\nIf the view is based on a complex query that joins many tables or performs many calculations, it can be slow to query. If the view is based on a large amount of data, it can also be slow to query."
      },
      {
        "date": "2022-10-31T10:37:00.000Z",
        "voteCount": 4,
        "content": "B is correct According to Stephane Maarek course on Udemy\nSince March 2020, Amazon Redshift supports column-level access control for data in Redshift. Customers can use column-level GRANT and REVOKE statements to help meet their security and compliance needs.\n\nRedshift's table-level access controls for the data in Redshift are already in use by many customers, but they also want the ability to control access in more detail. You can now control access to columns without having to implement view-based access control or use another system. Column-level access control is available in all Amazon Redshift regions.\n\nGRANT command defines access privileges for a user or user group. Privileges include access options such as being able to read data in tables and views, write data, create tables, and drop tables. Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column."
      },
      {
        "date": "2022-10-30T18:32:00.000Z",
        "voteCount": 1,
        "content": "B. Column level access control is available in redshift."
      },
      {
        "date": "2022-10-16T01:13:00.000Z",
        "voteCount": 1,
        "content": "the key is lowest maintenance overhead!!\nif you grant access permission using SQL, you will facing with endless maintenance"
      },
      {
        "date": "2022-08-02T03:30:00.000Z",
        "voteCount": 1,
        "content": "I will choose \"C\". Because its easy for me to grant read only access for any user for non sensitive data. And to allow only auditers to access sensitive data. Not other way around as given in \"B\"."
      },
      {
        "date": "2022-07-23T15:02:00.000Z",
        "voteCount": 2,
        "content": "Its B, remember that create a view is not a good practice and might have leak of data. The best practice is to GRANT"
      },
      {
        "date": "2022-07-21T19:02:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2022-05-22T01:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-05-21T17:52:00.000Z",
        "voteCount": 1,
        "content": "B and C are very similar. The only advantage for B is that there is a single role assigned to auditors to access all the columns. While in case of C, auditors will access few columns via public role and few senstive columns via another role created specific to them."
      },
      {
        "date": "2022-04-30T15:08:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2022-04-22T13:27:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html"
      },
      {
        "date": "2021-11-19T20:43:00.000Z",
        "voteCount": 1,
        "content": "B is the ans"
      },
      {
        "date": "2021-10-31T15:08:00.000Z",
        "voteCount": 1,
        "content": "It's D. Much easier to create a view than to insert in 1 new table."
      },
      {
        "date": "2021-11-05T09:22:00.000Z",
        "voteCount": 1,
        "content": "Changed for D"
      },
      {
        "date": "2021-11-06T22:06:00.000Z",
        "voteCount": 1,
        "content": "Changed for B"
      },
      {
        "date": "2021-10-30T02:00:00.000Z",
        "voteCount": 2,
        "content": "Why not C? all other team should have access to the table except for the sensitive data columns right? Options B is providing Audit team permission to the table and then granting  access to the column again. Is that second step even necessary?"
      },
      {
        "date": "2022-08-02T03:24:00.000Z",
        "voteCount": 1,
        "content": "That's what I thought too. Allowing a very limited set of users to access sensitive columns is much easier"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/amazon/view/29735-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A banking company wants to collect large volumes of transactional data using Amazon Kinesis Data Streams for real-time analytics. The company uses<br>PutRecord to send data to Amazon Kinesis, and has observed network outages during certain times of the day. The company wants to obtain exactly once semantics for the entire processing pipeline.<br>What should the company do to obtain these characteristics?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the application so it can remove duplicates during processing be embedding a unique ID in each record.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRely on the processing semantics of Amazon Kinesis Data Analytics to avoid duplicate processing of events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the data producer so events are not ingested into Kinesis Data Streams multiple times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRely on the exactly one processing semantics of Apache Flink and Apache Spark Streaming included in Amazon EMR."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T16:08:00.000Z",
        "voteCount": 17,
        "content": "Agree with A."
      },
      {
        "date": "2021-09-28T01:35:00.000Z",
        "voteCount": 2,
        "content": "me too!"
      },
      {
        "date": "2021-09-30T02:42:00.000Z",
        "voteCount": 9,
        "content": "A. \nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\n\"Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing.\""
      },
      {
        "date": "2023-11-03T06:09:00.000Z",
        "voteCount": 2,
        "content": "D \nApache Flink provides a powerful API to transform, aggregate, and enrich events, and supports exactly-once semantics. Apache Flink is therefore a good foundation for the core of your streaming architecture.\nhttps://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/"
      },
      {
        "date": "2023-05-01T10:17:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-02-27T05:31:00.000Z",
        "voteCount": 1,
        "content": "A - exactly what is requested in the description"
      },
      {
        "date": "2022-11-07T07:15:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A as producer retries can result in duplicates in Kinesis Data Streams and must be handled by the producer by using a unique key for each message.\n\nThere are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times.\n\nOptions B &amp; C are wrong as they would not handle the exactly-once processing semantics.\n\n\nOption D is wrong as although Apache Flink and Spark Streaming would work, it would need a complete change in the current application."
      },
      {
        "date": "2022-07-20T23:12:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-05-27T02:45:00.000Z",
        "voteCount": 1,
        "content": "application should be idempotent. Can be achieved by including a primary key in the record. Hence, A is correct answer"
      },
      {
        "date": "2022-05-11T18:35:00.000Z",
        "voteCount": 2,
        "content": "The problem is the question is, the producer may put the records several times. Donell explained this very well. \nKDA, Flink or Spark can only make sure 'Exactly once' for every record in stream. If the record is duplicated by producer, they will process exactly once for every record in stream.\nSo the answer should be A."
      },
      {
        "date": "2022-04-30T14:13:00.000Z",
        "voteCount": 1,
        "content": "Answer : A"
      },
      {
        "date": "2021-11-17T14:43:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2021-11-02T02:31:00.000Z",
        "voteCount": 8,
        "content": "Answer A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record.\nProducer Retries\nConsider a producer that experiences a network-related timeout after it makes a call to PutRecord, but before it can receive an acknowledgement from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both PutRecord calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.\n\nReference: https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html"
      },
      {
        "date": "2021-10-27T22:49:00.000Z",
        "voteCount": 4,
        "content": "ANSWER:D\nKDS and KDF has 'exactly once' semantics .Option A is a fail safe  mechanism when there is a choppy network while sending data to KDS with PutRecord.\nApache Flink and Apache Spark both guarantees 'Exactly once' semantics and which is what is the requirement as per the question ."
      },
      {
        "date": "2022-05-11T18:33:00.000Z",
        "voteCount": 1,
        "content": "The problem is the question is, the producer may put the records several times. Donell explained this very well. So the answer should be A.\nFlink or Spark can only make sure 'Exactly once' for every record in stream. If the record is duplicated by producer, they don't work."
      },
      {
        "date": "2021-10-27T13:36:00.000Z",
        "voteCount": 3,
        "content": "A is the right answer"
      },
      {
        "date": "2021-10-09T02:22:00.000Z",
        "voteCount": 2,
        "content": "A was a good choice until i search EMR Flink...: \nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-flink.html\nApache Flink is a streaming dataflow engine that you can use to run real-time stream processing on high-throughput data sources. Flink supports event time semantics for out-of-order events, exactly-once semantics, backpressure control, and APIs optimized for writing both streaming and batch applications. \nCan be connected to KDS.\nSo i will pick up D"
      },
      {
        "date": "2021-10-22T15:34:00.000Z",
        "voteCount": 2,
        "content": "According to Flink kinesis connector doc -- https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kinesis.html, in the Kinesis producer section, \nit states \"Note that the producer is not participating in Flink\u2019s checkpointing and doesn\u2019t provide exactly-once processing guarantees. Also, the Kinesis producer does not guarantee that records are written in order to the shards (See here and here for more details).\nIn case of a failure or a resharding, data will be written again to Kinesis, leading to duplicates. This behavior is usually called \u201cat-least-once\u201d semantics.\"\nSo I think the answer is A."
      },
      {
        "date": "2021-10-26T08:09:00.000Z",
        "voteCount": 1,
        "content": "There are documents that link  streaming (Kafka/Kinesis) and EMR/Spark/Flink to remove  deduplication in realtime. \nhttps://blog.griddynamics.com/in-stream-deduplication-with-spark-amazon-kinesis-and-s3/ \n\nin addition to the difficulty of changing running application, I would say D is a good potential candidate"
      },
      {
        "date": "2021-10-07T15:42:00.000Z",
        "voteCount": 1,
        "content": "Link provided supports A as the answer"
      },
      {
        "date": "2021-09-28T02:27:00.000Z",
        "voteCount": 1,
        "content": "I will also go with option A."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/amazon/view/30050-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company's data analyst needs to ensure that queries run in Amazon Athena cannot scan more than a prescribed amount of data for cost control purposes.<br>Queries that exceed the prescribed threshold must be canceled immediately.<br>What should the data analyst do to achieve this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Athena to invoke an AWS Lambda function that terminates queries when the prescribed threshold is crossed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each workgroup, set the control limit for each query to the prescribed threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce the prescribed threshold on all Amazon S3 bucket policies",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each workgroup, set the workgroup-wide data usage control limit to the prescribed threshold."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-31T12:47:00.000Z",
        "voteCount": 20,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-28T15:10:00.000Z",
        "voteCount": 15,
        "content": "From the link: https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html\n\"The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled\"\nAnswer is B"
      },
      {
        "date": "2023-05-01T10:18:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-11-07T07:20:00.000Z",
        "voteCount": 8,
        "content": "Correct answer is B as Athena Workgroups help set control limits and per-query control limit helps specific a limit which if exceeded by a query it would be canceled.\n\n\nA is wrong as you can't configure Atehna for this purpose\n\nC is incorrect because you can't set a threshold in Athena using S3 bucket policies.\n\nD is incorrect because the workgroup-wide data usage control limit specifies the total amount of data scanned for all queries that run in the entire workgroup, and not on a specific query only. Remember that the requirement is to immediately cancel queries that exceed the recommended threshold."
      },
      {
        "date": "2022-07-25T21:10:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-05-21T04:48:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-12-12T19:27:00.000Z",
        "voteCount": 1,
        "content": "why not A or C? Can we set RecordMaxBufferTime to control scan?"
      },
      {
        "date": "2021-11-19T20:54:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-11-02T13:47:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2021-11-02T07:31:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-19T08:07:00.000Z",
        "voteCount": 2,
        "content": "Will go with B for this scenario. I think the use case if to set a per query limit"
      },
      {
        "date": "2021-10-18T23:50:00.000Z",
        "voteCount": 2,
        "content": "\"per-query control limit\" vs \"workgroup-wide data usage control limit\"\nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html"
      },
      {
        "date": "2021-10-18T07:45:00.000Z",
        "voteCount": 3,
        "content": "B and D both serve similar purpose. However in this example B is the right option.\nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html"
      },
      {
        "date": "2021-10-12T22:22:00.000Z",
        "voteCount": 3,
        "content": "It is B based on this link:\nhttps://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html"
      },
      {
        "date": "2021-10-01T23:30:00.000Z",
        "voteCount": 3,
        "content": "I apologize, the Answer is B."
      },
      {
        "date": "2021-09-22T04:03:00.000Z",
        "voteCount": 1,
        "content": "Agree with D."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/amazon/view/29836-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A marketing company is using Amazon EMR clusters for its workloads. The company manually installs third-party libraries on the clusters by logging in to the master nodes. A data analyst needs to create an automated solution to replace the manual process.<br>Which options can fulfill these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the required installation scripts in Amazon S3 and execute them using custom bootstrap actions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the required installation scripts in Amazon S3 and execute them through Apache Spark in Amazon EMR.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the required third-party libraries in the existing EMR master node. Create an AMI out of that master node and use that custom AMI to re-create the EMR cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon DynamoDB table to store the list of required applications. Trigger an AWS Lambda function with DynamoDB Streams to install the software.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI and use that AMI to create the EMR cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T12:47:00.000Z",
        "voteCount": 25,
        "content": "I will choose A and E.\nhttps://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/\n\nhttps://docs.aws.amazon.com/de_de/emr/latest/ManagementGuide/emr-plan-bootstrap.html"
      },
      {
        "date": "2021-09-21T14:49:00.000Z",
        "voteCount": 3,
        "content": "Doubt in this one... Documentation says you use E as a option to avoid A.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html"
      },
      {
        "date": "2021-10-01T18:01:00.000Z",
        "voteCount": 4,
        "content": "Its A and E, if you do one or the other, but you shouldn't do both (as if it was a step 1 and after that step 2)"
      },
      {
        "date": "2021-10-02T18:57:00.000Z",
        "voteCount": 1,
        "content": "Agreed!"
      },
      {
        "date": "2021-10-04T06:54:00.000Z",
        "voteCount": 8,
        "content": "A and E."
      },
      {
        "date": "2023-11-20T01:07:00.000Z",
        "voteCount": 3,
        "content": "AE: I'll pass the test."
      },
      {
        "date": "2023-11-09T03:55:00.000Z",
        "voteCount": 2,
        "content": "I think existing cluster must not change.\nIt is danger.\nInstalling software for new EC2 instance  is more safe.\nso A and E correct.I think."
      },
      {
        "date": "2023-07-30T10:39:00.000Z",
        "voteCount": 2,
        "content": "AE\nCustom AMIs created from the base EMR AMI are not supported and will lead to application provisioning errors upon cluster startup.\nhttps://medium.com/@amberrunnels/creating-a-custom-ami-on-amazon-emr-a60ddeb7821b"
      },
      {
        "date": "2023-05-27T13:21:00.000Z",
        "voteCount": 3,
        "content": "A is very obvious. Between C and E - I will prefer E , as creating AMI from a master node may create a bulky AMI with lot of redundant hadoop libraries that can be done during bootstrap process."
      },
      {
        "date": "2023-05-01T10:19:00.000Z",
        "voteCount": 1,
        "content": "AE: I passed the test"
      },
      {
        "date": "2022-12-22T04:39:00.000Z",
        "voteCount": 1,
        "content": "AE\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html#emr-custom-ami-preconfigure"
      },
      {
        "date": "2022-10-31T11:01:00.000Z",
        "voteCount": 1,
        "content": "A AND E  According to Stephane Maarek Udemy course \nA= You can use a bootstrap action to install additional software or customize the configuration of the EMR cluster instances. Bootstrap actions are scripts that run on the cluster after Amazon EMR launches the instance using the Amazon Linux Amazon Machine Image (AMI). Bootstrap actions run before Amazon EMR installs the applications that you specify when you create the cluster and before cluster nodes begin processing data.\nE= You can create Amazon EMR clusters that have custom Amazon Machine Images (AMI) running Amazon Linux. You can create the AMI from an EC2 instance running Amazon Linux. Make sure that you have installed all the required third-party libraries on this EC2 instance. This allows you to preload additional software on your AMI and use these AMIs to launch your EMR clusters."
      },
      {
        "date": "2022-09-26T12:29:00.000Z",
        "voteCount": 1,
        "content": "AC Confirmed by paid dumps"
      },
      {
        "date": "2022-10-20T19:27:00.000Z",
        "voteCount": 1,
        "content": "Could you give the reason for C?"
      },
      {
        "date": "2023-03-30T11:46:00.000Z",
        "voteCount": 1,
        "content": "C instead of E because it's absolutely redundant and inefficient to launch another instance when you already have the same master node."
      },
      {
        "date": "2022-09-16T18:28:00.000Z",
        "voteCount": 1,
        "content": "Answer A &amp; E"
      },
      {
        "date": "2022-07-19T23:21:00.000Z",
        "voteCount": 1,
        "content": "A and E."
      },
      {
        "date": "2022-05-22T01:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is A &amp; E"
      },
      {
        "date": "2022-05-20T21:15:00.000Z",
        "voteCount": 3,
        "content": "A and E are right answers. How would installing libraries on Master Nodes resolve anything? Computation happens on Data nodes (slaves) and all required packages should be installed there."
      },
      {
        "date": "2022-11-12T19:24:00.000Z",
        "voteCount": 2,
        "content": "But the quention says the company manually installs third-party libraries on the clusters \"by logging in to the master nodes\". Does it mean that they log in there but install the libraries in slave-nodes?"
      },
      {
        "date": "2022-04-27T22:30:00.000Z",
        "voteCount": 2,
        "content": "Although most of others choose A.E. But I think C is right instead of E.\nFor E: Launch EC2 instance and install these softwares are not easy for EMR. I have installed hadoop and skark one time. And it took my much time. And if I want to make a hadoop/hive/spark... environment to be used as AWS EMR, it will take much efford.\nBut C: I can login into master node with ssh, install the lib, and use the master node EC2 instance to cretae a custom AMI. Although it will waste the previous EMR cluster, but the if I want to establish an hadoop/spark/hive clusters using EC2, I still need several instances to prepare AMI.\nSo, I vote for AC."
      },
      {
        "date": "2022-04-27T22:29:00.000Z",
        "voteCount": 1,
        "content": "Although most of others choose A.E. But I think C is right instead of E.\nFor E:  Launch EC2 instance and install these softwares are not easy for EMR. I have installed hadoop and skark one time. And it took my much time. And if I want to make a hadoop/hive/spark... environment to be used as AWS EMR, it will take much efford.\nBut C: I can login into master node with ssh, install the lib, and use the master node EC2 instance to cretae a custom AMI. Although it will waste the previous EMR cluster, but the if I want to establish an hadoop/spark/hive clusters using EC2, I still need several instances to prepare AMI.\nSo, I vote for AC."
      },
      {
        "date": "2022-02-19T16:20:00.000Z",
        "voteCount": 2,
        "content": "A and E is correct"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/amazon/view/30035-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data engineering team within a shared workspace company wants to build a centralized logging system for all weblogs generated by the space reservation system. The company has a fleet of Amazon EC2 instances that process requests for shared space reservations on its website. The data engineering team wants to ingest all weblogs into a service that will provide a near-real-time search engine. The team does not want to manage the maintenance and operation of the logging system.<br>Which solution allows the data engineering team to efficiently set up the web logging system within AWS?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis data stream to CloudWatch. Choose Amazon OpenSearch Service (Amazon Elasticsearch Service) as the end destination of the weblogs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis Data Firehose delivery stream to CloudWatch. Choose Amazon OpenSearch Service (Amazon Elasticsearch Service) as the end destination of the weblogs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis data stream to CloudWatch. Configure Splunk as the end destination of the weblogs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis Firehose delivery stream to CloudWatch. Configure Amazon DynamoDB as the end destination of the weblogs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T22:12:00.000Z",
        "voteCount": 25,
        "content": "My answer is B"
      },
      {
        "date": "2021-09-25T02:37:00.000Z",
        "voteCount": 3,
        "content": "Agreed with B."
      },
      {
        "date": "2021-10-07T02:27:00.000Z",
        "voteCount": 3,
        "content": "Do you have a link to back this up?  check https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html"
      },
      {
        "date": "2023-05-01T10:20:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-11-07T07:24:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is B as Kinesis Data Firehose provides a managed solution to integrate with CloudWatch logs and stream data into ElasticSearch.\n\nOption A is wrong as Kinesis Data Stream would increase the maintenance and operation of the logging system in terms of data collections and ingestion to ElasticSearch.\n\nOption C is wrong as Kinesis Data Stream would increase the maintenance and operation of the logging system and Splunk would need a separate purchase.\n\nOption D is wrong as Kinesis Firehose does not support DynamoDB as its destination and DynamoDB is not an ideal storage solution for logs."
      },
      {
        "date": "2022-10-30T06:38:00.000Z",
        "voteCount": 2,
        "content": "near real-time search engine for weblogs -&gt; Opensearch. KDF can ingest the logs directly into OS."
      },
      {
        "date": "2022-09-27T05:56:00.000Z",
        "voteCount": 3,
        "content": "---------------------------------------------------------------\nAnswer B - KDS cannot write to open search\n------------------------------------------------------------------------"
      },
      {
        "date": "2022-08-06T14:29:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2022-05-21T05:42:00.000Z",
        "voteCount": 2,
        "content": "I think Answer should be B"
      },
      {
        "date": "2021-11-05T02:08:00.000Z",
        "voteCount": 2,
        "content": "it's A or B because of Elasticsearch as destination; KDS cannot feed from CWLogs but Firehose can, hence the answer is B"
      },
      {
        "date": "2021-10-29T22:41:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-29T18:23:00.000Z",
        "voteCount": 3,
        "content": "It is B. They should use Firehose, not Data Streams (A). This sentence gives it away: \"The team does not want to manage the maintenance and operation of the logging system.\""
      },
      {
        "date": "2021-10-09T08:35:00.000Z",
        "voteCount": 1,
        "content": "I agree with A; Firehose can't stream to CloudWatch"
      },
      {
        "date": "2021-10-22T06:08:00.000Z",
        "voteCount": 3,
        "content": "Sorry I meant B"
      },
      {
        "date": "2021-10-06T04:34:00.000Z",
        "voteCount": 3,
        "content": "B is the right option."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/amazon/view/29829-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to research user turnover by analyzing the past 3 months of user activities. With millions of users, 1.5 TB of uncompressed data is generated each day. A 30-node Amazon Redshift cluster with 2.56 TB of solid state drive (SSD) storage for each node is required to meet the query performance goals.<br>The company wants to run an additional analysis on a year's worth of historical data to examine trends indicating which features are most popular. This analysis will be done once a week.<br>What is the MOST cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the Amazon Redshift cluster to 120 nodes so it has enough storage capacity to hold 1 year of data. Then use Amazon Redshift for the additional analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the data from the last 90 days in Amazon Redshift. Move data older than 90 days to Amazon S3 and store it in Apache Parquet format partitioned by date. Then use Amazon Redshift Spectrum for the additional analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the data from the last 90 days in Amazon Redshift. Move data older than 90 days to Amazon S3 and store it in Apache Parquet format partitioned by date. Then provision a persistent Amazon EMR cluster and use Apache Presto for the additional analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResize the cluster node type to the dense storage node type (DS2) for an additional 16 TB storage capacity on each individual node in the Amazon Redshift cluster. Then use Amazon Redshift for the additional analysis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T19:37:00.000Z",
        "voteCount": 33,
        "content": "B. Redshift Spectrum. \"Amazon Redshift Spectrum executes queries across thousands of parallelized nodes to deliver fast results, regardless of the complexity of the query or the amount of data. \"\nhttps://aws.amazon.com/redshift/features/"
      },
      {
        "date": "2021-10-01T07:32:00.000Z",
        "voteCount": 1,
        "content": "Agree!"
      },
      {
        "date": "2021-10-07T08:28:00.000Z",
        "voteCount": 1,
        "content": "why 30 node can save 90 days data?"
      },
      {
        "date": "2021-10-31T09:11:00.000Z",
        "voteCount": 5,
        "content": "You are right, 30 nodes cannot save 90 days uncompressed data. But we can always compress the data while storing in Redshift. So that will definitely reduce the storage requirement and can be managed by the 30 nodes."
      },
      {
        "date": "2021-11-01T14:24:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/aws/data-compression-improvements-in-amazon-redshift/"
      },
      {
        "date": "2023-05-01T10:21:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-11-07T07:42:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B as the data can be stored in Redshift for 90 days for analyzing the past 3 months of user activities. Data older than 90 days can be moved to S3 and analyzed using Redshift Spectrum once a week. This provides the most cost-effective solution.\n\n\nUsing Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.\nOptions A &amp; D are wrong as increasing the size of the Redshift cluster would not be cost-effective.\n\n\nOption C is wrong as analyzing data using a persistent EMR cluster would not be cost-effective."
      },
      {
        "date": "2022-07-21T17:09:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2022-04-30T14:42:00.000Z",
        "voteCount": 2,
        "content": "Answer - B"
      },
      {
        "date": "2021-11-19T20:29:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2021-11-01T09:55:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-23T11:55:00.000Z",
        "voteCount": 3,
        "content": "B for sure."
      },
      {
        "date": "2021-09-22T13:38:00.000Z",
        "voteCount": 3,
        "content": "Agree its B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/amazon/view/30037-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A bank operates in a regulated environment. The compliance requirements for the country in which the bank operates say that customer data for each state should only be accessible by the bank's employees located in the same state. Bank employees in one state should NOT be able to access data for customers who have provided a home address in a different state.<br>The bank's marketing team has hired a data analyst to gather insights from customer data for a new campaign being launched in certain states. Currently, data linking each customer account to its home state is stored in a tabular .csv file within a single Amazon S3 folder in a private S3 bucket. The total size of the S3 folder is 2 GB uncompressed. Due to the country's compliance requirements, the marketing team is not able to access this folder.<br>The data analyst is responsible for ensuring that the marketing team gets one-time access to customer data for their campaign analytics project, while being subject to all the compliance requirements and controls.<br>Which solution should the data analyst implement to meet the desired requirements with the LEAST amount of setup effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-arrange data in Amazon S3 to store customer data about each state in a different S3 folder within the same bucket. Set up S3 bucket policies to provide marketing employees with appropriate data access under compliance controls. Delete the bucket policies after the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad tabular data from Amazon S3 to an Amazon EMR cluster using s3DistCp. Implement a custom Hadoop-based row-level security solution on the Hadoop Distributed File System (HDFS) to provide marketing employees with appropriate data access under compliance controls. Terminate the EMR cluster after the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad tabular data from Amazon S3 to Amazon Redshift with the COPY command. Use the built-in row-level security feature in Amazon Redshift to provide marketing employees with appropriate data access under compliance controls. Delete the Amazon Redshift tables after the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad tabular data from Amazon S3 to Amazon QuickSight Enterprise edition by directly importing it as a data source. Use the built-in row-level security feature in Amazon QuickSight to provide marketing employees with appropriate data access under compliance controls. Delete Amazon QuickSight data sources after the project is complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T04:01:00.000Z",
        "voteCount": 26,
        "content": "My answer is D"
      },
      {
        "date": "2021-09-20T07:18:00.000Z",
        "voteCount": 4,
        "content": "agreed with D."
      },
      {
        "date": "2022-11-07T07:44:00.000Z",
        "voteCount": 8,
        "content": "Correct answer is D as using QuickSight with its built-in-row-level security features allows the data analyst to provide limited one-time access while maintaining data compliance requirements and controls and a minimal amount of setup.\n\nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. Only the people whom you shared with can see any of the data. By adding row-level security, you can further control their access.\n\n\nOption A is wrong as it would take some amount of setup to repartition the data in S3.\n\nOptions B &amp; C are wrong as using EMR and Redshift would need set up and provisioning effort."
      },
      {
        "date": "2024-01-05T13:01:00.000Z",
        "voteCount": 1,
        "content": "I think option A is right answer.\nOption A proposes reorganizing the data by storing customer data for each state in a different S3 folder within the same bucket. This makes it easier to manage access control at the folder level.\n\nSetting up S3 bucket policies allows for controlling access to specific folders, meeting the compliance requirements without requiring additional services.\n\nAfter the project is complete, the bucket policies can be deleted, ensuring that access control is removed as needed.\nOption C and D would be costlier as we need to spin up redshift or Quicksight for 2 GB data"
      },
      {
        "date": "2023-05-01T10:22:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-02-05T07:54:00.000Z",
        "voteCount": 1,
        "content": "Least operational overhead is D"
      },
      {
        "date": "2022-11-06T00:43:00.000Z",
        "voteCount": 1,
        "content": "Redshift now has RLS. Can't the answer be C as well?\nhttps://aws.amazon.com/about-aws/whats-new/2022/07/amazon-redshift-row-level-security/\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_rls.html\nhttps://aws.amazon.com/blogs/big-data/achieve-fine-grained-data-security-with-row-level-access-control-in-amazon-redshift/"
      },
      {
        "date": "2022-11-21T00:23:00.000Z",
        "voteCount": 2,
        "content": "The solution should be with least efforts. Setting up redshift is big task hence C is incorrect."
      },
      {
        "date": "2022-10-29T19:27:00.000Z",
        "voteCount": 1,
        "content": "answer is D"
      },
      {
        "date": "2022-10-02T19:23:00.000Z",
        "voteCount": 1,
        "content": "Will go with D."
      },
      {
        "date": "2022-07-26T19:27:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-05-22T01:12:00.000Z",
        "voteCount": 1,
        "content": "D, everything else has too much setup or is not usable (A)"
      },
      {
        "date": "2021-11-07T09:09:00.000Z",
        "voteCount": 4,
        "content": "Answer: D Quicksight Enterprise provides row-level security.\nhttps://docs.aws.amazon.com/ja_jp/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2021-11-02T03:05:00.000Z",
        "voteCount": 3,
        "content": "for C there is no built-in row-level security feature in Amazon Redshift it is in quicksight\nso i think C is not the correct answer"
      },
      {
        "date": "2024-01-18T06:23:00.000Z",
        "voteCount": 1,
        "content": "There is! https://docs.aws.amazon.com/redshift/latest/dg/t_rls.html\n\"Using row-level security (RLS) in Amazon Redshift, you can have granular access control over your sensitive data. You can decide which users or roles can access specific records of data within schemas or tables, based on security policies that are defined at the database objects level. In addition to column-level security, where you can grant users permissions to a subset of columns, use RLS policies to further restrict access to particular rows of the visible columns.\""
      },
      {
        "date": "2021-10-31T20:21:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-28T03:56:00.000Z",
        "voteCount": 3,
        "content": "Topic 2 - Question 57\nA healthcare company uses Amazon S3 to store all its data and is planning to use Amazon EMR, backed with EMR File System (EMRFS), to process and transform the data. The company data is stored in multiple buckets and encrypted using different encryption keys for each bucket. How can the EMR Cluster be configured to access the encrypted data?\n\nA) Modify the S3 bucket policies to grant public access to the S3 buckets.\nB) Create a security configuration that specifies the encryption keys for the buckets using per bucket encryption overrides.\nC) Configure the cluster to use 53 Select to access the data in the buckets and specify the encryption keys as options.\nD) Copy the encryption keys to the master node and create a security configuration that references the keys."
      },
      {
        "date": "2021-10-28T22:37:00.000Z",
        "voteCount": 3,
        "content": "Again BrainCert practice question:\nCorrect Answer\nB. Create a security configuration that specifies the encryption keys for the buckets using per bucket encryption overrides.\nExplanation\nCorrect answer is B as EMR provides per bucket encryption overrides option. Refer AWS documentation - EMR Securing Data\nhttps://aws.amazon.com/blogs/big-data/secure-your-data-on-amazon-emr-using-native-ebs-and-per-bucket-s3-encryption-options/\nWith S3 encryption on Amazon EMR, all the encryption modes use a single CMK by default to encrypt objects in S3. If you have highly sensitive content in specific S3 buckets, you may want to manage the encryption of these buckets separately by using different CMKs or encryption modes for individual buckets. You can accomplish this using the per bucket encryption overrides option in Amazon EMR."
      },
      {
        "date": "2021-10-30T00:33:00.000Z",
        "voteCount": 1,
        "content": "Are BrainCert practice question not real exam questions?"
      },
      {
        "date": "2022-07-26T19:25:00.000Z",
        "voteCount": 1,
        "content": "@LRyan2020  Why are these questions being posted here ? Are they sample questions or exam practice questions ?"
      },
      {
        "date": "2021-10-27T22:24:00.000Z",
        "voteCount": 1,
        "content": "Topic 2 - Question 56\nA company currently processes real-time streaming data using Apache Kafka. The company is facing challenges with managing, setting up, and scaling during production. The company want to optimize the deployment with the Kafka brokers. The solution should be managed by AWS, secure, and require minimal changes to the current client code.\n\nWhich solution meets these requirements?\n\nA) Use Apache Zookeeper to scale Kafka installed on Amazon EC2 instances.\nB) Use Amazon Managed Streaming for Kafka to scale the brokers.\nC) Use Apache Zookeeper to scale the brokers of Amazon Managed Streaming for Kafka\nD) Scale the number of client machines, and use a single broker with Amazon Managed Streaming for Kafka."
      },
      {
        "date": "2021-10-28T19:38:00.000Z",
        "voteCount": 3,
        "content": "@LRyan2020. These are BrainCert practice questions buddy !!!! \nCorrect Answer\nB. Use Amazon Managed Streaming for Kafka to scale the brokers.\nExplanation\nCorrect answer is B as Amazon Managed Streaming is fully managed, secure Kafka streaming solution with no operation overhead. Refer AWS documentation - Managed Streaming Kafka https://aws.amazon.com/msk/ Amazon MSK lets you focus on creating your streaming applications without having to worry about the operational overhead of managing your Apache Kafka environment. Amazon MSK manages the provisioning, configuration, and maintenance of Apache Kafka clusters and Apache ZooKeeper nodes for you. Amazon MSK also shows key Apache Kafka performance metrics in the AWS console. Options A, C &amp; D are wrong as they are not managed by AWS and would need user involvement."
      },
      {
        "date": "2021-10-27T03:30:00.000Z",
        "voteCount": 1,
        "content": "Question 55\nAn online retailer is planning to capture clickstream data from its ecommerce website and then use the data to drive a new custom-built recommendation engine that provides product recommendations to online users. The retailer will use Amazon Kinesis Data Streams to ingest the streaming data and Amazon Kinesis Data Analytics to perform SQL queries on the stream, using windowed queries to process the data that arrives at inconsistent intervals.\nWhat type of windowed query must be used to aggregate the data using time-based windows that open as data arrives?\n\nA) Continuous queries\n B) Tumbling window queries\nC) Sliding window queries\nD) Stagger window queries"
      },
      {
        "date": "2021-10-27T04:27:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer\nD. Stagger window queries\nExplanation\nCorrect answer is D as using stagger windows is a windowing method that is suited for\nanalyzing groups of data that arrive at inconsistent times. It is well suited for any timeseries\nanalytics use case, such as a set of related sales or log records.\nRefer AWS documentation - Kinesis Stagger Window Concepts\nOption A is wrong as Continuous queries is a query over a stream that executes\ncontinuously over streaming data. This continuous execution enables scenarios, such as\nthe ability for applications to continuously query a stream and generate alerts.\nOption B is wrong as Tumbling window queries are suitable when a windowed query\nprocesses each window in a non-overlapping manner\nOption C is wrong as Sliding window queries help define a time-based or row-based\nwindow, instead of grouping records using GROUP BY"
      },
      {
        "date": "2021-10-27T11:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/kinesisanalytics/latest/dev/stagger-window-concepts.html"
      },
      {
        "date": "2021-10-23T05:00:00.000Z",
        "voteCount": 1,
        "content": "Question-54\nA marketing company is storing its campaign response data in Amazon S3. A consistent set of sources has generated the data for each campaign. The data is saved into Amazon S3 as .csv files. A business analyst will use Amazon Athena to analyze each campaign's data. The company needs the cost of ongoing data analysis with Athena to be minimized. Which combination of actions should a data analytics specialist take to meet these requirements? (Select TWO.)\nA. Convert the .csv files to Apache Parquet. \nB. Convert the .csv files to Apache Avro. \nC. Partition the data by campaign. \nD. Partition the data by source. \nE. Compress the .csv files."
      },
      {
        "date": "2021-10-24T09:11:00.000Z",
        "voteCount": 4,
        "content": "Answer : A, C"
      },
      {
        "date": "2021-11-02T08:58:00.000Z",
        "voteCount": 1,
        "content": "E also works.\n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2021-11-03T18:27:00.000Z",
        "voteCount": 2,
        "content": "A and C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/amazon/view/51699-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online gaming company is using an Amazon Kinesis Data Analytics SQL application with a Kinesis data stream as its source. The source sends three non-null fields to the application: player_id, score, and us_5_digit_zip_code.<br>A data analyst has a .csv mapping file that maps a small number of us_5_digit_zip_code values to a territory code. The data analyst needs to include the territory code, if one exists, as an additional output of the Kinesis Data Analytics application.<br>How should the data analyst meet this requirement while minimizing costs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the contents of the mapping file in an Amazon DynamoDB table. Preprocess the records as they arrive in the Kinesis Data Analytics application with an AWS Lambda function that fetches the mapping and supplements each record to include the territory code, if one exists. Change the SQL query in the application to include the new field in the SELECT statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the mapping file in an Amazon S3 bucket and configure the reference data column headers for the .csv file in the Kinesis Data Analytics application. Change the SQL query in the application to include a join to the file's S3 Amazon Resource Name (ARN), and add the territory code field to the SELECT columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the mapping file in an Amazon S3 bucket and configure it as a reference data source for the Kinesis Data Analytics application. Change the SQL query in the application to include a join to the reference table and add the territory code field to the SELECT columns.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the contents of the mapping file in an Amazon DynamoDB table. Change the Kinesis Data Analytics application to send its output to an AWS Lambda function that fetches the mapping and supplements each record to include the territory code, if one exists. Forward the record from the Lambda function to the original application destination."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-29T19:07:00.000Z",
        "voteCount": 23,
        "content": "ANSWER:C\nEXPLANATION:Since ,Kinesis data analytics only consume the data from KDS and KDF ,so S3 Data reference can be attached to it as a datasource ."
      },
      {
        "date": "2021-10-07T01:47:00.000Z",
        "voteCount": 7,
        "content": "C - https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html"
      },
      {
        "date": "2023-05-01T10:23:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-11-07T07:46:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is C as Kinesis Data Analytics allows adding S3 source for reference data which can be referred by Kinesis Data Analytics for data enrichment. Kinesis Data Analytics stores it as an in-application reference table.\n\nOptions A &amp; D are wrong as they are not cost-effective.\n\nOption B is wrong as Kinesis Data Analytics stores the reference data as an in-application reference table."
      },
      {
        "date": "2022-08-06T14:28:00.000Z",
        "voteCount": 1,
        "content": "ANSWER:C"
      },
      {
        "date": "2021-12-05T16:48:00.000Z",
        "voteCount": 1,
        "content": "I think B is more concise than C. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/sch-mapping.html\nSince the mapping file is a .csv file, I think we need the columns headers to avoid schema discovery for kda joins. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html\nAlso IAM role ARN is needed to Add the Reference Data Source to the Application Configuration"
      },
      {
        "date": "2021-12-14T09:08:00.000Z",
        "voteCount": 2,
        "content": "My bad! C is correct"
      },
      {
        "date": "2021-10-07T09:55:00.000Z",
        "voteCount": 1,
        "content": "I think A is correct."
      },
      {
        "date": "2021-10-14T08:32:00.000Z",
        "voteCount": 1,
        "content": "sorry,change to C"
      },
      {
        "date": "2021-09-21T00:30:00.000Z",
        "voteCount": 1,
        "content": "C should be on one."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/amazon/view/51700-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has collected more than 100 TB of log files in the last 24 months. The files are stored as raw text in a dedicated Amazon S3 bucket. Each object has a key of the form year-month-day_log_HHmmss.txt where HHmmss represents the time the log file was initially created. A table was created in Amazon Athena that points to the S3 bucket. One-time queries are run against a subset of columns in the table several times an hour.<br>A data analyst must make changes to reduce the cost of running these queries. Management wants a solution with minimal maintenance overhead.<br>Which combination of steps should the data analyst take to meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the log files to Apace Avro format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a key prefix of the form date=year-month-day/ to the S3 objects to partition the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the log files to Apache Parquet format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a key prefix of the form year-month-day/ to the S3 objects to partition the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop and recreate the table with the PARTITIONED BY clause. Run the ALTER TABLE ADD PARTITION statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop and recreate the table with the PARTITIONED BY clause. Run the MSCK REPAIR TABLE statement.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T05:06:00.000Z",
        "voteCount": 30,
        "content": "ANSWER:B,C,F\nOPTION B: Add a key prefix of the form date=year-month-day/ to the S3 objects to partition the data.\nEXPLAINATION: Your Amazon S3 bucket can support 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned prefix.So with every partition prefix we get additional support and that is why it is wise to add prefix especially when we have large set of data .\nLINK:https://aws.amazon.com/premiumsupport/knowledge-center/s3-object-key-naming-pattern/\n\nOPTION C:Convert the log files to Apache Parquet format.\nEXPLAINATION:Parquet format is columnar based and which improves your query performance when done for Athena .\n\nOPTION:F:Drop and recreate the table with the PARTITIONED BY clause. Run the MSCK REPAIR TABLE statement.\nEXPLAINATION:MSCK REPAIR TABLE compares the partitions in the table metadata and the partitions in S3. If new partitions are present in the S3 location that you specified when you created the table, it adds those partitions to the metadata and to the Athena table."
      },
      {
        "date": "2021-12-02T21:49:00.000Z",
        "voteCount": 2,
        "content": "Agree - B C F\nif it was case of removing partitions  - D would have been better\nMSCK REPAIR TABLE only adds partitions to metadata; it does not remove them. To remove partitions from metadata after the partitions have been manually deleted in Amazon S3, run the command ALTER TABLE table-name DROP PARTITION. For more information see ALTER TABLE DROP PARTITION."
      },
      {
        "date": "2022-09-04T12:10:00.000Z",
        "voteCount": 1,
        "content": "Agree - Answer is BCF"
      },
      {
        "date": "2021-11-01T21:22:00.000Z",
        "voteCount": 7,
        "content": "Details why B and not D\nhttps://docs.aws.amazon.com/athena/latest/ug/partitions.html"
      },
      {
        "date": "2023-05-01T10:24:00.000Z",
        "voteCount": 1,
        "content": "BCF: I passed the test"
      },
      {
        "date": "2022-07-19T23:04:00.000Z",
        "voteCount": 1,
        "content": "ANSWER: B,C,F"
      },
      {
        "date": "2022-07-14T09:21:00.000Z",
        "voteCount": 2,
        "content": "initially went with BCE\nbut E is wrong as Athena favors \"=\" hive style partition year=2021/month=01/day=26/ with MSCK repair \n\nfor non-hive partition style data/2021/01/26/ have to use ALTER TABLE"
      },
      {
        "date": "2021-11-17T13:16:00.000Z",
        "voteCount": 1,
        "content": "ans is B,C,F"
      },
      {
        "date": "2021-11-03T17:43:00.000Z",
        "voteCount": 1,
        "content": "B, C, F is the answer. C, D, E is a valid solution, but would in this case be more work. MSCK REPAIR TABLE scans through the directories on S3 to find partitions, but requires Hive-style partitioning, i. e. date=Y-M-D. You should almost never rely on MSCK REPAIR TABLE, it\u2019s extremely inefficient, but the docs are full of examples using it so an exam would be too. The real way to do this is to use a Y-M-D/ partitioning scheme and partition projection."
      },
      {
        "date": "2021-10-05T06:43:00.000Z",
        "voteCount": 1,
        "content": "Can anyone please explain why B and not D?\nI understand C&amp;F."
      },
      {
        "date": "2021-10-28T08:16:00.000Z",
        "voteCount": 2,
        "content": "If you use a prefix of the form 'date=2020-11-11/' , you can use Athena to filter by date field.\n\nhttps://www.mikulskibartosz.name/partitioning-s3-data-by-date/"
      },
      {
        "date": "2021-10-28T18:18:00.000Z",
        "voteCount": 1,
        "content": "Can you please explain why do you need \"date=\" as a prefix?"
      },
      {
        "date": "2023-01-25T21:29:00.000Z",
        "voteCount": 1,
        "content": "Query possible like \"..date &gt;= 2020-11-11\""
      },
      {
        "date": "2022-04-22T15:59:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/partitions.html"
      },
      {
        "date": "2021-09-24T10:05:00.000Z",
        "voteCount": 5,
        "content": "B, C, F"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/amazon/view/51701-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has an application that ingests streaming data. The company needs to analyze this stream over a 5-minute timeframe to evaluate the stream for anomalies with Random Cut Forest (RCF) and summarize the current count of status codes. The source and summarized data should be persisted for future use.<br>Which approach would enable the desired outcome while keeping data persistence costs low?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data stream with Amazon Kinesis Data Streams. Have an AWS Lambda consumer evaluate the stream, collect the number status codes, and evaluate the data against a previously trained RCF model. Persist the source and results as a time series to Amazon DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data stream with Amazon Kinesis Data Streams. Have a Kinesis Data Analytics application evaluate the stream over a 5-minute window using the RCF function and summarize the count of status codes. Persist the source and results to Amazon S3 through output delivery to Kinesis Data Firehouse.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data stream with Amazon Kinesis Data Firehose with a delivery frequency of 1 minute or 1 MB in Amazon S3. Ensure Amazon S3 triggers an event to invoke an AWS Lambda consumer that evaluates the batch data, collects the number status codes, and evaluates the data against a previously trained RCF model. Persist the source and results as a time series to Amazon DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data stream with Amazon Kinesis Data Firehose with a delivery frequency of 5 minutes or 1 MB into Amazon S3. Have a Kinesis Data Analytics application evaluate the stream over a 1-minute window using the RCF function and summarize the count of status codes. Persist the results to Amazon S3 through a Kinesis Data Analytics output to an AWS Lambda integration."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-05T22:16:00.000Z",
        "voteCount": 20,
        "content": "Answer is B.\nFirst of all the question mentions about analyzing the stream over a 5-minute timeframe.\nOnly option B satisfies the above condition. Also KDA uses RCF.\n\nQ: How can I perform real-time anomaly detection in Kinesis Data Analytics?\nKinesis Data Analytics includes pre-built SQL functions for several advanced analytics including one for anomaly detection. You can simply make a call to this function from your SQL code for detecting anomalies in real-time. Kinesis Data Analytics uses the Random Cut Forest algorithm to implement anomaly detection. For more information on Random Cut Forests, see the Streaming Data Anomaly Detection whitepaper."
      },
      {
        "date": "2021-10-13T07:20:00.000Z",
        "voteCount": 6,
        "content": "Looks like 'B' it is."
      },
      {
        "date": "2023-10-26T17:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is . B\nIngest the data stream with Amazon Kinesis Data Streams. Have a Kinesis Data Analytics application evaluate the stream over a 5-minute window using the RCF function and summarize the count of status codes. Persist the source and results to Amazon S3 through output delivery to Kinesis Data Firehouse"
      },
      {
        "date": "2023-05-01T10:25:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-12-02T09:00:00.000Z",
        "voteCount": 1,
        "content": "Why further questions are not available?"
      },
      {
        "date": "2023-02-20T00:49:00.000Z",
        "voteCount": 1,
        "content": "Because you don't pay for further pages."
      },
      {
        "date": "2022-11-07T07:54:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B as Kinesis Data Streams can be used to capture the data with Kinesis Data Analytics to perform RCF over a 5 minutes window pushing the source and results data into Kinesis Data Firehose for S3 persistence.\n\nhttps://aws.amazon.com/blogs/big-data/perform-near-real-time-analytics-on-streaming-data-with-amazon-kinesis-and-amazon-elasticsearch-service/\n\nOption A is wrong as it would require more effort and the cost would be higher if storing the data in DynamoDB.\n\nOption C is wrong as Kinesis Data Firehose with a delivery frequency of 1 minute or 1 MB would not be able to meet the requirement accurately. Also, the cost would be higher if storing the data in DynamoDB.\n\nOption D is wrong as having the Kinesis Data Analytics application evaluate the stream over a 1-minute window would not meet the 5 minutes requirement."
      },
      {
        "date": "2022-10-29T19:27:00.000Z",
        "voteCount": 2,
        "content": "answer is B"
      },
      {
        "date": "2022-10-02T19:22:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-08-06T13:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-11-04T02:23:00.000Z",
        "voteCount": 3,
        "content": "ANSWER:OPTION B\nEXPLAINATION: Since it's a real time streaming data ,so filter out option C,D as KFH is near real time .Now out of option A and B ,S3 has a low data persistence cost ."
      },
      {
        "date": "2021-11-04T13:33:00.000Z",
        "voteCount": 6,
        "content": "Answer is B. However no where it is mentioned about real-time in question. Streaming Data can also be near real-time.Ruling out KDF due to the above reason is incorrect."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/amazon/view/51702-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online retailer needs to deploy a product sales reporting solution. The source data is exported from an external online transaction processing (OLTP) system for reporting. Roll-up data is calculated each day for the previous day's activities. The reporting system has the following requirements:<br>\u2711 Have the daily roll-up data readily available for 1 year.<br>\u2711 After 1 year, archive the daily roll-up data for occasional but immediate access.<br>\u2711 The source data exports stored in the reporting system must be retained for 5 years. Query access will be needed only for re-evaluation, which may occur within the first 90 days.<br>Which combination of actions will meet these requirements while keeping storage costs to a minimum? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the source data initially in the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Glacier Deep Archive 90 days after creation, and then deletes the data 5 years after creation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the source data initially in the Amazon S3 Glacier storage class. Apply a lifecycle configuration that changes the storage class from Amazon S3 Glacier to Amazon S3 Glacier Deep Archive 90 days after creation, and then deletes the data 5 years after creation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the daily roll-up data initially in the Amazon S3 Standard storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Glacier Deep Archive 1 year after data creation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the daily roll-up data initially in the Amazon S3 Standard storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) 1 year after data creation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the daily roll-up data initially in the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Apply a lifecycle configuration that changes the storage class to Amazon S3 Glacier 1 year after data creation."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T17:52:00.000Z",
        "voteCount": 28,
        "content": "A,D for sure"
      },
      {
        "date": "2021-12-27T08:16:00.000Z",
        "voteCount": 14,
        "content": "B and D\nHave the daily roll-up data readily available for 1 year.  After 1 year, archive the daily roll-up data for occasional but immediate access -&gt; D: Standard and Standard IA after 1 year.\nQuery access will be needed ONLY for re-evaluation, which MAY occur within the first 90 days -&gt; Glacier is enough, there is no require for immediate access, and we also don't know it's frequency, but \"ONLY\" and \"MAY\" keywords can indicate that it's rare to do re-evaluation.\nQuestion asks for minimizing the cost, so B, not A"
      },
      {
        "date": "2023-08-30T09:48:00.000Z",
        "voteCount": 1,
        "content": "B and D, Objects cannot be directly transitioned to Amazon S3 Standard-Infrequent Access (S3 Standard-IA), objects must be stored for a minimum of 30 days before they can be transitioned to this storage class- So it has to be stored as Amazon S3 Standard storage class for the first year to satisfy immediate access requirement."
      },
      {
        "date": "2023-08-04T01:19:00.000Z",
        "voteCount": 2,
        "content": "The right answer is BE. \nS3 Glacier has instant access."
      },
      {
        "date": "2023-08-04T01:21:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/"
      },
      {
        "date": "2023-08-04T01:22:00.000Z",
        "voteCount": 1,
        "content": "But the occasional keyword points to S3 IA, so probable BD"
      },
      {
        "date": "2023-05-01T10:26:00.000Z",
        "voteCount": 1,
        "content": "BD: I passed the test"
      },
      {
        "date": "2023-04-02T07:16:00.000Z",
        "voteCount": 1,
        "content": "This seems more appropriate to me"
      },
      {
        "date": "2023-04-02T15:01:00.000Z",
        "voteCount": 1,
        "content": "I meant BD and not BC"
      },
      {
        "date": "2023-04-01T19:35:00.000Z",
        "voteCount": 4,
        "content": "It is B instead of A because the source data is not really used for any immediate querying ever. It is processing to create the roll-up data and that's it. So it makes sense to store it in the cheapest storage, which is Glacier."
      },
      {
        "date": "2023-02-04T06:37:00.000Z",
        "voteCount": 2,
        "content": "Two kinds of data. For the roll-up data everyone agrees on D instead of E.\nFor the source data it is B not A because the source data does not have to be accessed immediately."
      },
      {
        "date": "2023-01-06T14:40:00.000Z",
        "voteCount": 3,
        "content": "B &amp; D https://aws.amazon.com/s3/pricing/\nB because S3 Glacier Instant Retrieval - For long-lived archive data accessed once a quarter with instant retrieval in milliseconds"
      },
      {
        "date": "2022-12-11T08:29:00.000Z",
        "voteCount": 3,
        "content": "As per Udemy preparation course, the Glacier that these questions mention must be the \"Glacier Flexible Retrieval\" that has 3 options: expedited (1 to 5 minutes), standard (3 to 5 hours) and bulk (5 to 12 hours). Only the bulk option is free.\nSo, the question here about A vs B is the cost of possible retrievals vs cost of IA. I think that possibly the glacier can become costlier as we MAY have to access the data a uncertain number of times.\nHaving this said I would pick A &amp; D, but I'm still very doubtfull tbh..."
      },
      {
        "date": "2022-11-18T09:45:00.000Z",
        "voteCount": 1,
        "content": "Option E is obvious for roll-up data and seems clear from the comments here. Between A and B, I would choose B as it has lesser cost, which is clearly stated in the question. Refer here for cost structure: https://aws.amazon.com/s3/pricing/"
      },
      {
        "date": "2022-11-07T10:13:00.000Z",
        "voteCount": 7,
        "content": "Correct answers are A &amp; D\n\nOption A as the source data exports can be stored in (S3 Standard-IA) storage class for 90 days if needed, and then moved to S3 Glacier Deep Archive for 5 years before it is expired.\n\nOption D as the roll-up data can be stored in Standard class for a year and then moved to S3 Standard-IA to save cost but still provide immediate access.\n\nOption B is wrong as although you can transition the objects directly to the S3 Glacier storage class, the data would not be readily available for roll-up calculation and would cost more to perform the roll-up.\n\nOptions C &amp; E are wrong as S3 Glacier and Glacier Deep Archive would not be able to provide immediate access to the roll-up data."
      },
      {
        "date": "2022-11-06T01:13:00.000Z",
        "voteCount": 3,
        "content": "B and D\nB (S3 Glacier) instead of A (S3 Standard-IA) keeps storage costs to a minimum. Query access to the source data may or may not occur within the first 90 days. S3 Glacier will do."
      },
      {
        "date": "2022-09-27T06:31:00.000Z",
        "voteCount": 1,
        "content": "\u2711 Have the daily roll-up data readily available for 1 year.\n\u2711 After 1 year, archive the daily roll-up data for occasional but immediate access.\nWhich combination of actions will meet these requirements while keeping storage costs to a minimum?\n\n Answer E - Cost is low and readily available \nStore the daily roll-up data initially in the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. \nApply a lifecycle configuration that changes the storage class to Amazon S3 Glacier 1 year after data creation."
      },
      {
        "date": "2022-09-26T22:17:00.000Z",
        "voteCount": 1,
        "content": "Even S3 Glacier can have instant retrieval\nhttps://aws.amazon.com/s3/storage-classes/glacier/\nS3 Glacier Instant Retrieval delivers the lowest cost storage, up to 68% lower cost (than S3 Standard-Infrequent Access), for long-lived data that is accessed once per quarter and requires millisecond retrieval.."
      },
      {
        "date": "2022-09-26T22:18:00.000Z",
        "voteCount": 1,
        "content": "Sorry, should be B&amp;D"
      },
      {
        "date": "2022-09-23T19:00:00.000Z",
        "voteCount": 3,
        "content": "Between A and B, I think for the context of the exam, it should be B. 90 days is a keyword for Glacier as all Glacier tiers require 90 days min storage"
      },
      {
        "date": "2022-07-27T23:42:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: AD"
      },
      {
        "date": "2023-01-18T02:07:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer : B &amp; D \nLink : https://aws.amazon.com/s3/pricing/\nB because S3 Glacier Instant Retrieval - For long-lived archive data accessed once a quarter with instant retrieval in milliseconds."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/amazon/view/51698-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company needs to store objects containing log data in JSON format. The objects are generated by eight applications running in AWS. Six of the applications generate a total of 500 KiB of data per second, and two of the applications can generate up to 2 MiB of data per second.<br>A data engineer wants to implement a scalable solution to capture and store usage data in an Amazon S3 bucket. The usage data objects need to be reformatted, converted to .csv format, and then compressed before they are stored in Amazon S3. The company requires the solution to include the least custom code possible and has authorized the data engineer to request a service quota increase if needed.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Kinesis Data Firehose delivery stream for each application. Write AWS Lambda functions to read log data objects from the stream for each application. Have the function perform reformatting and .csv conversion. Enable compression on all the delivery streams.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Kinesis data stream with one shard per application. Write an AWS Lambda function to read usage data objects from the shards. Have the function perform .csv conversion, reformatting, and compression of the data. Have the function store the output in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Kinesis data stream for each application. Write an AWS Lambda function to read usage data objects from the stream for each application. Have the function perform .csv conversion, reformatting, and compression of the data. Have the function store the output in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore usage data objects in an Amazon DynamoDB table. Configure a DynamoDB stream to copy the objects to an S3 bucket. Configure an AWS Lambda function to be triggered when objects are written to the S3 bucket. Have the function convert the objects into .csv format."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-07T05:49:00.000Z",
        "voteCount": 17,
        "content": "ANSWER:A\nEXPLANATION:\nFirehose can invoke an AWS Lambda function to transform incoming data before delivering it to destinations\nAmazon Kinesis Data Firehose allows you to compress your data before delivering it to Amazon S3 ."
      },
      {
        "date": "2023-01-16T12:19:00.000Z",
        "voteCount": 2,
        "content": "You can use the Amazon Kinesis Data Firehose API to send data to a Kinesis Data Firehose delivery stream using the AWS SDK\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-sdk.html \n\nAs explicitly mentioned in the case: \nTo request an increase in quota, use the Amazon Kinesis Data Firehose Limits form\nhttps://docs.aws.amazon.com/firehose/latest/dev/limits.html"
      },
      {
        "date": "2023-07-05T11:01:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why not opcion B yet."
      },
      {
        "date": "2023-07-05T11:03:00.000Z",
        "voteCount": 1,
        "content": "aaaaaaaah, I got it. AWS KDS + AWS Lambda means Lambda polling from AWS KDS steam, but in the case of AWS KDF + AWS Lambda, Lambda is not polling from AWS KDF, so it's faster than the other solution. It also requires custom code, because you have to create 8 shards..."
      },
      {
        "date": "2023-12-19T03:59:00.000Z",
        "voteCount": 1,
        "content": "The requirements can be met by using a combination of AWS services: Amazon Kinesis Data Firehose, AWS Lambda, and Amazon S3.\n\nHere's a high-level overview of how these services can be used to meet the requirements:\nThis solution requires minimal custom code - only the Lambda function to convert JSON to .csv needs to be written. The rest of the pipeline can be configured using AWS Management Console or AWS CLI."
      },
      {
        "date": "2023-11-08T22:41:00.000Z",
        "voteCount": 1,
        "content": "I choose A.\nBecause of A has minimum custom code.\nD is wrong . not compress.\nBC are wrong. They have compress code in Lambda."
      },
      {
        "date": "2023-11-08T22:43:00.000Z",
        "voteCount": 1,
        "content": "Probably, This question asks that we know Firehose's compress feature or not."
      },
      {
        "date": "2023-05-01T10:27:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-02T07:19:00.000Z",
        "voteCount": 2,
        "content": "Least cutom code"
      },
      {
        "date": "2023-01-15T09:27:00.000Z",
        "voteCount": 1,
        "content": "Firehose can it connect directly connect to source ? by that rule - A does not seem to be right .. C and B - KDS are required for large volume 10GB and above in general .. D seems to be the closest though compression to parquet is not mentioned .."
      },
      {
        "date": "2023-01-26T18:37:00.000Z",
        "voteCount": 3,
        "content": "A source can be a logging server running on Amazon EC2 instances, an application running on mobile devices, or a sensor on an IoT device. You can connect your sources to Kinesis Data Firehose using 1) Amazon Kinesis Data Firehose API, which uses the AWS SDK for Java, .NET, Node.js, Python, or Ruby. 2) Kinesis Data Stream, where Kinesis Data Firehose reads data easily from an existing Kinesis data stream and load it into Kinesis Data Firehose destinations...\nin Kinesis Data Firehose FAQ"
      },
      {
        "date": "2022-11-07T10:15:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A as Kinesis Data Firehose can be used to collect the data, it integrates with Lambda to provide custom transformation and then stores the compressed data to S3. There is a limit of throughput that KDF can support and it can be increased using quota increase requests.\n\nOptions B &amp; C are wrong as Kinesis Data Stream would require more handling and custom code.\n\nOption D is wrong as it does not meet the requirement that the usage data objects need to be reformatted, converted to .csv format, and then compressed before they are stored in Amazon S3."
      },
      {
        "date": "2022-10-09T00:15:00.000Z",
        "voteCount": 4,
        "content": "Go with C:\nA\u2013 Doesn\u2019t mention saving to S3, and the Lambda read log data instead of usage data. (unless this is a typo, then A is about right) \nB-  Cannot be done as the Shards have a hard limit of 1MB/Sec. (Can AWS increase that quota?)\nC- Correct.  With Custom code on Lambda (which I can live with). \nD \u2013 no compression was mentioned."
      },
      {
        "date": "2022-09-26T10:20:00.000Z",
        "voteCount": 1,
        "content": "my answer is 'A' considering the least custom code possible condition."
      },
      {
        "date": "2022-08-06T03:00:00.000Z",
        "voteCount": 3,
        "content": "KDF + lambda +compression"
      },
      {
        "date": "2022-07-25T21:43:00.000Z",
        "voteCount": 3,
        "content": "ANSWER : A"
      },
      {
        "date": "2022-06-16T16:17:00.000Z",
        "voteCount": 1,
        "content": "AWS Lambda supports Parallelization Factor, a feature that allows you to process DynamoDB data stream with more than one Lambda invocation simultaneously."
      },
      {
        "date": "2022-03-15T23:30:00.000Z",
        "voteCount": 2,
        "content": "Ans: A\nB is wrong because:\n1. It will create 2 hot shards\n2. Max object size per shard is 1MB in Kinesis Data Stream."
      },
      {
        "date": "2022-07-31T23:15:00.000Z",
        "voteCount": 2,
        "content": "The question says 2 MiB per second, not 2 MiB of payload"
      },
      {
        "date": "2021-12-18T22:09:00.000Z",
        "voteCount": 1,
        "content": "A is really make sense... but wondering why cant we use option B with enhance fan out ?"
      },
      {
        "date": "2021-11-02T20:55:00.000Z",
        "voteCount": 3,
        "content": "A is my answer: Forehose provides way to introduce lambda for transformations before loading files to s3 and it clearly states before loading files to s3 which removes option D for sure. Option C is close but question asks clearly finding a solution to store and does not ask for any real-time processing, keeping cost in mind, A stands."
      },
      {
        "date": "2021-11-01T11:55:00.000Z",
        "voteCount": 3,
        "content": "The question clearly states - \"The usage data objects need to be reformatted..... \" Emphasis on the phrase USAGE DATA OBJECTS. Why are you guys suggesting option A in that case. Option A talks of LOG DATA OBJECTS not USAGE DATA OBJECTS.\nThis leaves B &amp; C - with my vote on C."
      },
      {
        "date": "2021-10-31T10:20:00.000Z",
        "voteCount": 3,
        "content": "A because:\nAmazon Kinesis Data Firehose allows you to compress your data before delivering it to Amazon S3. The service currently supports GZIP, ZIP, and SNAPPY compression formats. \nFirehose can invoke an AWS Lambda function to transform incoming data before delivering it to destinations. You can configure a new Lambda function using one of the Lambda blueprints we provide or choose an existing Lambda function."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/amazon/view/51410-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analytics specialist is building an automated ETL ingestion pipeline using AWS Glue to ingest compressed files that have been uploaded to an Amazon S3 bucket. The ingestion pipeline should support incremental data processing.<br>Which AWS Glue feature should the data analytics specialist use to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkflows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTriggers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob bookmarks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClassifiers"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T11:22:00.000Z",
        "voteCount": 20,
        "content": "C it is."
      },
      {
        "date": "2021-09-20T10:17:00.000Z",
        "voteCount": 9,
        "content": "I am ok with c as job bookmarks handle  incremental data processing."
      },
      {
        "date": "2023-03-08T03:27:00.000Z",
        "voteCount": 1,
        "content": "Definitely.."
      },
      {
        "date": "2023-05-01T10:28:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-02-04T06:42:00.000Z",
        "voteCount": 1,
        "content": "Textbook question"
      },
      {
        "date": "2023-01-15T16:45:00.000Z",
        "voteCount": 1,
        "content": "Tracking processed data using job bookmarks - AWS Glue\nJob bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources (JSON, CSV, Avro, XML, Parquite, ORC)."
      },
      {
        "date": "2022-11-07T10:17:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C as Job Bookmarks supports incremental data processing by persisting the state information.\n\nOption A is wrong as Glue Workflow helps create and visualize complex extract, transform, and load (ETL) activities involving multiple crawlers, jobs, and triggers.\n\nOption B is wrong a Glue trigger can start specified jobs and crawlers. A trigger fires on-demand, based on a schedule, or based on a combination of events.\n\n\nOption D is wrong as the Glue classifier reads the data in a data store. If it recognizes the format of the data, it generates a schema. The classifier also returns a certain number to indicate how certain the format recognition was."
      },
      {
        "date": "2022-07-27T23:03:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-03-17T22:36:00.000Z",
        "voteCount": 1,
        "content": "Definitely Bookmarks"
      },
      {
        "date": "2022-03-02T09:40:00.000Z",
        "voteCount": 1,
        "content": "C for sure i.e. bookmark"
      },
      {
        "date": "2021-11-02T05:45:00.000Z",
        "voteCount": 2,
        "content": "C as question is clearly asking for support on incremental data processing"
      },
      {
        "date": "2021-10-07T04:35:00.000Z",
        "voteCount": 4,
        "content": "C for sure."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/amazon/view/51704-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A telecommunications company is looking for an anomaly-detection solution to identify fraudulent calls. The company currently uses Amazon Kinesis to stream voice call records in a JSON format from its on-premises database to Amazon S3. The existing dataset contains voice call records with 200 columns. To detect fraudulent calls, the solution would need to look at 5 of these columns only.<br>The company is interested in a cost-effective solution using AWS that requires minimal effort and experience in anomaly-detection algorithms.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue job to transform the data from JSON to Apache Parquet. Use AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Use Amazon Athena to create a table with a subset of columns. Use Amazon QuickSight to visualize the data and then use Amazon QuickSight machine learning-powered anomaly detection.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Firehose to detect anomalies on a data stream from Kinesis by running SQL queries, which compute an anomaly score for all calls and store the output in Amazon RDS. Use Amazon Athena to build a dataset and Amazon QuickSight to visualize the results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue job to transform the data from JSON to Apache Parquet. Use AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Use Amazon SageMaker to build an anomaly detection model that can detect fraudulent calls by ingesting data from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Analytics to detect anomalies on a data stream from Kinesis by running SQL queries, which compute an anomaly score for all calls. Connect Amazon QuickSight to Kinesis Data Analytics to visualize the anomaly scores."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T20:38:00.000Z",
        "voteCount": 29,
        "content": "OPTION A: Amazon QuickSight uses proven Amazon technology to continuously run ML-powered anomaly detection across millions of metrics to discover hidden trends and outliers in your data. This anomaly detection enables you to get deep insights that are often buried in the aggregates and not scalable with manual analysis."
      },
      {
        "date": "2022-10-27T04:21:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is A as only limited columns are required it would be best to convert the data in columnar format and expose it through Athena to QuickSight for anomaly detection. Using parquet with Athena helps provide a cost-effective solution using QuickSight requires minimal effort and experience in anomaly-detection algorithms.\n\nLink : https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html\n\nOption B is wrong as Kinesis Data Firehose does not provide analytic functions and cannot be used for anamoly detection\n\nOption C is wrong as using SageMaker would require more effort.\n\nOption D is wrong as Kinesis Data Analytics does not integrate with QuickSight directly."
      },
      {
        "date": "2023-05-28T00:20:00.000Z",
        "voteCount": 1,
        "content": "A is the best among worst but still wrong. Why the need of creating Athena table again on top of Catalog? Its already Parquet."
      },
      {
        "date": "2023-05-01T10:29:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-09-19T13:09:00.000Z",
        "voteCount": 1,
        "content": "C :\nhttps://aws.amazon.com/blogs/big-data/detecting-anomalous-values-by-invoking-the-amazon-athena-machine-learning-inference-function/"
      },
      {
        "date": "2022-07-25T21:12:00.000Z",
        "voteCount": 1,
        "content": "Answer - A"
      },
      {
        "date": "2022-04-30T16:23:00.000Z",
        "voteCount": 1,
        "content": "Answer - A"
      },
      {
        "date": "2022-04-02T08:25:00.000Z",
        "voteCount": 3,
        "content": "ML-powered anomaly detection."
      },
      {
        "date": "2021-12-27T05:56:00.000Z",
        "voteCount": 3,
        "content": "The question is based on below blog and answer is A:\nhttps://aws.amazon.com/blogs/big-data/detect-fraudulent-calls-using-amazon-quicksight-ml-insights/"
      },
      {
        "date": "2021-11-29T07:57:00.000Z",
        "voteCount": 4,
        "content": "Answer is A\nOption D also wrong as QuickSight cannot connect to any of Kinesis services."
      },
      {
        "date": "2021-11-20T09:03:00.000Z",
        "voteCount": 1,
        "content": "You can use D also but less effort = Option A (Quicksight)\nhttps://aws.amazon.com/blogs/big-data/detect-fraudulent-calls-using-amazon-quicksight-ml-insights/"
      },
      {
        "date": "2021-11-17T22:35:00.000Z",
        "voteCount": 1,
        "content": "A because : Quicksight Machine Learning Insights can use random_cut_forest ."
      },
      {
        "date": "2021-10-30T19:18:00.000Z",
        "voteCount": 1,
        "content": "Option A based on https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html"
      },
      {
        "date": "2021-10-26T17:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is A."
      },
      {
        "date": "2021-10-23T12:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nB wrong. Kinesis Data Firehose has no analytic function\nD wrong. SQL is not for anomaly detection and can't output to Quicksight\nC works but more effort."
      },
      {
        "date": "2021-10-22T08:19:00.000Z",
        "voteCount": 2,
        "content": "Best answer is A. kindly refer to documentation below for confirmation thanks.\nhttps://aws.amazon.com/quicksight/features-ml/"
      },
      {
        "date": "2021-10-05T10:26:00.000Z",
        "voteCount": 3,
        "content": "D should be the right answer, considering that the requirement is develop a cost-effective solution using AWS that requires minimal effort and experience in anomaly-detection algorithms. With the option A we need the glue, athena and quicksight. With option D we only need stream analytics (using SQL sintax, very easy) and the quicksight.\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html"
      },
      {
        "date": "2021-10-07T14:27:00.000Z",
        "voteCount": 2,
        "content": "but what about \"Connect Amazon QuickSight to Kinesis Data Analytics to visualize the anomaly scores\" ? Not sure if you can connect Quicksight to Data Analytics like this. So A seems to be a more logical answer."
      },
      {
        "date": "2021-10-23T04:05:00.000Z",
        "voteCount": 3,
        "content": "Kinesis Data Analytics can write only to three services:\n - Firehose\n - Kinesis data streams\n - lambda\nSee https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output.html\n\nThere is no way to write to QuickSight"
      },
      {
        "date": "2021-12-12T14:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. from d above quicksight docs: External destination \u2013 You can persist data to a Kinesis data stream, a Kinesis Data Firehose delivery stream, or a Lambda function."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/amazon/view/51409-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online retailer is rebuilding its inventory management system and inventory reordering system to automatically reorder products by using Amazon Kinesis Data<br>Streams. The inventory management system uses the Kinesis Producer Library (KPL) to publish data to a stream. The inventory reordering system uses the<br>Kinesis Client Library (KCL) to consume data from the stream. The stream has been configured to scale as needed. Just before production deployment, the retailer discovers that the inventory reordering system is receiving duplicated data.<br>Which factors could be causing the duplicated data? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe producer has a network-related timeout.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe stream's value for the IteratorAgeMilliseconds metric is too high.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere was a change in the number of shards, record processors, or both.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe AggregationEnabled configuration property was set to true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe max_records configuration property was set to a number that is too high."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-01T14:11:00.000Z",
        "voteCount": 32,
        "content": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\nA,C"
      },
      {
        "date": "2022-06-18T22:26:00.000Z",
        "voteCount": 15,
        "content": "Duplication can happen in two ways, either in producer side or consumer side, obviously. \nIn proudcer side it happens due to network delays/timeouts, specifically producer is waiting for an successful acknowledgement, yet it is lost due to network failure, and producer sends data again until it receives acknowledgement.\nIn consumer side it happens due to record processor restart. This can happen due to 4 reasons. A worker terminates unexpectedly, Worker instances are added or removed, Shards are merged or split, application is deployed.\nIn any of the situations the best way to prevent duplicates is to have a unique identifier in data.\nAns should be A, C"
      },
      {
        "date": "2023-09-10T14:57:00.000Z",
        "voteCount": 1,
        "content": "Correct ans is BC, A network-related timeout for the producer could potentially lead to failed record ingestion, but it is less likely to cause duplicated data."
      },
      {
        "date": "2023-05-01T10:30:00.000Z",
        "voteCount": 1,
        "content": "AC: I passed the test"
      },
      {
        "date": "2023-01-06T14:19:00.000Z",
        "voteCount": 2,
        "content": "A&amp;C are the correct answers.\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html"
      },
      {
        "date": "2022-10-27T04:24:00.000Z",
        "voteCount": 2,
        "content": "Correct answers are A &amp; C as there is a chance of duplicates in case a producer retries cause of network connectivity issues or consumer retries when a shard is merged or split. \n\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\n\nThere are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times."
      },
      {
        "date": "2022-10-27T04:24:00.000Z",
        "voteCount": 2,
        "content": "Consider a producer that experiences a network-related timeout after it makes a call to PutRecord, but before it can receive an acknowledgement from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both PutRecord calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.\n\nConsumer (data processing application) retries happen when record processors restart."
      },
      {
        "date": "2022-10-27T04:25:00.000Z",
        "voteCount": 2,
        "content": "Record processors for the same shard restart in the following cases:\n\n    A worker terminates unexpectedly\n    Worker instances are added or removed\n    Shards are merged or split\n    The application is deployed"
      },
      {
        "date": "2022-07-21T12:29:00.000Z",
        "voteCount": 1,
        "content": "answer is A - AS Archive objects that are queried by S3 Glacier Select must be in uncompressed comma-separated values (CSV)."
      },
      {
        "date": "2022-06-22T04:09:00.000Z",
        "voteCount": 1,
        "content": "A&amp;C is the correct answer"
      },
      {
        "date": "2022-06-14T10:46:00.000Z",
        "voteCount": 1,
        "content": "AC should be the correct one"
      },
      {
        "date": "2022-05-24T15:43:00.000Z",
        "voteCount": 1,
        "content": "AC is the correct answer"
      },
      {
        "date": "2022-02-27T00:17:00.000Z",
        "voteCount": 2,
        "content": "it's made clear in the statement that the problem showed up when the solution was deployed to production, and connection timeouts are another option that can cause that issue"
      },
      {
        "date": "2022-02-20T09:24:00.000Z",
        "voteCount": 1,
        "content": "A and C are correct."
      },
      {
        "date": "2022-02-13T03:22:00.000Z",
        "voteCount": 1,
        "content": "A and C Are correct"
      },
      {
        "date": "2021-11-14T14:54:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2021-11-01T21:07:00.000Z",
        "voteCount": 3,
        "content": "For sure A and C"
      },
      {
        "date": "2021-10-24T23:33:00.000Z",
        "voteCount": 3,
        "content": "I am ok for A and C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/amazon/view/51408-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large retailer has successfully migrated to an Amazon S3 data lake architecture. The company's marketing team is using Amazon Redshift and Amazon<br>QuickSight to analyze data, and derive and visualize insights. To ensure the marketing team has the most up-to-date actionable information, a data analyst implements nightly refreshes of Amazon Redshift using terabytes of updates from the previous day.<br>After the first nightly refresh, users report that half of the most popular dashboards that had been running correctly before the refresh are now running much slower. Amazon CloudWatch does not show any alerts.<br>What is the MOST likely cause for the performance degradation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe dashboards are suffering from inefficient SQL queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster is undersized for the queries being run by the dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe nightly data refreshes are causing a lingering transaction that cannot be automatically closed by Amazon Redshift due to ongoing user workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-15T00:19:00.000Z",
        "voteCount": 23,
        "content": "Looks like it's D.\nhttps://github.com/awsdocs/amazon-redshift-developer-guide/issues/21"
      },
      {
        "date": "2021-12-20T22:55:00.000Z",
        "voteCount": 4,
        "content": "Agree \nfor further reading please see \nhttps://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/"
      },
      {
        "date": "2023-05-01T10:31:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-10-27T05:25:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D as the reason for the drop in query performance might because of the long nightly data refreshes operation which blocked the vacuum operation and left it in the need of the vacuum operation. \n\nAmazon Redshift can automatically sort and perform a VACUUM DELETE operation on tables in the background. To clean up tables after a load or a series of incremental updates, you can also run the VACUUM command, either against the entire database or against individual tables.\n\nA vacuum operation might not be able to start if a load or insert operation is already in progress. Vacuum operations temporarily require exclusive access to tables in order to start. This exclusive access is required briefly, so vacuum operations don't block concurrent loads and inserts for any significant period of time."
      },
      {
        "date": "2022-09-26T22:33:00.000Z",
        "voteCount": 1,
        "content": "C. if there is a lingering transaction, it will block other read queries or transactions. The Question said the next day query ran slow, not being blcoked, so C is out."
      },
      {
        "date": "2022-08-01T04:46:00.000Z",
        "voteCount": 1,
        "content": "A is always can be but as fas as it worked before night transaction A and B is out.\nC is rear  case because typically  transactions block dara read and not making redshift slower( due to wlm and queues it hardly a case) after any DML or DDL operation there is a lot of junk an need of vacume, During vacuum there can be seom perfomance issue for queries that tryes to use that table that is why redshift autowacuum doesn't start when there is high user activity. So we need to vacuum explisitly to make things faster."
      },
      {
        "date": "2022-07-25T21:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D"
      },
      {
        "date": "2022-05-21T19:52:00.000Z",
        "voteCount": 1,
        "content": "C sounds correct answer. It could be between C and D. But considering Redshift automatically vacuumes the tables periodically, and that user work load is the reason for the slowness, I would pick C over D."
      },
      {
        "date": "2022-05-21T04:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-05-22T01:36:00.000Z",
        "voteCount": 1,
        "content": "Sorry, Answer could be B"
      },
      {
        "date": "2022-04-30T16:14:00.000Z",
        "voteCount": 2,
        "content": "Answer-D"
      },
      {
        "date": "2022-03-20T04:47:00.000Z",
        "voteCount": 1,
        "content": "C is equally possible.. Cannot rule it out...."
      },
      {
        "date": "2021-11-20T08:19:00.000Z",
        "voteCount": 2,
        "content": "ans cloud be D \nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html"
      },
      {
        "date": "2021-10-31T22:10:00.000Z",
        "voteCount": 1,
        "content": "D should be the answer but the Correct Answer shows B. Ridiculous."
      },
      {
        "date": "2021-11-04T14:58:00.000Z",
        "voteCount": 14,
        "content": "welcome to examtopics...."
      },
      {
        "date": "2021-10-26T03:27:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is D"
      },
      {
        "date": "2021-09-27T20:06:00.000Z",
        "voteCount": 3,
        "content": "I choose D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/amazon/view/51407-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A marketing company is storing its campaign response data in Amazon S3. A consistent set of sources has generated the data for each campaign. The data is saved into Amazon S3 as .csv files. A business analyst will use Amazon Athena to analyze each campaign's data. The company needs the cost of ongoing data analysis with Athena to be minimized.<br>Which combination of actions should a data analytics specialist take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the .csv files to Apache Parquet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the .csv files to Apache Avro.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by campaign.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the .csv files."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-15T13:56:00.000Z",
        "voteCount": 33,
        "content": "A,C\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nRefer: \nPartition your data\nOptimize columnar data store generation"
      },
      {
        "date": "2021-11-03T15:41:00.000Z",
        "voteCount": 5,
        "content": "How about compression? Compression take higher chance for optimizing however A, C is correct because Apache Parquet has data compressed as default."
      },
      {
        "date": "2023-05-01T10:32:00.000Z",
        "voteCount": 1,
        "content": "AC: I passed the test"
      },
      {
        "date": "2023-03-25T04:04:00.000Z",
        "voteCount": 1,
        "content": "A: for reducing data size\nC: since the user will query the data by campaign"
      },
      {
        "date": "2022-10-27T05:28:00.000Z",
        "voteCount": 3,
        "content": "Correct answers are A &amp; C as it is recommended to partition the data as per the requirement and use columnar data store like parquet which compresses the data as well as allows splitting. \n\nCorrect answers are A &amp; C as it is recommended to partition the data as per the requirement and use columnar data store like parquet which compresses the data as well as allows splitting. \n\n\nOption D is wrong as partitioning of the data should be as per the requirement which currently is queried as per the campaigns.\n\nOptions B &amp; E are wrong as compressing the .csv files or using Avro would not provide as many benefits as parquet files."
      },
      {
        "date": "2022-10-27T05:29:00.000Z",
        "voteCount": 2,
        "content": "Apache Parquet and Apache ORC are popular columnar data stores. They provide features that store data efficiently by employing column-wise compression, different encoding, compression based on data type, and predicate pushdown. They are also splittable. Generally, better compression ratios or skipping blocks of data means reading fewer bytes from Amazon S3, leading to better query performance."
      },
      {
        "date": "2022-10-27T05:29:00.000Z",
        "voteCount": 2,
        "content": "Partitioning divides your table into parts and keeps the related data together based on column values such as date, country, region, etc. Partitions act as virtual columns. You define them at table creation, and they can help reduce the amount of data scanned per query, thereby improving performance. You can restrict the amount of data scanned by a query by specifying filters based on the partition."
      },
      {
        "date": "2022-10-27T05:29:00.000Z",
        "voteCount": 2,
        "content": "Compressing your data can speed up your queries significantly, as long as the files are either of an optimal size (see the next section), or the files are splittable. The smaller data sizes reduce network traffic from Amazon S3 to Athena.\n\nSplittable files allow the execution engine in Athena to split the reading of a single file by multiple readers to increase parallelism. If you have a single unsplittable file, then only a single reader can read the file while all other readers may sit idle. Not all compression algorithms are splittable.\n\nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable."
      },
      {
        "date": "2022-07-21T12:38:00.000Z",
        "voteCount": 1,
        "content": "its A &amp; C."
      },
      {
        "date": "2022-07-19T21:51:00.000Z",
        "voteCount": 1,
        "content": "A: Because columnar format helps to improve performance. Parquet is splittable and compresses by default hence option e is already taken care here.\nC: Partitioning improves performance. Here since the bulk of the analysis activity is dependent on campaign hence this will be ideal for partitioning (limited partitions and low cardinality)"
      },
      {
        "date": "2022-07-02T01:09:00.000Z",
        "voteCount": 1,
        "content": "AC is correct."
      },
      {
        "date": "2022-05-29T10:07:00.000Z",
        "voteCount": 2,
        "content": "A: Because columnar format helps to improve performance. Parquet is splittable and compresses by default hence option e is already taken care here.\nC: Partitioning improves performance. Here since the bulk of the analysis activity is dependent on campaign hence this will be ideal for partitioning (limited partitions and low cardinality)"
      },
      {
        "date": "2022-05-27T21:15:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B &amp; C"
      },
      {
        "date": "2022-05-24T15:37:00.000Z",
        "voteCount": 1,
        "content": "AC is the correct answer"
      },
      {
        "date": "2022-05-14T17:15:00.000Z",
        "voteCount": 1,
        "content": "C,E seems to be correct answer. Since all sources are consistent, it is better to partition based on campaign. Also, compression means less data will be scanned which will save costs. Why would you change format to either Avro or Parquet? You would need to build an etl job using Glue etc. to do this which would add to the cost."
      },
      {
        "date": "2022-03-26T12:54:00.000Z",
        "voteCount": 1,
        "content": "A and C\n\nas the data analyst will query per campaign, then it is a good partition key\nAlso, columnar data types such as Parquet are better for analysis than row-based such as Avro."
      },
      {
        "date": "2022-02-28T20:21:00.000Z",
        "voteCount": 1,
        "content": "A and C it is"
      },
      {
        "date": "2021-12-05T13:51:00.000Z",
        "voteCount": 4,
        "content": "A &amp; C is correct"
      },
      {
        "date": "2021-11-14T14:52:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2021-10-18T17:29:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A&amp;C"
      },
      {
        "date": "2021-10-02T20:42:00.000Z",
        "voteCount": 2,
        "content": "A,C it is."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/amazon/view/51706-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online retail company is migrating its reporting system to AWS. The company's legacy system runs data processing on online transactions using a complex series of nested Apache Hive queries. Transactional data is exported from the online system to the reporting system several times a day. Schemas in the files are stable between updates.<br>A data analyst wants to quickly migrate the data processing to AWS, so any code changes should be minimized. To keep storage costs low, the data analyst decides to store the data in Amazon S3. It is vital that the data from the reports and associated analytics is completely up to date based on the data in Amazon S3.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue Data Catalog to manage the Hive metadata. Create an AWS Glue crawler over Amazon S3 that runs when data is refreshed to ensure that data changes are updated. Create an Amazon EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue Data Catalog to manage the Hive metadata. Create an Amazon EMR cluster with consistent view enabled. Run emrfs sync before each analytics step to ensure data changes are updated. Create an EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Athena table with CREATE TABLE AS SELECT (CTAS) to ensure data is refreshed from underlying queries against the raw dataset. Create an AWS Glue Data Catalog to manage the Hive metadata over the CTAS table. Create an Amazon EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 Select query to ensure that the data is properly updated. Create an AWS Glue Data Catalog to manage the Hive metadata over the S3 Select table. Create an Amazon EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T13:39:00.000Z",
        "voteCount": 25,
        "content": "looks like 'A' it is."
      },
      {
        "date": "2021-10-21T18:23:00.000Z",
        "voteCount": 3,
        "content": "wrong; it says schema is stable so you dont need to jerk crawlers every time\nB"
      },
      {
        "date": "2022-01-28T08:55:00.000Z",
        "voteCount": 1,
        "content": "B is wrong, they did not even mention the usage of S3 dude"
      },
      {
        "date": "2021-11-29T21:22:00.000Z",
        "voteCount": 5,
        "content": "Answer should be A, \n1. Consistent View is no more required\n2. Though schema is stable in this running Glue Crawler is one of the way to get the partition metadata updated"
      },
      {
        "date": "2023-07-02T06:23:00.000Z",
        "voteCount": 2,
        "content": "I think this question is just outdated. For what the question says, B should be the answer. A is too broad, as it doesn't mention that it needs something to trigger the crawler (like a Lambda). And it states that the schema is stable, so no need to run crawler all the time a file is updated. B doesn't need to mention s3 because the \"enable consistent view\" already means that!  It's out dated because Amazon got rid off the consistent view from EMR. But looking 3 years back, B would be the perfect answer."
      },
      {
        "date": "2023-05-01T10:33:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-07-26T19:31:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-11-09T17:17:00.000Z",
        "voteCount": 2,
        "content": "I think answer is  A. \nIt can also be that the question is bit outdated , as now S3 has  strong read after write consistency , EMRFS consistent view might not make sense now."
      },
      {
        "date": "2021-11-04T05:05:00.000Z",
        "voteCount": 3,
        "content": "I think it's A\nYou no longer need to use EMRFS Consistent View as Amazon S3 supports strong read-after-write Consistency. See Strong read-after-write consistency. This works with all Amazon EMR versions.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-consistent-view.html"
      },
      {
        "date": "2021-10-23T11:50:00.000Z",
        "voteCount": 4,
        "content": "I believe it is B. \nThe key point is \"It is vital that the data from the reports and associated analytics is completely up to date based on the data in Amazon S3.\"  \nEMRFS Consistent view allows EMR clusters to check for list and read-after-write consistency for Amazon S3 objects written by or synced with EMRFS. \nIf you directly delete objects from Amazon S3 that are tracked in EMRFS metadata, EMRFS treats the object as inconsistent and throws an exception after it has exhausted retries. Use EMRFS to delete objects in Amazon S3 that are tracked using consistent view. Alternatively, you can use the emrfs command line to purge metadata entries for objects that have been directly deleted, or you can sync the consistent view with Amazon S3 immediately after you delete the objects.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-consistent-view.html\n\nTo run Glue Crawlers when S3 is refreshed needs S3 Trigger Lambda functions that run the crawler. It seems too much of an overhead especially when EMRFS consistent view / sync command can maintain consistency with already provisioned EMR cluster."
      },
      {
        "date": "2021-10-19T04:48:00.000Z",
        "voteCount": 4,
        "content": "Glue crawler is responsible for the schema definition , which is mentioned as stable in this case .A glue ETL job could have moved the data .Having said that , I go for B"
      },
      {
        "date": "2021-10-07T22:57:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Because schema is stable, you don't need to run Glue crawler again. Moreover, data stored in S3 therefore EMRFS is needed and with consistent view, data is updated."
      },
      {
        "date": "2021-10-13T16:27:00.000Z",
        "voteCount": 1,
        "content": "Sorry, may be A is correct. When you use AWS Glue Data Catalog as the metastore for Hive, no need to configure EMRFS."
      },
      {
        "date": "2021-09-29T16:47:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2021-09-24T15:28:00.000Z",
        "voteCount": 1,
        "content": "Agree with A"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/amazon/view/73937-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A media company is using Amazon QuickSight dashboards to visualize its national sales data. The dashboard is using a dataset with these fields: ID, date, time_zone, city, state, country, longitude, latitude, sales_volume, and number_of_items.<br>To modify ongoing campaigns, the company wants an interactive and intuitive visualization of which states across the country recorded a significantly lower sales volume compared to the national average.<br>Which addition to the company's QuickSight dashboard will meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA geospatial color-coded chart of sales volume data across the country.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA pivot table of sales volume data summed up at the state level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA drill-down layer for state-level sales volume data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA drill through to other dashboards containing state-level sales volume data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-20T11:54:00.000Z",
        "voteCount": 15,
        "content": "A provides needful"
      },
      {
        "date": "2023-05-01T10:34:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-26T14:47:00.000Z",
        "voteCount": 1,
        "content": "A pivot table of sales volume data summed up at the state level will meet the requirement of an interactive and intuitive visualization of which states across the country recorded a significantly lower sales volume compared to the national average. The pivot table can be sorted by the sales volume column and filtered to show only the states with lower sales volume than the national average, making it easy to identify the problem areas. A geospatial color-coded chart (option A) could also be useful, but it may not be as intuitive for identifying the states with lower sales volume."
      },
      {
        "date": "2023-01-25T04:51:00.000Z",
        "voteCount": 1,
        "content": "I think it is C, since with A you are not comparing the states against the national average"
      },
      {
        "date": "2023-01-06T14:51:00.000Z",
        "voteCount": 4,
        "content": "A \"intuitive visualization\""
      },
      {
        "date": "2022-10-27T05:37:00.000Z",
        "voteCount": 4,
        "content": "Answer is A\nData visualization depends on the story you want to tell. Maps are great at visualizing your geographic data by location. The data on a map is often displayed in a colored area map or a bubble map. If the location is not part of the story, a map could be messy. With a table, you can display a large number of precise measures and dimensions. You can quickly look up or compare individual values while also showing grand totals. However, given the amount of data, tables take longer to digest.\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers because color makes these easier to spot. Use a pivot table if you're going to further analyze data on the visual, for example, by changing column sort order or applying aggregate functions across rows or columns."
      },
      {
        "date": "2022-10-27T01:15:00.000Z",
        "voteCount": 3,
        "content": "A is more intuitive in this context"
      },
      {
        "date": "2022-09-26T22:44:00.000Z",
        "voteCount": 2,
        "content": "The question ask for \" interactive and intuitive visualization\".\nBoth A and B works, but with a Pivot table it's hard to identify the states with the largest drop. You have to compare the numbers across the entire table. Not good"
      },
      {
        "date": "2022-09-14T20:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-08-13T01:26:00.000Z",
        "voteCount": 1,
        "content": "The answer is A."
      },
      {
        "date": "2022-07-20T23:35:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-07-14T23:11:00.000Z",
        "voteCount": 2,
        "content": "They just want State names with low total sales. there is no need to show those on a map"
      },
      {
        "date": "2023-01-24T03:10:00.000Z",
        "voteCount": 1,
        "content": "it says \"interactive and intuitive visualization\""
      },
      {
        "date": "2022-07-05T15:45:00.000Z",
        "voteCount": 1,
        "content": "I support C.\ndrill-down meets the need for interactive; state-level sales volume data meets the need for display of which states achieved considerably lower sales volumes than the national average"
      },
      {
        "date": "2022-06-16T01:05:00.000Z",
        "voteCount": 2,
        "content": "I think both a Pivot Table and a geo-spatial chart would be nice in this case. However, because the geo-spatial chart would work because we have lat/lon coordinates, and because I feel like these are considered more comprehensible by the exam-makers, I will choose A."
      },
      {
        "date": "2022-05-27T04:36:00.000Z",
        "voteCount": 1,
        "content": "A will only show the volume for each state and location of the state in the map. How does it compare against the national average? I think ans is D."
      },
      {
        "date": "2022-05-21T05:57:00.000Z",
        "voteCount": 2,
        "content": "We only need to compare the sales volume for each state. So there is no need for pivot table here as no requirement for slice and dice of data. GeoSpatial graph will do the job nicely"
      },
      {
        "date": "2022-05-05T19:26:00.000Z",
        "voteCount": 2,
        "content": "I think it should be B. Using pivot table, we can display sales by city, and sum up with state level. We can use \"collapse\", \"sort\"  to get an \"interactive and comprehensive\" display."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/amazon/view/51708-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company hosts an on-premises PostgreSQL database that contains historical data. An internal legacy application uses the database for read-only activities. The company's business team wants to move the data to a data lake in Amazon S3 as soon as possible and enrich the data for analytics.<br>The company has set up an AWS Direct Connect connection between its VPC and its on-premises network. A data analytics specialist must design a solution that achieves the business team's goals with the least operational overhead.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data from the on-premises PostgreSQL database to Amazon S3 by using a customized batch upload process. Use the AWS Glue crawler to catalog the data in Amazon S3. Use an AWS Glue job to enrich and store the result in a separate S3 bucket in Apache Parquet format. Use Amazon Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS for PostgreSQL database and use AWS Database Migration Service (AWS DMS) to migrate the data into Amazon RDS. Use AWS Data Pipeline to copy and enrich the data from the Amazon RDS for PostgreSQL table and move the data to Amazon S3. Use Amazon Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Glue crawler to use a JDBC connection to catalog the data in the on-premises database. Use an AWS Glue job to enrich the data and save the result to Amazon S3 in Apache Parquet format. Create an Amazon Redshift cluster and use Amazon Redshift Spectrum to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Glue crawler to use a JDBC connection to catalog the data in the on-premises database. Use an AWS Glue job to enrich the data and save the result to Amazon S3 in Apache Parquet format. Use Amazon Athena to query the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-10T06:09:00.000Z",
        "voteCount": 30,
        "content": "The question says least operational overhead...so it should be D"
      },
      {
        "date": "2021-10-30T16:03:00.000Z",
        "voteCount": 18,
        "content": "Answer is D\n\nAWS Glue can communicate with an on-premises data store over VPN or DX connectivity. An AWS Glue crawler uses an S3 or JDBC connection to catalog the data source, and the AWS Glue ETL job uses S3 or JDBC connections as a source or target data store.\nhttps://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/"
      },
      {
        "date": "2021-11-05T02:25:00.000Z",
        "voteCount": 5,
        "content": "that blog post literally says \"In this post, I describe a solution for transforming and moving data from an on-premises data store to Amazon S3 using AWS Glue\""
      },
      {
        "date": "2023-12-25T03:52:00.000Z",
        "voteCount": 2,
        "content": "D is wrong, the requirement is \"move the data to a data lake\". D doesn't store the source data, but only \"save the result to Amazon S3\"."
      },
      {
        "date": "2023-08-05T00:49:00.000Z",
        "voteCount": 2,
        "content": "I would go with B because of the requirement for minimum overhead.\nD is also correct but needs more work and services involved, we are using DMS for a data migration scenario and datapipeline for a transformation and movement scenario, it makes sense."
      },
      {
        "date": "2023-08-03T16:53:00.000Z",
        "voteCount": 1,
        "content": "AWS Glue can also connect to a variety of on-premises JDBC data stores such as PostgreSQL, MySQL, Oracle, Microsoft SQL Server, and MariaDB.\n\nAWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target."
      },
      {
        "date": "2023-07-04T08:21:00.000Z",
        "voteCount": 9,
        "content": "pk349 user is stupid. In all questions posts X: I passed the test. Stupid."
      },
      {
        "date": "2022-10-27T05:41:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D as AWS Glue can be used to extract from the on-premises data store using JDBC connection, transform data and store the data in S3 with the least operational overhead. \nCorrect answer is D as AWS Glue can be used to extract from the on-premises data store using JDBC connection, transform data and store the data in S3 with the least operational overhead. \n\nOption A is wrong as using a customized batch upload process would add to the operational overhead.\n\nOption B is wrong as creating an intermediate RDS instance with Data Pipeline jobs added to the operational overhead.\n\nOption C is wrong as creating a Redshift cluster would add to the operational overhead."
      },
      {
        "date": "2022-10-27T01:27:00.000Z",
        "voteCount": 1,
        "content": "I think B is not correct. If the goal is to migrate this database to RDS, then is B. But we want to move it to S3, AWS Glue can do ETL. It is less overhead"
      },
      {
        "date": "2022-07-19T22:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-06-19T02:53:00.000Z",
        "voteCount": 4,
        "content": "I think most of us missing the point that we should understand what each solution is designed for. Clearly this is a database migration task so in that case I would defiinetely use DMS, coz its optimized for such tasks. Considering that I would say this is not and ETL task so that Glue ETL isn't an option. Moreover Glue ETL uses \"serverless spark platform\" and therefore will be expensive for sure. Therefore I go with option B."
      },
      {
        "date": "2022-05-27T01:27:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer. For any form of analytics, RedShift is a preferred choice. Athena use case is for AdHoc query."
      },
      {
        "date": "2022-09-18T02:15:00.000Z",
        "voteCount": 1,
        "content": "Redshift and redshift spectrum adds significantly more operational overhead!"
      },
      {
        "date": "2022-02-13T03:57:00.000Z",
        "voteCount": 4,
        "content": "Explanation\nCorrect answer is D as AWS Glue can be used to extract from the on-premises data store using JDBC connection, transform data and store the data in S3 with the least operational overhead.\nRefer AWS documentation - Glue Analyze On-premises Data Store (https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/)\nAWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3,\ndata stores in a VPC, or on-premises JDBC data stores as a target.\nOption A is wrong as using a customized batch upload process would add to the operational overhead.\nOption B is wrong as creating an intermediate RDS instance with Data Pipeline jobs added to the operational overhead.\nOption C is wrong as creating a Redshift cluster would add to the operational overhead."
      },
      {
        "date": "2022-02-09T08:27:00.000Z",
        "voteCount": 4,
        "content": "We want to reduce operational costs  and overhead.\n\nA - Sounds good but the beginning is not right, we can do better than customized batch.\nB - That's too much work, we are not interested in keeping the PSQL Engine, so if we can remove that's step from the equation we are good to go.\nC - We directly get the table from the On-prem DB with Glue and move it directly to S3. That's good, but we are adding spectrum to the mix, which is a more expensive solution.\nD - Yes, we are moving directly from on-prem to S3, but using athena to query the data. This is the response."
      },
      {
        "date": "2022-02-04T16:10:00.000Z",
        "voteCount": 1,
        "content": "I believe answer is 'B'"
      },
      {
        "date": "2021-11-17T13:00:00.000Z",
        "voteCount": 2,
        "content": "It is close, but I am leaning towards \"D\""
      },
      {
        "date": "2021-10-27T22:04:00.000Z",
        "voteCount": 3,
        "content": "I believe answer is D.\nAWS Glue can communicate with an on-premises data store over VPN or DX connectivity.\n\nAn AWS Glue crawler uses an S3 or JDBC connection to catalog the data source, and the AWS Glue ETL job uses S3 or JDBC connections as a source or target data store.\n\nhttps://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/"
      },
      {
        "date": "2021-10-24T02:25:00.000Z",
        "voteCount": 2,
        "content": "The question asks to achieve the goals with \"least operational overhead\".\nAnswer is D....B cannot be the answer as DMS require EC2 instance to be created for the replication task, this is an operational overhead. Same for A, customized batch process will be an operational overhead. C is not an option as you need to maintain a redshift cluster."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/amazon/view/51709-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A medical company has a system with sensor devices that read metrics and send them in real time to an Amazon Kinesis data stream. The Kinesis data stream has multiple shards. The company needs to calculate the average value of a numeric metric every second and set an alarm for whenever the value is above one threshold or below another threshold. The alarm must be sent to Amazon Simple Notification Service (Amazon SNS) in less than 30 seconds.<br>Which architecture meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Kinesis Data Firehose delivery stream to read the data from the Kinesis data stream with an AWS Lambda transformation function that calculates the average per second and sends the alarm to Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function to read from the Kinesis data stream to calculate the average per second and sent the alarm to Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Kinesis Data Firehose deliver stream to read the data from the Kinesis data stream and store it on Amazon S3. Have Amazon S3 trigger an AWS Lambda function that calculates the average per second and sends the alarm to Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Kinesis Data Analytics application to read from the Kinesis data stream and calculate the average per second. Send the results to an AWS Lambda function that sends the alarm to Amazon SNS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T03:09:00.000Z",
        "voteCount": 31,
        "content": "It should be D, as KDF cannot achieve 30 Seconds SLA"
      },
      {
        "date": "2021-12-21T03:14:00.000Z",
        "voteCount": 8,
        "content": "Agree\nAlso it can not be B because I think average needs to be calculated across different shards and Lambda processes one batch of records at a time from each shard.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2021-09-21T00:06:00.000Z",
        "voteCount": 7,
        "content": "Should be 'D'."
      },
      {
        "date": "2023-06-18T06:06:00.000Z",
        "voteCount": 3,
        "content": "This question could be deprecated. Legacy KDA for SQL could invoke Lambda, not KDA for Flink . So, D is not feasible today. KDS to Lambda has always been possible and the batch window in the event source mapping can be set to 1 second."
      },
      {
        "date": "2023-02-04T07:52:00.000Z",
        "voteCount": 2,
        "content": "Firehose is too slow, D it is."
      },
      {
        "date": "2022-10-27T05:48:00.000Z",
        "voteCount": 6,
        "content": "D is the correct answer\n\nOptions A &amp; C is wrong as Kinesis Data Firehose would not be able to achieve the 30 seconds requirement. \n\nOption B is wrong as Lambda function works with a single shard and works on a batch or batch window which would not provide the average."
      },
      {
        "date": "2022-10-27T05:48:00.000Z",
        "voteCount": 3,
        "content": "Continuous metric generation applications enable you to monitor and understand how your data is trending over time. Your applications can aggregate streaming data into critical information and seamlessly integrate it with reporting databases and monitoring services to serve your applications and users in real-time. With Kinesis Data Analytics, you can use SQL or Apache Flink code to continuously generate time-series analytics over time windows. For example, you can build a live leaderboard for a mobile game by computing the top players every minute and then sending it to Amazon DynamoDB. Or, you can track the traffic to your website by calculating the number of unique website visitors every five minutes and then sending the processed results to Amazon Redshift."
      },
      {
        "date": "2022-10-27T05:48:00.000Z",
        "voteCount": 1,
        "content": "Continuous metric generation applications enable you to monitor and understand how your data is trending over time. Your applications can aggregate streaming data into critical information and seamlessly integrate it with reporting databases and monitoring services to serve your applications and users in real-time. With Kinesis Data Analytics, you can use SQL or Apache Flink code to continuously generate time-series analytics over time windows. For example, you can build a live leaderboard for a mobile game by computing the top players every minute and then sending it to Amazon DynamoDB. Or, you can track the traffic to your website by calculating the number of unique website visitors every five minutes and then sending the processed results to Amazon Redshift."
      },
      {
        "date": "2022-08-03T21:39:00.000Z",
        "voteCount": 2,
        "content": "Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch. Each batch contains records from a single shard/data stream"
      },
      {
        "date": "2022-07-25T21:52:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-06-28T07:26:00.000Z",
        "voteCount": 1,
        "content": "As others have explained it should be D."
      },
      {
        "date": "2022-03-15T23:40:00.000Z",
        "voteCount": 2,
        "content": "Ans: D\nB is wrong because Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch. Each batch contains records from a single shard/data stream."
      },
      {
        "date": "2022-01-20T10:17:00.000Z",
        "voteCount": 1,
        "content": "D\nnot A &amp; C - KDF cannot achieve 30 seconds\nnot B - If I understood correctly It had to compute average value, and I don't think that it is for Lambda case"
      },
      {
        "date": "2021-10-31T04:59:00.000Z",
        "voteCount": 1,
        "content": "I think B is correct."
      },
      {
        "date": "2021-11-03T16:46:00.000Z",
        "voteCount": 1,
        "content": "I Changed for D, because lambda is taking 30 seconds to calculate average. We need &lt; 30 seconds."
      },
      {
        "date": "2021-12-06T07:04:00.000Z",
        "voteCount": 1,
        "content": "why you say the lambda takes 30 seconds to calculate an average?"
      },
      {
        "date": "2021-10-18T07:43:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D. KDS-&gt;KDA-&gt;Lambda-&gt;SNS"
      },
      {
        "date": "2021-10-12T12:39:00.000Z",
        "voteCount": 4,
        "content": "Why not B? Lambda can consume data from Kinesis Data Stream and process it. It can send alarm to SNS if limit breached. What is the problem in this simple solution?"
      },
      {
        "date": "2022-11-18T07:09:00.000Z",
        "voteCount": 1,
        "content": "This is what I do not understand as well. Lambda can act as a consumer to Kinesis Data Streams so why would you need Kinesis Data Analysis as an intermediary to calculate the metric and send it to SNS??"
      },
      {
        "date": "2023-11-08T19:48:00.000Z",
        "voteCount": 1,
        "content": "yeah me too."
      },
      {
        "date": "2021-10-04T23:01:00.000Z",
        "voteCount": 1,
        "content": "I think B is correct."
      },
      {
        "date": "2021-10-07T03:10:00.000Z",
        "voteCount": 1,
        "content": "Don't think KDA can directly talk to SNS.."
      },
      {
        "date": "2021-10-20T13:50:00.000Z",
        "voteCount": 1,
        "content": "In option D, KDA uses lambda to connect to SNS"
      },
      {
        "date": "2021-10-09T09:35:00.000Z",
        "voteCount": 1,
        "content": "sorry, change to D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/amazon/view/51710-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An IoT company wants to release a new device that will collect data to track sleep overnight on an intelligent mattress. Sensors will send data that will be uploaded to an Amazon S3 bucket. About 2 MB of data is generated each night for each bed. Data must be processed and summarized for each user, and the results need to be available as soon as possible. Part of the process consists of time windowing and other functions. Based on tests with a Python script, every run will require about 1 GB of memory and will complete within a couple of minutes.<br>Which solution will run the script in the MOST cost-effective way?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Lambda with a Python script\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue with a Scala job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EMR with an Apache Spark script",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue with a PySpark job"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-16T12:37:00.000Z",
        "voteCount": 23,
        "content": "I think that answer should be \"A\". Lambda is the most cost effective solution and satisfies both memory and time requirements. Additionally lambda support \"tumbling windows\" https://aws.amazon.com/blogs/compute/using-aws-lambda-for-streaming-analytics/ so in my opinion \"A\" is the best option in this question."
      },
      {
        "date": "2022-06-18T22:23:00.000Z",
        "voteCount": 3,
        "content": "D\nBecause:\nTumbling windows are distinct time windows that open and close at regular intervals. By default, Lambda invocations are stateless\u2014you cannot use them for processing data across multiple continuous invocations without an external database. However, with tumbling windows, you can maintain your state across invocations. This state contains the aggregate result of the messages previously processed for the current window. Your state can be a maximum of 1 MB per shard. If it exceeds that size, Lambda terminates the window early.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-windows"
      },
      {
        "date": "2023-01-16T22:46:00.000Z",
        "voteCount": 2,
        "content": "Tumbling is for streaming analytics, and here it's a batch as data is in S3 source."
      },
      {
        "date": "2021-10-15T02:47:00.000Z",
        "voteCount": 22,
        "content": "Glue DataBrew supports window functions\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.window.html\nwill go with D"
      },
      {
        "date": "2023-08-05T01:00:00.000Z",
        "voteCount": 2,
        "content": "A Python script is already provided, and rewriting it in Scala or Spark would be unnecessary work. Time windowing here is not referencing streaming, the files are already inside S3."
      },
      {
        "date": "2023-08-03T17:10:00.000Z",
        "voteCount": 1,
        "content": "A - lambda invocation tumbling window functions"
      },
      {
        "date": "2023-04-01T14:00:00.000Z",
        "voteCount": 4,
        "content": "A, not D. Confucius says \n\"Do not use a cannon to kill a mosquito.\""
      },
      {
        "date": "2023-03-27T08:44:00.000Z",
        "voteCount": 4,
        "content": "In normal answer sure I will go for D, but most cost effective and only require 1GB of memory, I will go for A\n\nAWS Lambda is a serverless compute service that runs code in response to events and automatically scales based on the incoming traffic. With Lambda, the user only pays for the compute time that the function uses, making it a cost-effective option. Since the Python script has been tested to run within a few minutes with 1 GB of memory, AWS Lambda can easily handle the processing requirements for this project.\n\nIn addition, since the data generated each night is relatively small (2 MB per bed), AWS Lambda's maximum payload size of 512 MB is more than enough to handle the incoming data. The processed data can also be easily uploaded to the Amazon S3 bucket.\n\nTherefore, AWS Lambda with a Python script would be the most cost-effective solution for this project, as it provides a serverless and scalable environment for running the Python script with the required memory and processing capabilities."
      },
      {
        "date": "2023-03-24T07:06:00.000Z",
        "voteCount": 1,
        "content": "MOST cost-effective way is Lambda"
      },
      {
        "date": "2023-01-22T05:42:00.000Z",
        "voteCount": 2,
        "content": "Answer A was impossible at the time exam DAS-C01 came out (13 APR 2020.) The AWS blog post announcing the new windowing feature for Lambda is dated 15 DEC 2020."
      },
      {
        "date": "2023-01-19T06:51:00.000Z",
        "voteCount": 2,
        "content": "in my opinion is D. time windowing , 1 GB of memory and a couple of minutes of execution...Glue is better"
      },
      {
        "date": "2023-01-16T12:38:00.000Z",
        "voteCount": 1,
        "content": "Lamda to handle tumbling window - data should be limited to 1MB \nThese include:\n\nWindow start and end: the beginning and ending timestamps for the current tumbling window.\nState: an object containing the state returned from the previous window, which is initially empty. The state object can contain up to 1 MB of data.\nisFinalInvokeForWindow: indicates if this is the last invocation for the tumbling window. This only occurs once per window period.\nisWindowTerminatedEarly: a window ends early only if the state exceeds the maximum allowed size of 1 MB.\n\nTherefore its D - As glue job with Pyspark can handle this volume of data aggregation by each bed."
      },
      {
        "date": "2023-01-09T12:20:00.000Z",
        "voteCount": 1,
        "content": "Maybe the question is testing do you know PySpark can do windowing function. PySpark is also Python script. In term of cost effective, Glue is cheaper cpmpare to EMR. So overall, I vote for D."
      },
      {
        "date": "2022-12-11T09:41:00.000Z",
        "voteCount": 3,
        "content": "The process is already tested with python, so there is not a concern on the fact that we might need a PySpark job to work with dataframes, etc.\nAlso it seems a bit overkill to set up a Spark job to process 2mb files. Again, if the quote saying that \"the script is tested with python\" was not present, I would choose PySpark or Scala. \nThe Python mention might also be for us to choose PySpark over Scala, but it is not a easy question and I think both answers are quite right.\n\nI think it must be A"
      },
      {
        "date": "2022-11-27T08:25:00.000Z",
        "voteCount": 2,
        "content": "A is way cost effective."
      },
      {
        "date": "2022-10-27T05:57:00.000Z",
        "voteCount": 3,
        "content": "Go for A due to cost effectiveness as Glue is much expensive as compare to Lambda"
      },
      {
        "date": "2022-10-27T01:45:00.000Z",
        "voteCount": 1,
        "content": "The data size for every bed is 2 MB. In this case, Lambda should faster than Glub job"
      },
      {
        "date": "2022-10-25T09:32:00.000Z",
        "voteCount": 1,
        "content": "A: This new feature introduces the concept of a tumbling window, which is a fixed-size, non-overlapping time interval of up to 15 minutes. To use this, you specify a tumbling window duration in the event-source mapping between the stream and the Lambda function. When you apply a tumbling window to a stream, items in the stream are grouped by window and sent to the processing Lambda function. The function returns a state value that is passed to the next invocation of the tumbling window.\nhttps://aws.amazon.com/blogs/compute/using-aws-lambda-for-streaming-analytics/"
      },
      {
        "date": "2022-10-20T03:23:00.000Z",
        "voteCount": 1,
        "content": "D over A for tumbling window"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/amazon/view/51711-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to provide its data analysts with uninterrupted access to the data in its Amazon Redshift cluster. All data is streamed to an Amazon S3 bucket with Amazon Kinesis Data Firehose. An AWS Glue job that is scheduled to run every 5 minutes issues a COPY command to move the data into Amazon Redshift.<br>The amount of data delivered is uneven throughout the day, and cluster utilization is high during certain periods. The COPY command usually completes within a couple of seconds. However, when load spike occurs, locks can exist and data can be missed. Currently, the AWS Glue job is configured to run without retries, with timeout at 5 minutes and concurrency at 1.<br>How should a data analytics specialist configure the AWS Glue job to optimize fault tolerance and improve data availability in the Amazon Redshift cluster?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of retries. Decrease the timeout value. Increase the job concurrency.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the number of retries at 0. Decrease the timeout value. Increase the job concurrency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the number of retries at 0. Decrease the timeout value. Keep the job concurrency at 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the number of retries at 0. Increase the timeout value. Keep the job concurrency at 1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-28T07:07:00.000Z",
        "voteCount": 17,
        "content": "A is wrong, read that locks means: Locking is a protection mechanism that controls how many sessions can access a table at the same time. To solve a locking problem, identify the session (PID) that is holding the lock and then terminate the session. If the session doesn't terminate, reboot your cluster.\nSo... If you increase concurrency, you increase a number of sessions that makes things worse. \nHence C"
      },
      {
        "date": "2023-04-18T19:33:00.000Z",
        "voteCount": 1,
        "content": "Please explain how decreasing timeout help in this case?"
      },
      {
        "date": "2022-08-03T04:11:00.000Z",
        "voteCount": 2,
        "content": "You are only suggesting to decrease the timeout value. How does that help if data is missing to be copied? The best way to solve this problem is to increase re tries, wait less for a re try to complete and concurrently copy data to redshift. So ans is \"A\"."
      },
      {
        "date": "2021-10-02T21:29:00.000Z",
        "voteCount": 11,
        "content": "Not sure but should be A for number of re-tries and increased concurrency."
      },
      {
        "date": "2023-08-05T01:18:00.000Z",
        "voteCount": 2,
        "content": "D makes sense, increasing timeout leaves time for locks to resolve, also, job concurrency should be 1 for COPY commands, it doesnt make sense to have multiple ones running."
      },
      {
        "date": "2023-04-03T07:01:00.000Z",
        "voteCount": 5,
        "content": "A is the least wrong, since it theoretically will increase fault tolerance. But it's an incorrect by design. Increasing concurrency will lead to an increase in locks, leading to more timeouts, leading to more retries, so once we reach the max retries, some data still might be missed. The right answer would be to set concurrency to 1 and increase retries while decreasing timeouts.\nB, C and D: Keep the number of retries at 0. - whatever the other configuration you set, when the job fails due to a lock that job will never be retried and data will be missed 100%."
      },
      {
        "date": "2023-04-03T07:09:00.000Z",
        "voteCount": 4,
        "content": "Actually, timeout jobs aren't retried as per the docs. So even A is wrong from a retry point of view. The only thing that will allow us to wait for the lock to resolve themselves is an increase in timeout then - so really all answers are bad, but D is less bad then. Also the default timeout is 48 hours, so the fact that the job was set to 5 min timeout is short."
      },
      {
        "date": "2023-03-26T16:24:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best solution to optimize fault tolerance and improve data availability. Keeping the number of retries at 0 will ensure that the job does not attempt to retry if it fails, which may cause further locks and missed data. Decreasing the timeout value will ensure that the job fails quickly if it is unable to complete the COPY command. Keeping the job concurrency at 1 will ensure that the job runs on a single node, which will reduce the chances of locks occurring."
      },
      {
        "date": "2023-03-20T05:02:00.000Z",
        "voteCount": 2,
        "content": "ets go in reverse to answer this.\nD is out - Glue job that is scheduled to run every 5 minutes so timeout can't increase\nC is out - locks will still exist when spike is high\nB is out - jobs will fail due to decreased timeout when there is no concurrency."
      },
      {
        "date": "2023-03-20T05:00:00.000Z",
        "voteCount": 2,
        "content": "lets go in reverse to answer this.\nD is out - Glue job that is scheduled to run every 5 minutes\nC is out - locks will still exist\nB is out - jobs will due to decreased timeout when there is no concurrency."
      },
      {
        "date": "2023-02-17T22:35:00.000Z",
        "voteCount": 4,
        "content": "Locks generally happen in case of DDL statements. Refer this link - https://docs.aws.amazon.com/redshift/latest/dg/r_LOCK.html\nQuestion mentions about Glue job reading data every 5 mins and inserting it in Redshift. In case of lock present on table, the job would timeout. If we don't retry, there is obviously data loss. \nJob generally completes in few seconds so decreasing timeout definitely makes sense and everyone agree. \nWe definitely need to increase retry count so if lock exist, job will retry with same data insertion in hope that lock is released.\nEven if lock is not released in next attempt and by the time next glue job sequence will pick the data for insertion and if we don't increase concurrency, initial failed job would lead to data loss.\nSo important to increase concurrency as well so that previous instance and current instance of glue job can run together."
      },
      {
        "date": "2022-12-01T03:00:00.000Z",
        "voteCount": 1,
        "content": "Suggested Concurrency is 1"
      },
      {
        "date": "2022-08-04T02:32:00.000Z",
        "voteCount": 4,
        "content": "D.\nIncreasing the timeout is not conflict with \"typically finish with a few seconds\", and it can be helpful in the peak time"
      },
      {
        "date": "2022-08-03T04:11:00.000Z",
        "voteCount": 2,
        "content": "The best way to solve this problem is to increase re tries, wait less for a re try to complete and concurrently copy data to redshift. So ans is \"A\"."
      },
      {
        "date": "2022-07-21T19:15:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-05-21T18:10:00.000Z",
        "voteCount": 3,
        "content": "A looks right. Timeout should be reduced and retries should be increased. Also, concurrency should be increased so that the copy operation completes quickly. Concurrency will not cause locking as the copy command running in concurrency mode should be part of the same transaction. Hence there is no question of locking."
      },
      {
        "date": "2022-05-17T05:04:00.000Z",
        "voteCount": 2,
        "content": "Increasing number of retries will add stress on locks when Copy command completes in few seconds, there is no need to wait till 5 min rather, TimeOut should be decreased. Concurrency at 1 is perfect"
      },
      {
        "date": "2022-05-21T18:08:00.000Z",
        "voteCount": 1,
        "content": "if a copy command completes, system will not wait for 5 mins and there is no timeout scenario"
      },
      {
        "date": "2022-05-06T01:01:00.000Z",
        "voteCount": 2,
        "content": "Agree with NTP's first answer, and added some comments.\nThe question requires \"Data avaibility\" and \"fault tolerant\", so data loss should be avoid.\nI aggree with your previoud answer:\n1. COPY finish within a few seconds, so decrease timeout and increase retries. It can reduce the table lock as you said.\n2. increase concurrent jobs, it can execute COPY and QUERY at the same time, if only they are not on the same table. And if only most of the COPY can finish in seconds, there will be no LOCK because of this."
      },
      {
        "date": "2022-03-23T01:19:00.000Z",
        "voteCount": 2,
        "content": "If locks exists, reads/writes are made to wait till the session holding the lock completes\nRef: https://docs.aws.amazon.com/redshift/latest/dg/r_LOCK.html\n\nTherefore, increasing timeout will help. Keeping concurrency at 1 is recommended to avoid multiple COPY command and prevent multiple instances of glue task. \n\nMaybe answer = D"
      },
      {
        "date": "2022-03-23T04:03:00.000Z",
        "voteCount": 1,
        "content": "increased timeout will help because longer timeout means more waiting for glue job which means more chance for session holding the lock to complete before timeout occurs."
      },
      {
        "date": "2021-12-21T19:58:00.000Z",
        "voteCount": 1,
        "content": "A\nCOPY commands typically finish within a few seconds -&gt; decrease timeout to 30 seconds. If there is a lock, this task will fail and release a lock, but we still loose the data. So we need to add more retries, e.g. 2 retries. If we add so many retires, there could be concurrent jobs, concurrent COPY is not recommended but this case of lock happens many times continuously is rare"
      },
      {
        "date": "2021-12-21T20:04:00.000Z",
        "voteCount": 3,
        "content": "Change to C\nNumber of retries - Specify the number of times, from 0 to 10, that AWS Glue should automatically restart the job if it fails. Jobs that reach the timeout limit are not restarted.\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html\nSo retry will not help in case of timeout. With only reducing timeout, we can only reduce the locks and the amount of data lost, it's ok because the question does not require no data lost"
      },
      {
        "date": "2022-05-06T01:00:00.000Z",
        "voteCount": 1,
        "content": "The question requires \"Data avaibility\" and \"fault tolerant\", so data loss should be avoid.\nI aggree with your previoud answer:\n1. COPY finish within a few seconds, so decrease timeout and increase retries. It can reduce the table lock as you said.\n2. increase concurrent jobs, it can execute COPY and QUERY at the same time, if only they are not on the same table. And if only most of the COPY can finish in seconds, there will be no LOCK because of this."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/amazon/view/51714-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company leverages Amazon Athena for ad-hoc queries against an AWS Glue Data Catalog. The data analytics team manages the data catalog and data access for the company. The data analytics team wants to separate queries and manage the cost of running those queries by different workloads and teams.<br>Ideally, the data analysts want to group the queries run by different users within a team, store the query results in individual Amazon S3 buckets specific to each team, and enforce cost constraints on the queries run against the Data Catalog.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM groups and resource tags for each team within the company. Set up IAM policies that control user access and actions on the Data Catalog resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Athena resource groups for each team within the company and assign users to these groups. Add S3 bucket names and other query configurations to the properties list for the resource groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Athena workgroups for each team within the company. Set up IAM workgroup policies that control user access and actions on the workgroup resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Athena query groups for each team within the company and assign users to the groups."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T07:44:00.000Z",
        "voteCount": 24,
        "content": "C should be the one."
      },
      {
        "date": "2021-10-29T22:48:00.000Z",
        "voteCount": 5,
        "content": "C. Should be the right answer. https://docs.aws.amazon.com/athena/latest/ug/workgroups.html"
      },
      {
        "date": "2023-11-08T19:06:00.000Z",
        "voteCount": 1,
        "content": "Athena resouce group is not found.\nIs this correct recognization?\nso answer is C.I think."
      },
      {
        "date": "2023-05-01T10:38:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-25T04:36:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2022-10-27T06:04:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C as Athena workgroups can help isolate queries for teams, applications, and workloads. It also helps store the results in a separate S3 bucket and enforce cost constraints. Access to workgroups can be controlled using IAM. \nAWS doc says :\nWe recommend using workgroups to isolate queries for teams, applications, or different workloads. For example, you may create separate workgroups for two different teams in your organization. You can also separate workloads. For example, you can create two independent workgroups, one for automated scheduled applications, such as report generation, and another for ad-hoc usage by analysts. You can switch between workgroups. \n\nTo control access to workgroups, use resource-level IAM permissions or identity-based IAM policies. \nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups.html"
      },
      {
        "date": "2022-08-06T14:11:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-05-21T05:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-11-30T10:43:00.000Z",
        "voteCount": 2,
        "content": "C is the answer"
      },
      {
        "date": "2021-11-05T02:10:00.000Z",
        "voteCount": 3,
        "content": "C my option - Workgroups to isolate queries for teams, applications, or different workloads. For example, you may create separate workgroups for two different teams in your organization"
      },
      {
        "date": "2021-10-04T08:47:00.000Z",
        "voteCount": 1,
        "content": "Looks like its B\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/"
      },
      {
        "date": "2021-10-10T07:32:00.000Z",
        "voteCount": 2,
        "content": "I mean C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/amazon/view/51715-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A manufacturing company uses Amazon S3 to store its data. The company wants to use AWS Lake Formation to provide granular-level security on those data assets. The data is in Apache Parquet format. The company has set a deadline for a consultant to build a data lake.<br>How should the consultant create the MOST cost-effective solution that meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun Lake Formation blueprints to move the data to Lake Formation. Once Lake Formation has the data, apply permissions on Lake Formation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTo create the data catalog, run an AWS Glue crawler on the existing Parquet data. Register the Amazon S3 path and then apply permissions through Lake Formation to provide granular-level security.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Apache Ranger on an Amazon EC2 instance and integrate with Amazon EMR. Using Ranger policies, create role-based access control for the existing data assets in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple IAM roles for different users and groups. Assign IAM roles to different data assets in Amazon S3 to create table-based and column-based access controls."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-17T15:19:00.000Z",
        "voteCount": 39,
        "content": "I will go with Option B. Here Data is already there in S3 bucket in parquet format. We just need to register the S3 bucket with Lake Formation after the catalogue creation. Blueprints are Ideal way if the Data has to be brought to S3 from other sources. \nFrom Lake Formation FAQ:\n\nHow does Lake Formation organize my data in a data lake?\n\nA: You can use one of the blueprints available in Lake Formation to ingest data into your data lake. Lake Formation creates Glue workflows that crawl source tables, extract the data, and load it to S3. In S3, Lake Formation organizes the data for you, setting up partitions and data formats for optimized performance and cost. For data already in Amazon S3, you can register those buckets with Lake Formation to manage them.\n\nLake Formation also crawls your data lake to maintain a data catalog and provides an intuitive user interface for you to search entities (by type, classification, attribute, or free-form text.)"
      },
      {
        "date": "2021-10-23T20:18:00.000Z",
        "voteCount": 3,
        "content": "Makes sense, B for me"
      },
      {
        "date": "2021-12-03T00:39:00.000Z",
        "voteCount": 3,
        "content": "Agree Option B"
      },
      {
        "date": "2021-09-24T13:57:00.000Z",
        "voteCount": 15,
        "content": "Looks like 'A'.\nhttps://aws.amazon.com/blogs/big-data/building-securing-and-managing-data-lakes-with-aws-lake-formation/"
      },
      {
        "date": "2022-06-15T02:22:00.000Z",
        "voteCount": 1,
        "content": "I agree that it looks attractive. However, I think that the fact that the data is all already in S3 doesn't require Blueprints. The Blueprints section mentions importing data into your datalake, and your datalake will store the data in S3."
      },
      {
        "date": "2022-11-18T10:05:00.000Z",
        "voteCount": 1,
        "content": "It can't be A because your data is already in S3. Blueprints only work if you do not have an S3 data lake in place which is not the case for this question."
      },
      {
        "date": "2023-08-03T17:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lake-formation/latest/dg/security-permissions-example-scenario.html"
      },
      {
        "date": "2023-05-01T10:40:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-05-04T00:07:00.000Z",
        "voteCount": 1,
        "content": "Hi, Are these questions still valid for the exams ? Please respond."
      },
      {
        "date": "2023-05-23T03:38:00.000Z",
        "voteCount": 1,
        "content": "Did he respond?"
      },
      {
        "date": "2023-03-25T08:47:00.000Z",
        "voteCount": 2,
        "content": "It's B\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html"
      },
      {
        "date": "2022-12-13T20:18:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html"
      },
      {
        "date": "2022-10-27T06:06:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer as per doc:\nYou can use one of the blueprints available in Lake Formation to ingest data into your data lake. Lake Formation creates Glue workflows that crawl source tables, extract the data, and load it to S3. In S3, Lake Formation organizes the data for you, setting up partitions and data formats for optimized performance and cost. For data already in Amazon S3, you can register those buckets with Lake Formation to manage them.\nhttps://aws.amazon.com/lake-formation/faqs/\nOption A is wrong as duplicating the data in AWS Lake Formation would not be the most cost-effective approach.\n\nOption C is wrong as using EMR would not be the most cost-effective approach also it would take time to set up.\n\nOption D is wrong as S3 does not provide table-based or column-based access control."
      },
      {
        "date": "2022-09-14T22:30:00.000Z",
        "voteCount": 1,
        "content": "A: move data takes more effort and cost\nC: doesn't need EMR\nD: We should use Lake Formation not IAM, and IAM can't offer column level control.\nSo I like B"
      },
      {
        "date": "2022-09-14T22:30:00.000Z",
        "voteCount": 1,
        "content": "A: move data takes more effort and cost\nC: doesn't need EMR\nD: We should use Lake Formation not IAM, and IAM can't offer column level control.\nSo I like B"
      },
      {
        "date": "2022-07-20T23:20:00.000Z",
        "voteCount": 1,
        "content": "Option B."
      },
      {
        "date": "2022-06-15T02:23:00.000Z",
        "voteCount": 2,
        "content": "Use Lake Formation for the permissions. No need to use Blueprints because all the data is already in S3. Only thing that is required to get the data into the data lake is use a Glue Crawler."
      },
      {
        "date": "2022-06-14T12:21:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-05-27T03:44:00.000Z",
        "voteCount": 1,
        "content": "it's A. B is ruled out as you don't have to run crawler etc. when you are using LakeFormation. It will do everything for you"
      },
      {
        "date": "2022-05-22T01:50:00.000Z",
        "voteCount": 1,
        "content": "I think Answer should be B"
      },
      {
        "date": "2022-04-30T14:15:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      },
      {
        "date": "2022-03-05T05:34:00.000Z",
        "voteCount": 1,
        "content": "A - https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html\nLake formation can move data as well as provide granular access from its blue prints - which is mainly intended for cost effective/time efficient way to implement faster data lakes. Whereas option B is building datalake on our own. build vs buy analogy wise A will suit the question from quick turnaround point of view from deployment."
      },
      {
        "date": "2022-03-04T06:51:00.000Z",
        "voteCount": 1,
        "content": "B - the data is already in S3, no need to use a blueprint to import it"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/amazon/view/51718-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has an application that uses the Amazon Kinesis Client Library (KCL) to read records from a Kinesis data stream.<br>After a successful marketing campaign, the application experienced a significant increase in usage. As a result, a data analyst had to split some shards in the data stream. When the shards were split, the application started throwing an ExpiredIteratorExceptions error sporadically.<br>What should the data analyst do to resolve this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of threads that process the stream records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the provisioned read capacity units assigned to the stream's Amazon DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the provisioned write capacity units assigned to the stream's Amazon DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the provisioned write capacity units assigned to the stream's Amazon DynamoDB table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-11T01:57:00.000Z",
        "voteCount": 23,
        "content": "If the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table."
      },
      {
        "date": "2021-09-30T15:19:00.000Z",
        "voteCount": 11,
        "content": "Should be 'C'."
      },
      {
        "date": "2023-06-18T07:18:00.000Z",
        "voteCount": 1,
        "content": "If the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table."
      },
      {
        "date": "2023-05-01T10:40:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-29T13:15:00.000Z",
        "voteCount": 1,
        "content": "A. Seems like A per this https://github.com/awslabs/amazon-kinesis-client/issues/701"
      },
      {
        "date": "2023-03-29T13:17:00.000Z",
        "voteCount": 2,
        "content": "No it's wrong.\n\"A new shard iterator is returned by every GetRecords request (as NextShardIterator), which you then use in the next GetRecords request (as ShardIterator). Typically, this shard iterator does not expire before you use it. However, you may find that shard iterators expire because you have not called GetRecords for more than 5 minutes, or because you've performed a restart of your consumer application.\n\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. For more information, see Using a Lease Table to Track the Shards Processed by the KCL Consumer Application.\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly\nHence C"
      },
      {
        "date": "2023-03-04T05:19:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly"
      },
      {
        "date": "2023-02-15T12:55:00.000Z",
        "voteCount": 1,
        "content": "The ExpiredIteratorExceptions error indicates that the KCL is attempting to read records from a shard that has been closed or split. When a shard is split, two new shards are created, and records are redistributed among them. Any open iterator to the original shard becomes invalid and may return this error.\n\nTo resolve this issue, the data analyst should increase the number of worker threads that process the stream records. This will help ensure that records are read from the new shards as quickly as possible and reduce the likelihood of the KCL attempting to read from a closed or split shard.\n\nOption A is therefore the correct answer. Increasing the provisioned read or write capacity units assigned to the stream's Amazon DynamoDB table would not resolve this issue."
      },
      {
        "date": "2022-10-27T06:08:00.000Z",
        "voteCount": 4,
        "content": "C is the right answer\nFor each Amazon Kinesis Data Streams application, KCL uses a unique lease table (stored in a Amazon DynamoDB table) to keep track of the shards in a KDS data stream that are being leased and processed by the workers of the KCL consumer application.\n\nFor each Amazon Kinesis Data Streams application, KCL uses a unique lease table (stored in a Amazon DynamoDB table) to keep track of the shards in a KDS data stream that are being leased and processed by the workers of the KCL consumer application.\n\nFor each Amazon Kinesis Data Streams application, KCL uses a unique lease table (stored in a Amazon DynamoDB table) to keep track of the shards in a KDS data stream that are being leased and processed by the workers of the KCL consumer application."
      },
      {
        "date": "2022-07-19T21:55:00.000Z",
        "voteCount": 3,
        "content": "Should be 'C'."
      },
      {
        "date": "2022-06-18T22:42:00.000Z",
        "voteCount": 2,
        "content": "KCL uses DynamoDB for checkpointing and coordination. There for DynamoDB table shoud be fast enough to handle the throughput. This exception means that it is not, so we need to incrase WCUs."
      },
      {
        "date": "2022-06-16T10:05:00.000Z",
        "voteCount": 1,
        "content": "\"C\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly"
      },
      {
        "date": "2022-05-24T15:49:00.000Z",
        "voteCount": 1,
        "content": "Go with answer C"
      },
      {
        "date": "2022-05-18T04:39:00.000Z",
        "voteCount": 3,
        "content": "such a vague question. The question does not even mention that kinesis data stream is writing to Dynamodb. It just mentions some application is reading data from Kinesis using KCL. But C looks most sensible with assumption that application is writing output to DynamoDb"
      },
      {
        "date": "2022-01-30T11:36:00.000Z",
        "voteCount": 1,
        "content": "Shard Iterator Expires Unexpectedly\nA new shard iterator is returned by every GetRecords request (as NextShardIterator), which you then use in the next GetRecords request (as ShardIterator). Typically, this shard iterator does not expire before you use it. However, you may find that shard iterators expire because you have not called GetRecords for more than 5 minutes, or because you've performed a restart of your consumer application.\n\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. For more information, see Using a Lease Table to Track the Shards Processed by the KCL Consumer Application."
      },
      {
        "date": "2021-11-14T14:56:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2021-10-27T17:55:00.000Z",
        "voteCount": 5,
        "content": "C; read text from blog\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. For more information, see Using a Lease Table to Track the Shards Processed by the KCL Consumer Application."
      },
      {
        "date": "2021-10-14T12:02:00.000Z",
        "voteCount": 1,
        "content": "Go with C:\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/amazon/view/74081-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is building a service to monitor fleets of vehicles. The company collects IoT data from a device in each vehicle and loads the data into Amazon<br>Redshift in near-real time. Fleet owners upload .csv files containing vehicle reference data into Amazon S3 at different times throughout the day. A nightly process loads the vehicle reference data from Amazon S3 into Amazon Redshift. The company joins the IoT data from the device and the vehicle reference data to power reporting and dashboards. Fleet owners are frustrated by waiting a day for the dashboards to update.<br>Which solution would provide the SHORTEST delay between uploading reference data to Amazon S3 and the change showing up in the owners' dashboards?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 event notifications to trigger an AWS Lambda function to copy the vehicle reference data into Amazon Redshift immediately when the reference data is uploaded to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and schedule an AWS Glue Spark job to run every 5 minutes. The job inserts reference data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend reference data to Amazon Kinesis Data Streams. Configure the Kinesis data stream to directly load the reference data into Amazon Redshift in real time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the reference data to an Amazon Kinesis Data Firehose delivery stream. Configure Kinesis with a buffer interval of 60 seconds and to directly load the data into Amazon Redshift."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T06:32:00.000Z",
        "voteCount": 8,
        "content": "A until the 29th of May 2023, afterwards C, since streaming ingestion with be GA for 6 months https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-redshift-real-time-streaming-ingestion-kds-msk/"
      },
      {
        "date": "2022-10-27T06:11:00.000Z",
        "voteCount": 6,
        "content": "A is the right answer\n\nYou can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.\n\nOption B is wrong as an AWS Glue Spark job running every 5 mins is not the quickest way.\n\nOption C is wrong as Kinesis Data Streams does not integrate with Redshift, also it would have a limit on message size.\n\nOption D is wrong as Kinesis Data Firehose would still add a delay of min 60 secs, also it would have a limit on message size. \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"
      },
      {
        "date": "2023-11-08T18:39:00.000Z",
        "voteCount": 1,
        "content": "I think that this question already make near realtime solution.\nso A is correct answer.\nthis is worst question I think."
      },
      {
        "date": "2023-05-01T10:41:00.000Z",
        "voteCount": 1,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-10-27T02:43:00.000Z",
        "voteCount": 1,
        "content": "I think D is faster."
      },
      {
        "date": "2022-10-21T01:34:00.000Z",
        "voteCount": 1,
        "content": "KDS can't send data directly to RD so ruled out C.\nD has a 60 sec buffer time so ruled out\nSo A"
      },
      {
        "date": "2022-07-25T21:14:00.000Z",
        "voteCount": 2,
        "content": "Answer - A"
      },
      {
        "date": "2022-06-21T01:13:00.000Z",
        "voteCount": 1,
        "content": "I guess it should be A as well. Triggering a data upload upon S3 upload seems like a perfect solution in this case.\n\nHowever, I'm thrown off a little bit by the SHORTEST TIME INTERVAL. Because in theory streaming the data directly into Redshift could be faster. This should then be done via FireHose, because Kinesis Data Streams cannot stream into Redshift directly without some other type of service."
      },
      {
        "date": "2022-05-21T07:52:00.000Z",
        "voteCount": 1,
        "content": "For now A, later C ;)\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html"
      },
      {
        "date": "2022-04-30T16:29:00.000Z",
        "voteCount": 1,
        "content": "Answer - A"
      },
      {
        "date": "2022-04-21T21:32:00.000Z",
        "voteCount": 4,
        "content": "Event driven requirement =&gt; S3 event stream"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/amazon/view/51719-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is migrating from an on-premises Apache Hadoop cluster to an Amazon EMR cluster. The cluster runs only during business hours. Due to a company requirement to avoid intraday cluster failures, the EMR cluster must be highly available. When the cluster is terminated at the end of each business day, the data must persist.<br>Which configurations would enable the EMR cluster to meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEMR File System (EMRFS) for storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHadoop Distributed File System (HDFS) for storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue Data Catalog as the metastore for Apache Hive\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMySQL database on the master node as the metastore for Apache Hive",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMultiple master nodes in a single Availability Zone\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMultiple master nodes in multiple Availability Zones"
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T13:42:00.000Z",
        "voteCount": 35,
        "content": "yes.. I go with ACE.. \nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html\n\"Note : The cluster can reside only in one Availability Zone or subnet.\""
      },
      {
        "date": "2023-07-07T05:35:00.000Z",
        "voteCount": 1,
        "content": "I did not know that, thank you very much. Undoubtedly the correct answer is ACF."
      },
      {
        "date": "2021-12-27T05:10:00.000Z",
        "voteCount": 9,
        "content": "ACE, for those in doubts for F - EMR cluster can only be launched in single availability zone, if availability zone failure is to be considered then a read replica of EMR cluster is configured in another availability zone with shared storage space. But this option is not there in the choice. so ACE is the correct answer\nhttps://aws.amazon.com/getting-started/hands-on/optimize-amazon-emr-clusters-with-ec2-spot/"
      },
      {
        "date": "2023-05-01T10:43:00.000Z",
        "voteCount": 1,
        "content": "ACE: I passed the test"
      },
      {
        "date": "2023-04-03T06:33:00.000Z",
        "voteCount": 1,
        "content": "E not F because Amazon EMR clusters with multiple primary nodes are not tolerant to Availability Zone failures. \n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha-considerations.html"
      },
      {
        "date": "2023-04-01T19:57:00.000Z",
        "voteCount": 1,
        "content": "This question appears in the Stephane Maarek's Udemy course."
      },
      {
        "date": "2023-11-26T09:45:00.000Z",
        "voteCount": 2,
        "content": "where ? I must have missed it."
      },
      {
        "date": "2023-01-26T09:26:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/questions/QUvOaZvA5BT56skWO0iu2kZA/emr-in-2-a-zs-and-high-availability"
      },
      {
        "date": "2023-01-24T11:57:00.000Z",
        "voteCount": 1,
        "content": "Amazon EMR clusters with multiple primary nodes are not tolerant to Availability Zone failures. In the case of an Availability Zone outage, you lose access to the Amazon EMR cluster.\nE - is not correct here"
      },
      {
        "date": "2022-10-27T06:14:00.000Z",
        "voteCount": 6,
        "content": "Correct answers are A, C &amp; E\nOption A as the cluster is not persistent and terminated each business day, it would be best to use EMRFS and S3 as an external persistence layer.\nOption C as AWS Glue Data Catalog can be used as the metastore for Apache Hive.\nOption E as Multiple master nodes are hosted in a single AZ or subnet. \n\n\nOption B is wrong as HDFS would need a persistent cluster.\nOption D is wrong as the MySQL database should be external and not installed on the master nodes.\nOption F is wrong as multiple master nodes cannot be hosted in multiple AZs but in a single AZ."
      },
      {
        "date": "2022-09-23T11:34:00.000Z",
        "voteCount": 1,
        "content": "E is correct (ACE) \nAmazon docs lists E"
      },
      {
        "date": "2022-08-14T22:14:00.000Z",
        "voteCount": 1,
        "content": "I go with ACE"
      },
      {
        "date": "2022-07-25T21:25:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: ACE"
      },
      {
        "date": "2022-06-16T15:48:00.000Z",
        "voteCount": 1,
        "content": "Amazon EMR supports multiple master nodes to enable high availability for EMR applications. EMR clusters with multiple master nodes are not tolerant of Availability Zone failures. In the case of an Availability Zone outage, you lose access to the EMR cluster. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously."
      },
      {
        "date": "2022-05-21T04:52:00.000Z",
        "voteCount": 1,
        "content": "My Answer is A, C &amp; E"
      },
      {
        "date": "2022-04-28T02:03:00.000Z",
        "voteCount": 1,
        "content": "Many explaination below."
      },
      {
        "date": "2022-02-20T09:14:00.000Z",
        "voteCount": 1,
        "content": "A, C, F\nhttps://aws.amazon.com/jp/blogs/news/setting-up-read-replica-clusters-with-hbase-on-amazon-s3/"
      },
      {
        "date": "2022-02-20T09:16:00.000Z",
        "voteCount": 1,
        "content": "I'm not a fluent speaker of English, so it's possible that I didn't understand the intent of the question."
      },
      {
        "date": "2021-11-20T21:19:00.000Z",
        "voteCount": 1,
        "content": "I go with ACE.."
      },
      {
        "date": "2021-11-03T01:27:00.000Z",
        "voteCount": 1,
        "content": "The correct answer shown is BCF but in discussions it\u2019s apparent the answe should be ACF. What\u2019s the actual answer?"
      },
      {
        "date": "2022-02-23T08:38:00.000Z",
        "voteCount": 3,
        "content": "You must be new here... Always open the discussion to see the actual, correct answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/amazon/view/51724-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company wants to use Amazon QuickSight to generate dashboards for web and in-store sales. A group of 50 business intelligence professionals will develop and use the dashboards. Once ready, the dashboards will be shared with a group of 1,000 users.<br>The sales data comes from different stores and is uploaded to Amazon S3 every 24 hours. The data is partitioned by year and month, and is stored in Apache<br>Parquet format. The company is using the AWS Glue Data Catalog as its main data catalog and Amazon Athena for querying. The total size of the uncompressed data that the dashboards query from at any point is 200 GB.<br>Which configuration will provide the MOST cost-effective solution that meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into an Amazon Redshift cluster by using the COPY command. Configure 50 author users and 1,000 reader users. Use QuickSight Enterprise edition. Configure an Amazon Redshift data source with a direct query option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse QuickSight Standard edition. Configure 50 author users and 1,000 reader users. Configure an Athena data source with a direct query option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse QuickSight Enterprise edition. Configure 50 author users and 1,000 reader users. Configure an Athena data source and import the data into SPICE. Automatically refresh every 24 hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse QuickSight Enterprise edition. Configure 1 administrator and 1,000 reader users. Configure an S3 data source and import the data into SPICE. Automatically refresh every 24 hours."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-21T18:50:00.000Z",
        "voteCount": 26,
        "content": "I think its C.\n\nWhen you create or edit a dataset, you choose to use either SPICE or a direct query, unless the dataset contains uploaded files. Importing (also called ingesting) your data into SPICE can save time and money:\n\nYour analytical queries process faster.\n\nYou don't need to wait for a direct query to process.\n\nData stored in SPICE can be reused multiple times without incurring additional costs. If you use a data source that charges per query, you're charged for querying the data when you first create the dataset and later when you refresh the dataset.\n\nQuotas for SPICE are as follows:\n\n2,047 Unicode characters for each field\n\n127 Unicode characters for each column name\n\n2,000 columns for each file\n\n1,000 files for each manifest\n\nFor Standard edition, 25 million (25,000,000) rows or 25 GB for each dataset\n\nFor Enterprise edition, 250 million (250,000,000) rows or 500 GB for each dataset\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/spice.html\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html"
      },
      {
        "date": "2021-10-24T15:44:00.000Z",
        "voteCount": 1,
        "content": "Answer C might be the best for saving time and money but Standard edition is best for saving money, and since the solution is expected for Cost effective, shouldn't B is the correct answer?"
      },
      {
        "date": "2021-12-12T21:27:00.000Z",
        "voteCount": 2,
        "content": "yeah, but what about refresh after 24 hrs? it is there in option C."
      },
      {
        "date": "2021-09-21T14:36:00.000Z",
        "voteCount": 12,
        "content": "Looks like C."
      },
      {
        "date": "2024-01-24T09:14:00.000Z",
        "voteCount": 2,
        "content": "standard edition can only have 100 users \nhttps://docs.aws.amazon.com/quicksight/latest/user/managing-users-standard.html"
      },
      {
        "date": "2023-05-01T10:43:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-02-15T12:35:00.000Z",
        "voteCount": 1,
        "content": "B is not the answer. SPICE can load upto 50GB of data as per the below link: Amazon QuickSight now supports larger SPICE datasets on the Enterprise Edition. Earlier each SPICE dataset could hold up to 250 million rows and 500GB of data. Now, all new SPICE datasets can accommodate up to 500 million rows (or 500GB) of data in the Enterprise Edition and 25 million rows (or 25GB) for Standard Edition. This raises the limit for your datasets, letting you accelerate dashboards with more data. See here for details."
      },
      {
        "date": "2022-07-21T19:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-04-29T01:25:00.000Z",
        "voteCount": 1,
        "content": "as explained by others"
      },
      {
        "date": "2022-03-20T04:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-12-22T18:54:00.000Z",
        "voteCount": 5,
        "content": "C\nStandard edition does not support Readers\nhttps://aws.amazon.com/vi/quicksight/pricing/\nCompare table"
      },
      {
        "date": "2021-12-20T22:32:00.000Z",
        "voteCount": 7,
        "content": "Answer C\nOnly Enterprise Edition supports Readers. So eliminate B. \nNeed 50 authors so eliminate D \nBetween A and C - SPICE is most cost effective."
      },
      {
        "date": "2021-11-20T07:32:00.000Z",
        "voteCount": 1,
        "content": "C seems correct."
      },
      {
        "date": "2021-10-17T14:49:00.000Z",
        "voteCount": 1,
        "content": "C seems correct. As mentioned by @aqs:\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html"
      },
      {
        "date": "2021-10-13T13:13:00.000Z",
        "voteCount": 2,
        "content": "Importing to SPICE still has the limit of 10GB. I think the answer should be B.\nhttps://aws.amazon.com/quicksight/pricing/?nc=sn&amp;loc=2&amp;dn=1"
      },
      {
        "date": "2021-10-08T10:01:00.000Z",
        "voteCount": 2,
        "content": "C , based on article : https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html  , Spice Enterprise edition now allows upto 500 GB Dataset."
      },
      {
        "date": "2021-10-06T16:21:00.000Z",
        "voteCount": 2,
        "content": "After intense research B is the most cost effective option check out the links below and see for yourselves. thanks\nhttps://aws.amazon.com/quicksight/pricing/\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html"
      },
      {
        "date": "2021-10-16T01:22:00.000Z",
        "voteCount": 4,
        "content": "Standard version supports only 100 users. Direct query option doesn't guarantee it is cheaper because there are 1000 users who scan your data via Athena."
      },
      {
        "date": "2021-10-22T00:28:00.000Z",
        "voteCount": 1,
        "content": "Where did you see that ?"
      },
      {
        "date": "2021-11-15T09:19:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/quicksight/latest/user/managing-users-standard.html"
      },
      {
        "date": "2021-10-04T22:05:00.000Z",
        "voteCount": 3,
        "content": "Cant import more than 10GB into spice. How then do you visualize without bringing it into SPICE. B looks like the best option from all"
      },
      {
        "date": "2021-09-26T00:00:00.000Z",
        "voteCount": 4,
        "content": "ANSWER:C\nEXPLANATION:We do have author ,reader and admin type of roles /users defined in QuickSight and  the question doesn't talk about the admin role .50 BI professional who developed and will be using the dashboard (Author) and 1000 users(Reader )."
      },
      {
        "date": "2021-09-30T22:00:00.000Z",
        "voteCount": 4,
        "content": "It wont be a cost effective solution as data is being brought into SPICE which is almost 200GB. only 10 GB spice data is give free of cost per user, for rest 190GB charge will be applied. So it should ideally be B."
      },
      {
        "date": "2021-10-27T23:21:00.000Z",
        "voteCount": 4,
        "content": "SPICE means the data will be reused; whereas 1000 users quering 200GB each with Athena.. that'll be costly.\nAnswer C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/amazon/view/51725-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A central government organization is collecting events from various internal applications using Amazon Managed Streaming for Apache Kafka (Amazon MSK).<br>The organization has configured a separate Kafka topic for each application to separate the data. For security reasons, the Kafka cluster has been configured to only allow TLS encrypted data and it encrypts the data at rest.<br>A recent application update showed that one of the applications was configured incorrectly, resulting in writing data to a Kafka topic that belongs to another application. This resulted in multiple errors in the analytics pipeline as data from different applications appeared on the same topic. After this incident, the organization wants to prevent applications from writing to a topic different than the one they should write to.<br>Which solution meets these requirements with the least amount of effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a different Amazon EC2 security group for each application. Configure each security group to have access to a specific topic in the Amazon MSK cluster. Attach the security group to each application based on the topic that the applications should read and write to.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Kafka Connect on each application instance and configure each Kafka Connect instance to write to a specific topic only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kafka ACLs and configure read and write permissions for each topic. Use the distinguished name of the clients' TLS certificates as the principal of the ACL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a different Amazon EC2 security group for each application. Create an Amazon MSK cluster and Kafka topic for each application. Configure each security group to have access to the specific cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-31T18:05:00.000Z",
        "voteCount": 31,
        "content": "Looks like C is the right option.\nhttps://docs.aws.amazon.com/msk/latest/developerguide/msk-acls.html"
      },
      {
        "date": "2021-10-05T03:16:00.000Z",
        "voteCount": 8,
        "content": "C should be the answer."
      },
      {
        "date": "2024-02-28T08:01:00.000Z",
        "voteCount": 1,
        "content": "Option B:\nWhile Kafka Connect itself doesn't directly \"bind\" to a topic, you can configure connectors to interact with specific MSK topics in various ways:\n\n1. Connector Configuration:\n\nEach connector type has specific configuration options that define the source or sink topics it interacts with.\nFor source connectors, you'll typically specify the topic name where the connector will read data from. This could be done through properties like topics or source.topics depending on the connector type.\nFor sink connectors, you'll typically specify the topic name where the connector will write data to. This could be done through properties like topic or sink.topics based on the connector type."
      },
      {
        "date": "2023-05-01T10:47:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-10-27T06:22:00.000Z",
        "voteCount": 5,
        "content": "C is the correct as per doc\nApache Kafka has a pluggable authorizer and ships with an out-of-box authorizer implementation that uses Apache ZooKeeper to store all ACLs. Amazon MSK enables this authorizer in the server.properties file on the brokers. For Apache Kafka version 2.4.1, the authorizer is AclAuthorizer. For earlier versions of Apache Kafka, it is SimpleAclAuthorizer.\n\nOption A is wrong as the Security group cannot be used to control which instance can access which topic.\n\nOption B is wrong as it does not restrict access and the applications can still push the data to other topics.\n\nOption D is wrong as it does not meet the least amount of effort requirement."
      },
      {
        "date": "2022-07-19T23:26:00.000Z",
        "voteCount": 1,
        "content": "C is correct one"
      },
      {
        "date": "2022-04-20T09:32:00.000Z",
        "voteCount": 1,
        "content": "I vote C"
      },
      {
        "date": "2022-03-01T00:49:00.000Z",
        "voteCount": 3,
        "content": "c is the answer"
      },
      {
        "date": "2022-01-26T11:40:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-12-15T17:18:00.000Z",
        "voteCount": 2,
        "content": "C is correct one"
      },
      {
        "date": "2021-11-17T13:26:00.000Z",
        "voteCount": 1,
        "content": "C should be the Ans.."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/amazon/view/51728-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to collect and process events data from different departments in near-real time. Before storing the data in Amazon S3, the company needs to clean the data by standardizing the format of the address and timestamp columns. The data varies in size based on the overall load at each particular point in time. A single data record can be 100 KB-10 MB.<br>How should a data analytics specialist design the solution for data ingestion?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams. Configure a stream for the raw data. Use a Kinesis Agent to write data to the stream. Create an Amazon Kinesis Data Analytics application that reads data from the raw stream, cleanses it, and stores the output to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose. Configure a Firehose delivery stream with a preprocessing AWS Lambda function for data cleansing. Use a Kinesis Agent to write data to the delivery stream. Configure Kinesis Data Firehose to deliver the data to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Streaming for Apache Kafka. Configure a topic for the raw data. Use a Kafka producer to write data to the topic. Create an application on Amazon EC2 that reads data from the topic by using the Apache Kafka consumer API, cleanses the data, and writes to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Simple Queue Service (Amazon SQS). Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-19T16:49:00.000Z",
        "voteCount": 26,
        "content": "C. Should be the right answer, because of the main requirement \"A single data record can be 100 KB-10 MB.\"\n- Kinesis firehose - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB.\u2028\n- Kinesis stream - The maximum size of the data payload of a record before base64-encoding is up to 1 MB.\n - SQS - https://aws.amazon.com/pt/about-aws/whats-new/2015/10/now-send-payloads-up-to-2gb-with-amazon-sqs/"
      },
      {
        "date": "2021-12-20T20:36:00.000Z",
        "voteCount": 5,
        "content": "I think answer should be  D\nYou rightly mentioned above about Firehose and Stream limits. However MSK has maximum record size of 8MB  - https://docs.aws.amazon.com/msk/latest/developerguide/limits.html\nSQS now allow payloads upto 2GB -\nUsing the Extended Client Library, message payloads larger than 256KB are stored in an Amazon Simple Storage Service (S3) bucket, using SQS to send and receive a reference to the payload location."
      },
      {
        "date": "2022-02-14T05:19:00.000Z",
        "voteCount": 3,
        "content": "The Kafka record size depends on message size. It is up to 100MB. I have experience setting the configuration for sending the encoded image through kafka."
      },
      {
        "date": "2023-01-17T01:15:00.000Z",
        "voteCount": 5,
        "content": "The limit for MSK mentioned here (8MB) is for MSK serverless only"
      },
      {
        "date": "2022-06-05T23:19:00.000Z",
        "voteCount": 2,
        "content": "It should be D.\nMaximum message size for Amazon MSK is 8MB.\nhttps://docs.aws.amazon.com/msk/latest/developerguide/limits.html"
      },
      {
        "date": "2021-10-19T23:41:00.000Z",
        "voteCount": 8,
        "content": "It should be C. 1 MB is the soft limit for the Kafka which can be increased up to 10 MB. That's the main difference b/w KDA and Kafka"
      },
      {
        "date": "2023-08-05T03:10:00.000Z",
        "voteCount": 1,
        "content": "MSK is the only one that can handle single put of 10MB.\nFirehose can handle up to 128 MB Buffered data, but it just catches single records of up to 3MB in that window and sends them in batch, you can't put a record that is higher than 3MB"
      },
      {
        "date": "2023-08-05T03:12:00.000Z",
        "voteCount": 1,
        "content": "But answer C really needs a lot of setup, if the question was about 1 MB records it would be Firehose immediately."
      },
      {
        "date": "2023-05-01T10:49:00.000Z",
        "voteCount": 3,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-10-27T11:28:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB.\n\nAmazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters. It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka. This means existing applications, tooling, and plugins from partners and the Apache Kafka community are supported without requiring changes to application code.\n\nOptions A &amp; B are wrong as the maximum size of a record sent to both Kinesis Data Stream and Kinesis Data Firehose base64-encoding, is 1,000 KiB.\n\nOption D is wrong SQS data size limit is 256KB."
      },
      {
        "date": "2022-10-27T11:26:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB. \nCorrect answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB. \n\nCorrect answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB. \n\nOption D is wrong SQS data size limit is 256KB."
      },
      {
        "date": "2022-10-24T02:24:00.000Z",
        "voteCount": 1,
        "content": "Kalka can handle the limit, Firehose is 8mb max"
      },
      {
        "date": "2022-10-24T02:24:00.000Z",
        "voteCount": 1,
        "content": "Sorry i meant C not D"
      },
      {
        "date": "2022-07-21T17:19:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-06-28T07:10:00.000Z",
        "voteCount": 1,
        "content": "KDS, KDF and MSK all have record/message limit lower than 10MB. https://aws.amazon.com/kinesis/data-streams/faqs/, https://aws.amazon.com/kinesis/data-streams/faqs/, https://docs.aws.amazon.com/msk/latest/developerguide/limits.html. SQS is recommended for \"Using the ability of Amazon SQS to scale transparently. For example, you buffer requests and the load changes as a result of occasional load spikes or the natural growth of your business. Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.\": https://aws.amazon.com/kinesis/data-streams/faqs/"
      },
      {
        "date": "2022-05-27T05:08:00.000Z",
        "voteCount": 1,
        "content": "Although looks unnecessary to use EC2 to clean up data, C is still the right answer. Both Kinesis data streams and firehose have upper limit of 1 mb record size. Ideally, instead of using ec2, I would prefer to use something like Kafka Streams to perform required transformation."
      },
      {
        "date": "2022-05-13T00:30:00.000Z",
        "voteCount": 1,
        "content": "C\nThe maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB."
      },
      {
        "date": "2022-04-30T14:54:00.000Z",
        "voteCount": 1,
        "content": "Answer - C"
      },
      {
        "date": "2022-02-09T01:28:00.000Z",
        "voteCount": 2,
        "content": "C. For AWS Lambda processing, you can set a buffering hint between 1 MiB and 3 MiB using the BufferSizeInMBs processor parameter."
      },
      {
        "date": "2021-12-21T18:24:00.000Z",
        "voteCount": 1,
        "content": "C - cleanses the data, and writes to Amazon S3.\nD does not mention cleaning the data"
      },
      {
        "date": "2021-11-28T17:53:00.000Z",
        "voteCount": 1,
        "content": "why not B ? Buffer size can be set up to 128 MB"
      },
      {
        "date": "2021-11-03T20:17:00.000Z",
        "voteCount": 1,
        "content": "Between B and C, I chose C because\nThe maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB."
      },
      {
        "date": "2021-10-27T04:39:00.000Z",
        "voteCount": 2,
        "content": "Why not B? \nQuestion is saying about near real time. and Firehose quota can be increased using Amazon Kinesis data firehose limit form."
      },
      {
        "date": "2022-11-18T10:47:00.000Z",
        "voteCount": 1,
        "content": "Kinesis firehose - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/amazon/view/51727-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An operations team notices that a few AWS Glue jobs for a given ETL application are failing. The AWS Glue jobs read a large number of small JSON files from an<br>Amazon S3 bucket and write the data to a different S3 bucket in Apache Parquet format with no major transformations. Upon initial investigation, a data engineer notices the following error message in the History tab on the AWS Glue console: `Command Failed with Exit Code 1.`<br>Upon further investigation, the data engineer notices that the driver memory profile of the failed jobs crosses the safe threshold of 50% usage quickly and reaches<br>90`\"95% soon after. The average memory usage across all executors continues to be less than 4%.<br>The data engineer also notices the following error while examining the related Amazon CloudWatch Logs.<br><img src=\"/assets/media/exam-media/04144/0006000001.png\" class=\"in-exam-image\"><br>What should the data engineer do to solve the failure in the MOST cost-effective way?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the worker type from Standard to G.2X.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the AWS Glue ETL code to use the 'groupFiles': 'inPartition' feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the fetch size setting by using AWS Glue dynamics frame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify maximum capacity to increase the total maximum data processing units (DPUs) used."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T12:43:00.000Z",
        "voteCount": 33,
        "content": "Bssed on the link, I will go for B\n\nhttps://awsfeed.com/whats-new/big-data/optimize-memory-management-in-aws-glue"
      },
      {
        "date": "2021-12-04T00:03:00.000Z",
        "voteCount": 7,
        "content": "B It is\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"
      },
      {
        "date": "2022-10-27T11:29:00.000Z",
        "voteCount": 16,
        "content": "Correct answer is B as the key issue is the driver memory problem caused because of the glue job processing multiple small files. Grouping of the files helps group the files and hence Spark driver stores significantly less state in memory.\n\nIn this scenario, a Spark job is reading a large number of small files from Amazon Simple Storage Service (Amazon S3). It converts the files to Apache Parquet format and then writes them out to Amazon S3. The Spark driver is running out of memory. The input Amazon S3 data has more than 1 million files in different Amazon S3 partitions.\n\nYou can fix the processing of the multiple files by using the grouping feature in AWS Glue. Grouping is automatically enabled when you use dynamic frames and when the input dataset has a large number of files (more than 50,000). Grouping allows you to coalesce multiple files together into a group, and it allows a task to process the entire group instead of a single file. As a result, the Spark driver stores significantly less state in memory to track fewer tasks."
      },
      {
        "date": "2022-10-27T11:29:00.000Z",
        "voteCount": 3,
        "content": "df = glueContext.create_dynamic_frame_from_options\n    (\"s3\", {'paths': [\"s3://input_path\"], \"recurse\":True, 'groupFiles': 'inPartition'}, format=\"json\")\ndatasink = glueContext.write_dynamic_frame.from_options\n    (frame = df, connection_type = \"s3\", connection_options = {\"path\": output_path}, format = \"parquet\", transformation_ctx = \"datasink\")"
      },
      {
        "date": "2022-10-27T11:30:00.000Z",
        "voteCount": 2,
        "content": "The driver runs below the threshold of 50 percent memory usage over the entire duration of the AWS Glue job. The executors stream the data from Amazon S3, process it, and write it out to Amazon S3. As a result, they consume less than 5 percent memory at any point in time."
      },
      {
        "date": "2023-08-05T07:21:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"
      },
      {
        "date": "2023-05-01T10:49:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-01-25T22:57:00.000Z",
        "voteCount": 2,
        "content": "B, more details you can find here\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-oom-abnormalities.html"
      },
      {
        "date": "2022-12-21T03:25:00.000Z",
        "voteCount": 4,
        "content": "B\nhttps://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/"
      },
      {
        "date": "2022-09-13T12:17:00.000Z",
        "voteCount": 3,
        "content": "D would work, but it's not the most COST EFFECTIVE way. So B"
      },
      {
        "date": "2022-07-25T21:14:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-06-16T15:28:00.000Z",
        "voteCount": 1,
        "content": "A data processing unit (DPU) is a relative measure of processing power that consists of vCPUs and memory. Change the maximum capacity parameter value and set it to a higher number"
      },
      {
        "date": "2022-06-08T14:35:00.000Z",
        "voteCount": 1,
        "content": "B -- Groupfiles will help here .."
      },
      {
        "date": "2022-05-22T01:33:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D"
      },
      {
        "date": "2022-05-21T04:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-05-06T18:06:00.000Z",
        "voteCount": 2,
        "content": "No doubt B."
      },
      {
        "date": "2022-04-30T16:24:00.000Z",
        "voteCount": 2,
        "content": "Answer - B"
      },
      {
        "date": "2022-03-10T05:10:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-oom-abnormalities.html#monitor-debug-oom-fix"
      },
      {
        "date": "2022-02-19T16:39:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-12-04T00:03:00.000Z",
        "voteCount": 1,
        "content": "B\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/amazon/view/74931-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A transport company wants to track vehicular movements by capturing geolocation records. The records are 10 B in size and up to 10,000 records are captured each second. Data transmission delays of a few minutes are acceptable, considering unreliable network conditions. The transport company decided to use<br>Amazon Kinesis Data Streams to ingest the data. The company is looking for a reliable mechanism to send data to Kinesis Data Streams while maximizing the throughput efficiency of the Kinesis shards.<br>Which solution will meet the company's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis Agent",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis Producer Library (KPL)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis Data Firehose",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis SDK"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-sdk.htmls",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-27T11:32:00.000Z",
        "voteCount": 10,
        "content": "Correct answer is B as Kinesis Producer Library (KPL) is a wrapper over Kinesis Agent provides the ability to buffer, batch and retry functionality and helps achieve high write throughput. \n\nThe Amazon Kinesis Producer Library (KPL) simplifies the producer application development by enabling developers to achieve high write throughput to one or more Kinesis data streams. The KPL is an easy to use, highly configurable library that you install on your hosts. It acts as an intermediary between your producer application code and the Kinesis Streams API actions. \n\nThe KPL can help build high-performance producers. Consider a situation where your Amazon EC2 instances serve as a proxy for collecting 100-byte events from hundreds or thousands of low power devices and writing records into a Kinesis data stream. These EC2 instances must each write thousands of events per second to your data stream. To achieve the throughput needed, producers must implement complicated logic, such as batching or multithreading, in addition to retry logic and record de-aggregation at the consumer side. The KPL performs all of these tasks for you."
      },
      {
        "date": "2022-10-27T11:33:00.000Z",
        "voteCount": 3,
        "content": "Because the KPL may buffer records before sending them to Kinesis Data Streams, it does not force the caller application to block and wait for a confirmation that the record has arrived at the server before continuing execution. A call to put a record into the KPL always returns immediately and does not wait for the record to be sent or a response to be received from the server. Instead, a Future object is created that receives the result of sending the record to Kinesis Data Streams at a later time. This is the same behavior as asynchronous clients in the AWS SDK."
      },
      {
        "date": "2022-10-27T11:33:00.000Z",
        "voteCount": 3,
        "content": "The KPL provides the following features out of the box:\n\n    Batching of puts using PutRecords (the Collector in the architecture diagram)\n    Tracking of record age and enforcement of maximum buffering times (all components)\n    Per-shard record aggregation (the Aggregator)\n    Retries in case of errors, with ability to distinguish between retryable and non-retryable errors (the Retrier)\n    Per-shard rate limiting to prevent excessive and pointless spamming (the Limiter)\n    Useful metrics and a highly efficient CloudWatch client"
      },
      {
        "date": "2023-05-01T10:50:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-08-06T14:15:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2022-04-29T15:57:00.000Z",
        "voteCount": 4,
        "content": "B.\nhttps://docs.aws.amazon.com/ja_jp/streams/latest/dev/developing-producers-with-kpl.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/amazon/view/60900-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company has 15 stores across 6 cities in the United States. Once a month, the sales team requests a visualization in Amazon QuickSight that provides the ability to easily identify revenue trends across cities and stores. The visualization also helps identify outliers that need to be examined with further analysis.<br>Which visual type in QuickSight meets the sales team's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGeospatial chart",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLine chart",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHeat map\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTree map"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T13:29:00.000Z",
        "voteCount": 25,
        "content": "Answer is C\nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\nUse a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot."
      },
      {
        "date": "2022-10-27T11:39:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is C as Heat Map can help measure the intersection between two dimensions i.e cities and stores, with color-coding to easily differentiate the patterns.\n\nUse heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range. Heat maps can also be used to show the count of values for the intersection of the two dimensions.\n\nEach rectangle on a heat map represents the value for the specified measure for the intersection of the selected dimensions. Rectangle color represents where the value falls in the range for the measure, with darker colors indicating higher values and lighter colors indicating lower ones.\n\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to further analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns."
      },
      {
        "date": "2023-05-01T12:25:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-04-18T22:16:00.000Z",
        "voteCount": 1,
        "content": "Correct ans is C"
      },
      {
        "date": "2023-02-04T08:36:00.000Z",
        "voteCount": 4,
        "content": "C is the right answer.\nWe have the requirements \"easily identify revenue trends across cities and stores\" and \"identify outliers\"\nA: first requirement not fulfilled, there are more stores than cities so it is possible that two stores have the sam postal code, which means they are not distinguishable with geospatial charts\nB - first requirement not fulfilled, a line chart with 15 lines is impossible to understand \"easily\"\nC - both requirements fulfilled, it's easy to tell apart stores / cities and outliers are easily visible, also you can view trends over time\nD - first requirement not fulfilled, it's not possible to see trends over time"
      },
      {
        "date": "2023-01-31T09:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is B \nThe best type of visualization for the sales team's requirements is a line chart. A line chart provides an easy way to identify revenue trends across cities and stores, as well as outliers that need to be examined with further analysis. It is also suited to compare multiple sets of data easily. With a line chart, users can simply follow the line to identify where the revenue for each city and store is trending up or down."
      },
      {
        "date": "2023-01-09T03:02:00.000Z",
        "voteCount": 1,
        "content": "Use heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range. Heat maps can also be used to show the count of values for the intersection of the two dimensions."
      },
      {
        "date": "2022-12-10T22:14:00.000Z",
        "voteCount": 1,
        "content": "Option \"C\". \nQuote below from AWS :-\n\"Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to further analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns.\""
      },
      {
        "date": "2022-12-08T01:41:00.000Z",
        "voteCount": 1,
        "content": "If the option is base on product or store (only) then I select heat map, but in here we have location information"
      },
      {
        "date": "2022-11-14T12:02:00.000Z",
        "voteCount": 2,
        "content": "B: All other options fail to show a trend over time. Outliers are also clearly visible in line chart."
      },
      {
        "date": "2022-11-06T23:27:00.000Z",
        "voteCount": 1,
        "content": "I vote for C.\n\"Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot.\"\nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\n\nI don't think A is the answer because a geospatial chart would display the total revenue in each city and would not be able to identify outliers at the store level."
      },
      {
        "date": "2022-09-19T11:55:00.000Z",
        "voteCount": 1,
        "content": "Heat map - \nthe rectangle gives - value (revenue range per city can be identified) , trend/outlier (darkness of the colour)"
      },
      {
        "date": "2022-09-13T19:44:00.000Z",
        "voteCount": 2,
        "content": "my vote for B"
      },
      {
        "date": "2022-07-20T23:17:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2022-12-10T22:13:00.000Z",
        "voteCount": 1,
        "content": "Updated answer =&gt; Option \"C\". \nQuote below from AWS :-\n\"Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to further analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns.\""
      },
      {
        "date": "2022-07-05T14:05:00.000Z",
        "voteCount": 3,
        "content": "I suppose ans should be A.\nYou can create two types of maps in Amazon QuickSight: point maps and filled maps. Point maps show the difference between data values for each location by size. Filled maps show the difference between data values for each location by varying shades of color.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/geospatial-charts.html"
      },
      {
        "date": "2022-06-15T02:13:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A. The questions states that it wants to quickly identify patterns across locations, this is what a geospatial chart is for. I would use colors and circle size to quickly show the patterns."
      },
      {
        "date": "2022-06-27T23:17:00.000Z",
        "voteCount": 2,
        "content": "To add to my earlier suggestion. I've also done AWS-licensed practice exams and similar questions also stated to use geospatial charts. If you see: \"across locations on a map\" then think \"geospatial chart\""
      },
      {
        "date": "2022-05-27T03:40:00.000Z",
        "voteCount": 1,
        "content": "It's B. I.e. line chart. You can display revenue patterns with each line representing each location. A is ruled out as you can't show any pattern using Geospacial. Also, there are multiple locations within each city so this type of chart won't make sense. C and D are ruled out as they can't show any pattern."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/amazon/view/60189-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A marketing company has data in Salesforce, MySQL, and Amazon S3. The company wants to use data from these three locations and create mobile dashboards for its users. The company is unsure how it should create the dashboards and needs a solution with the least possible customization and coding.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena federated queries to join the data sources. Use Amazon QuickSight to generate the mobile dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the mobile dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift federated queries to join the data sources. Use Amazon QuickSight to generate the mobile dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight to connect to the data sources and generate the mobile dashboards.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T10:07:00.000Z",
        "voteCount": 14,
        "content": "I vote for D\nIt haven't mention the requirement of data joining, and it support salesforce as data source\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-salesforce.html"
      },
      {
        "date": "2022-10-27T11:41:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is D as QuickSight supports all the above data sources and can help create the dashboards and needs a solution with the least possible customization and coding. \n\nhttps://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\n\n\nOptions A, B &amp; C are wrong as they would not support all data sources."
      },
      {
        "date": "2023-08-03T18:50:00.000Z",
        "voteCount": 1,
        "content": "Amazon QuickSight announced the launch of Cross Data Source Join, which allows you to connect to multiple data sources and join data across these sources in Amazon QuickSight directly to create data sets used to build dashboards."
      },
      {
        "date": "2023-05-01T12:26:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-02-04T08:38:00.000Z",
        "voteCount": 2,
        "content": "D is right. Least operational overhead"
      },
      {
        "date": "2023-01-19T13:38:00.000Z",
        "voteCount": 1,
        "content": "D is the answer https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html"
      },
      {
        "date": "2022-08-17T02:03:00.000Z",
        "voteCount": 2,
        "content": "Definitely D"
      },
      {
        "date": "2022-07-01T16:18:00.000Z",
        "voteCount": 1,
        "content": "Ans is A: Athena federated query can connect to JDBC compliant data sources along with S3. It can join or union the data to provide single dataset for Quicksight,\nB and C: Requires more effort and coding.\nD: I'm not sure if Quicksight can join or consolidate the results from multiple sources into single dataset"
      },
      {
        "date": "2022-05-30T09:34:00.000Z",
        "voteCount": 3,
        "content": "D: Quicksight connects to all of the mentioned data sources"
      },
      {
        "date": "2022-05-27T03:52:00.000Z",
        "voteCount": 1,
        "content": "D. Quicksight has capability to connect to multiple sources at the same time and generation visualization."
      },
      {
        "date": "2022-04-30T14:17:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2021-12-01T10:31:00.000Z",
        "voteCount": 2,
        "content": "Vote for D. https://aws.amazon.com/blogs/big-data/joining-across-data-sources-on-amazon-quicksight/"
      },
      {
        "date": "2021-11-27T11:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is D, QuickSight support cross-data source joins across all three data sources mentioned in the question."
      },
      {
        "date": "2021-11-19T14:21:00.000Z",
        "voteCount": 2,
        "content": "Ans is \"D\" Quick sight has integration capabilities with all 3"
      },
      {
        "date": "2021-11-10T19:15:00.000Z",
        "voteCount": 1,
        "content": "the answer should be A leveraging Athena's federated query feature. Scroll down (almost) to the the end of this blog to see the screenshot showing which data sources can be connected via Athena"
      },
      {
        "date": "2021-11-05T03:46:00.000Z",
        "voteCount": 3,
        "content": "\"the least possible customization and coding\" - for me this points to option D"
      },
      {
        "date": "2021-10-01T19:28:00.000Z",
        "voteCount": 3,
        "content": "Why not D?\nhttps://aws.amazon.com/blogs/big-data/joining-across-data-sources-on-amazon-quicksight/"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/amazon/view/60219-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company uses Amazon Redshift for its data warehousing needs. ETL jobs run every night to load data, apply business rules, and create aggregate tables for reporting. The company's data analysis, data science, and business intelligence teams use the data warehouse during regular business hours. The workload management is set to auto, and separate queues exist for each team with the priority set to NORMAL.<br>Recently, a sudden spike of read queries from the data analysis team has occurred at least twice daily, and queries wait in line for cluster resources. The company needs a solution that enables the data analysis team to avoid query queuing without impacting latency and the query times of other teams.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the query priority to HIGHEST for the data analysis queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the data analysis queue to enable concurrency scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a query monitoring rule to add more cluster capacity for the data analysis queue when queries are waiting for resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse workload management query queue hopping to route the query to the next matching queue."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T01:39:00.000Z",
        "voteCount": 11,
        "content": "Answer is B\nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html\nYou manage which queries are sent to the concurrency-scaling cluster by configuring WLM queues. When you turn on concurrency scaling, eligible queries are sent to the concurrency-scaling cluster instead of waiting in a queue."
      },
      {
        "date": "2021-11-06T22:33:00.000Z",
        "voteCount": 9,
        "content": "correct answer is B. (https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html)\nD is wrong because it works only for manual WLM config. (https://docs.aws.amazon.com/redshift/latest/dg/wlm-queue-hopping.html)"
      },
      {
        "date": "2023-05-01T12:27:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-10-27T11:43:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is B as concurrency scaling can help scale the avoiding query queuing without impacting latency and the query times of other teams.\n\nWith the Concurrency Scaling feature, you can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance.When you turn on concurrency scaling, Amazon Redshift automatically adds additional cluster capacity to process an increase in both read and write queries. Users see the most current data, whether the queries run on the main cluster or a concurrency-scaling cluster. You're charged for concurrency-scaling clusters only for the time they're actively running queries.\n\nOption D is wrong as workload management query queue hopping works only with manual WLM config.\n\nOption A is wrong as it does not help avoid query queuing.\n\nOption C is wrong as concurrency scaling does the same automatically."
      },
      {
        "date": "2022-08-06T14:23:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-03-20T04:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-09-25T23:38:00.000Z",
        "voteCount": 1,
        "content": "I think B is right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/amazon/view/60222-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company owns facilities with IoT devices installed across the world. The company is using Amazon Kinesis Data Streams to stream data from the devices to<br>Amazon S3. The company's operations team wants to get insights from the IoT data to monitor data quality at ingestion. The insights need to be derived in near- real time, and the output must be logged to Amazon DynamoDB for further analysis.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Amazon Kinesis Data Analytics to analyze the stream data. Save the output to DynamoDB by using the default output from Kinesis Data Analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Amazon Kinesis Data Analytics to analyze the stream data. Save the output to DynamoDB by using an AWS Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Amazon Kinesis Data Firehose to analyze the stream data by using an AWS Lambda function. Save the output to DynamoDB by using the default output from Kinesis Data Firehose.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Amazon Kinesis Data Firehose to analyze the stream data by using an AWS Lambda function. Save the data to Amazon S3. Then run an AWS Glue job on schedule to ingest the data into DynamoDB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-29T07:51:00.000Z",
        "voteCount": 15,
        "content": "It is B. https://aws.amazon.com/about-aws/whats-new/2017/12/amazon-kinesis-data-analytics-can-now-output-real-time-sql-results-to-aws-lambda/"
      },
      {
        "date": "2022-10-27T11:46:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is B as Kinesis Data Analytics would help generate near-real time and using Lambda function the output can be saved to DynamoDB. Using AWS Lambda as a destination allows you to more easily perform post-processing of your SQL results before sending them to a final destination.Lambda functions can deliver analytic information to a variety of AWS services and other destinations,\n\nOption A is wrong as Kinesis Data Analytics does not support DynamoDB as its default output destination.\n\nOptions C &amp; D are wrong as Kinesis Data Firehose does not support DynamoDB as its default output destination."
      },
      {
        "date": "2023-05-01T12:27:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-01-26T11:39:00.000Z",
        "voteCount": 1,
        "content": "Kinesis data analytic can send output to lambda and lambda send it DynamoDB\n\nhttps://aws.amazon.com/about-aws/whats-new/2017/12/amazon-kinesis-data-analytics-can-now-output-real-time-sql-results-to-aws-lambda/"
      },
      {
        "date": "2022-12-20T15:55:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-11-06T13:50:00.000Z",
        "voteCount": 1,
        "content": "It\u2019s C https://www.teamdatascience.com/post/how-to-write-kinesis-data-stream-to-dynamodb"
      },
      {
        "date": "2022-11-18T11:14:00.000Z",
        "voteCount": 2,
        "content": "If you want to query/analyse streaming data in near real time you need to use KDA rather than lambda."
      },
      {
        "date": "2022-08-06T14:24:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-05-21T05:38:00.000Z",
        "voteCount": 1,
        "content": "I think the Answer should be B"
      },
      {
        "date": "2021-12-28T17:39:00.000Z",
        "voteCount": 2,
        "content": "Why not D ?? \"The insights must be extracted in near-real time\" and KDF is the right candidate for this."
      },
      {
        "date": "2022-02-24T03:08:00.000Z",
        "voteCount": 2,
        "content": "Glue is not near-real time"
      },
      {
        "date": "2022-05-10T01:42:00.000Z",
        "voteCount": 1,
        "content": "KDF can do some data transformation, but if you want to do analysis, DF is not suitable."
      },
      {
        "date": "2021-11-01T18:34:00.000Z",
        "voteCount": 1,
        "content": "I agree with B"
      },
      {
        "date": "2021-10-25T14:36:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      },
      {
        "date": "2021-10-17T00:09:00.000Z",
        "voteCount": 2,
        "content": "I think answer is C. Kinesis analytics can help in performing near real time analysis. Then to storing it to DDB. Kinesis analytics can send data to lambda and lambda can help write to DDB table."
      },
      {
        "date": "2021-10-23T23:36:00.000Z",
        "voteCount": 1,
        "content": "*answer is B sorry for confusion"
      },
      {
        "date": "2021-10-03T02:02:00.000Z",
        "voteCount": 1,
        "content": "why not b ?"
      },
      {
        "date": "2021-09-25T01:35:00.000Z",
        "voteCount": 3,
        "content": "I agree C is answer"
      },
      {
        "date": "2021-12-26T18:29:00.000Z",
        "voteCount": 3,
        "content": "C is ruled out because KDF default output does not support direct write to DynamoDB."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/amazon/view/60220-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has a data lake on AWS that ingests sources of data from multiple business units and uses Amazon Athena for queries. The storage layer is Amazon<br>S3 using the AWS Glue Data Catalog. The company wants to make the data available to its data scientists and business analysts. However, the company first needs to manage data access for Athena based on user roles and responsibilities.<br>What should the company do to apply these access controls with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine security policy-based rules for the users and applications by role in AWS Lake Formation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine security policy-based rules for the users and applications by role in AWS Identity and Access Management (IAM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine security policy-based rules for the tables and columns by role in AWS Glue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine security policy-based rules for the tables and columns by role in AWS Identity and Access Management (IAM)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-25T10:51:00.000Z",
        "voteCount": 15,
        "content": "https://aws.amazon.com/lake-formation/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\n\n\"You can use Lake Formation to centrally define security, governance, and auditing policies in one place, versus doing these tasks per service, and then enforce those policies for your users across their analytics applications. Your policies are consistently implemented, eliminating the need to manually configure them across security services like AWS Identity and Access Management and AWS Key Management Service, storage services like S3, and analytics and machine learning services like Redshift, Athena, and (in beta) EMR for Apache Spark. This reduces the effort in configuring policies across services and provides consistent enforcement and compliance.\""
      },
      {
        "date": "2021-10-25T20:04:00.000Z",
        "voteCount": 1,
        "content": "I meant to put A"
      },
      {
        "date": "2021-09-20T22:49:00.000Z",
        "voteCount": 12,
        "content": "I think A is right answer"
      },
      {
        "date": "2023-05-01T12:28:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-11-08T07:23:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is A as AWS Lake Formation helps define security policy-based rules for the users and applications by role centrally and easily with the LEAST operational overhead."
      },
      {
        "date": "2022-09-19T11:28:00.000Z",
        "voteCount": 1,
        "content": "A is the answer. - Multiple BUs, Data Calatlog. - permission handling. - use LKF"
      },
      {
        "date": "2022-08-23T11:21:00.000Z",
        "voteCount": 2,
        "content": "the answer is B. least operational overhead means \"don't set up anything new\". \nAthena has capability to provide access control via IAM (resource-based, action based)\n\nhttps://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonathena.html"
      },
      {
        "date": "2022-08-06T14:20:00.000Z",
        "voteCount": 2,
        "content": "A is right answer"
      },
      {
        "date": "2022-06-23T02:33:00.000Z",
        "voteCount": 3,
        "content": "As already explained in this topic. The answer should be A. Using Lake Formation it is much easier to assign data access to roles. Glue can also work, but it is harder. Using IAM I don't think you can achieve fine-grained access."
      },
      {
        "date": "2022-05-21T05:35:00.000Z",
        "voteCount": 1,
        "content": "Agree. Answer D is correct"
      },
      {
        "date": "2022-10-21T02:30:00.000Z",
        "voteCount": 4,
        "content": "IAM policy can't do column-level security for athena, only table level."
      },
      {
        "date": "2021-10-24T21:00:00.000Z",
        "voteCount": 1,
        "content": "A is the answer for new data lake builder to maintain access to lake."
      },
      {
        "date": "2021-09-29T04:28:00.000Z",
        "voteCount": 1,
        "content": "Why is it A? Not B?"
      },
      {
        "date": "2021-10-06T00:58:00.000Z",
        "voteCount": 1,
        "content": "A is easier"
      },
      {
        "date": "2021-09-21T06:55:00.000Z",
        "voteCount": 1,
        "content": "I think B is the correct answer."
      },
      {
        "date": "2021-10-18T11:10:00.000Z",
        "voteCount": 2,
        "content": "Changed A is correct."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/amazon/view/60223-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has an encrypted Amazon Redshift cluster. The company recently enabled Amazon Redshift audit logs and needs to ensure that the audit logs are also encrypted at rest. The logs are retained for 1 year. The auditor queries the logs once a month.<br>What is the MOST cost-effective way to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the Amazon S3 bucket where the logs are stored by using AWS Key Management Service (AWS KMS). Copy the data into the Amazon Redshift cluster from Amazon S3 on a daily basis. Query the data as required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable encryption on the Amazon Redshift cluster, configure audit logging, and encrypt the Amazon Redshift cluster. Use Amazon Redshift Spectrum to query the data as required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable default encryption on the Amazon S3 bucket where the logs are stored by using AES-256 encryption. Copy the data into the Amazon Redshift cluster from Amazon S3 on a daily basis. Query the data as required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable default encryption on the Amazon S3 bucket where the logs are stored by using AES-256 encryption. Use Amazon Redshift Spectrum to query the data as required.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T23:54:00.000Z",
        "voteCount": 26,
        "content": "I think D is right answer"
      },
      {
        "date": "2022-02-09T09:09:00.000Z",
        "voteCount": 6,
        "content": "A business owns an Amazon Redshift cluster that is encrypted. The organization just enabled audit logs in Amazon Redshift and wants to guarantee that audit logs are likewise encrypted at rest. The logs are kept for one year. The auditor conducts a monthly audit of the logs.\n\nHow might these needs be met in the MOST cost-effective manner possible?\n\nD is the best answer.  We want to have our logs in S3 and be able to query them. Using the S3 encryption is enough for our security requirements. Now, the logs are audited once a month, meaning we need to extract meaningful information from them. We already have a Redshift cluster, so using spectrum is a bliss for this task."
      },
      {
        "date": "2023-05-01T12:29:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-12-26T09:28:00.000Z",
        "voteCount": 2,
        "content": "A can't be correct because of the following: \"Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.\"\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html#db-auditing-logs\n\nHence, D is the right answer."
      },
      {
        "date": "2022-10-27T11:52:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D as S3 default encryption helps data at rest encryption. As the logs are queried once a month, it would be cost-effective to store the data in S3 and have it queried using Redshift Spectrum. \n\nOptions A &amp; C are wrong as loading the data in Redshift would result in the most cost-effective solution.\n\nOption B is wrong as Redshift Audit logs are stored in S3."
      },
      {
        "date": "2022-10-27T11:52:00.000Z",
        "voteCount": 1,
        "content": "Amazon Redshift logs information about connections and user activities in your database. These logs help you to monitor the database for security and troubleshooting purposes, a process called database auditing. The logs are stored in Amazon S3 buckets. These provide convenient access with data-security features for users who are responsible for monitoring activities in the database."
      },
      {
        "date": "2022-10-27T11:52:00.000Z",
        "voteCount": 1,
        "content": "Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to execute very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster."
      },
      {
        "date": "2022-06-28T16:13:00.000Z",
        "voteCount": 2,
        "content": "Option C. AES 256 - Free, S3 select can query the data directly. Instead KMS additional cost and Spectrum needs cluster, costly."
      },
      {
        "date": "2022-06-20T03:19:00.000Z",
        "voteCount": 1,
        "content": "I doubt that D is the most cost effective answer since it involves expensive \"Amazon Redshift Spectrum\". Guys this is an additional cost as given in \"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html\". Its original purpose is to query exabytes of data resides in S3 without loading to Redshift. But copying an audit log will be of no cost compared to that, coz its small in size. So I guess it should be A. We can use copy command to load data and its capable of automatically decrypting data in S3 while copying."
      },
      {
        "date": "2022-06-30T01:17:00.000Z",
        "voteCount": 1,
        "content": "Agree to Sen5476. KMS is additional cost so I can go with C"
      },
      {
        "date": "2022-06-07T00:35:00.000Z",
        "voteCount": 3,
        "content": "It is D! AWS KMS is more expensive then using the default SSE-S3 (using AES-256). Loading the logs into Redshift Cluster is more expensive then querying the logs via Redshift Spectrum."
      },
      {
        "date": "2022-04-27T21:25:00.000Z",
        "voteCount": 3,
        "content": "D. As  CHRIS12722222 commented:\n\"Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.\""
      },
      {
        "date": "2022-04-06T02:11:00.000Z",
        "voteCount": 1,
        "content": "AES-256 not supported"
      },
      {
        "date": "2022-04-22T03:32:00.000Z",
        "voteCount": 6,
        "content": "This statement is wrong. \n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\n\"Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.\""
      },
      {
        "date": "2022-03-28T18:26:00.000Z",
        "voteCount": 1,
        "content": "No need to COPY the data into Redshift. You can use Redshift Spectrum to query the data in S3 since it is used monthly only."
      },
      {
        "date": "2022-03-17T17:55:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2021-12-14T09:03:00.000Z",
        "voteCount": 3,
        "content": "D\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html"
      },
      {
        "date": "2021-11-17T13:20:00.000Z",
        "voteCount": 1,
        "content": "ans is D"
      },
      {
        "date": "2021-10-29T15:35:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer as auditor is only looking to query data once a month. Data can stay on s3."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/amazon/view/60221-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analytics specialist is setting up workload management in manual mode for an Amazon Redshift environment. The data analytics specialist is defining query monitoring rules to manage system performance and user experience of an Amazon Redshift cluster.<br>Which elements must each query monitoring rule include?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA unique rule name, a query runtime condition, and an AWS Lambda function to resubmit any failed queries in off hours",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA queue name, a unique rule name, and a predicate-based stop condition",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA unique rule name, one to three predicates, and an action\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA workload name, a unique rule name, and a query runtime-based condition"
    ],
    "answer": "C",
    "answerDescription": "Reference:<br>https://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-06T09:35:00.000Z",
        "voteCount": 10,
        "content": "C is the answer; textbook question\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html"
      },
      {
        "date": "2023-05-01T12:30:00.000Z",
        "voteCount": 3,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-10-27T11:58:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as a query monitoring rule includes a unique rule name, one to three predicates, and an action. \nTo define a query monitoring rule, you specify the following elements: \n1-A rule name \n2-One or more predicates \n3- An action \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html"
      },
      {
        "date": "2022-07-26T19:30:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2021-12-03T13:42:00.000Z",
        "voteCount": 4,
        "content": "C is correct as per:\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html\nEach rule includes up to three conditions, or predicates, and one action."
      },
      {
        "date": "2021-10-14T19:28:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer."
      },
      {
        "date": "2021-10-13T18:17:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html"
      },
      {
        "date": "2021-10-07T10:31:00.000Z",
        "voteCount": 1,
        "content": "help me.. what is answer?"
      },
      {
        "date": "2021-10-04T23:43:00.000Z",
        "voteCount": 2,
        "content": "I agree C is answer"
      },
      {
        "date": "2021-09-30T13:38:00.000Z",
        "voteCount": 1,
        "content": "I think A is right answer"
      },
      {
        "date": "2021-10-05T21:02:00.000Z",
        "voteCount": 1,
        "content": "sorry mistake"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/amazon/view/60224-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A market data company aggregates external data sources to create a detailed view of product consumption in different countries. The company wants to sell this data to external parties through a subscription. To achieve this goal, the company needs to make its data securely available to external parties who are also AWS users.<br>What should the company do to meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Amazon S3. Share the data by using presigned URLs for security.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Amazon S3. Share the data by using S3 bucket ACLs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to AWS Data Exchange for storage. Share the data by using presigned URLs for security.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to AWS Data Exchange for storage. Share the data by using the AWS Data Exchange sharing wizard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T00:08:00.000Z",
        "voteCount": 7,
        "content": "Between C or D, I don't think can generate pre-signed URL from Data Exchange...\nI'll choose D, however there're no such thing called AWS Data Exchange sharing wizard\nPerhaps this question is outdated\nHere's the standard flow of Providing data products on AWS Data Exchange\nhttps://docs.aws.amazon.com/es_es/data-exchange/latest/userguide/providing-data-sets.html"
      },
      {
        "date": "2021-12-04T10:32:00.000Z",
        "voteCount": 3,
        "content": "Pressigned URL can be generated https://boto3.amazonaws.com/v1/documentation/api/1.18.43/reference/services/dataexchange.html#DataExchange.Client.generate_presigned_url"
      },
      {
        "date": "2021-12-21T20:14:00.000Z",
        "voteCount": 1,
        "content": "Presigned URL can be generated, the assets are not shared using that. it works using subscription model"
      },
      {
        "date": "2022-10-27T12:00:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is D as AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud with least operational overhead. \n\nOption A is wrong as S3 Presigned URLs provide access to anyone who has access to the URL. As the customers are already AWS users, the access can be provided securely through Data exchange. \n\nOption B is wrong as the only recommended use case for the bucket ACL is to grant write permission to the S3 Log Delivery group to write access log objects to the bucket. \n\nOption C is wrong as AWS Data Exchange does not support resigned URLs."
      },
      {
        "date": "2022-10-27T12:00:00.000Z",
        "voteCount": 1,
        "content": "AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. Qualified data providers include category-leading brands such as Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages; Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually; Dun &amp; Bradstreet, who maintain a database of more than 330 million global business records; and Foursquare, whose location data is derived from 220 million unique consumers and includes more than 60 million global commercial venues."
      },
      {
        "date": "2022-10-27T12:00:00.000Z",
        "voteCount": 2,
        "content": "AWS Data Exchange now gives customers an easy way to set up export jobs upon subscribing to products. Instead of navigating to separate screens, subscribers can use the AWS Data Exchange console to configure export jobs that will begin automatically after their subscription is completed. This functionality reduces friction and time to value for customers. For subscribers just getting started on AWS Data Exchange, setting up data exports to Amazon S3 is a critical first step towards downstream analysis in a variety of AWS services."
      },
      {
        "date": "2022-10-27T12:00:00.000Z",
        "voteCount": 1,
        "content": "AWS Data Exchange now gives customers an easy way to set up export jobs upon subscribing to products. Instead of navigating to separate screens, subscribers can use the AWS Data Exchange console to configure export jobs that will begin automatically after their subscription is completed. This functionality reduces friction and time to value for customers. For subscribers just getting started on AWS Data Exchange, setting up data exports to Amazon S3 is a critical first step towards downstream analysis in a variety of AWS services."
      },
      {
        "date": "2023-05-01T12:30:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-11-30T02:10:00.000Z",
        "voteCount": 1,
        "content": "i think its A"
      },
      {
        "date": "2022-07-27T23:17:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-05-22T02:16:00.000Z",
        "voteCount": 2,
        "content": "... based on a subscription model ... -&gt; Must be Data Exchange Service -&gt; ... also AWS Customer...  -&gt;  no need for a presigned url &gt; so should be D"
      },
      {
        "date": "2022-03-19T18:01:00.000Z",
        "voteCount": 2,
        "content": "I think Answer is D."
      },
      {
        "date": "2022-02-26T09:00:00.000Z",
        "voteCount": 1,
        "content": "C: But I think it's called the \"Publish new product wizard\". See step 5 in https://docs.aws.amazon.com/data-exchange/latest/userguide/publishing-products.html#publish-products"
      },
      {
        "date": "2022-02-26T09:04:00.000Z",
        "voteCount": 1,
        "content": "I mean D"
      },
      {
        "date": "2021-12-24T17:05:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html"
      },
      {
        "date": "2021-12-25T20:05:00.000Z",
        "voteCount": 1,
        "content": "Change to D\nPreSignedURL for S3, so it can be A\nBut the question mentions the least operational overhead, so it's D. There is AWS Data Exchange post-subscription wizard, maybe it's the sharing wizard in the answer D\nhttps://aws.amazon.com/about-aws/whats-new/2021/04/aws-data-exchange-launches-post-subscription-wizard-to-configure-revision-exports-to-amazon-s3/"
      },
      {
        "date": "2021-12-17T10:58:00.000Z",
        "voteCount": 1,
        "content": "i guess both C and D will work but question says least operational  so that i chose D since C need code changes to generate preassigned url for data exchange. pls correct me if i am wrong ."
      },
      {
        "date": "2021-12-13T18:09:00.000Z",
        "voteCount": 1,
        "content": "I bend towards D\nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/publishing-products.html\nTo create a data set product on Data Exchange: \nOpen your web browser and go to the AWS Data Exchange console.\nOn the left side navigation pane, under Publish data, choose Owned data sets.\nIn Owned data sets, choose Create data set to open the Data set creation steps wizard***."
      },
      {
        "date": "2021-12-06T01:07:00.000Z",
        "voteCount": 4,
        "content": "I will go with A\nData Exchange doesn't support pre-signed URL, neither there is any wizard to share data.\nThis is the way to share data using Data exchange \nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/publishing-products.html#publish-data-product"
      },
      {
        "date": "2021-11-29T21:29:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      },
      {
        "date": "2021-12-06T10:28:00.000Z",
        "voteCount": 1,
        "content": "why you say that?"
      },
      {
        "date": "2021-11-04T14:16:00.000Z",
        "voteCount": 1,
        "content": "In the blog for exchange, they do talk about some wizard but not really some exchange wizard feature. I would still go with D. Although, presigned URLs sounds like a right way to share data with external aws account securely specially when you're selling content. Answer is actually C or D, need more clarifications."
      },
      {
        "date": "2021-09-22T11:40:00.000Z",
        "voteCount": 1,
        "content": "I think C is right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/amazon/view/66163-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has a marketing department and a finance department. The departments are storing data in Amazon S3 in their own AWS accounts in AWS<br>Organizations. Both departments use AWS Lake Formation to catalog and secure their data. The departments have some databases and tables that share common names.<br>The marketing department needs to securely access some tables from the finance department.<br>Which two steps are required for this process? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe finance department grants Lake Formation permissions for the tables to the external account for the marketing department.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe finance department creates cross-account IAM permissions to the table for the marketing department role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe marketing department creates an IAM role that has permissions to the Lake Formation tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-27T04:51:00.000Z",
        "voteCount": 7,
        "content": "Why are there only 3 options?\nI think the answer is A and C."
      },
      {
        "date": "2022-02-12T12:02:00.000Z",
        "voteCount": 5,
        "content": "Answer is A &amp; B"
      },
      {
        "date": "2022-03-26T07:19:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A &amp; C. Read links below\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/cross-account-permissions.html\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/regranting-locations.html"
      },
      {
        "date": "2023-07-03T08:05:00.000Z",
        "voteCount": 3,
        "content": "Option B is not required for this process. Cross-account IAM permissions are not directly used for granting access to Lake Formation tables. Instead, Lake Formation permissions are managed within the Lake Formation service itself, and access to the tables is granted through IAM roles and permissions configured within the respective AWS accounts.\n\nTo summarize, the finance department grants Lake Formation permissions to the marketing department's account, and the marketing department creates an IAM role with the necessary permissions to access the shared tables. These two steps enable secure access to the required tables between the departments."
      },
      {
        "date": "2023-05-01T12:31:00.000Z",
        "voteCount": 4,
        "content": "AC: I passed the test"
      },
      {
        "date": "2022-08-16T13:48:00.000Z",
        "voteCount": 1,
        "content": "I think A and B. Why is the marketing department the one creating the IAM role"
      },
      {
        "date": "2022-07-27T23:35:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: AC"
      },
      {
        "date": "2022-07-27T23:35:00.000Z",
        "voteCount": 2,
        "content": "There are usually 5 options given in a question."
      },
      {
        "date": "2022-05-22T02:20:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C - why was already explained"
      },
      {
        "date": "2021-11-16T11:13:00.000Z",
        "voteCount": 2,
        "content": "B , C  ?"
      },
      {
        "date": "2021-12-04T19:18:00.000Z",
        "voteCount": 9,
        "content": "I disagree. Answer is A &amp; B. \nhttps://docs.aws.amazon.com/lake-formation/latest/dg/lake-formation-permissions.html\n...Lake Formation uses a combination of Lake Formation permissions and IAM permissions. The IAM permissions model consists of IAM policies."
      },
      {
        "date": "2021-12-21T20:33:00.000Z",
        "voteCount": 5,
        "content": "Agree A&amp;B\nFor further reading please see \nhttps://docs.aws.amazon.com/lake-formation/latest/dg/sharing-catalog-resources.html"
      },
      {
        "date": "2022-02-17T20:26:00.000Z",
        "voteCount": 13,
        "content": "By this link, it has to be A &amp; C \nA: finance department grants Lake Formation permissions for the tables to the external account for the marketing department.\n[aws link: You can share Data Catalog resources (databases and tables) with external AWS accounts by granting Lake Formation permissions on the resources to the external accounts]\nC: The marketing department creates an IAM role that has permissions to the Lake Formation tables\n[aws link: When you share a resource with an AWS organization, you're sharing the resource with all accounts at all levels in that organization. The data lake administrator in each external account must then grant permissions on the shared resources to principals in their account.]\nA would share the access to lake formation to the marketing account, &amp; the administrator from marketing account creates a role with permission to lake formation table ( role can be assigned to any principal )"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/amazon/view/65691-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A human resources company maintains a 10-node Amazon Redshift cluster to run analytics queries on the company's data. The Amazon Redshift cluster contains a product table and a transactions table, and both tables have a product_sku column. The tables are over 100 GB in size. The majority of queries run on both tables.<br>Which distribution style should the company use for the two tables to achieve optimal query performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn EVEN distribution style for both tables",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA KEY distribution style for both tables\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn ALL distribution style for the product table and an EVEN distribution style for the transactions table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn EVEN distribution style for the product table and an KEY distribution style for the transactions table"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-02T07:24:00.000Z",
        "voteCount": 16,
        "content": "B - KEY - both tables are huge and have common key . \nALL-distribution style for the product table not correct because of size\nEVEN distribution style for the product table  - may not necessarily help."
      },
      {
        "date": "2022-03-23T17:14:00.000Z",
        "voteCount": 3,
        "content": "ref: https://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html"
      },
      {
        "date": "2021-11-08T22:28:00.000Z",
        "voteCount": 5,
        "content": "Option B - As both tables have a common key and are used widely in reports.https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html"
      },
      {
        "date": "2023-05-01T12:32:00.000Z",
        "voteCount": 3,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-27T18:31:00.000Z",
        "voteCount": 4,
        "content": "B. A KEY distribution style for both tables would be the best option for optimal query performance.\n\nExplanation:\nWhen using a KEY distribution style, Redshift distributes the rows according to the values in one column that serves as the distribution key. If the product_sku column is used as the distribution key for both the product table and the transactions table, rows with the same product_sku value will be stored on the same node. This will reduce the amount of data that needs to be moved between nodes during a query, resulting in faster query performance."
      },
      {
        "date": "2023-01-10T12:12:00.000Z",
        "voteCount": 2,
        "content": "Even Walmart product table might not reach 100G size...I think the question is not well-stated"
      },
      {
        "date": "2022-11-17T01:32:00.000Z",
        "voteCount": 2,
        "content": "This question is very tricky. Usually product table is a dimension table and it should be small.\nBut here \"The tables are over 100GB in size\"\nSo B."
      },
      {
        "date": "2022-10-27T12:12:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as the key requirement is to have optimal query performance, it would be better to use ALL distribution style for the product dimension table and EVEN distribution style for the transactions fact table. \n\nOptions A &amp; D are wrong as they would not provide optimal query performance.\n\nOption B is wrong as the KEY distribution style for both tables can be used but it won't provide an optimal query performance."
      },
      {
        "date": "2022-10-27T12:12:00.000Z",
        "voteCount": 1,
        "content": "Distribute the fact table and its largest dimension table on their common columns. Choose the largest dimension based on the size of dataset that participates in the most common join, not just the size of the table. If a table is commonly filtered, using a WHERE clause, only a portion of its rows participate in the join. Such a table has less impact on redistribution than a smaller table that contributes more data. Designate both the dimension table's primary key and the fact table's corresponding foreign key as DISTKEY. If multiple tables use the same distribution key, they are also collocated with the fact table. Your fact table can have only one distribution key. Any tables that join on another key isn't collocated with the fact table."
      },
      {
        "date": "2022-12-05T16:14:00.000Z",
        "voteCount": 1,
        "content": "Agree with everything you said but the 100GB size in both tables makes it better to distribute it by Key."
      },
      {
        "date": "2022-08-08T05:48:00.000Z",
        "voteCount": 1,
        "content": "Option B - As both tables have a common key and are used widely in reports."
      },
      {
        "date": "2022-07-19T21:13:00.000Z",
        "voteCount": 1,
        "content": "Option B - As both tables have a common key and are used widely in reports."
      },
      {
        "date": "2022-07-01T21:46:00.000Z",
        "voteCount": 1,
        "content": "The question doesn't tell what type of queries are common. Key distribution is good if there are joins. All-style can be ruled out due to the size concerns. So in this case I would use Even distribution style. There is no evidence to tell that Product column is the most suitable colum for partitioning."
      },
      {
        "date": "2022-06-03T04:29:00.000Z",
        "voteCount": 1,
        "content": "I also think answer B. But the question is unclear in my opinion and it could be C. \nWhat pushes me to B is the fact that it is not mentioned explicitly that the product table is small or can be considered a Fact-Table. It is explicitly mentioned that both tables share a common field."
      },
      {
        "date": "2022-05-27T21:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-04-27T01:26:00.000Z",
        "voteCount": 1,
        "content": "B - KEY - both tables are huge and have common key ."
      },
      {
        "date": "2022-04-19T08:21:00.000Z",
        "voteCount": 2,
        "content": "Choose B based on lakeswimmer  's comment."
      },
      {
        "date": "2022-04-06T01:39:00.000Z",
        "voteCount": 1,
        "content": "B is correct because there is common column between table to join and query"
      },
      {
        "date": "2022-03-26T12:42:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html\n\nDistribute the fact table and one dimension table on their common columns.\n\nYour fact table can have only one distribution key. Any tables that join on another key aren't collocated with the fact table. Choose one dimension to collocate based on how frequently it is joined and the size of the joining rows. Designate both the dimension table's primary key and the fact table's corresponding foreign key as the DISTKEY."
      },
      {
        "date": "2022-03-26T12:39:00.000Z",
        "voteCount": 2,
        "content": "The question is not clear enough.\nIt really depends on the queries run and the size of EACH table.\n\nI would answer B assuming that when you query for a product, you might want to see all transactions of this product. All product is a dimension, it could still be a really large table to have an ALL distribution. Maybe if both tables are partitioned by the sku, performance would be better."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/amazon/view/66044-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company receives data from its vendor in JSON format with a timestamp in the file name. The vendor uploads the data to an Amazon S3 bucket, and the data is registered into the company's data lake for analysis and reporting. The company has configured an S3 Lifecycle policy to archive all files to S3 Glacier after 5 days.<br>The company wants to ensure that its AWS Glue crawler catalogs data only from S3 Standard storage and ignores the archived files. A data analytics specialist must implement a solution to achieve this goal without changing the current S3 bucket configuration.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the exclude patterns feature of AWS Glue to identify the S3 Glacier files for the crawler to exclude.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule an automation job that uses AWS Lambda to move files from the original S3 bucket to a new S3 bucket for S3 Glacier storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the excludeStorageClasses property in the AWS Glue Data Catalog table to exclude files on S3 Glacier storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the include patterns feature of AWS Glue to identify the S3 Standard files for the crawler to include."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-14T12:14:00.000Z",
        "voteCount": 12,
        "content": "C - https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"
      },
      {
        "date": "2022-10-27T12:17:00.000Z",
        "voteCount": 5,
        "content": "C is the right answer\nTo exclude Amazon S3 storage classes while creating a dynamic frame, use excludeStorageClasses in additionalOptions. AWS Glue automatically uses its own Amazon S3 Lister implementation to list and exclude files corresponding to the specified storage classes.\nglueContext.create_dynamic_frame.from_catalog(\n    database = \"my_database\",\n    tableName = \"my_table_name\",\n    redshift_tmp_dir = \"\",\n    transformation_ctx = \"my_transformation_context\",\n    additional_options = {\n        \"excludeStorageClasses\" : [\"GLACIER\", \"DEEP_ARCHIVE\"]\n    }\n)\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"
      },
      {
        "date": "2023-05-01T12:33:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-12-13T19:51:00.000Z",
        "voteCount": 1,
        "content": "Selected C"
      },
      {
        "date": "2022-10-31T13:40:00.000Z",
        "voteCount": 1,
        "content": "Selected C"
      },
      {
        "date": "2022-08-23T12:04:00.000Z",
        "voteCount": 1,
        "content": "C \n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"
      },
      {
        "date": "2022-08-21T23:49:00.000Z",
        "voteCount": 1,
        "content": "C please \nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html#aws-glue-programming-etl-storage-classes-table"
      },
      {
        "date": "2022-08-18T03:46:00.000Z",
        "voteCount": 1,
        "content": "due to https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"
      },
      {
        "date": "2022-08-08T22:48:00.000Z",
        "voteCount": 1,
        "content": "Go with C - due to https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html\nWhy A wrong? - Exclude patterns can be used to ignore the file-paths/folders which are already been crawled before(or files from which data is not required). Since this reduces the number of files that crawlers needs to list everytime, crawler runtime is also reduced accordingly"
      },
      {
        "date": "2022-08-05T05:58:00.000Z",
        "voteCount": 1,
        "content": "A - WRONG, exclude patterns are configured at the crawler level with static basic patterns, they can't be parametrized with the current date. To accomplish this, you would need to re-create the same crawler every day with a new pattern.\n\nB - CORRECT, the only available way to exclude objects to be crawled is to physically moving them to a separate prefix or bucket.\n\nC - WRONG, excludeStorageClasses applies to a Glue ETL job that reads the table, but the table has already been crawled.\n\nD - WRONG, exclude patterns are configured at the crawler level with static basic patterns, they can't be parametrized with the current date. To accomplish this, you would need to re-create the same crawler every day with a new pattern."
      },
      {
        "date": "2022-08-03T05:46:00.000Z",
        "voteCount": 2,
        "content": "Answer is A \nIt allows you to \"exclude pattern\" only.\nFor Include, it is talking about \"include path\". \nhttps://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude"
      },
      {
        "date": "2022-07-22T04:52:00.000Z",
        "voteCount": 2,
        "content": "Guys Please READ. The question is AIMING to CRAWL data not PROCESS DATA with an ETL Process. The correct answer is \"A\". If you want to exclude certain files in your ETL process you might be using the \"C\" answer."
      },
      {
        "date": "2023-11-21T01:47:00.000Z",
        "voteCount": 1,
        "content": "Answer C:\nThe crawler can access data stores directly as the source of the crawl, or it can use existing tables in the Data Catalog as the source. If the crawler uses existing catalog tables, it crawls the data stores that are specified by those catalog tables.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/define-crawler.html\n\nYou can specify storage class exclusions to be used by an AWS Glue ETL job as a table parameter in the AWS Glue Data Catalog. You can include this parameter in the CreateTable operation using the AWS Command Line Interface (AWS CLI) or programmatically using the API. For more information, see Table Structure and CreateTable. \n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"
      },
      {
        "date": "2022-07-16T01:48:00.000Z",
        "voteCount": 2,
        "content": "The link provided talks about \"You can specify storage class exclusions to be used by an ***AWS Glue ETL job ****as a table parameter in the AWS Glue Data Catalog\".\nThe question is talking about how to make the ***Glue CRAWLER*** avoid files in those classes. The fact that the Q mentions a timestamp in the file name suggests there is a filename pattern based timestamp which can be used an include pattern in the crawler config. So shouldn't it be D?"
      },
      {
        "date": "2023-01-27T07:40:00.000Z",
        "voteCount": 1,
        "content": "@MBP911, We can also add exclude storage class option in Glue catalog table. So C makes sense.\nhttps://docs.amazonaws.cn/en_us/glue/latest/dg/aws-glue-programming-etl-storage-classes.html#aws-glue-programming-etl-storage-classes-table"
      },
      {
        "date": "2021-12-05T14:52:00.000Z",
        "voteCount": 5,
        "content": "Answer is C\nOption A is wrong. #Exclude patterns: These enable you to exclude certain files or tables from the crawl."
      },
      {
        "date": "2021-12-21T21:29:00.000Z",
        "voteCount": 4,
        "content": "Agree C\nfor further reading see this \nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/amazon/view/65984-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company analyzes historical data and needs to query data that is stored in Amazon S3. New data is generated daily as .csv files that are stored in Amazon S3.<br>The company's analysts are using Amazon Athena to perform SQL queries against a recent subset of the overall data. The amount of data that is ingested into<br>Amazon S3 has increased substantially over time, and the query latency also has increased.<br>Which solutions could the company implement to improve query performance? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MySQL Workbench on an Amazon EC2 instance, and connect to Athena by using a JDBC or ODBC connector. Run the query from MySQL Workbench instead of Athena directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Athena to extract the data and store it in Apache Parquet format on a daily basis. Query the extracted data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a daily AWS Glue ETL job to convert the data files to Apache Parquet and to partition the converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a daily AWS Glue ETL job to compress the data files by using the .gzip format. Query the compressed data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a daily AWS Glue ETL job to compress the data files by using the .lzo format. Query the compressed data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T00:15:00.000Z",
        "voteCount": 13,
        "content": "lzo format focuses on high compression &amp; decompression speed. So C &amp; E"
      },
      {
        "date": "2024-03-24T12:44:00.000Z",
        "voteCount": 1,
        "content": "The bzip2 and LZO compression formats are splittable, but are not recommended if you want performance and compatibility."
      },
      {
        "date": "2022-01-30T20:12:00.000Z",
        "voteCount": 12,
        "content": "C &amp; D . As per the link https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/ preferred compression is Gzip . \nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable. When they are not an option, then try BZip2 or Gzip with an optimal file size."
      },
      {
        "date": "2022-08-03T22:31:00.000Z",
        "voteCount": 1,
        "content": "Agree with C &amp; D. Please refer - https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable"
      },
      {
        "date": "2023-11-18T12:48:00.000Z",
        "voteCount": 1,
        "content": "C &amp; E are correct answers"
      },
      {
        "date": "2023-11-10T05:05:00.000Z",
        "voteCount": 1,
        "content": "Generrally, high compression equal low speed decompression.\ngzip is more high compression than lzo.\nso I choose E.\nC and E are correct I think."
      },
      {
        "date": "2023-09-19T12:52:00.000Z",
        "voteCount": 1,
        "content": "C AND D ARE THE CORRECT ANSWERS"
      },
      {
        "date": "2023-09-13T20:34:00.000Z",
        "voteCount": 1,
        "content": "Seems no one mentioned that LZO is splitable on text file while gzip is not. When we use parquet format, we intend to utilize its features that it is splitable, compressible. CSV are file is one example of the text file. So it seems LZO should be chosen over Gzip. Refer to  https://aws.amazon.com/cn/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2023-09-13T20:35:00.000Z",
        "voteCount": 1,
        "content": "My bad, I meant CE"
      },
      {
        "date": "2023-08-28T13:11:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D is the correct Answer for me"
      },
      {
        "date": "2023-08-03T19:36:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/building-a-performance-efficient-data-pipeline.html"
      },
      {
        "date": "2023-07-03T15:17:00.000Z",
        "voteCount": 1,
        "content": "D and E would not convert the data to a column format, so I am not sure why and how it would help improve the latency, therefore only viable options are B (use Athena to unload parquet files), and C (use glue to convert to parquet and partition the data). \n\nAlso the questions asks to choose two solutions."
      },
      {
        "date": "2023-05-01T12:34:00.000Z",
        "voteCount": 1,
        "content": "CE: I passed the test"
      },
      {
        "date": "2023-03-27T19:11:00.000Z",
        "voteCount": 2,
        "content": "GZip is often a good choice for cold data, which is accessed infrequently. Snappy or LZO are a better choice for hot data, which is accessed frequently."
      },
      {
        "date": "2022-12-13T19:57:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C and E."
      },
      {
        "date": "2022-10-27T12:30:00.000Z",
        "voteCount": 6,
        "content": "Correct answers are C &amp; D as AWS recommends using either Parquet or ORC columnar data stores and BZips or Gzip compression. \nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable. When they are not an option, then try BZip2 or Gzip with an optimal file size.\nOption A is wrong as MySQL Workbench does not improve query performance. It instead increases operational overhead.\n\nOption B is wrong as Athena is not ideal for running batch jobs. Use Glue instead.\n\nOption E is wrong as although .lzo is supported and can provide better compression and decompression speeds, Gzip is recommended for Athena as per the AWS documentation."
      },
      {
        "date": "2022-10-18T15:23:00.000Z",
        "voteCount": 1,
        "content": "E over D:\nhttps://docs.aws.amazon.com/athena/latest/ug/compression-formats.html\nIt is stated that Lzo compression format has less compression than Gzip, but this makes decompression much faster, which will result in improved query performance and since that's what's needed my answer is E. There is no cost concern in the question so it doesn't matter if files stored in s3 are bigger with Lzo compression."
      },
      {
        "date": "2022-09-14T21:19:00.000Z",
        "voteCount": 1,
        "content": "cd as exp[lained by others"
      },
      {
        "date": "2022-05-22T01:40:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B &amp; C"
      },
      {
        "date": "2022-05-17T05:10:00.000Z",
        "voteCount": 1,
        "content": "GZIP is default compression format for Parquet"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/amazon/view/65752-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is sending historical datasets to Amazon S3 for storage. A data engineer at the company wants to make these datasets available for analysis using<br>Amazon Athena. The engineer also wants to encrypt the Athena query results in an S3 results location by using AWS solutions for encryption. The requirements for encrypting the query results are as follows:<br>\u2711 Use custom keys for encryption of the primary dataset query results.<br>\u2711 Use generic encryption for all other query results.<br>\u2711 Provide an audit trail for the primary dataset queries that shows when the keys were used and by whom.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with S3 managed encryption keys (SSE-S3) for the primary dataset. Use SSE-S3 for the other datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with customer-provided encryption keys (SSE-C) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with AWS KMS managed customer master keys (SSE-KMS CMKs) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse client-side encryption with AWS Key Management Service (AWS KMS) customer managed keys for the primary dataset. Use S3 client-side encryption with client-side keys for the other datasets."
    ],
    "answer": "A",
    "answerDescription": "Reference:<br>https://d1.awsstatic.com/product-marketing/S3/Amazon_S3_Security_eBook_2020.pdf",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-29T15:40:00.000Z",
        "voteCount": 16,
        "content": "Its C - https://docs.aws.amazon.com/athena/latest/ug/encrypting-query-results-stored-in-s3.html\n\nAthena query result location supports CSE-KMS, SSE-KMS or SSE-S3 only not customer provided encryption keys"
      },
      {
        "date": "2023-11-10T05:36:00.000Z",
        "voteCount": 1,
        "content": "Its C - https://docs.aws.amazon.com/athena/latest/ug/encrypting-query-results-stored-in-s3.html\n\nAthena query result location supports CSE-KMS, SSE-KMS or SSE-S3 only not customer provided encryption keys"
      },
      {
        "date": "2023-05-01T12:34:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-02-14T14:19:00.000Z",
        "voteCount": 1,
        "content": "Use server-side encryption with AWS KMS managed customer master keys (SSE-KMS CMKs) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets.\n\nUsing SSE-KMS CMKs will allow the data engineer to use custom keys for encrypting the query results for the primary dataset, and AWS KMS provides a way to audit key usage through CloudTrail logs. Using SSE-S3 for other datasets will provide generic encryption. Server-side encryption is preferred over client-side encryption as it reduces the complexity and overhead of key management, and it enables other S3 features such as lifecycle policies, cross-region replication, and so on."
      },
      {
        "date": "2022-10-27T12:32:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C as AWS KMS managed customer master keys (SSE-KMS CMKs) can be used for encryption primary dataset as it would provide auditing as well. SSE-S3 is fine for other datasets. \nOption A is wrong as SSE-S3 should not be used for the primary dataset as it does not provide custom keys. \n\nOption B is wrong as Athena does not support server-side encryption with customer-provided encryption keys (SSE-C). \n\nOption D is wrong as generic encryption is fine for other datasets, so SSE-S3 should be fine."
      },
      {
        "date": "2022-08-09T17:54:00.000Z",
        "voteCount": 1,
        "content": "Answer - C"
      },
      {
        "date": "2022-08-06T14:36:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2021-11-14T14:57:00.000Z",
        "voteCount": 1,
        "content": "I reckon it should be B. We need custom keys so that should be SSE-C not KMS CMK. SSE-C events should be logged in cloudtrail for auditing purposes."
      },
      {
        "date": "2021-11-17T19:22:00.000Z",
        "voteCount": 1,
        "content": "Can't use customer-provided encryption keys (SSE-C) for Ahena"
      },
      {
        "date": "2021-11-10T02:31:00.000Z",
        "voteCount": 4,
        "content": "Answer is C &gt; Use server-side encryption with AWS KMS managed customer master keys (SSE-KMS CMKs) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets...Auditing of the encryptions are one of the requirement for primary data sets ,hence KMS for primary and SSE-S3 for rest"
      },
      {
        "date": "2021-11-13T21:15:00.000Z",
        "voteCount": 1,
        "content": "Agree - Answer C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/amazon/view/64637-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A large telecommunications company is planning to set up a data catalog and metadata management for multiple data sources running on AWS. The catalog will be used to maintain the metadata of all the objects stored in the data stores. The data stores are composed of structured sources like Amazon RDS and Amazon<br>Redshift, and semistructured sources like JSON and XML files stored in Amazon S3. The catalog must be updated on a regular basis, be able to detect the changes to object metadata, and require the least possible administration.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora as the data catalog. Create AWS Lambda functions that will connect and gather the metadata information from multiple sources and update the data catalog in Aurora. Schedule the Lambda functions periodically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog as the central metadata repository. Use AWS Glue crawlers to connect to multiple data stores and update the Data Catalog with metadata changes. Schedule the crawlers periodically to update the metadata catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB as the data catalog. Create AWS Lambda functions that will connect and gather the metadata information from multiple sources and update the DynamoDB catalog. Schedule the Lambda functions periodically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog as the central metadata repository. Extract the schema for RDS and Amazon Redshift sources and build the Data Catalog. Use AWS crawlers for data stored in Amazon S3 to infer the schema and automatically update the Data Catalog."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-19T12:53:00.000Z",
        "voteCount": 17,
        "content": "Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html"
      },
      {
        "date": "2022-10-27T12:41:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is B as AWS Glue Data Catalog can act as the central metadata repository with Glue Crawlers which can connect to multiple data stores and update the Data Catalog with metadata changes. \n\nOptions A &amp; C are wrong they would increase the administration work. \n\nOption D is wrong as Glue Crawlers can connect to all the mentioned datastores. \n\nhttps://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html"
      },
      {
        "date": "2022-10-27T12:46:00.000Z",
        "voteCount": 2,
        "content": "Option D Is wrong and extracting schema is a manual work"
      },
      {
        "date": "2023-05-01T12:35:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-07-27T23:07:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-05-21T05:12:00.000Z",
        "voteCount": 2,
        "content": "My Answer is B"
      },
      {
        "date": "2022-03-19T17:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-11-13T21:10:00.000Z",
        "voteCount": 4,
        "content": "Answer B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/amazon/view/64638-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An ecommerce company is migrating its business intelligence environment from on premises to the AWS Cloud. The company will use Amazon Redshift in a public subnet and Amazon QuickSight. The tables already are loaded into Amazon Redshift and can be accessed by a SQL tool.<br>The company starts QuickSight for the first time. During the creation of the data source, a data analytics specialist enters all the information and tries to validate the connection. An error with the following message occurs: `Creating a connection to your data source timed out.`<br>How should the data analytics specialist resolve this error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the SELECT permission on Amazon Redshift tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the QuickSight IP address range into the Amazon Redshift security group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role for QuickSight to access Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a QuickSight admin user for creating the dataset."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-27T13:13:00.000Z",
        "voteCount": 8,
        "content": "Correct answer is B as the error is time out and not permissions denied, the most likely reason is the Redshift Security Group does not allow QuickSight IP address range. \nFor Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region.\n\nOptions A, C &amp; D are wrong as the error is a timeout and not access denied.\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html"
      },
      {
        "date": "2021-12-20T21:46:00.000Z",
        "voteCount": 6,
        "content": "B it is \nsecurity group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region.\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html"
      },
      {
        "date": "2023-07-08T00:19:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      },
      {
        "date": "2023-05-01T12:37:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-07-20T23:09:00.000Z",
        "voteCount": 2,
        "content": "Answer : B"
      },
      {
        "date": "2022-07-04T02:48:00.000Z",
        "voteCount": 2,
        "content": "Answer seems to be B due to connection timeout issue"
      },
      {
        "date": "2022-05-21T03:12:00.000Z",
        "voteCount": 2,
        "content": "If IAM permission is not there, then it would error out immediately. Time out is usually caused by missing security group permissions. Hence B"
      },
      {
        "date": "2022-04-30T14:10:00.000Z",
        "voteCount": 1,
        "content": "ANswer : B"
      },
      {
        "date": "2022-04-28T20:10:00.000Z",
        "voteCount": 2,
        "content": "B. Connection timeout is always because of the network conenctivity. Check security group tp allow IP and port access."
      },
      {
        "date": "2022-04-28T17:51:00.000Z",
        "voteCount": 1,
        "content": "vote for B"
      },
      {
        "date": "2021-11-27T07:25:00.000Z",
        "voteCount": 3,
        "content": "Error message is \"Creating a connection to your data source timed out\". Time out errors occur due to security group restrictions. Therefore, answer is B."
      },
      {
        "date": "2021-11-19T21:24:00.000Z",
        "voteCount": 3,
        "content": "Answer : B"
      },
      {
        "date": "2021-11-13T21:09:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      },
      {
        "date": "2021-11-12T21:40:00.000Z",
        "voteCount": 2,
        "content": "the answer should be B. The other options can be ruled out easily"
      },
      {
        "date": "2021-10-25T07:24:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/amazon/view/64636-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A power utility company is deploying thousands of smart meters to obtain real-time updates about power consumption. The company is using Amazon Kinesis<br>Data Streams to collect the data streams from smart meters. The consumer application uses the Kinesis Client Library (KCL) to retrieve the stream data. The company has only one consumer application.<br>The company observes an average of 1 second of latency from the moment that a record is written to the stream until the record is read by a consumer application. The company must reduce this latency to 500 milliseconds.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse enhanced fan-out in Kinesis Data Streams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards for the Kinesis data stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the propagation delay by overriding the KCL default settings.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop consumers by using Amazon Kinesis Data Firehose."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-13T00:31:00.000Z",
        "voteCount": 10,
        "content": "Answer: C\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html"
      },
      {
        "date": "2024-01-27T09:33:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct. Checked in Bing app"
      },
      {
        "date": "2023-08-03T19:47:00.000Z",
        "voteCount": 3,
        "content": "Kinesis Data Streams records are available to be read immediately after they are written. There are some use cases that need to take advantage of this and require consuming data from the stream as soon as it is available. You can significantly reduce the propagation delay by overriding the KCL default settings to poll more frequently"
      },
      {
        "date": "2023-05-01T12:38:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-18T03:16:00.000Z",
        "voteCount": 2,
        "content": "I dug around the documentation, and the default is 1sec, but you can reduce to almost immediate (low millis). It's not recommended as sustainable on high traffic and you may end up with other issues on shards etc but the question doesnt care about this. Also the name of the property to change is not the same as in the question which is a little misleading, but the effect/solution is right (C)."
      },
      {
        "date": "2023-02-14T13:57:00.000Z",
        "voteCount": 1,
        "content": "Option A, using enhanced fan-out, can help achieve the desired reduction in latency by allowing multiple consumers to receive data from the same stream at the same time without having to share the same shard. This can help increase the overall throughput of the stream and reduce the time it takes for records to be processed and delivered to the consumer application."
      },
      {
        "date": "2023-03-20T10:14:00.000Z",
        "voteCount": 5,
        "content": "\"The company has only one consumer application.\""
      },
      {
        "date": "2022-10-27T13:15:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as the default propagation delay for the KCL consumer is 1 second and reducing the delay can help reduce the latency. \nPropagation delay is defined as the end-to-end latency from the moment a record is written to the stream until it is read by a consumer application. This delay varies depending upon a number of factors, but it is primarily affected by the polling interval of consumer applications. \nOption A is wrong as enhanced fan-out in Kinesis Data Streams enables consumers to receive records from a stream with a throughput of up to 2 MB of data per second per shard. It doesn't reduce the latency for KCL.\n\nOption B is wrong as increasing shards doesn't reduce the latency for KCL.\n\nOption D is wrong as Kinesis Data Firehose calls Kinesis Data Streams GetRecords() once every second for each Kinesis shard."
      },
      {
        "date": "2022-10-27T13:16:00.000Z",
        "voteCount": 1,
        "content": "For most applications, we recommend polling each shard one time per second per application. This enables you to have multiple consumer applications processing a stream concurrently without hitting Amazon Kinesis Data Streams limits of 5 GetRecords calls per second. Additionally, processing larger batches of data tends to be more efficient at reducing network and other downstream latencies in your application. \nThe KCL defaults are set to follow the best practice of polling every 1 second. This default results in average propagation delays that are typically below 1 second.\n\nKinesis Data Streams records are available to be read immediately after they are written. There are some use cases that need to take advantage of this and require consuming data from the stream as soon as it is available. You can significantly reduce the propagation delay by overriding the KCL default settings to poll more frequently, \n\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html"
      },
      {
        "date": "2022-07-19T22:26:00.000Z",
        "voteCount": 1,
        "content": "Answer: C\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html"
      },
      {
        "date": "2021-12-02T19:15:00.000Z",
        "voteCount": 4,
        "content": "Answer C\nidleTimeBetweenReadsInMillis  set between 200ms and 500ms"
      },
      {
        "date": "2021-11-26T19:52:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2021-11-13T21:07:00.000Z",
        "voteCount": 3,
        "content": "Answer C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/amazon/view/64635-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company needs to collect streaming data from several sources and store the data in the AWS Cloud. The dataset is heavily structured, but analysts need to perform several complex SQL queries and need consistent performance. Some of the data is queried more frequently than the rest. The company wants a solution that meets its performance requirements in a cost-effective manner.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Streaming for Apache Kafka to ingest the data to save it to Amazon S3. Use Amazon Athena to perform SQL queries over the ingested data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Streaming for Apache Kafka to ingest the data to save it to Amazon Redshift. Enable Amazon Redshift workload management (WLM) to prioritize workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to ingest the data to save it to Amazon Redshift. Enable Amazon Redshift workload management (WLM) to prioritize workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to ingest the data to save it to Amazon S3. Load frequently queried data to Amazon Redshift using the COPY command. Use Amazon Redshift Spectrum for less frequently queried data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-17T14:14:00.000Z",
        "voteCount": 16,
        "content": "D is right"
      },
      {
        "date": "2021-12-02T23:35:00.000Z",
        "voteCount": 6,
        "content": "I think it should be D\n\"multiple sophisticated SQL queries with consistent performance\" means Redshift. \nWLM just prioritizes query. frequently queried can be in Redshift"
      },
      {
        "date": "2023-05-01T12:38:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-26T02:19:00.000Z",
        "voteCount": 2,
        "content": "D is the most logical answer"
      },
      {
        "date": "2022-10-27T13:19:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D as Kinesis Data Firehose can be used to push data directly to S3. Frequently queried data can be loaded into Redshift for querying. Less frequent data can be still stored in S3 and queried using Redshift Spectrum. \n\nUsing Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\n\nOption A is wrong as Athena is not ideal for performing complex queries.\n\nOptions B &amp; C are wrong as using Redshift for all the data would not be cost-effective."
      },
      {
        "date": "2022-07-20T23:05:00.000Z",
        "voteCount": 3,
        "content": "Answer is D"
      },
      {
        "date": "2022-06-04T08:53:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2022-05-22T01:51:00.000Z",
        "voteCount": 2,
        "content": "My Answer is D"
      },
      {
        "date": "2022-04-01T05:47:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2022-03-14T23:39:00.000Z",
        "voteCount": 1,
        "content": "Ans: C\nThe question is focused on  performance. Hence using WLM to fulfill the purpose."
      },
      {
        "date": "2022-04-28T05:17:00.000Z",
        "voteCount": 2,
        "content": "WLM prioritizes workloads mostly according to the run time, like long/short run. If only the data is well patitioned, and formatted, Athena on S3 will also get result in \"consistent performance\". So I think the question is mainly asking about the \"cost\"."
      },
      {
        "date": "2022-02-27T13:43:00.000Z",
        "voteCount": 3,
        "content": "when kinesis firehost can directly write to Redshift why write to s3 and then copy command then wlm for queries ? I think answer C"
      },
      {
        "date": "2022-03-09T04:39:00.000Z",
        "voteCount": 5,
        "content": "Kinesis Data Firehose delivers your data to your S3 bucket first and then issues an Amazon Redshift COPY command to load the data into your Amazon Redshift cluster.\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-redshift"
      },
      {
        "date": "2021-11-14T19:02:00.000Z",
        "voteCount": 3,
        "content": "I think it is A. Cost-effective is the requirement, so RD cannot be the one."
      },
      {
        "date": "2021-12-07T19:58:00.000Z",
        "voteCount": 2,
        "content": "But, how you are going to ingest the data from MSK to S3 ? I think this is pub-sub model, someone has to consume the Kafka Topic to put there in S3. Doest it ? \nit must be D or B. Where B might take more cost to put data on Redshift cluster."
      },
      {
        "date": "2022-04-28T05:19:00.000Z",
        "voteCount": 1,
        "content": "And you will par for Athena based on data scanning. if you query frequently on some data, you will pay much for the data scanning."
      },
      {
        "date": "2023-03-18T03:19:00.000Z",
        "voteCount": 1,
        "content": "PERFORMANCE is the requirement not COST?"
      },
      {
        "date": "2021-11-13T21:06:00.000Z",
        "voteCount": 1,
        "content": "Answer : D"
      },
      {
        "date": "2021-11-09T23:28:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer should be A. thoughts?"
      },
      {
        "date": "2021-11-13T21:06:00.000Z",
        "voteCount": 3,
        "content": "wrong - Amazon Athena can't be used for complex queries"
      },
      {
        "date": "2021-10-17T08:21:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/amazon/view/64639-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A manufacturing company uses Amazon Connect to manage its contact center and Salesforce to manage its customer relationship management (CRM) data. The data engineering team must build a pipeline to ingest data from the contact center and CRM system into a data lake that is built on Amazon S3.<br>What is the MOST efficient way to collect data in the data lake with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to ingest Amazon Connect data and Amazon AppFlow to ingest Salesforce data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to ingest Amazon Connect data and Amazon Kinesis Data Streams to ingest Salesforce data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to ingest Amazon Connect data and Amazon AppFlow to ingest Salesforce data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon AppFlow to ingest Amazon Connect data and Amazon Kinesis Data Firehose to ingest Salesforce data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-08T09:37:00.000Z",
        "voteCount": 14,
        "content": "Answer: C\nhttps://aws.amazon.com/blogs/apn/building-secure-and-private-data-flows-between-aws-and-salesforce-using-amazon-appflow/;https://aws.amazon.com/connect/faqs/"
      },
      {
        "date": "2022-12-11T08:18:00.000Z",
        "voteCount": 2,
        "content": "A is wrong as it's required to persist the data to S3, which can't be directly from KDS"
      },
      {
        "date": "2023-01-17T06:15:00.000Z",
        "voteCount": 1,
        "content": "The native integration of Amazon Connect and Amazon Kinesis to upload the Contact Trace Records (CTR) to Amazon Simple Storage Service (Amazon S3) bucket in near-real-time\n\nhttps://aws.amazon.com/blogs/contact-center/analyzing-amazon-connect-usage-with-agent-desktop-and-streaming-data/#:~:text=native%20integration%20of%20Amazon%20Connect%20and%20Amazon%20Kinesis%20to%20upload%20the%20Contact%20Trace%20Records%20(CTR)%20to%20Amazon%20Simple%20Storage%20Service%20(Amazon%20S3)%20bucket%20in%20near%2Dreal%2Dtime"
      },
      {
        "date": "2021-12-25T21:12:00.000Z",
        "voteCount": 7,
        "content": "Both A and C are right\nAmazon Connect can stream metrics and agent event data to Amazon Kinesis Data Stream or Amazon Kinesis Data Firehose\nhttps://aws.amazon.com/connect/faqs/\nBut the question asks for the least operational overhead, so Firehose is more suitable for the data pipeline as its further sources and sinks"
      },
      {
        "date": "2023-11-06T06:01:00.000Z",
        "voteCount": 1,
        "content": "Amazon Connect has data streaming feature.\nso Amazon Connect can stream Firehose and Streams.\nWhich is more unmanaged tool?\nIt is Firehose.\nC is correct."
      },
      {
        "date": "2023-05-01T12:39:00.000Z",
        "voteCount": 3,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-19T08:05:00.000Z",
        "voteCount": 3,
        "content": "The most efficient way to collect data in the data lake with the least operational overhead is option C: Use Amazon Kinesis Data Firehose to ingest Amazon Connect data and Amazon AppFlow to ingest Salesforce data.\n\nAmazon Kinesis Data Firehose is a fully managed service that can reliably capture and load streaming data into Amazon S3 with no operational overhead. It can also transform the incoming data before storing it in S3.\n\nAmazon AppFlow is a fully managed integration service that can securely transfer data between different SaaS applications, including Salesforce and S3. It supports incremental data transfers and can also transform the data during the transfer process.\n\nUsing Kinesis Data Firehose for Amazon Connect data and AppFlow for Salesforce data provides a simple and scalable solution that requires minimal operational overhead, allowing the data engineering team to focus on other tasks."
      },
      {
        "date": "2022-12-14T15:36:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as the most efficient way is to use Amazon Connect integration with Kinesis Data Firehose to stream data directly to S3 and AppFlow to ingest data from Salesforce data to S3.\n\nOption A is also correct, although it's valid, it is not the most efficient solution."
      },
      {
        "date": "2022-10-27T13:23:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as the most efficient way is to use Amazon Connect integration with Kinesis Data Firehose to stream data directly to S3 and AppFlow to ingest data from Salesforce data to S3. \n\nOption A is wrong as although it's valid, it is not the most efficient solution.\n\nOptions B &amp; D are wrong as Kinesis does not integrate directly with Salesforce and AppFlow is a better solution. \n\nhttps://aws.amazon.com/blogs/apn/building-secure-and-private-data-flows-between-aws-and-salesforce-using-amazon-appflow/\n\nhttps://aws.amazon.com/connect/faqs/"
      },
      {
        "date": "2022-10-06T18:37:00.000Z",
        "voteCount": 2,
        "content": "Answer: C \nBoth A and C are correct but C provides a reduced operational overhead as it  is fully managed"
      },
      {
        "date": "2023-11-06T06:02:00.000Z",
        "voteCount": 1,
        "content": "This is correct answer."
      },
      {
        "date": "2022-09-14T21:27:00.000Z",
        "voteCount": 1,
        "content": "c to use  KDF"
      },
      {
        "date": "2022-08-21T12:23:00.000Z",
        "voteCount": 2,
        "content": "Sounds like C for s3 destinations"
      },
      {
        "date": "2022-08-17T13:19:00.000Z",
        "voteCount": 2,
        "content": "C, since the kinesis data stream can not write to S3 directly."
      },
      {
        "date": "2022-08-06T14:32:00.000Z",
        "voteCount": 1,
        "content": "Both A and C are the same option. \nAdmins, please look into the missing answer option."
      },
      {
        "date": "2022-12-14T15:34:00.000Z",
        "voteCount": 1,
        "content": "Ah nevermind, One is Firehose, the other is Data Streams"
      },
      {
        "date": "2022-06-30T02:22:00.000Z",
        "voteCount": 1,
        "content": "A is the answer https://aws.amazon.com/quickstart/connect/data-streaming/"
      },
      {
        "date": "2022-12-11T08:20:00.000Z",
        "voteCount": 2,
        "content": "does KDS persist to S3 directly?"
      },
      {
        "date": "2022-06-25T00:31:00.000Z",
        "voteCount": 2,
        "content": "A.\nAmazon Connect feeds to Kinesis Data Stream.  The other 3 answer is incorrectly using Firehose.\nhttps://aws.amazon.com/quickstart/connect/data-streaming/"
      },
      {
        "date": "2021-11-30T15:15:00.000Z",
        "voteCount": 1,
        "content": "C is the Answer."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/amazon/view/64640-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A manufacturing company wants to create an operational analytics dashboard to visualize metrics from equipment in near-real time. The company uses Amazon<br>Kinesis Data Streams to stream the data to other applications. The dashboard must automatically refresh every 5 seconds. A data analytics specialist must design a solution that requires the least possible implementation effort.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to store the data in Amazon S3. Use Amazon QuickSight to build the dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Spark Streaming on Amazon EMR to read the data in near-real time. Develop a custom application for the dashboard by using D3.js.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Visualize the data by using an OpenSearch Dashboards (Kibana).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue streaming ETL to store the data in Amazon S3. Use Amazon QuickSight to build the dashboard."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-27T08:12:00.000Z",
        "voteCount": 18,
        "content": "Whenever the requirement states it is Operational Analytics, I consider using ES. Also, here QuickSight cannot refresh data for every 5 secs automatically, which in case of ELK is possible. So, Answer is C."
      },
      {
        "date": "2024-01-03T03:31:00.000Z",
        "voteCount": 1,
        "content": "A,C is wrong because \"You can configure the values for OpenSearch buffer size (1 MB to 100 MB) or buffer interval (60 to 900 seconds)\", the requirement is \"5 seconds\" to refresh data.\nhttps://aws.amazon.com/tw/kinesis/data-firehose/faqs/\nB is wrong since EMR requires more effort.\nhence D."
      },
      {
        "date": "2024-01-03T03:30:00.000Z",
        "voteCount": 1,
        "content": "A,C is wrong because \"You can configure the values for OpenSearch buffer size (1 MB to 100 MB) or buffer interval (60 to 900 seconds)\", the requirement is \"5 seconds\" to refresh data.\nhttps://aws.amazon.com/tw/kinesis/data-firehose/faqs/\nB is wrong since EMR requires more effort.\nhence D."
      },
      {
        "date": "2023-05-01T12:40:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-19T08:10:00.000Z",
        "voteCount": 1,
        "content": "The solution that meets the requirements and requires the least possible implementation effort is option C: Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Visualize the data by using an OpenSearch Dashboards (Kibana).\n\nAmazon Kinesis Data Firehose can be used to stream data to an Amazon Elasticsearch Service cluster with minimal configuration. Amazon OpenSearch Service is an open-source, distributed search and analytics engine that is fully compatible with Elasticsearch. OpenSearch Dashboards (Kibana) can be used to visualize and explore the data in near-real time, with automatic refreshing every 5 seconds.\n\nThis solution eliminates the need for a custom application or custom coding, and it requires minimal configuration and maintenance effort. It also provides a scalable, fully managed, and cost-effective way to store, analyze, and visualize streaming data."
      },
      {
        "date": "2023-01-28T05:25:00.000Z",
        "voteCount": 4,
        "content": "What is the need of refreshing the dashboard for every 5seconds if Firehose can deliver for every 60 seconds? Go with D(don't use quicksight spice, query the data directly from quicksight) or go with B(but more dev effort)"
      },
      {
        "date": "2022-07-19T23:38:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. Simple to deploy."
      },
      {
        "date": "2022-06-15T01:33:00.000Z",
        "voteCount": 2,
        "content": "It is C. Simple to deploy = not a custom Java application. QuickSight can't be refreshed fast enough."
      },
      {
        "date": "2022-05-27T21:22:00.000Z",
        "voteCount": 1,
        "content": "Answer B is correct"
      },
      {
        "date": "2022-05-21T02:07:00.000Z",
        "voteCount": 2,
        "content": "It should be C. There is no need to use JavaScript libraries to generate a dashboard as the end user is not public using a webpage but data analysts within the organization. Firehose, ElasticSearch and Kibana are able to handle this requirement very well. And there is no custom code development required either."
      },
      {
        "date": "2022-05-05T00:48:00.000Z",
        "voteCount": 2,
        "content": "In the requirement, it want to refresh the dashboard every 5 seconds, but not need to process data in 5 seconds."
      },
      {
        "date": "2022-04-27T23:27:00.000Z",
        "voteCount": 1,
        "content": "I vote for B.\nFor C, it use kinesis anadata firehose. But the data firehose has a buffer with a minimum 60 seconds or 1 MB. We are not sure about the message size and frequence, so the deliver interval maybe much more than 5 seconds(it is required in the question). So B will not match the requirement.\nFor B, the spark in a EMR can process data in realtime. It will develop application using d3.js for visualization. But we still need a storage for the realtime data, which is output of spark stream, and used by d3.js to get and display. Although C isn't clear enough and needs much efford, but it match the requirement."
      },
      {
        "date": "2022-05-05T00:47:00.000Z",
        "voteCount": 1,
        "content": "Some typo in my comments.\nBut I want to change my vote. In the requirement, it want to refresh the dashboard every 5 seconds, but not need to process data in 5 seconds. \nSo I changed to C."
      },
      {
        "date": "2022-03-19T03:05:00.000Z",
        "voteCount": 1,
        "content": "QuickSight can only be updated every 15 minutes. So the answer is C."
      },
      {
        "date": "2022-03-01T18:48:00.000Z",
        "voteCount": 4,
        "content": "B and C are close. As simple to deploy rules out D3.js as it's not simple and quicksight is not meant for real-time (or near real-time) refresh! hence answer is C"
      },
      {
        "date": "2022-02-27T13:30:00.000Z",
        "voteCount": 2,
        "content": "Kinesis firehose can write to destination RES R - REdshift , E - elastic search , S - S3 . when it can write directly to elastic search why write to S3 and then connect to elastic search. So the answer is C directly write to elastic search and use kibana to refresh every 5 secs and reporting display."
      },
      {
        "date": "2022-02-27T02:06:00.000Z",
        "voteCount": 2,
        "content": "I think it's B because Quicksight can't refresh data every 5sec and Firehose has a minila batch size of 60sec so the only remaining option is B even if it requires custom code"
      },
      {
        "date": "2022-01-29T01:23:00.000Z",
        "voteCount": 4,
        "content": "B and C are close.\nBut B is not simple to deploy.\nso C."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/amazon/view/64641-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analyst is designing an Amazon QuickSight dashboard using centralized sales data that resides in Amazon Redshift. The dashboard must be restricted so that a salesperson in Sydney, Australia, can see only the Australia view and that a salesperson in New York can see only United States (US) data.<br>What should the data analyst do to ensure the appropriate data security is in place?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the data sources for Australia and the US into separate SPICE capacity pools.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Redshift VPC security group for Australia and the US.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy QuickSight Enterprise edition to implement row-level security (RLS) to the sales table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy QuickSight Enterprise edition and set up different VPC security groups for Australia and the US."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-23T20:13:00.000Z",
        "voteCount": 8,
        "content": "Should be C,  standard exmaple given in AWS website\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2023-05-01T12:41:00.000Z",
        "voteCount": 3,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-12T06:16:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2022-12-27T15:26:00.000Z",
        "voteCount": 2,
        "content": "Should be C"
      },
      {
        "date": "2022-10-27T13:35:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C as QuickSight Enterprise edition provides row-level security (RLS) which can be configured for the sales table to restrict access. \nTo do this, you create a query or file that has one column named UserName, GroupName, or both. Or you can create a query or file that has one column named UserARN, GroupARN, or both. You can think of this as adding a rule for that user or group. Then you can add one column to the query or file for each field that you want to grant or restrict access to. For each user or group name that you add, you add the values for each field. You can use NULL (no value) to mean all values.\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2022-07-27T23:12:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-05-21T05:14:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C"
      },
      {
        "date": "2021-11-23T14:35:00.000Z",
        "voteCount": 2,
        "content": "Answer : C"
      },
      {
        "date": "2021-11-16T00:37:00.000Z",
        "voteCount": 4,
        "content": "Answer is C \nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. When you share a dataset with RLS with dataset owners, they can still see all the data. When you share it with readers, however, they can only see the data restricted by the permission dataset rules. By adding row-level security, you can further control their access.\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html"
      },
      {
        "date": "2021-10-15T11:15:00.000Z",
        "voteCount": 3,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/amazon/view/64642-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company wants to run analytics on its Elastic Load Balancing logs stored in Amazon S3. A data analyst needs to be able to query all data from a desired year, month, or day. The data analyst should also be able to query a subset of the columns. The company requires minimal operational overhead and the most cost- effective solution.<br>Which approach meets these requirements for optimizing and querying the log data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue job nightly to transform new log files into .csv format and partition by year, month, and day. Use AWS Glue crawlers to detect new partitions. Use Amazon Athena to query data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a long-running Amazon EMR cluster that continuously transforms new log files from Amazon S3 into its Hadoop Distributed File System (HDFS) storage and partitions by year, month, and day. Use Apache Presto to query the optimized format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a transient Amazon EMR cluster nightly to transform new log files into Apache ORC format and partition by year, month, and day. Use Amazon Redshift Spectrum to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue job nightly to transform new log files into Apache Parquet format and partition by year, month, and day. Use AWS Glue crawlers to detect new partitions. Use Amazon Athena to query data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T06:55:00.000Z",
        "voteCount": 20,
        "content": "Answer: D"
      },
      {
        "date": "2021-11-30T08:03:00.000Z",
        "voteCount": 15,
        "content": "A - .csv format is not optimal\nB - long running EMR is not cost-effective and has operational over-head of cluster management\nC - Again EMR with running Redshift Cluster is not cost-effective\nSo, Answer is Option D -  low-cost and no operational over-head. Data Scanning cost by Athena can be minimized by partition pruning and subset of columns from parquet file."
      },
      {
        "date": "2023-05-01T12:42:00.000Z",
        "voteCount": 3,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-26T02:57:00.000Z",
        "voteCount": 1,
        "content": "answer is d"
      },
      {
        "date": "2023-01-21T18:06:00.000Z",
        "voteCount": 2,
        "content": "Agree D"
      },
      {
        "date": "2022-10-27T13:37:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D as the Glue job can be used to transform and partition the logs files. Athena can be used to query the day. Glue and Athena are cost-effective with low operational overhead. Parquet data format can help query on a subset of the columns \n\nOption A is wrong as the CSV format is not the optimal format for storing and querying data using Athena.\n\nOption B is wrong as the long-running Amazon EMR cluster would not be a cost-effective option.\n\nOption C is wrong as using EMR and Redshift would not be as cost-effective or reduce operational cost as compared to Athena and Glue. \n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2022-07-27T23:39:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-06-24T04:06:00.000Z",
        "voteCount": 1,
        "content": "D as my other comment explains"
      },
      {
        "date": "2022-06-24T04:05:00.000Z",
        "voteCount": 2,
        "content": "Answer D is obvious. \nA: we don't want csv. we want ORC or Parquet\nB: Long-running EMR cluster is expensive. Storing the data in HDFS is expensive\nC: Can work. But Using Redshift Spectrum is only logical if we want to combine the data with other data in Redshift. \nD: This is optimal. Glue works well. Parquet with Y/M/D partitions is optimal. Athena to query the data is perfect!"
      },
      {
        "date": "2021-11-28T12:58:00.000Z",
        "voteCount": 3,
        "content": "Very tricky question, Option C has some operational over-head but D is AWS glue is server less, considering that I might go for D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/amazon/view/64643-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An education provider's learning management system (LMS) is hosted in a 100 TB data lake that is built on Amazon S3. The provider's LMS supports hundreds of schools. The provider wants to build an advanced analytics reporting platform using Amazon Redshift to handle complex queries with optimal performance.<br>System users will query the most recent 4 months of data 95% of the time while 5% of the queries will leverage data from the previous 12 months.<br>Which solution meets these requirements in the MOST cost-effective way?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the most recent 4 months of data in the Amazon Redshift cluster. Use Amazon Redshift Spectrum to query data in the data lake. Use S3 lifecycle management rules to store data from the previous 12 months in Amazon S3 Glacier storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage DS2 nodes for the Amazon Redshift cluster. Migrate all data from Amazon S3 to Amazon Redshift. Decommission the data lake.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the most recent 4 months of data in the Amazon Redshift cluster. Use Amazon Redshift Spectrum to query data in the data lake. Ensure the S3 Standard storage class is in use with objects in the data lake.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the most recent 4 months of data in the Amazon Redshift cluster. Use Amazon Redshift federated queries to join cluster data with the data lake to reduce costs. Ensure the S3 Standard storage class is in use with objects in the data lake."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T19:08:00.000Z",
        "voteCount": 16,
        "content": "Answer: C"
      },
      {
        "date": "2022-10-27T13:41:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is C as only the last 4 months of data is required for 95%, the data can be stored in the Redshift cluster. The data covering the 12 months can be moved to S3 and queried using Redshift Spectrum. A mix of S3 and Redshift would provide the most cost-effective option. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\n\nOption A is wrong as Redshift Spectrum cannot be used to query S3 Glacier storage data.\n\nOption B is wrong as using Redshift for all the data is not cost-effective.\n\nOption D is wrong as although Redshift federated queries would work, however, for 5% it would be cost-effective to query S3 directly instead of joining data from cluster and S3."
      },
      {
        "date": "2024-02-23T06:00:00.000Z",
        "voteCount": 2,
        "content": "Point A cannot be answer because 5% user/query will be frequently used and Glacier need 90 to 180 days period. Point D : Federated query make sense when multiple data source because it required lots of authentication and authorization process , But here we have only  S3 , so we can go with Point C"
      },
      {
        "date": "2023-11-21T05:40:00.000Z",
        "voteCount": 1,
        "content": "I don't get it:\n\nThe question says:\n\"System users will query the most recent 4 months of data 95% of the time while 5% of the queries will leverage data from the previous 12 months.\"\n\n95% using data from the previous 4 month\n5 % using data from the previous 12 month\n0 % using data that is older than 12 month. \n\nSo why not archive them?"
      },
      {
        "date": "2023-11-21T05:43:00.000Z",
        "voteCount": 1,
        "content": "Ahh now I got it: They want tu use the glacier to store the data from the previous 12 months."
      },
      {
        "date": "2023-05-01T12:43:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-02-14T13:14:00.000Z",
        "voteCount": 4,
        "content": "Option C suggests storing the most recent 4 months of data in the Amazon Redshift cluster and using Amazon Redshift Spectrum to query data in the data lake, while ensuring that the S3 Standard storage class is in use with objects in the data lake. While this approach could work, it may not be the most cost-effective way to meet the requirements, because storing data in the Amazon Redshift cluster can be more expensive than storing data in S3. Additionally, by storing all data in the data lake, you may be able to use other data analysis services to query the data, which can be more cost-effective than using Amazon Redshift. Therefore, Option A, which leverages Amazon Redshift Spectrum to query the data in the data lake and uses S3 lifecycle management rules to move data from the previous 12 months to Amazon S3 Glacier, is likely a more cost-effective solution."
      },
      {
        "date": "2022-07-25T21:38:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-07-09T09:21:00.000Z",
        "voteCount": 1,
        "content": "C is the answer. If you have to join old data coming from S3 with new data coming from the Redshift cluster, you can do that. It is described here: https://catalog.us-east-1.prod.workshops.aws/workshops/e5548031-3004-49ad-89be-a13e8cd616f6/en-US/perform-analytics-on-your-data/join-and-query-data-with-redshift-spectrum"
      },
      {
        "date": "2022-06-21T17:59:00.000Z",
        "voteCount": 1,
        "content": "Not D. https://docs.aws.amazon.com/redshift/latest/dg/federated-limitations.html"
      },
      {
        "date": "2022-05-27T22:00:00.000Z",
        "voteCount": 1,
        "content": "Answer C should be correct"
      },
      {
        "date": "2022-05-21T21:51:00.000Z",
        "voteCount": 1,
        "content": "C. There is no mention of joining old and new data. Hence no need for federated queries."
      },
      {
        "date": "2022-05-27T20:10:00.000Z",
        "voteCount": 1,
        "content": "federated queries are for databases and not specific to s3. Here requirement is s3 so spectrum should work okay."
      },
      {
        "date": "2022-05-17T07:58:00.000Z",
        "voteCount": 1,
        "content": "Its C , textbook case for spectrum"
      },
      {
        "date": "2022-05-06T20:52:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct.\nUsing federated queries to combine redshift and Spectrum (from S3) will be more cost effective."
      },
      {
        "date": "2021-12-25T09:10:00.000Z",
        "voteCount": 2,
        "content": "A  - Data from past 12 months is not required so glacier is most cost effective, Redshift spectrum to be used for querying data 4 - 12 months older, and upto 4 months old data is to queried from Redshift DB"
      },
      {
        "date": "2021-12-29T05:41:00.000Z",
        "voteCount": 2,
        "content": "I correct myself for misreading data from last 12 months to be in glacier, answer is C"
      },
      {
        "date": "2021-12-06T14:45:00.000Z",
        "voteCount": 3,
        "content": "The question says \"efficiently handle complicated queries\", A. recommend S3 lifecycle, but I don\u00b4t think Glacier be efficient, even Expedited retrieval (1-5) minutes. I will go for C."
      },
      {
        "date": "2021-12-05T13:52:00.000Z",
        "voteCount": 1,
        "content": "ali98 on point. Answer is C"
      },
      {
        "date": "2021-11-28T13:08:00.000Z",
        "voteCount": 4,
        "content": "Isn't D? The 5% of the user should be able to query both. moving the 4 months data to redshift makes sense and at the same time for 12 months data you need to query Redshift + Data lake data so in that federated queries  can help to do that.\n\nIn option C if they have provided Redshift Spectrum queries S3 as an external also then it make sense to choose C, But I do not read that way."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/amazon/view/64645-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is hosting an enterprise reporting solution with Amazon Redshift. The application provides reporting capabilities to three main groups: an executive group to access financial reports, a data analyst group to run long-running ad-hoc queries, and a data engineering group to run stored procedures and ETL processes. The executive team requires queries to run with optimal performance. The data engineering team expects queries to take minutes.<br>Which Amazon Redshift feature meets the requirements for this task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConcurrency scaling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShort query acceleration (SQA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkload management (WLM)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaterialized views"
    ],
    "answer": "D",
    "answerDescription": "Materialized views:<br>Reference:<br>https://aws.amazon.com/redshift/faqs/",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-20T23:42:00.000Z",
        "voteCount": 10,
        "content": "Answer C\nWLM is main feature which supports Concurrency scaling and SQA."
      },
      {
        "date": "2021-11-16T01:02:00.000Z",
        "voteCount": 6,
        "content": "Answer: C\n\nIn some cases, you might have multiple sessions or users running queries at the same time. In these cases, some queries might consume cluster resources for long periods of time and affect the performance of other queries. For example, suppose that one group of users submits occasional complex, long-running queries that select and sort rows from several large tables. Another group frequently submits short queries that select only a few rows from one or two tables and run in a few seconds. In this situation, the short-running queries might have to wait in a queue for a long-running query to complete. WLM helps manage this situation."
      },
      {
        "date": "2023-05-01T12:44:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-11-02T02:41:00.000Z",
        "voteCount": 1,
        "content": "Are all the questions from here?"
      },
      {
        "date": "2022-10-27T13:47:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C as Redshift Workload Management can help define multiple query queues and manage the scheduling between long and short running workloads. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html\n\nOption A is wrong as Concurrency scaling helps support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance.\n\nOption B is wrong as Short query acceleration (SQA) prioritizes selected short-running queries ahead of longer-running queries.\n\nOption D is wrong as materialized view contains a precomputed result set, based on an SQL query over one or more base tables."
      },
      {
        "date": "2022-07-25T21:12:00.000Z",
        "voteCount": 1,
        "content": "Answer : C"
      },
      {
        "date": "2022-06-08T14:24:00.000Z",
        "voteCount": 1,
        "content": "WLM will manage query execution"
      },
      {
        "date": "2022-06-08T14:24:00.000Z",
        "voteCount": 1,
        "content": "C - WLM addresses exactly that (managing query workload)"
      },
      {
        "date": "2022-04-30T16:19:00.000Z",
        "voteCount": 1,
        "content": "Answer-C"
      },
      {
        "date": "2021-11-20T09:10:00.000Z",
        "voteCount": 3,
        "content": "Answer : C"
      },
      {
        "date": "2021-11-13T20:41:00.000Z",
        "voteCount": 2,
        "content": "Answer : C"
      },
      {
        "date": "2021-10-03T17:56:00.000Z",
        "voteCount": 3,
        "content": "Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/amazon/view/64703-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A global pharmaceutical company receives test results for new drugs from various testing facilities worldwide. The results are sent in millions of 1 KB-sized JSON objects to an Amazon S3 bucket owned by the company. The data engineering team needs to process those files, convert them into Apache Parquet format, and load them into Amazon Redshift for data analysts to perform dashboard reporting. The engineering team uses AWS Glue to process the objects, AWS Step<br>Functions for process orchestration, and Amazon CloudWatch for job scheduling.<br>More testing facilities were recently added, and the time to process files is increasing.<br>What will MOST efficiently decrease the data processing time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to group the small files into larger files. Write the files back to Amazon S3. Process the files using AWS Glue and load them into Amazon Redshift tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue dynamic frame file grouping option while ingesting the raw input files. Process the files and load them into Amazon Redshift tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Redshift COPY command to move the files from Amazon S3 into Amazon Redshift tables directly. Process the files in Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR instead of AWS Glue to group the small input files. Process the files in Amazon EMR and load them into Amazon Redshift tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T11:16:00.000Z",
        "voteCount": 18,
        "content": "Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"
      },
      {
        "date": "2021-12-20T22:09:00.000Z",
        "voteCount": 3,
        "content": "Agree \n\ndf = glueContext.create_dynamic_frame.from_options(\"s3\", {'paths': [\"s3://s3path/\"], 'recurse':True, 'groupFiles': 'inPartition', 'groupSize': '1048576'}, format=\"json\""
      },
      {
        "date": "2024-03-02T09:41:00.000Z",
        "voteCount": 1,
        "content": "The data engineering team needs to process those files, convert them into Apache Parquet format---so answer is A?"
      },
      {
        "date": "2023-05-01T12:44:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-10-27T13:53:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B as the AWS Glue job can be updated to group files to create larger files which can help improve the processing time without any additional steps or changes. \n\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html\n\nOptions A &amp; D are wrong as using a staging space or EMR would add additional steps to the processing.\n\nOption C is wrong as this only performs the loading of data, not processing before the load."
      },
      {
        "date": "2022-07-21T19:47:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-07-06T17:55:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2022-04-30T15:53:00.000Z",
        "voteCount": 1,
        "content": "Answer-B"
      },
      {
        "date": "2022-04-29T00:28:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"
      },
      {
        "date": "2022-02-27T05:30:00.000Z",
        "voteCount": 2,
        "content": "I'm confused between B and C. But I think I will vote for B, since the input files are in json so I think they need to be processed before loading to redshift (flatten the data ...). We need ETL instead of ELT."
      },
      {
        "date": "2021-12-15T04:12:00.000Z",
        "voteCount": 2,
        "content": "groupFiles is supported for DynamicFrames created from the following data formats: csv, ion, grokLog, json, and xml. This option is not supported for avro, parquet, and orc.\nfrom https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html. B is wrong.\nWhy not C?"
      },
      {
        "date": "2021-12-22T18:01:00.000Z",
        "voteCount": 1,
        "content": "B is right. The question says that input file format is json, output is parquet, so DynamicFrames with groupFiles can help input json format"
      },
      {
        "date": "2021-11-20T07:00:00.000Z",
        "voteCount": 2,
        "content": "This is a weird question as both A and B are equally efficient"
      },
      {
        "date": "2021-12-03T18:54:00.000Z",
        "voteCount": 1,
        "content": "\"most effectively\" might be be A"
      },
      {
        "date": "2022-02-08T18:06:00.000Z",
        "voteCount": 3,
        "content": "A is not as efficient since you're creating an additional step (a \"staging\" location in S3 to store those large files). Not to mention now you're paying double the storage cost"
      },
      {
        "date": "2021-11-22T11:28:00.000Z",
        "voteCount": 5,
        "content": "You are correct, but the question says they already have Glue job. So, I would choose B."
      },
      {
        "date": "2021-11-13T20:39:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/amazon/view/64706-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company operates toll services for highways across the country and collects data that is used to understand usage patterns. Analysts have requested the ability to run traffic reports in near-real time. The company is interested in building an ingestion pipeline that loads all the data into an Amazon Redshift cluster and alerts operations personnel when toll traffic for a particular toll station does not meet a specified threshold. Station data and the corresponding threshold values are stored in Amazon S3.<br>Which approach is the MOST efficient way to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to collect data and deliver it to Amazon Redshift and Amazon Kinesis Data Analytics simultaneously. Create a reference data source in Kinesis Data Analytics to temporarily store the threshold values from Amazon S3 and compare the count of vehicles for a particular toll station against its corresponding threshold value. Use AWS Lambda to publish an Amazon Simple Notification Service (Amazon SNS) notification if the threshold is not met.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to collect all the data from toll stations. Create a stream in Kinesis Data Streams to temporarily store the threshold values from Amazon S3. Send both streams to Amazon Kinesis Data Analytics to compare the count of vehicles for a particular toll station against its corresponding threshold value. Use AWS Lambda to publish an Amazon Simple Notification Service (Amazon SNS) notification if the threshold is not met. Connect Amazon Kinesis Data Firehose to Kinesis Data Streams to deliver the data to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to collect data and deliver it to Amazon Redshift. Then, automatically trigger an AWS Lambda function that queries the data in Amazon Redshift, compares the count of vehicles for a particular toll station against its corresponding threshold values read from Amazon S3, and publishes an Amazon Simple Notification Service (Amazon SNS) notification if the threshold is not met.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to collect data and deliver it to Amazon Redshift and Amazon Kinesis Data Analytics simultaneously. Use Kinesis Data Analytics to compare the count of vehicles against the threshold value for the station stored in a table as an in-application stream based on information stored in Amazon S3. Configure an AWS Lambda function as an output for the application that will publish an Amazon Simple Queue Service (Amazon SQS) notification to alert operations personnel if the threshold is not met."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-13T20:38:00.000Z",
        "voteCount": 19,
        "content": "Answer : A \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html"
      },
      {
        "date": "2022-12-13T02:11:00.000Z",
        "voteCount": 1,
        "content": "Load from KDF to KDA : https://aws.amazon.com/kinesis/data-firehose/features/#:~:text=Support%20for%20multiple%20data%20destinations,MongoDB%2C%20and%20Splunk%20as%20destinations."
      },
      {
        "date": "2021-11-27T13:57:00.000Z",
        "voteCount": 8,
        "content": "answer : A"
      },
      {
        "date": "2023-07-03T16:49:00.000Z",
        "voteCount": 2,
        "content": "I'd go with A, but it's not word correctly, KDF can't deliver simultaneously. It's KDA who is set up to read data from KDF (not KDF delivering to KDA)\n\nEverything else doesn't seem to be as efficient."
      },
      {
        "date": "2023-05-01T12:47:00.000Z",
        "voteCount": 3,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-01-19T14:48:00.000Z",
        "voteCount": 3,
        "content": "Answer : A\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html"
      },
      {
        "date": "2023-01-10T13:55:00.000Z",
        "voteCount": 1,
        "content": "It is Kinesis analytics that can deliver up to 3 destinations, not firehose and Firehose multiple destinations means it can dump to those locations but it cannot deliver to multiple destinations at same time"
      },
      {
        "date": "2022-12-11T03:44:00.000Z",
        "voteCount": 2,
        "content": "KDF can have only one destination"
      },
      {
        "date": "2022-12-13T02:10:00.000Z",
        "voteCount": 2,
        "content": "Amazon Kinesis Data Firehose is the easiest way to load streaming data  Kinesis Data Firehose is a fully managed service that makes it easy to capture, transform, and load massive volumes of streaming data ... into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Kinesis Data Analytics,"
      },
      {
        "date": "2022-07-27T23:10:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2022-05-22T02:02:00.000Z",
        "voteCount": 1,
        "content": "A: You can use KDF with Redshift and KDA at the same stream, is the most efficient and works \nB: Not effective at all and KDF to KDS makes no sense\nC: Even less effective\nD: SQS? will not send a notification and the in-app data makes no sense too\n\nSo it must be A"
      },
      {
        "date": "2022-05-17T12:09:00.000Z",
        "voteCount": 1,
        "content": "Its going to have to be C,\nA: KDF, a single stream cannot have more than one destination\nB. KDS cannot cannot send to S3\nD: Same as before KDF cannot have more than one destination"
      },
      {
        "date": "2022-04-05T00:16:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\nIt seems to me that KDA can output into 2 destinations, so it is A for me\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output.html"
      },
      {
        "date": "2022-04-09T01:09:00.000Z",
        "voteCount": 2,
        "content": "ANSWER: B\nSorry for the confusion, but via process of elimination I now see B as most likely\nA: Firehose cannot deliver to 2 destinations at the same time\nB: S3 can be used for reference data from KDA (https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html)\nC: First store everything in Redshift and then near-real-time check? Unlikely!\nD: No SQS, but SNS makes sense"
      },
      {
        "date": "2022-08-16T22:29:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/kinesis/data-firehose/features/#:~:text=Support%20for%20multiple%20data%20destinations&amp;text=You%20can%20specify%20the%20destination,the%20data%20should%20be%20loaded.\n\nSupport for multiple data destinations\nAmazon Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, HTTP endpoints, Datadog, New Relic, MongoDB, and Splunk as destinations. You can specify the destination Amazon S3 bucket, the Amazon Redshift table, the Amazon OpenSearch Service domain, generic HTTP endpoints, or a service provider where the data should be loaded."
      },
      {
        "date": "2022-03-29T14:23:00.000Z",
        "voteCount": 2,
        "content": "C:\nQ: What is a destination?\nA destination is the data store where your data will be delivered. Amazon Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, and Amazon OpenSearch Service as destinations.\nhttps://www.amazonaws.cn/en/kinesis/data-firehose/faqs/"
      },
      {
        "date": "2022-02-09T11:06:00.000Z",
        "voteCount": 4,
        "content": "How can KDF sends to two data streams? I am not aware of it. The answer is C"
      },
      {
        "date": "2022-01-12T22:26:00.000Z",
        "voteCount": 5,
        "content": "I think C is correct. I get the reference data in KDA thing, however KDF can only have one consumer for a stream. i.e. Redshift and KDA both cant be consumers for the same delivery stream. Thoughts?"
      },
      {
        "date": "2022-01-12T08:07:00.000Z",
        "voteCount": 5,
        "content": "Firehose cant delivery to 2 destinations and it doesnt support KDA delivery. So option C"
      },
      {
        "date": "2022-03-26T02:49:00.000Z",
        "voteCount": 1,
        "content": "Absolutely incorrect !\n1. When you create KDF, configure it so send data to redshift destination. \n2. Create legacy SQL KDA application Configure KDA to use KDF as the streaming source."
      },
      {
        "date": "2022-08-16T22:29:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/kinesis/data-firehose/features/#:~:text=Support%20for%20multiple%20data%20destinations&amp;text=You%20can%20specify%20the%20destination,the%20data%20should%20be%20loaded.\n\nSupport for multiple data destinations\nAmazon Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, HTTP endpoints, Datadog, New Relic, MongoDB, and Splunk as destinations. You can specify the destination Amazon S3 bucket, the Amazon Redshift table, the Amazon OpenSearch Service domain, generic HTTP endpoints, or a service provider where the data should be loaded."
      },
      {
        "date": "2021-12-17T11:02:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works.html"
      },
      {
        "date": "2021-12-17T11:05:00.000Z",
        "voteCount": 2,
        "content": "Answer: A"
      },
      {
        "date": "2021-11-09T16:56:00.000Z",
        "voteCount": 4,
        "content": "Would vote for Option A - As its near real time ,Firehose can do the job . Also KDA can join the stream data with reference data in S3. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-add-reference-data.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/amazon/view/64708-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online retail company uses Amazon Redshift to store historical sales transactions. The company is required to encrypt data at rest in the clusters to comply with the Payment Card Industry Data Security Standard (PCI DSS). A corporate governance policy mandates management of encryption keys using an on- premises hardware security module (HSM).<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and manage encryption keys using AWS CloudHSM Classic. Launch an Amazon Redshift cluster in a VPC with the option to use CloudHSM Classic for key management.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC and establish a VPN connection between the VPC and the on-premises network. Create an HSM connection and client certificate for the on- premises HSM. Launch a cluster in the VPC with the option to use the on-premises HSM to store keys.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an HSM connection and client certificate for the on-premises HSM. Enable HSM encryption on the existing unencrypted cluster by modifying the cluster. Connect to the VPC where the Amazon Redshift cluster resides from the on-premises network using a VPN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replica of the on-premises HSM in AWS CloudHSM. Launch a cluster in a VPC with the option to use CloudHSM to store keys."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-19T23:59:00.000Z",
        "voteCount": 18,
        "content": "I will go with B\nA- Wrong because on-premise HCM is required\nC-  Encryption can not be enabled on the existing unencrypted cluster\nD - Redshift doesn't support AWS CloudHSM."
      },
      {
        "date": "2021-11-04T03:09:00.000Z",
        "voteCount": 8,
        "content": "Answer: B"
      },
      {
        "date": "2021-11-16T22:17:00.000Z",
        "voteCount": 1,
        "content": "Could you please explain why B is the best option? I See this link - https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html but it's not clear as to whether or not an On premises HSM can be used? - \"Amazon Redshift supports only AWS CloudHSM Classic. We don't support the newer AWS CloudHSM service.\""
      },
      {
        "date": "2021-11-23T00:27:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html\n\nAmazon Redshift supports management of encryption keys in external hardware security modules (HSMs). The HSM can be on-premises or can be AWS CloudHSM."
      },
      {
        "date": "2021-11-22T07:17:00.000Z",
        "voteCount": 2,
        "content": "From my understanding AWS CloudHSM Classic is an old service and AWS CloudHSM is newer. So, Redshift supports old AWS CloudHSM Classic or on-premise HSM:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html\n\"The HSM can be on-premises or can be AWS CloudHSM. When you use an HSM, you must use client and server certificates to configure a trusted connection between Amazon Redshift and your HSM. Amazon Redshift supports only AWS CloudHSM Classic for key management.\"\nAgree the answer should be B"
      },
      {
        "date": "2023-05-01T12:47:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-26T03:17:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2022-10-28T04:55:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B as Redshift can be configured to use on-premises HSM using a VPN connection. \nOption A is wrong as AWS Classic CloudHSM does not meet the on-premises HSM requirement.\n\nOption C is wrong as enabling encryption on an existing cluster cannot be done with HSM.\n\nYou can enable encryption when you launch your cluster, or you can modify an unencrypted cluster to use AWS Key Management Service (AWS KMS) encryption. To do so, you can use either an AWS-managed key or a customer managed key. When you modify your cluster to enable AWS KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster. Snapshots created from the encrypted cluster are also encrypted\n\nOption D is wrong as Redshift only supports classic CloudHSM."
      },
      {
        "date": "2022-07-19T23:35:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-04-27T05:56:00.000Z",
        "voteCount": 1,
        "content": "B - https://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html"
      },
      {
        "date": "2022-01-29T01:10:00.000Z",
        "voteCount": 3,
        "content": "C is wrong \nBecause \u201cWhen you modify your cluster to enable AWS KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster.\u201d"
      },
      {
        "date": "2021-12-11T05:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is C."
      },
      {
        "date": "2021-12-11T14:13:00.000Z",
        "voteCount": 1,
        "content": "You can't enable hardware security module (HSM) encryption by modifying the cluster. Instead, create a new, HSM-encrypted cluster and migrate your data to the new cluste"
      },
      {
        "date": "2021-11-15T16:41:00.000Z",
        "voteCount": 1,
        "content": "A : [Amazon Redshift supports only AWS CloudHSM Classic. We don't support the newer AWS CloudHSM service.] It looks like this is outdated question. see the article:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with-HSM"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/amazon/view/64707-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A hospital is building a research data lake to ingest data from electronic health records (EHR) systems from multiple hospitals and clinics. The EHR systems are independent of each other and do not have a common patient identifier. The data engineering team is not experienced in machine learning (ML) and has been asked to generate a unique patient identifier for the ingested records.<br>Which solution will accomplish this task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn AWS Glue ETL job with the FindMatches transform\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Kendra",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon SageMaker Ground Truth",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn AWS Glue ETL job with the ResolveChoice transform"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-16T01:39:00.000Z",
        "voteCount": 11,
        "content": "You can now use AWS Glue to find matching records across a dataset (including ones without identifiers) by using the new FindMatches ML Transform, a custom machine learning transformation that helps you identify matching records. By adding the FindMatches transformation to your Glue ETL jobs, you can find related products, places, suppliers, customers, and more.\n\nYou can also use the FindMatches transformation for deduplication, such as to identify customers who have signed up more than once, products that have accidentally been added to your product catalog more than once, and so forth. You can teach the FindMatches ML Transform your definition of a \u201cduplicate\u201d through examples, and it will use machine learning to identify other potential duplicates in your dataset."
      },
      {
        "date": "2024-01-16T00:09:00.000Z",
        "voteCount": 2,
        "content": "I also agree with A. But I have a doubt about whether it can \"generate\" a unique identifier."
      },
      {
        "date": "2023-05-01T12:48:00.000Z",
        "voteCount": 3,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-10-28T04:57:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A as Glue can be used to perform matching across data stores using the FinMatches API. \nhttps://docs.aws.amazon.com/glue/latest/dg/machine-learning.html\nOption B is wrong as Amazon Kendra is an intelligent search service powered by machine learning (ML). Kendra reimagines enterprise search for your websites and applications so your employees and customers can easily find the content they\u2019re looking for, even when it\u2019s scattered across multiple locations and content repositories within your organization.\n\nOption C is wrong as Amazon SageMaker Ground Truth is a data labeling service that makes it easy to label data and gives you the option to use human annotators through Amazon Mechanical Turk.\n\nOption D is wrong as ResolveChoice helps resolve a choice type within a DynamicFrame. It is ideal for format changes."
      },
      {
        "date": "2022-08-03T21:42:00.000Z",
        "voteCount": 2,
        "content": "A is the answer"
      },
      {
        "date": "2022-07-25T21:56:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2021-11-29T20:28:00.000Z",
        "voteCount": 2,
        "content": "A is the answer"
      },
      {
        "date": "2021-11-15T22:52:00.000Z",
        "voteCount": 3,
        "content": "Answer should be A as FindMatches completely addresses the use case in question"
      },
      {
        "date": "2021-11-15T13:40:00.000Z",
        "voteCount": 1,
        "content": "OPTION A: FindMatches ML: identify duplicate or matching records in your dataset,\neven when the records do not have a common unique identifier and no fields\nmatch exactly."
      },
      {
        "date": "2021-11-09T07:29:00.000Z",
        "voteCount": 4,
        "content": "Option A - https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/"
      },
      {
        "date": "2021-09-27T20:08:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\nhttps://aws.amazon.com/kendra/"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/amazon/view/74178-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is Running Apache Spark on an Amazon EMR cluster. The Spark job writes to an Amazon S3 bucket. The job fails and returns an HTTP 503 `Slow<br>Down` AmazonS3Exception error.<br>Which actions will resolve this error? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd additional prefixes to the S3 bucket\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of prefixes in the S3 bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the EMR File System (EMRFS) retry limit\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable dynamic partition pruning in the Spark configuration for the cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more partitions in the Spark configuration for the cluster"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-07-27T23:11:00.000Z",
        "voteCount": 8,
        "content": "There are three ways to resolve this problem:\n\nAdd more prefixes to the S3 bucket.\nReduce the number of Amazon S3 requests.\nIncrease the EMR File System (EMRFS) retry limit."
      },
      {
        "date": "2022-08-02T21:27:00.000Z",
        "voteCount": 8,
        "content": "A - CORRECT, limit of S3 are defined on a per-prefix basis. ( https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html , \"3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned prefix\" ). If you \"add\" more prefixes, meaning you change the logic to make tasks write to \"less shared\" prefixes, then the limit is less likely to be hit.\nB - WRONG, limit of S3 are defined on a per-prefix basis. If you reduce the prefixes, then the limit is more likely to be hit.\nC - CORRECT, EMRFS uses an exponential backoff strategy to retry requests to Amazon S3 with default value 15. To increase the retry limit, change the value of fs.s3.maxRetries parameter. ( https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ )\nD - WRONG, dynamic partition pruning decreases the number of requests to S3, because it helps select which prefixes to read\nE - WRONG, increasing the partitions increases the number of Spark tasks, hence the number of write requests to S3"
      },
      {
        "date": "2023-05-01T12:50:00.000Z",
        "voteCount": 2,
        "content": "AC: I passed the test"
      },
      {
        "date": "2023-03-26T03:24:00.000Z",
        "voteCount": 1,
        "content": "AC for me"
      },
      {
        "date": "2022-04-22T16:11:00.000Z",
        "voteCount": 2,
        "content": "A C\nhttps://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/\n\n\nThis error occurs when you exceed the Amazon Simple Storage Service (Amazon S3) request rate. The request rate is 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket.\n\nThere are three ways to resolve this problem:\n\n    Add more prefixes to the S3 bucket.\n    Reduce the number of Amazon S3 requests.\n    Increase the EMR File System (EMRFS) retry limit."
      },
      {
        "date": "2022-05-12T03:38:00.000Z",
        "voteCount": 1,
        "content": "Agree AC"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/amazon/view/74200-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company recently created a test AWS account to use for a development environment. The company also created a production AWS account in another AWS<br>Region. As part of its security testing, the company wants to send log data from Amazon CloudWatch Logs in its production account to an Amazon Kinesis data stream in its test account.<br>Which solution will allow the company to accomplish this goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a subscription filter in the production account's CloudWatch Logs to target the Kinesis data stream in the test account as its destination. In the test account, create an IAM role that grants access to the Kinesis data stream and the CloudWatch Logs resources in the production account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the test account, create an IAM role that grants access to the Kinesis data stream and the CloudWatch Logs resources in the production account. Create a destination data stream in Kinesis Data Streams in the test account with an IAM role and a trust policy that allow CloudWatch Logs in the production account to write to the test account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the test account, create an IAM role that grants access to the Kinesis data stream and the CloudWatch Logs resources in the production account. Create a destination data stream in Kinesis Data Streams in the test account with an IAM role and a trust policy that allow CloudWatch Logs in the production account to write to the test account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination data stream in Kinesis Data Streams in the test account with an IAM role and a trust policy that allow CloudWatch Logs in the production account to write to the test account. Create a subscription filter in the production account's CloudWatch Logs to target the Kinesis data stream in the test account as its destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-25T06:26:00.000Z",
        "voteCount": 8,
        "content": "Why is B and C the exact same answer?"
      },
      {
        "date": "2022-05-19T08:22:00.000Z",
        "voteCount": 6,
        "content": "Option A doesn't have Trust Policy , so its D"
      },
      {
        "date": "2023-08-14T06:11:00.000Z",
        "voteCount": 3,
        "content": "correct answer is D , Here is the Link\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
      },
      {
        "date": "2023-05-01T12:51:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-26T03:29:00.000Z",
        "voteCount": 1,
        "content": "selected D"
      },
      {
        "date": "2022-07-27T23:16:00.000Z",
        "voteCount": 4,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-04-23T00:05:00.000Z",
        "voteCount": 4,
        "content": "Options B &amp; C do not mention the use of subscription filters so they are eliminated.\nOption A does not mention Trust policy which is a required step in the link below.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CreateDestination.html"
      },
      {
        "date": "2022-04-23T00:05:00.000Z",
        "voteCount": 1,
        "content": "Answer = D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/amazon/view/74024-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data architect is building an Amazon S3 data lake for a bank. The goal is to provide a single data repository for customer data needs, such as personalized recommendations. The bank uses Amazon Kinesis Data Firehose to ingest customers' personal information bank accounts, and transactions in near-real time from a transactional relational database. The bank requires all personally identifiable information (PII) that is stored in the AWS Cloud to be masked.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Lambda function from Kinesis Data Firehose to mask PII before delivering the data into Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Made, and configure it to discover and mask PII.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable server-side encryption (SSE) in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke Amazon Comprehend from Kinesis Data Firehose to detect and mask PII before delivering the data into Amazon S3."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-05T05:28:00.000Z",
        "voteCount": 11,
        "content": "A - CORRECT, Lambda function is used as on-the-fly masking due to the requirement \"All PII on the AWS Cloud must be hidden\"\nB - WRONG, the requirement \"All PII on the AWS Cloud must be hidden\" imposes that masking must be performed on-the-fly, while Macie can be applied to data already in S3. https://aws.amazon.com/blogs/big-data/automate-the-archival-and-deletion-of-sensitive-data-using-amazon-macie/\nC - WRONG, encryption != masking\nD - WRONG, Amazon Comprehend PII API has no integration with KDF"
      },
      {
        "date": "2023-07-11T01:37:00.000Z",
        "voteCount": 2,
        "content": "\"that is stored in the AWS Cloud to be masked\" is not on the fly\nI am still between A and B."
      },
      {
        "date": "2023-08-14T06:39:00.000Z",
        "voteCount": 1,
        "content": "D : Correct answer and we do have intergration \nhttps://aws.amazon.com/blogs/machine-learning/redact-sensitive-data-from-streaming-data-in-near-real-time-using-amazon-comprehend-and-amazon-kinesis-data-firehose/"
      },
      {
        "date": "2023-05-01T12:51:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-08T18:45:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/machine-learning/redact-sensitive-data-from-streaming-data-in-near-real-time-using-amazon-comprehend-and-amazon-kinesis-data-firehose/\n\nFrom the link above, A makes more sense."
      },
      {
        "date": "2023-01-08T04:03:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/machine-learning/protect-pii-using-amazon-s3-object-lambda-to-process-and-modify-data-during-retrieval/"
      },
      {
        "date": "2023-07-11T01:39:00.000Z",
        "voteCount": 2,
        "content": "Yes it should be A. But they should also write the question better. As \"that is stored in the AWS Cloud to be masked\" is not clear. Be masked before or could be after stored."
      },
      {
        "date": "2022-12-19T13:22:00.000Z",
        "voteCount": 1,
        "content": "A https://aws.amazon.com/blogs/machine-learning/protect-pii-using-amazon-s3-object-lambda-to-process-and-modify-data-during-retrieval/"
      },
      {
        "date": "2022-12-18T21:09:00.000Z",
        "voteCount": 2,
        "content": "A might be the answer. Amazon Comprehend is used for unstructured data."
      },
      {
        "date": "2022-10-25T05:23:00.000Z",
        "voteCount": 1,
        "content": "the source is a relational database, so you can get to know which column has PII and need masked. So A. And aws comprehend is for text"
      },
      {
        "date": "2022-09-14T11:19:00.000Z",
        "voteCount": 3,
        "content": "Between A and D, don't think Comprehend is invokable by KDF, and shouldn't be used for structured data (at least not efficient), hence A"
      },
      {
        "date": "2022-09-06T08:02:00.000Z",
        "voteCount": 3,
        "content": "A Correct.  Amazon Comprehend is used for unstructured data. Apparently, not the case here"
      },
      {
        "date": "2022-08-06T13:53:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-12-18T21:08:00.000Z",
        "voteCount": 1,
        "content": "Given the use-case, i guess A could be a better option."
      },
      {
        "date": "2022-07-24T05:52:00.000Z",
        "voteCount": 1,
        "content": "D \nhttps://aws.amazon.com/about-aws/whats-new/2020/09/amazon-comprehend-helps-mask-personally-identifiable-information-from-text-documents/"
      },
      {
        "date": "2022-06-24T04:37:00.000Z",
        "voteCount": 4,
        "content": "I feel like there is a mistake in this question. Without looking at the answers I would choose to use Amazon Macie. Maybe Answer B was supposed to say Amazon Macie instead of Amazon Made?? Amazon Made is not a real service. \n\nIf Macie is not an option I would choose A. Because just using encryption is not hiding PII and I don't think it is possible invoke Amazon Comprehend from FireHose."
      },
      {
        "date": "2023-05-21T07:45:00.000Z",
        "voteCount": 1,
        "content": "That's what I was thinking, it should be Amazon MACIE"
      },
      {
        "date": "2022-05-12T07:23:00.000Z",
        "voteCount": 1,
        "content": "A -  Kinesis Firehorse invoke lambda"
      },
      {
        "date": "2022-04-25T03:13:00.000Z",
        "voteCount": 2,
        "content": "B - I think we can use aws Maice"
      },
      {
        "date": "2022-04-23T13:05:00.000Z",
        "voteCount": 2,
        "content": "D - Would go for this as Amazon Comprehend can mask PII - https://docs.aws.amazon.com/comprehend/latest/dg/pii.html"
      },
      {
        "date": "2022-04-23T00:47:00.000Z",
        "voteCount": 1,
        "content": "I think answer = A\nData encryption and masking are not the same thing"
      },
      {
        "date": "2022-04-28T13:20:00.000Z",
        "voteCount": 1,
        "content": "Changed to D\n\nhttps://medium.com/fernando-pereiro/analyzing-twitter-on-real-time-with-aws-big-data-and-machine-learning-services-1fa888f962cf"
      },
      {
        "date": "2022-08-22T00:05:00.000Z",
        "voteCount": 1,
        "content": "but the comprehend is not in the KDF as D states"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/amazon/view/74012-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An analytics software as a service (SaaS) provider wants to offer its customers business intelligence (BI) reporting capabilities that are self-service. The provider is using Amazon QuickSight to build these reports. The data for the reports resides in a multi-tenant database, but each customer should only be able to access their own data.<br>The provider wants to give customers two user role options:<br>\u2711 Read-only users for individuals who only need to view dashboards.<br>\u2711 Power users for individuals who are allowed to create and share new dashboards with other users.<br>Which QuickSught feature allows the provider to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbedded dashboards",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable calculations",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIsolated namespaces\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSPICE"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T09:12:00.000Z",
        "voteCount": 1,
        "content": "C, Namespaces are logical containers to achieve multi-tenancy."
      },
      {
        "date": "2023-05-01T12:52:00.000Z",
        "voteCount": 3,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-20T23:13:00.000Z",
        "voteCount": 4,
        "content": "The QuickSight feature that allows the provider to meet these requirements is C. Isolated namespaces. Isolated namespaces provide a way to isolate the data sources and user permissions for different customers or tenants in a multi-tenant environment. With isolated namespaces, the provider can ensure that each customer only has access to their own data, while still allowing them to create and share dashboards if they are power users. Isolated namespaces also allow for fine-grained control over user permissions, so the provider can grant read-only access to some users and power user access to others."
      },
      {
        "date": "2022-12-18T21:12:00.000Z",
        "voteCount": 1,
        "content": "Amazon QuickSight Enterprise edition supports multitenancy through namespaces."
      },
      {
        "date": "2022-12-12T05:10:00.000Z",
        "voteCount": 2,
        "content": "C: Amazon QuickSight Enterprise edition supports multitenancy through namespaces. A QuickSight namespace is a logical container that you can use to organize clients, subsidiaries, \nteams, and so on. Namespaces can help you achieve the following goals:\nYou can allow the users of your QuickSight subscription to discover shared content and share with other users. At the same time, you can be sure that users in one namespace \ncan't see or interact with users in another namespace.\nYou can securely isolate data and also support diverse workloads without adding additional AWS accounts."
      },
      {
        "date": "2022-12-13T07:08:00.000Z",
        "voteCount": 1,
        "content": "C https://docs.aws.amazon.com/quicksight/latest/user/namespaces.html"
      },
      {
        "date": "2022-07-25T21:37:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2022-12-18T21:11:00.000Z",
        "voteCount": 2,
        "content": "Amazon QuickSight Enterprise edition supports multi-tenancy through namespaces."
      },
      {
        "date": "2022-05-21T04:56:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      },
      {
        "date": "2022-04-23T02:10:00.000Z",
        "voteCount": 1,
        "content": "Answer = A\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/embedded-analytics-dashboards-for-everyone.html"
      },
      {
        "date": "2022-04-28T10:33:00.000Z",
        "voteCount": 1,
        "content": "chnged to C - namespaces"
      },
      {
        "date": "2022-04-22T20:32:00.000Z",
        "voteCount": 4,
        "content": "C - https://docs.aws.amazon.com/quicksight/latest/user/namespaces.html"
      },
      {
        "date": "2022-04-21T05:14:00.000Z",
        "voteCount": 1,
        "content": "It's A.\nhttps://docs.aws.amazon.com/quicksight/latest/user/namespaces.html"
      },
      {
        "date": "2022-04-23T13:11:00.000Z",
        "voteCount": 3,
        "content": "How is the answer A if you're referring to the Isolated namespace?"
      },
      {
        "date": "2022-11-21T03:20:00.000Z",
        "voteCount": 1,
        "content": "the web must have changed the order of options before. So some people choosed A which was namespace at that time."
      },
      {
        "date": "2022-04-23T17:01:00.000Z",
        "voteCount": 2,
        "content": "You mean C - isolated namespace ?"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/amazon/view/74206-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is providing analytics services to its sales and marketing departments. The departments can access the data only through their business intelligence<br>(BI) tools, which run queries on Amazon Redshift using an Amazon Redshift internal user to connect. Each department is assigned a user in the Amazon Redshift database with the permissions needed for that department. The marketing data analysts must be granted direct access to the advertising table, which is stored in<br>Apache Parquet format in the marketing S3 bucket of the company data lake. The company data lake is managed by AWS Lake Formation. Finally, access must be limited to the three promotion columns in the table.<br>Which combination of steps will meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant permissions in Amazon Redshift to allow the marketing Amazon Redshift user to access the three promotion columns of the advertising external table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Redshift Spectrum IAM role with permissions for Lake Formation. Attach it to the Amazon Redshift cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Redshift Spectrum IAM role with permissions for the marketing S3 bucket. Attach it to the Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external schema in Amazon Redshift by using the Amazon Redshift Spectrum IAM role. Grant usage to the marketing Amazon Redshift user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant permissions in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns of the advertising table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant permissions in Lake Formation to allow the marketing IAM group to access the three promotion columns of the advertising table."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "CDE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CDF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-17T10:56:00.000Z",
        "voteCount": 6,
        "content": "BDE makes the most sense to me"
      },
      {
        "date": "2023-02-07T21:03:00.000Z",
        "voteCount": 4,
        "content": "Lake Formation provides the security and governance of the Data Catalog. Within Lake Formation, you can grant and revoke permissions to the Data Catalog objects, such as databases, tables, columns, and underlying Amazon S3 storage. hence B over C."
      },
      {
        "date": "2023-03-27T20:42:00.000Z",
        "voteCount": 1,
        "content": "But option B doesn't mention anything about the permission to S3."
      },
      {
        "date": "2023-08-06T09:20:00.000Z",
        "voteCount": 1,
        "content": "Lake Formation handles the permissions to S3 once the buckets are registered in the datalake."
      },
      {
        "date": "2023-12-22T01:59:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C, D, F\nCreate an IAM role with the necessary permissions to access the S3 bucket and assume this role in Amazon Redshift.** This role should have permissions to access the specific S3 bucket where the Parquet files are stored. You can then associate this role with Amazon Redshift, allowing Redshift to access the data in the S3 bucket.\nCreate an external schema in Amazon Redshift pointing to the data lake.** This allows Redshift to access the data in the data lake. You can use the `CREATE EXTERNAL SCHEMA` SQL command in Redshift to do this.\nGrant the necessary permissions in AWS Lake Formation.** You need to grant the marketing data analysts the necessary permissions to the specific columns in the table. You can do this in the AWS Lake Formation console or using the AWS CLI."
      },
      {
        "date": "2023-05-01T12:54:00.000Z",
        "voteCount": 3,
        "content": "BDE: I passed the test"
      },
      {
        "date": "2023-04-02T01:29:00.000Z",
        "voteCount": 2,
        "content": "I think it is BDE\nLake formation will be used to grant access for redshift spectrum to the s3 bucket"
      },
      {
        "date": "2023-02-13T15:03:00.000Z",
        "voteCount": 3,
        "content": "To meet the requirements, the combination of the following steps should be taken:\nC. Create an Amazon Redshift Spectrum IAM role with permissions for the marketing S3 bucket. Attach it to the Amazon Redshift cluster.\nD. Create an external schema in Amazon Redshift by using the Amazon Redshift Spectrum IAM role. Grant usage to the marketing Amazon Redshift user.\nE. Grant permissions in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns of the advertising table.\n\nThese steps would grant the necessary permissions for the marketing Amazon Redshift user to access the data stored in the marketing S3 bucket through Amazon Redshift Spectrum, while limiting access to only the three promotion columns of the advertising table. The Amazon Redshift Spectrum IAM role with the necessary permissions for the marketing S3 bucket would be attached to the Amazon Redshift cluster, and then an external schema would be created in Amazon Redshift using that role. Finally, the necessary permissions would be granted in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns."
      },
      {
        "date": "2023-02-05T21:53:00.000Z",
        "voteCount": 2,
        "content": "Role access to S3 buckets are required hence C over B. \nD &amp; E anyways are correct."
      },
      {
        "date": "2022-04-23T02:26:00.000Z",
        "voteCount": 2,
        "content": "Agreed BDE"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/amazon/view/73987-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A web retail company wants to implement a near-real-time clickstream analytics solution. The company wants to analyze the data with an open-source package.<br>The analytics application will process the raw data only once, but other applications will need immediate access to the raw data for up to 1 year.<br>Which solution meets these requirements with the LEAST amount of operational effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to collect the data. Use Amazon EMR with Apache Flink to consume and process the data from the Kinesis data stream. Set the retention period of the Kinesis data stream to 8.760 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to collect the data. Use Amazon Kinesis Data Analytics with Apache Flink to process the data in real time. Set the retention period of the Kinesis data stream to 8,760 hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Streaming for Apache Kafka (Amazon MSK) to collect the data. Use Amazon EMR with Apache Flink to consume and process the data from the Amazon MSK stream. Set the log retention hours to 8,760.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to collect the data. Use Amazon EMR with Apache Flink to consume and process the data from the Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Set an S3 Lifecycle policy to delete the data after 365 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T01:37:00.000Z",
        "voteCount": 10,
        "content": "store data in S3 not in Kinesis"
      },
      {
        "date": "2022-05-06T03:49:00.000Z",
        "voteCount": 8,
        "content": "The question requires to store \"raw data\" for 1 year, but for the \"processed data\"."
      },
      {
        "date": "2022-04-23T13:22:00.000Z",
        "voteCount": 8,
        "content": "A &amp; B out as you cannot store data for more than 7 days in KDS.\nC - Possibly but won't be cost-effective.\nD - Cost-Effective with least amount of operational overhead."
      },
      {
        "date": "2022-04-23T13:24:00.000Z",
        "voteCount": 1,
        "content": "Plus C only stores the log data, not the data itself."
      },
      {
        "date": "2022-04-30T16:04:00.000Z",
        "voteCount": 12,
        "content": "KDS can store 1 Year data:\nhttps://aws.amazon.com/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/"
      },
      {
        "date": "2023-08-06T09:52:00.000Z",
        "voteCount": 4,
        "content": "B is the right answer, Kinesis Data Stream can keep raw data up to 365 days.\nAnytime they ask about click-stream analytics it should go to Kinesis Data Analytics."
      },
      {
        "date": "2023-05-01T12:54:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-24T07:09:00.000Z",
        "voteCount": 2,
        "content": "analytics application will process the raw data only once so why store in S3?"
      },
      {
        "date": "2023-03-21T01:28:00.000Z",
        "voteCount": 6,
        "content": "The solution that meets the requirements with the least amount of operational effort is Option B: Use Amazon Kinesis Data Streams to collect the data. Use Amazon Kinesis Data Analytics with Apache Flink to process the data in real-time. Set the retention period of the Kinesis data stream to 8,760 hours.\n\nThis solution uses Amazon Kinesis Data Streams to collect the data and processes it in real-time using Amazon Kinesis Data Analytics with Apache Flink. This allows for near-real-time clickstream analytics without the need for additional data processing or storage. Additionally, the retention period of the Kinesis data stream can be set to 8,760 hours (1 year), which allows other applications to have immediate access to the raw data for up to 1 year without the need for additional storage or processing. This solution requires the least amount of operational effort as it does not require additional steps for data processing or storage."
      },
      {
        "date": "2023-03-18T05:06:00.000Z",
        "voteCount": 2,
        "content": "For those debating B and D, I am going with B. Least operational overhead, and the giveway is the hours : \"The maximum value of a stream's retention period is 8760 hours (365 days).\" https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html.   Be wary of the attempt in the question to use 8.760 hours which i think is sneaky."
      },
      {
        "date": "2023-02-14T19:52:00.000Z",
        "voteCount": 2,
        "content": "Focus on \"LEAST amount of operational effort\" so you should eliminate EMR from the picture. D recommends EMR and it is operational overhead. \n\nB - make sense. Since it can keep data for 365 days and there is no operational overhead."
      },
      {
        "date": "2023-01-11T10:27:00.000Z",
        "voteCount": 2,
        "content": "It says the raw data need to be accessed by other applications, if we put in the Kinesis Stream, how other application can access it? Besides, KDS is not good for store data. S3 is the right choice."
      },
      {
        "date": "2022-12-13T07:27:00.000Z",
        "voteCount": 1,
        "content": "A Kinesis data stream stores records for 24 hours by default, up to 365 days (8,760 hours). \nhttps://aws.amazon.com/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/"
      },
      {
        "date": "2023-01-09T13:58:00.000Z",
        "voteCount": 1,
        "content": "KDS can keep between 24 hours to 7 days."
      },
      {
        "date": "2023-03-18T05:07:00.000Z",
        "voteCount": 1,
        "content": "Incorrect. https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html"
      },
      {
        "date": "2022-12-13T05:03:00.000Z",
        "voteCount": 1,
        "content": "C. Using Amazon Managed Streaming for Apache Kafka (Amazon MSK) to collect the data, Amazon EMR with Apache Flink to process the data, and setting the log retention hours to 8,760 would meet the requirements with the least amount of operational effort. Amazon MSK is a fully managed service that makes it easy to set up, maintain, and scale Apache Kafka clusters. Amazon EMR can be used to process data from an Amazon MSK stream in real time, and the log retention hours can be set to 8,760 to retain the data for up to 1 year. This solution would require minimal effort to set up and maintain, and would allow other applications to access the raw data for up to 1 year."
      },
      {
        "date": "2022-12-04T06:23:00.000Z",
        "voteCount": 3,
        "content": "B for {{  LEAST amount of operational effort  }}"
      },
      {
        "date": "2022-12-13T07:27:00.000Z",
        "voteCount": 1,
        "content": "A Kinesis data stream stores records for 24 hours by default, up to 365 days (8,760 hours). \nhttps://aws.amazon.com/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/"
      },
      {
        "date": "2022-11-08T16:55:00.000Z",
        "voteCount": 1,
        "content": "I vote for B. I think C with EMR requires more operational effort."
      },
      {
        "date": "2022-10-25T05:40:00.000Z",
        "voteCount": 1,
        "content": "KDS can keep the data for one year."
      },
      {
        "date": "2022-08-17T07:13:00.000Z",
        "voteCount": 3,
        "content": "Answer is B, KDS can natively store raw data for up to 1 year"
      },
      {
        "date": "2022-07-21T19:55:00.000Z",
        "voteCount": 1,
        "content": "Answer-B"
      },
      {
        "date": "2022-12-18T21:28:00.000Z",
        "voteCount": 1,
        "content": "Kinesis data stream stores records for 24 hours by default, up to 365 days (8,760 hours)."
      },
      {
        "date": "2022-07-15T05:40:00.000Z",
        "voteCount": 2,
        "content": "Retention and Minimal Operational \nKDS(8760 hours (365 days)) + KDA(Flink available out of the box) &gt;&gt; KDS + EMR with Flink + S3"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/amazon/view/74208-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data analyst runs a large number of data manipulation language (DML) queries by using Amazon Athena with the JDBC driver. Recently, a query failed after it ran for 30 minutes. The query returned the following message: java.sql.SQLException: Query timeout<br>The data analyst does not immediately need the query results. However, the data analyst needs a long-term solution for this problem.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the query into smaller queries to search smaller subsets of data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the settings for Athena, adjust the DML query timeout limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Service Quotas console, request an increase for the DML query timeout\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the tables as compressed .csv files"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-21T01:51:00.000Z",
        "voteCount": 10,
        "content": "Not sure if C fulfills the \"long term solution\" part of the question. In the worst case, he needs to adjust it every time the data grows and the query takes again longer. What about A?"
      },
      {
        "date": "2023-08-06T10:13:00.000Z",
        "voteCount": 5,
        "content": "Right answer is A but exam answer is C, they want you to know about service quotas."
      },
      {
        "date": "2023-11-05T22:33:00.000Z",
        "voteCount": 1,
        "content": "Default Athena\u2019s max query time out minute is 30min.\nso we needs to up Service Quoata."
      },
      {
        "date": "2023-10-09T07:19:00.000Z",
        "voteCount": 1,
        "content": "I believe the best choice is A. From the given information, we aren't clear on how much we can extend the timeout\u2014would 60 minutes suffice? It's uncertain. Increasing timeouts without a clear rationale isn't a recommended approach, as there are costs associated with prolonged query times, both in terms of resources and expenses.\n\nOperational Best Practices: Increasing timeouts without understanding the actual need can be considered a \"band-aid\" solution. It might temporarily solve the symptom (i.e., timeout) but not the underlying problem causing the delay.:)"
      },
      {
        "date": "2023-05-17T05:36:00.000Z",
        "voteCount": 3,
        "content": "C is not a sustainable solution. So A is correct"
      },
      {
        "date": "2023-05-01T12:55:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-04-01T16:13:00.000Z",
        "voteCount": 2,
        "content": "A seems to be the long term fix here. I go with A"
      },
      {
        "date": "2023-03-26T04:10:00.000Z",
        "voteCount": 1,
        "content": "why not D?"
      },
      {
        "date": "2023-03-21T01:49:00.000Z",
        "voteCount": 4,
        "content": "Answer is C:\nBy default, the maximum timeout limit for a single query execution in Athena is 30 minutes. If the query requires more than 30 minutes to complete, it can be terminated with a \"Query timeout\" error.\n\nSplitting the query into smaller queries, as mentioned in option A, can work but may be time-consuming and not a scalable solution for large datasets. Requesting an increase in the DML query timeout via the Service Quotas console, as mentioned in option C, can provide a more scalable and long-term solution to the problem.\n\nOption D, saving tables as compressed .csv files, is not a solution to the problem described in the scenario. It will not address the query timeout issue and does not provide a long-term solution for the problem."
      },
      {
        "date": "2023-03-18T07:39:00.000Z",
        "voteCount": 1,
        "content": "I will go with A as that appears to be the long term fix. C talks about increasing the timeout but there is no mention of increase by what extent"
      },
      {
        "date": "2023-02-26T11:20:00.000Z",
        "voteCount": 1,
        "content": "The 30-minute DDL query limit is a soft limit, and an increase can be requested in the Service Quotas console."
      },
      {
        "date": "2023-02-09T00:28:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://docs.aws.amazon.com/athena/latest/ug/service-limits.html"
      },
      {
        "date": "2022-10-08T18:08:00.000Z",
        "voteCount": 3,
        "content": "For A, as there\u2019re a large number of queries that need to be split, the effort is huge. Some queries may still need a longer time to complete, and it\u2019s OK for the data analyst to wait for them.  So increasing the timeout limit is the long-term solution. I go with C."
      },
      {
        "date": "2022-10-09T01:25:00.000Z",
        "voteCount": 2,
        "content": "It says \"a query failed\" so effort is not to split many queries but one. Since \"data analyst\" is running the query, splitting query is also possible. Not sure between A or C but cannot rule out A."
      },
      {
        "date": "2022-07-20T23:33:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-05-21T05:49:00.000Z",
        "voteCount": 4,
        "content": "A looks correct option. timeout could occur if the spool size of a query is too much an spice is not able to generate output in specified time. Increasing timeout is not a long term solution as other queries may continue to timeout with such large data sets. So better to limit the scope by splitting the queries for specific limited dataset even if that means running query multiple times."
      },
      {
        "date": "2022-04-30T14:31:00.000Z",
        "voteCount": 1,
        "content": "Answer - C"
      },
      {
        "date": "2022-04-23T13:35:00.000Z",
        "voteCount": 3,
        "content": "C - Per amazon doc - https://docs.aws.amazon.com/athena/latest/ug/service-limits.html"
      },
      {
        "date": "2022-04-23T13:36:00.000Z",
        "voteCount": 2,
        "content": "The quota page specifically shows 30 mins max for DML if an increase isn't requested.\nhttps://docs.aws.amazon.com/general/latest/gr/athena.html#amazon-athena-limits"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/amazon/view/74209-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company is using an Amazon S3 bucket to host an ecommerce data lake. The company is using AWS Lake Formation to manage the data lake.<br>A data analytics specialist must provide access to a new business analyst team. The team will use Amazon Athena from the AWS Management Console to query data from existing web_sales and customer tables in the ecommerce database. The team needs read-only access and the ability to uniquely identify customers by using first and last names. However, the team must not be able to see any other personally identifiable data. The table structure is as follows:<br><img src=\"/assets/media/exam-media/04144/0008100001.png\" class=\"in-exam-image\"><br>Which combination of steps should the data analytics specialist take to provide the required permission by using the principle of least privilege? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS Lake Formation, grant the business_analyst group SELECT and ALTER permissions for the web_sales table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS Lake Formation, grant the business_analyst group the SELECT permission for the web_sales table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS Lake Formation, grant the business_analyst group the SELECT permission for the customer table. Under columns, choose filter type \u05d2\u20acInclude columns\u05d2\u20ac with columns fisrt_name, last_name, and customer_id.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS Lake Formation, grant the business_analyst group SELECT and ALTER permissions for the customer table. Under columns, choose filter type \u05d2\u20acInclude columns\u05d2\u20ac with columns fisrt_name and last_name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate users under a business_analyst IAM group. Create a policy that allows the lakeformation:GetDataAccess action, the athena:* action, and the glue:Get* action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate users under a business_analyst IAM group. Create a policy that allows the lakeformation:GetDataAccess action, the athena:* action, and the glue:Get* action. In addition, allow the s3:GetObject action, the s3:PutObject action, and the s3:GetBucketLocation action for the Athena query results S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-06-22T03:01:00.000Z",
        "voteCount": 8,
        "content": "It should be BCF. Athena always publishes the results of queries in a S3 bucket different from the source-bucket. To access the results you need permissions to this bucket. \nhttps://docs.aws.amazon.com/athena/latest/ug/querying.html \nQuote: To access and view query output files, IAM principals (users and roles) need permission to the Amazon S3 GetObject action for the query result location, as well as permission for the Athena GetQueryResults action"
      },
      {
        "date": "2022-08-02T12:14:00.000Z",
        "voteCount": 7,
        "content": "A - WRONG, ALTER grants write access to raw data.\nB - CORRECT, SELECT grants read access to the transaction data, where no customer information is present.\nC - CORRECT, Lake Formation allows specifying which columns are accessible. customer_id is needed in order to join with web_sales table.\nD - WRONG, ALTER grants write access to raw data.\nE - WRONG, s3:GetObject is needed to see Athena results ( https://docs.aws.amazon.com/athena/latest/ug/querying.html ).\nF - CORRECT, even though it's Athena that puts the objects in the bucket and not the IAM user itself, granting s3:PutObject on the Athena bucket doesn't provide for the original S3 bucket where raw data resides."
      },
      {
        "date": "2023-05-01T12:56:00.000Z",
        "voteCount": 2,
        "content": "BCF: I passed the test"
      },
      {
        "date": "2023-11-24T08:09:00.000Z",
        "voteCount": 3,
        "content": "In all the questions you mentioned you passed but please confirm if you  corrected any mistakes you had in the test?"
      },
      {
        "date": "2024-02-11T05:08:00.000Z",
        "voteCount": 3,
        "content": "pk349 has never given rationale behind the answers, but one has to admit that all the answers have consistently remained correct."
      },
      {
        "date": "2022-11-08T17:15:00.000Z",
        "voteCount": 1,
        "content": "F is definitely needed. Athena automatically saves query results in S3.\nhttps://docs.aws.amazon.com/athena/latest/ug/querying.html"
      },
      {
        "date": "2022-10-26T22:03:00.000Z",
        "voteCount": 1,
        "content": "BCF for sure. You need putobject permission to save athena query results."
      },
      {
        "date": "2022-04-23T03:22:00.000Z",
        "voteCount": 1,
        "content": "I think BCE.\nRead-only access mean no need for ALTER permission. No need to give write access (s3:putObject)"
      },
      {
        "date": "2022-04-29T05:58:00.000Z",
        "voteCount": 2,
        "content": "so the answer is BCF?"
      },
      {
        "date": "2022-05-22T01:40:00.000Z",
        "voteCount": 1,
        "content": "No its BCE\nF is granting Put...."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/amazon/view/74213-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has multiple data workflows to ingest data from its operational databases into its data lake on Amazon S3. The workflows use AWS Glue and Amazon<br>EMR for data processing and ETL. The company wants to enhance its architecture to provide automated orchestration and minimize manual intervention.<br>Which solution should the company use to manage the data workflows to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue workflows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Step Functions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Lambda",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Batch"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-23T13:45:00.000Z",
        "voteCount": 7,
        "content": "B - Step Functions\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-data-processing.html"
      },
      {
        "date": "2023-05-01T12:56:00.000Z",
        "voteCount": 1,
        "content": "S: I passed the test"
      },
      {
        "date": "2023-04-03T23:27:00.000Z",
        "voteCount": 1,
        "content": "B - AWS Step Funtions  \nhttps://aws.amazon.com/blogs/big-data/prepare-transform-and-orchestrate-your-data-using-aws-glue-databrew-aws-glue-etl-and-aws-step-functions/"
      },
      {
        "date": "2023-03-26T04:20:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2022-10-21T05:36:00.000Z",
        "voteCount": 1,
        "content": "B, AWS Step Functions"
      },
      {
        "date": "2022-09-27T13:11:00.000Z",
        "voteCount": 2,
        "content": "Answer : B \n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-data-processing.html\nDepending upon your data processing needs, Step Functions directly integrates with other data processing services provided by AWS such as AWS Batch for batch processing, Amazon EMR for big data processing, AWS Glue for data preparation, Athena for data analysis, and AWS Lambda for compute."
      },
      {
        "date": "2022-07-25T21:33:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-04-23T03:57:00.000Z",
        "voteCount": 3,
        "content": "I think A"
      },
      {
        "date": "2022-04-23T07:09:00.000Z",
        "voteCount": 5,
        "content": "Changed to B\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-emr.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/amazon/view/74215-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An online retail company is using Amazon Redshift to run queries and perform analytics on customer shopping behavior. When multiple queries are running on the cluster, runtime for small queries increases significantly. The company's data analytics team to decrease the runtime of these small queries by prioritizing them ahead of large queries.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift Spectrum for small queries",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the concurrency limit in workload management (WLM)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure short query acceleration in workload management (WLM)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a dedicated compute node for small queries"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-23T04:07:00.000Z",
        "voteCount": 8,
        "content": "C is ok"
      },
      {
        "date": "2022-04-23T14:03:00.000Z",
        "voteCount": 6,
        "content": "C - https://docs.aws.amazon.com/redshift/latest/dg/advisor-recommendations.html#enable-sqa-recommendation"
      },
      {
        "date": "2023-06-28T19:10:00.000Z",
        "voteCount": 1,
        "content": "C is ok. No doubts at all."
      },
      {
        "date": "2023-05-01T12:57:00.000Z",
        "voteCount": 2,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-03-26T04:19:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2022-07-21T17:19:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-06-16T02:49:00.000Z",
        "voteCount": 1,
        "content": "It is C"
      },
      {
        "date": "2022-04-30T14:48:00.000Z",
        "voteCount": 3,
        "content": "Answer - C"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/amazon/view/74484-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company uses Amazon Redshift as its data warehouse. A new table includes some columns that contain sensitive data and some columns that contain non- sensitive data. The data in the table eventually will be referenced by several existing queries that run many times each day.<br>A data analytics specialist must ensure that only members of the company's auditing team can read the columns that contain sensitive data. All other users must have read-only access to the columns that contain non-sensitive data.<br>Which solution will meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the auditing team permission to read from the table. Load the columns that contain non-sensitive data into a second table. Grant the appropriate users read-only permissions to the second table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant all users read-only permissions to the columns that contain non-sensitive data. Use the GRANT SELECT command to allow the auditing team to access the columns that contain sensitive data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant all users read-only permissions to the columns that contain non-sensitive data. Attach an IAM policy to the auditing team with an explicit. Allow action that grants access to the columns that contain sensitive data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the auditing team permission to read from the table. Create a view of the table that includes the columns that contain non-sensitive data. Grant the appropriate users read-only permissions to that view."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-28T05:40:00.000Z",
        "voteCount": 7,
        "content": "B - GRANT defines access privileges for a user or user group. https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html"
      },
      {
        "date": "2023-05-01T12:58:00.000Z",
        "voteCount": 3,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-10-20T08:26:00.000Z",
        "voteCount": 4,
        "content": "The question also says the least operational over head, that makes B the right answer."
      },
      {
        "date": "2022-12-13T07:58:00.000Z",
        "voteCount": 5,
        "content": "Views or AWS Lake Formation on Amazon Redshift Spectrum was used previously to manage such scenarios, however this adds extra overhead in creating and maintaining views or Amazon Redshift Spectrum. View based approach is also difficult to scale and can lead to lack of security controls. Amazon Redshift column-level access control is a new feature that supports access control at a column-level for data in Amazon Redshift. You can use column-level GRANT and REVOKE statements to help meet your security and compliance needs similar to managing any database object.\n\nhttps://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/"
      },
      {
        "date": "2022-07-20T23:26:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2022-06-08T10:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is  B"
      },
      {
        "date": "2022-05-22T01:48:00.000Z",
        "voteCount": 2,
        "content": "Answer should be B"
      },
      {
        "date": "2022-05-21T05:37:00.000Z",
        "voteCount": 1,
        "content": "It's B.  \nhttps://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/"
      },
      {
        "date": "2022-05-14T15:44:00.000Z",
        "voteCount": 3,
        "content": "Grant command give access to table or database level, not column level.\nView are selected projection of a table hence can be and shud be used here"
      },
      {
        "date": "2022-05-20T08:38:00.000Z",
        "voteCount": 2,
        "content": "Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column, https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html. so it is B"
      },
      {
        "date": "2022-08-11T15:02:00.000Z",
        "voteCount": 1,
        "content": "Grant command give access to table or database or column\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html"
      },
      {
        "date": "2022-08-17T22:25:00.000Z",
        "voteCount": 1,
        "content": "columm level is enabled according to https://aws.amazon.com/cn/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/"
      },
      {
        "date": "2022-04-30T14:20:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-04-25T08:30:00.000Z",
        "voteCount": 4,
        "content": "I think it's B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/amazon/view/74238-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company hosts an Apache Flink application on premises. The application processes data from several Apache Kafka clusters. The data originates from a variety of sources, such as web applications, mobile apps, and operational databases. The company has migrated some of these sources to AWS and now wants to migrate the Flink application. The company must ensure that data that resides in databases within the VPC does not traverse the internet. The application must be able to process all the data that comes from the company's AWS solution, on-premises resources, and the public internet.<br>Which solution will meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Flink on Amazon EC2 within the company's VPC. Create Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters in the VPC to collect data that comes from applications and databases within the VPC. Use Amazon Kinesis Data Streams to collect data that comes from the public internet. Configure Flink to have sources from Kinesis Data Streams Amazon MSK, and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Flink on Amazon EC2 within the company's VPC. Use Amazon Kinesis Data Streams to collect data that comes from applications and databases within the VPC and the public internet. Configure Flink to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Analytics application by uploading the compiled Flink .jar file. Use Amazon Kinesis Data Streams to collect data that comes from applications and databases within the VPC and the public internet. Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Analytics application by uploading the compiled Flink .jar file. Create Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters in the company's VPC to collect data that comes from applications and databases within the VPC. Use Amazon Kinesis Data Streams to collect data that comes from the public internet. Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams, Amazon MSK, and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-20T23:05:00.000Z",
        "voteCount": 7,
        "content": "A - WRONG, EC2 adds operational overhead.\nB - WRONG, EC2 adds operational overhead.\nC - CORRECT, You can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Streams from leaving the Amazon network. https://docs.aws.amazon.com/streams/latest/dev/vpc.html\nD - WRONG, even though MSK is used in this blog post ( https://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/ ), having MSK increases operational overhead compared to using KDS as in answer C."
      },
      {
        "date": "2022-04-25T08:38:00.000Z",
        "voteCount": 6,
        "content": "D makes sense based on the link provided"
      },
      {
        "date": "2023-05-01T13:35:00.000Z",
        "voteCount": 3,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-12-26T11:22:00.000Z",
        "voteCount": 2,
        "content": "I have a doubt about C and D:\n\"Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.\"\n\nKinesis Data Analytics can have, as a source, only Kinesis Data Stream, Kinesis Data Firehose and S3, so, as they are worded, it seems they are incorrect.\n\nAny thoughts?"
      },
      {
        "date": "2023-02-09T21:19:00.000Z",
        "voteCount": 4,
        "content": "C - KDA can have an on-premise Kafka cluster as a data source\nhttps://aws.amazon.com/ko/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/"
      },
      {
        "date": "2022-08-06T14:27:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-10-05T03:49:00.000Z",
        "voteCount": 1,
        "content": "(https://docs.aws.amazon.com/streams/latest/dev/vpc.html)\nYou can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Streams from leaving the Amazon network."
      },
      {
        "date": "2022-05-22T05:41:00.000Z",
        "voteCount": 5,
        "content": "C because you can have interface endpoints for KDS (https://docs.aws.amazon.com/streams/latest/dev/vpc.html) and its less overhead"
      },
      {
        "date": "2022-04-23T06:43:00.000Z",
        "voteCount": 3,
        "content": "Looks like D is right, based on the ref link provided in the question"
      },
      {
        "date": "2022-04-23T06:43:00.000Z",
        "voteCount": 1,
        "content": "KDS provides public endpoint so not use for vpc connectivity"
      },
      {
        "date": "2022-04-25T01:05:00.000Z",
        "voteCount": 2,
        "content": "Why not C?"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/amazon/view/74242-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A technology company has an application with millions of active users every day. The company queries daily usage data with Amazon Athena to understand how users interact with the application. The data includes the date and time, the location ID, and the services used. The company wants to use Athena to run queries to analyze the data with the lowest latency possible.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Apache Avro format with the date and time as the partition, with the data sorted by the location ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Apache Parquet format with the date and time as the partition, with the data sorted by the location ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Apache ORC format with the location ID as the partition, with the data sorted by the date and time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in .csv format with the location ID as the partition, with the data sorted by the date and time."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-20T08:35:00.000Z",
        "voteCount": 7,
        "content": "B is the right answer, partitioning by data and data type"
      },
      {
        "date": "2023-06-29T12:03:00.000Z",
        "voteCount": 1,
        "content": "Apache Parquet is a data format which allows fast retrieval. No doubts, it's C."
      },
      {
        "date": "2023-05-01T13:35:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-08-13T01:29:00.000Z",
        "voteCount": 1,
        "content": "B\nI think Parquet and ORC are the right choices but the partition will eliminate ORC."
      },
      {
        "date": "2022-07-20T23:36:00.000Z",
        "voteCount": 3,
        "content": "Answer - B"
      },
      {
        "date": "2022-07-08T02:44:00.000Z",
        "voteCount": 1,
        "content": "There is hardly the choice between Avro and ORC. The only thing we can use to eliminate the ORC option is that because of the wrong partitioning criteria given with that option."
      },
      {
        "date": "2022-04-30T14:38:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2022-04-25T16:49:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-04-23T14:12:00.000Z",
        "voteCount": 4,
        "content": "B - Parquet increases speed, partition by date &amp; time also brings better performance."
      },
      {
        "date": "2022-12-13T08:16:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2022-04-23T06:53:00.000Z",
        "voteCount": 3,
        "content": "B is ok"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/amazon/view/74013-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A real estate company maintains data about all properties listed in a market. The company receives data about new property listings from vendors who upload the data daily as compressed files into Amazon S3. The company's leadership team wants to see the most up-to-date listings as soon as the data is uploaded to<br>Amazon S3. The data analytics team must automate and orchestrate the data processing workflow of the listings to feed a dashboard. The team also must provide the ability to perform one-time queries and analytical reporting in a scalable manner.<br>Which solution meets these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR for processing incoming data. Use AWS Step Functions for workflow orchestration. Use Apache Hive for one-time queries and analytical reporting. Bulk ingest the data in Amazon OpenSearch Service (Amazon Elasticsearch Service). Use OpenSearch Dashboards (Kibana) on Amazon OpenSearch Service (Amazon Elasticsearch Service) for the dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR for processing incoming data. Use AWS Step Functions for workflow orchestration. Use Amazon Athena for one-time queries and analytical reporting. Use Amazon QuickSight for the dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue for processing incoming data. Use AWS Step Functions for workflow orchestration. Use Amazon Redshift Spectrum for one-time queries and analytical reporting. Use OpenSearch Dashboards (Kibana) on Amazon OpenSearch Service (Amazon Elasticsearch Service) for the dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue for processing incoming data. Use AWS Lambda and S3 Event Notifications for workflow orchestration. Use Amazon Athena for one-time queries and analytical reporting. Use Amazon QuickSight for the dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-22T10:51:00.000Z",
        "voteCount": 13,
        "content": "C seems correct\ncost-effective rules out emr [A&amp;B]\nscalable reporting suggests redshift spectrum instead of athena\nimmediate access to updated dashboards suggests kibana instead of quicksight\nd may appear less expensive than c, but does not quite meet the criteria"
      },
      {
        "date": "2022-04-21T05:25:00.000Z",
        "voteCount": 9,
        "content": "S3 Event Notifications to react to S3 updates"
      },
      {
        "date": "2023-08-28T12:17:00.000Z",
        "voteCount": 2,
        "content": "Vote D. \nAWS Step Function vs Lambda+S3 Events do not have much price difference. Although Redshift Spectrum charge very less money, to use Redshift Spectrum the customer needs to have an existed Redshift cluster (AWS did have serverless around $260; around $180 per month for dense compute), which definitely beat Athena for \"one-time\" query. Quicksight cost $18 per user per month whereas Opensearch charge $43 per month (serverless one even more expensive; any combination indexing+searching-n-query would give $172.8 per month).\nhttps://aws.amazon.com/quicksight/pricing/\nhttps://aws.amazon.com/opensearch-service/pricing/\nhttps://aws.amazon.com/redshift/pricing/"
      },
      {
        "date": "2023-07-14T01:21:00.000Z",
        "voteCount": 1,
        "content": "It is D\nCost effective\nWith Lambda  + S3 event notification, Latest data will be picked up \nAthena can help with ad hoc queries perfect for that\nQuickSight with Direct Query can handle real time refresh\nhttps://docs.aws.amazon.com/quicksight/latest/user/refreshing-data.html"
      },
      {
        "date": "2023-07-10T00:04:00.000Z",
        "voteCount": 2,
        "content": "I think its C due to the fact that Kibana provides real time dash board capabilities and glue is cheaper than emr."
      },
      {
        "date": "2023-05-01T13:36:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-02-15T09:12:00.000Z",
        "voteCount": 4,
        "content": "The question says \"cost effective\", so answer is D."
      },
      {
        "date": "2023-01-25T01:11:00.000Z",
        "voteCount": 1,
        "content": "I would think it was C"
      },
      {
        "date": "2022-12-19T03:56:00.000Z",
        "voteCount": 5,
        "content": "D is a better option. Lambda and S3 notifications make so much sense if the data is already uploaded to S3 everyday and you want to trigger the process upon the upload event."
      },
      {
        "date": "2022-12-12T02:03:00.000Z",
        "voteCount": 2,
        "content": "I would go for D, only in doubt regarding Athena. But I think that if we compare creating a OpenSearch cluster and a Redshift cluster vs having Athena and Quicksight it is more cost efectively."
      },
      {
        "date": "2022-12-04T09:19:00.000Z",
        "voteCount": 1,
        "content": "D is more cost-effective, however it can't meeting a must request \"must provide the ability to perform one-time queries and analytical reporting in a scalable manner.\" as athena is not scalable"
      },
      {
        "date": "2022-11-22T02:20:00.000Z",
        "voteCount": 4,
        "content": "D for sure. \nNot C because you need a redshift to use redshift spectrum and Opensearch+Kibana is used in near-realtime scenario but here the data is uploaded \"daily\""
      },
      {
        "date": "2022-11-08T17:52:00.000Z",
        "voteCount": 4,
        "content": "I vote for C because they want to see the up-to-date listings as soon as the data is uploaded to Amazon S3. OpenSearch Dashboards allows them to refresh and display the most up-to-date dashboard automatically right away. C may not be as cost-effective as D, but D does not meet the requirement of delivering the listings as soon as the data is uploaded."
      },
      {
        "date": "2022-10-20T08:38:00.000Z",
        "voteCount": 3,
        "content": "D because its most cost effective"
      },
      {
        "date": "2022-10-08T18:47:00.000Z",
        "voteCount": 1,
        "content": "Answer:D\nEMR is not cost-effective, AB is out; \nOpenSearch is also expensive compared to QuickSight. C is out.\nD is perfectly cost-effective."
      },
      {
        "date": "2022-07-27T23:00:00.000Z",
        "voteCount": 2,
        "content": "C seems correct\ncost-effective rules out emr [A&amp;B]"
      },
      {
        "date": "2022-07-27T23:02:00.000Z",
        "voteCount": 3,
        "content": "Maybe D is better in this solution. Lambda and S3 notifications make so much sense if the data is already uploaded to S3 everyday and you want to trigger the process upon the upload event."
      },
      {
        "date": "2022-07-25T08:57:00.000Z",
        "voteCount": 1,
        "content": "Most cost effective solution"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/amazon/view/74270-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A marketing company collects data from third-party providers and uses transient Amazon EMR clusters to process this data. The company wants to host an<br>Apache Hive metastore that is persistent, reliable, and can be accessed by EMR clusters and multiple AWS services and accounts simultaneously. The metastore must also be available at all times.<br>Which solution meets these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue Data Catalog as the metastore\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an external Amazon EC2 instance running MySQL as the metastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS for MySQL as the metastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 as the metastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-25T04:46:00.000Z",
        "voteCount": 11,
        "content": "answer: A\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html"
      },
      {
        "date": "2023-05-01T13:37:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-08-01T01:11:00.000Z",
        "voteCount": 2,
        "content": "A - Use AWS Glue Data Catalog"
      },
      {
        "date": "2022-07-31T08:09:00.000Z",
        "voteCount": 3,
        "content": "Use the AWS Glue Data Catalog as the metastore for Hive. \nUsing Amazon EMR version 5.8.0 or later, you can configure Hive to use the AWS Glue Data Catalog as its metastore. We recommend this configuration when you require a persistent metastore or a metastore shared by different clusters, services, applications, or AWS accounts."
      },
      {
        "date": "2022-07-31T08:10:00.000Z",
        "voteCount": 1,
        "content": "I mean, Ans A"
      },
      {
        "date": "2022-07-27T23:37:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2022-06-24T03:59:00.000Z",
        "voteCount": 1,
        "content": "Yes A is the answer. Clear as day. All the other options are not Apache Hive metatores"
      },
      {
        "date": "2022-06-22T19:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is A."
      },
      {
        "date": "2022-04-23T20:05:00.000Z",
        "voteCount": 2,
        "content": "external data store"
      },
      {
        "date": "2022-04-23T14:18:00.000Z",
        "voteCount": 1,
        "content": "C - persistent and can be queried all time."
      },
      {
        "date": "2023-03-18T05:23:00.000Z",
        "voteCount": 1,
        "content": "I have seen this mentioned a few times in other answers - curious as to why people think this is a least effort solution. Is there a direct AWS import/service level function using an RDS as a metadata store?"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/amazon/view/74271-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A data engineer is using AWS Glue ETL jobs to process data at frequent intervals. The processed data is then copied into Amazon S3. The ETL jobs run every 15 minutes. The AWS Glue Data Catalog partitions need to be updated automatically after the completion of each job.<br>Which solution will meet these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog to manage the data catalog. Define an AWS Glue workflow for the ETL process. Define a trigger within the workflow that can start the crawler when an ETL job run is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog to manage the data catalog. Use AWS Glue Studio to manage ETL jobs. Use the AWS Glue Studio feature that supports updates to the AWS Glue Data Catalog during job runs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Apache Hive metastore to manage the data catalog. Update the AWS Glue ETL code to include the enableUpdateCatalog and partitionKeys arguments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog to manage the data catalog. Update the AWS Glue ETL code to include the enableUpdateCatalog and partitionKeys arguments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-11T23:58:00.000Z",
        "voteCount": 2,
        "content": "D is the best for small effort"
      },
      {
        "date": "2023-05-01T13:38:00.000Z",
        "voteCount": 4,
        "content": "D: I passed the test"
      },
      {
        "date": "2022-12-12T02:09:00.000Z",
        "voteCount": 4,
        "content": "Although A can be a very good solution, as it is possible and is even more visual on what is happening, of course creating a workflow and a trigger will be more costly than putting this options in the ETL code.\nSo, D for sure."
      },
      {
        "date": "2022-11-08T12:14:00.000Z",
        "voteCount": 2,
        "content": "\"Business analysts use Amazon Athena to query the table and create monthly summary reports for the AWS accounts\" \nGiven the above, data should be partitioned by date first in order to calculate summary report for all accounts for a particular month. \nCorrect answer is B"
      },
      {
        "date": "2022-10-20T08:43:00.000Z",
        "voteCount": 1,
        "content": "I choose D as well"
      },
      {
        "date": "2022-07-19T22:29:00.000Z",
        "voteCount": 1,
        "content": "I agree with D"
      },
      {
        "date": "2022-07-17T03:20:00.000Z",
        "voteCount": 2,
        "content": "D is cost effective"
      },
      {
        "date": "2022-06-19T03:00:00.000Z",
        "voteCount": 1,
        "content": "I agree with D"
      },
      {
        "date": "2022-04-27T04:26:00.000Z",
        "voteCount": 3,
        "content": "D - most cost effective as not rerunning the crawler\nhttps://docs.aws.amazon.com/glue/latest/dg/update-from-job.html"
      },
      {
        "date": "2022-04-25T04:53:00.000Z",
        "voteCount": 2,
        "content": "answer: D\nhttps://docs.aws.amazon.com/glue/latest/dg/update-from-job.html"
      },
      {
        "date": "2022-04-25T00:48:00.000Z",
        "voteCount": 2,
        "content": "D seems correct"
      },
      {
        "date": "2022-04-23T20:06:00.000Z",
        "voteCount": 1,
        "content": "as per the document"
      },
      {
        "date": "2022-04-23T14:24:00.000Z",
        "voteCount": 2,
        "content": "D - According to doc- https://docs.aws.amazon.com/glue/latest/dg/update-from-job.html"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/amazon/view/74011-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A reseller that has thousands of AWS accounts receives AWS Cost and Usage Reports in an Amazon S3 bucket. The reports are delivered to the S3 bucket in the following format:<br>&lt;example-report-prefix&gt;/&lt;example-report-name&gt;/yyyymmdd-yyyymmdd/&lt;example-report-name&gt;.parquet<br>An AWS Glue crawler crawls the S3 bucket and populates an AWS Glue Data Catalog with a table. Business analysts use Amazon Athena to query the table and create monthly summary reports for the AWS accounts. The business analysts are experiencing slow queries because of the accumulation of reports from the last<br>5 years. The business analysts want the operations team to make changes to improve query performance.<br>Which action should the operations team take to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the file format to .csv.zip",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by date and account ID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by month and account ID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by account ID, year, and month\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-06-28T07:31:00.000Z",
        "voteCount": 8,
        "content": "Should be D. We want to create monthly reports for each account. So we want to query the data by account-id and month. \n\nOnly month is not enough, we have to add year, otherwise we query the previous year's months as well."
      },
      {
        "date": "2022-05-22T01:08:00.000Z",
        "voteCount": 6,
        "content": "should be D, by date is too precise and by account helps as well"
      },
      {
        "date": "2023-05-01T13:39:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-21T12:15:00.000Z",
        "voteCount": 1,
        "content": "Date/account id partitioning would create a partition for each day thus the analysts would need to ingest a different date range of partitions for each account int other analysis each month. Account ID/year/month would more accurately represent the query pattern and avoid the need for analysts to specify a data range. hence D"
      },
      {
        "date": "2022-07-25T22:00:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: D"
      },
      {
        "date": "2022-07-24T06:29:00.000Z",
        "voteCount": 1,
        "content": "Per account and monthly"
      },
      {
        "date": "2022-05-21T05:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-04-23T20:07:00.000Z",
        "voteCount": 2,
        "content": "monthly summary reports for the AWS accounts."
      },
      {
        "date": "2022-04-28T10:59:00.000Z",
        "voteCount": 1,
        "content": "I concur"
      },
      {
        "date": "2022-04-23T07:41:00.000Z",
        "voteCount": 3,
        "content": "B looks good to me"
      },
      {
        "date": "2022-04-28T10:59:00.000Z",
        "voteCount": 4,
        "content": "chnge to D"
      },
      {
        "date": "2022-04-21T05:02:00.000Z",
        "voteCount": 3,
        "content": "Partition by date is the good practice here"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/amazon/view/74405-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company using Amazon QuickSight Enterprise edition has thousands of dashboards, analyses, and datasets. The company struggles to manage and assign permissions for granting users access to various items within QuickSight. The company wants to make it easier to implement sharing and permissions management.<br>Which solution should the company implement to simplify permissions management?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse QuickSight folders to organize dashboards, analyses, and datasets. Assign individual users permissions to these folders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse QuickSight folders to organize dashboards, analyses, and datasets. Assign group permissions by using these folders.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IAM resource-based policies to assign group permissions to QuickSight items.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse QuickSight user management APIs to provision group permissions based on dashboard naming conventions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-01T13:40:00.000Z",
        "voteCount": 3,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-12-12T05:45:00.000Z",
        "voteCount": 1,
        "content": "A. Use QuickSight folders to organize dashboards, analytics, and datasets. Assign permissions to these folders for each user.\n\nUsing QuickSight folders to organize dashboards, analytics, and datasets, and assigning permissions to these folders for users is a way to simplify rights management. This allows the company to easily grant users access to QuickSight projects and make sharing easier.\n\nOptions B, C, and D all mention using AWS IAM and the QuickSight user management API, but do not mention using folders to organize projects. Option B mentions using folders to assign group permissions, but does not mention how to assign these permissions to users."
      },
      {
        "date": "2022-07-25T21:33:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-05-05T05:20:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/quicksight/latest/user/folders.html"
      },
      {
        "date": "2022-04-24T19:35:00.000Z",
        "voteCount": 2,
        "content": "as per the document"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/amazon/view/74396-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company has 10-15 \u05c0\u00a2\u05c0\u2019 of uncompressed .csv files in Amazon S3. The company is evaluating Amazon Athena as a one-time query engine. The company wants to transform the data to optimize query runtime and storage costs.<br>Which option for data format and compression meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV compressed with zip",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON compressed with bzip2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Parquet compressed with Snappy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Avro compressed with LZO"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-24T17:58:00.000Z",
        "voteCount": 8,
        "content": "Apache parquet"
      },
      {
        "date": "2023-05-01T13:41:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2023-01-23T20:21:00.000Z",
        "voteCount": 1,
        "content": "C it is"
      },
      {
        "date": "2022-08-06T14:01:00.000Z",
        "voteCount": 4,
        "content": "Parquet with snappy compression suits the requirement."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/amazon/view/74010-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company uses Amazon Redshift to store its data. The reporting team runs ad-hoc queries to generate reports from the Amazon Redshift database. The reporting team recently started to experience inconsistencies in report generation. Ad-hoc queries used to generate reports that would typically take minutes to run can take hours to run. A data analytics specialist debugging the issue finds that ad-hoc queries are stuck in the queue behind long-running queries.<br>How should the data analytics specialist resolve the issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate partitions in the tables queried in ad-hoc queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure automatic workload management (WLM) from the Amazon Redshift console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Simple Queue Service (Amazon SQS) queues with different priorities. Assign queries to a queue based on priority.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the VACUUM command for all tables in the database."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-27T05:39:00.000Z",
        "voteCount": 7,
        "content": "Amazon Redshift workload management (WLM) enables users to flexibly manage priorities within workloads so that short, fast-running queries won't get stuck in queues behind long-running queries.\n\nfrom https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html"
      },
      {
        "date": "2023-05-01T13:42:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2022-07-19T23:23:00.000Z",
        "voteCount": 2,
        "content": "Answer should be B"
      },
      {
        "date": "2022-07-17T04:13:00.000Z",
        "voteCount": 1,
        "content": "for sure"
      },
      {
        "date": "2022-06-15T00:52:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. It is the Redshift method for queueing queries."
      },
      {
        "date": "2022-05-27T21:20:00.000Z",
        "voteCount": 1,
        "content": "I think Answer should be B"
      },
      {
        "date": "2022-04-30T13:40:00.000Z",
        "voteCount": 1,
        "content": "B- Correct"
      },
      {
        "date": "2022-04-21T04:56:00.000Z",
        "voteCount": 4,
        "content": "Configure automatic workload management (WLM) from the Amazon Redshift console."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/amazon/view/74297-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company provides an incentive to users who are physically active. The company wants to determine how active the users are by using an application on their mobile devices to track the number of steps they take each day. The company needs to ingest and perform near-real-time analytics on live data. The processed data must be stored and must remain available for 1 year for analytics purposes.<br>Which solution will meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Cognito to write the data from the application to Amazon DynamoDB. Use an AWS Step Functions workflow to create a transient Amazon EMR cluster every hour and process the new data from DynamoDB. Output the processed data to Amazon Redshift for analytics. Archive the data from Amazon Redshift after 1 year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Amazon DynamoDB by using an Amazon API Gateway API as a DynamoDB proxy. Use an AWS Step Functions workflow to create a transient Amazon EMR cluster every hour and process the new data from DynamoDB. Output the processed data to Amazon Redshift to run analytics calculations. Archive the data from Amazon Redshift after 1 year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Amazon Kinesis Data Streams by using an Amazon API Gateway API as a Kinesis proxy. Run Amazon Kinesis Data Analytics on the stream data. Output the processed data into Amazon S3 by using Amazon Kinesis Data Firehose. Use Amazon Athena to run analytics calculations. Use S3 Lifecycle rules to transition objects to S3 Glacier after 1 year.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the data from the application into Amazon S3 by using Amazon Kinesis Data Firehose. Use Amazon Athena to run the analytics on the data in Amazon S3. Use S3 Lifecycle rules to transition objects to S3 Glacier after 1 year."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T02:32:00.000Z",
        "voteCount": 11,
        "content": "Near/Real-time analytics on live data is always done via KDA. Once the data is stored it cannot be seen as live data anymore.\nArguments regarding operational overhead are relevant and will also mix things up for me, but I think here we need KDA."
      },
      {
        "date": "2022-04-27T19:02:00.000Z",
        "voteCount": 7,
        "content": "I am not clear about C or D.\nFor C: it use API gateway as a proxy. ALthough it can, as mentioned beow by \"Teraxs\". But Is it a good idea to use API gateway for streaming data? For it's pricing, it will cost about for every million message. If the company has millions of customers, uploading activity stream every second. The cost will be huge.\nFor D: The question says they need analyze live data. So it needs kinesis analystics. But D doesn't mention about that. And answer D use kinesis firehose, it is also not near real-time.\nBut, anyway, C will meet the requirement, but D cannot. So I choose C."
      },
      {
        "date": "2024-02-13T14:31:00.000Z",
        "voteCount": 1,
        "content": "FSDFASDF"
      },
      {
        "date": "2023-11-05T21:58:00.000Z",
        "voteCount": 1,
        "content": "D is not correct.\nD cant solve realtime anallitics."
      },
      {
        "date": "2023-07-25T00:51:00.000Z",
        "voteCount": 2,
        "content": "I vote for D. D has the least operational overhead and fulfill the purpose. C need to manage KDS shard, API gateway, etc."
      },
      {
        "date": "2023-07-07T03:49:00.000Z",
        "voteCount": 3,
        "content": "\"the processed data must be stored\", so it is C. D stores the raw data directly to S3"
      },
      {
        "date": "2023-07-03T20:41:00.000Z",
        "voteCount": 2,
        "content": "Stored PROCESSED data, not raw data, hence C instead of D!"
      },
      {
        "date": "2023-05-01T13:43:00.000Z",
        "voteCount": 2,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-18T15:58:00.000Z",
        "voteCount": 2,
        "content": "I will go with C. Though D has least operational overhead, Athena can't be considered as the solution for Analytics"
      },
      {
        "date": "2023-01-26T13:02:00.000Z",
        "voteCount": 2,
        "content": "D sounds right"
      },
      {
        "date": "2023-01-08T01:57:00.000Z",
        "voteCount": 3,
        "content": "D has the least operational overhead"
      },
      {
        "date": "2022-11-08T18:27:00.000Z",
        "voteCount": 1,
        "content": "I vote for C. \n\nBecause the company needs to perform analytics on LIVE data, I don't think D can be the answer. D analyzes the data after it is stored in S3 and does not perform analytics on live data."
      },
      {
        "date": "2022-10-20T10:51:00.000Z",
        "voteCount": 1,
        "content": "C because it clearly says the company wants to ingest and perform analytics near-real time then the processed that is stored. So its KDA for near-real time analtics, although KDS is real-time, but it can also deliver near-real time."
      },
      {
        "date": "2022-10-08T19:19:00.000Z",
        "voteCount": 2,
        "content": "The key to selecting C instead of D is: \"the processed data must be stored\u2026\".  In C, the data has been processed through KDA, then saved in S3.  But in D, data is from the application to S3 directly, not being processed yet, Athena comes later. So I go with C."
      },
      {
        "date": "2022-09-14T12:27:00.000Z",
        "voteCount": 1,
        "content": "for the purpose of the exam, near-real time is generally KDF. Have to agree that arguments can be made for C"
      },
      {
        "date": "2022-07-19T22:35:00.000Z",
        "voteCount": 3,
        "content": "Option D"
      },
      {
        "date": "2022-07-19T22:41:00.000Z",
        "voteCount": 1,
        "content": "Assuming \"LEAST amount of operational overhead\", D seems to be the best option."
      },
      {
        "date": "2022-07-17T03:44:00.000Z",
        "voteCount": 2,
        "content": "For near real-time and with least operational overhead D is better answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/amazon/view/74253-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company needs to implement a near-real-time messaging system for hotel inventory. The messages are collected from 1,000 data sources and contain hotel inventory data. The data is then processed and distributed to 20 HTTP endpoint destinations. The range of data size for messages is 2-500 KB.<br>The messages must be delivered to each destination in order. The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations.<br>Which solution meets these requirements with the LOWEST latency from message ingestion to delivery?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream, and ingest the data for each source into the stream. Create 30 AWS Lambda functions to read these messages and send the messages to each destination endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream, and ingest the data for each source into the stream. Create a single enhanced fan-out AWS Lambda function to read these messages and send the messages to each destination endpoint. Register the function as an enhanced fan-out consumer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream, and ingest the data for each source into the stream. Configure Kinesis Data Firehose to deliver the data to an Amazon S3 bucket. Invoke an AWS Lambda function with an S3 event notification to read these messages and send the messages to each destination endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream, and ingest the data for each source into the stream. Create 20 enhanced fan-out AWS Lambda functions to read these messages and send the messages to each destination endpoint. Register the 20 functions as enhanced fan-out consumers."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T02:56:00.000Z",
        "voteCount": 12,
        "content": "I would pick D over B.\n\n1) The question is asking for the LEAST latency. It doesn't matter if we will have high costs, setup or complexity.\n2) What we will be the benefit of having 1 lambda (aka 1 consumer) with EFO? The EFO exists exatly to be possible to have multiple consumers without one affecting each other. 3) The parallelism factor will increase throughput, but we will still have 1 consumer getting data in parallel from the same shard, it is not like we will have multiple lambda executions writting to different HTTP endpoints.\n4) With 1 lambda, we will need to write to the HTTP sequentially, like http1.write, http2.write, etc.\n5) To finalize, I think that EFO with one consumer is irrelevant, because you will get 2MB/s even without EFO.\n\nI would go for D all the way. \n\nPlease, if there is people going around this, just share your thoughts."
      },
      {
        "date": "2022-12-19T05:29:00.000Z",
        "voteCount": 1,
        "content": "requirement =&gt; LOWEST latency from message ingestion to delivery ??\nHow will Option D be better over B ?"
      },
      {
        "date": "2022-12-19T05:31:00.000Z",
        "voteCount": 1,
        "content": "1 or 20 Fan-out, it'll still be the same latency from message ingestion to delivery. Right ??"
      },
      {
        "date": "2023-03-21T12:05:00.000Z",
        "voteCount": 8,
        "content": "Enhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with dedicated throughput of up to 2 MB of data per second per shard. When using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer. A consumer that uses enhanced fan-out doesn't have to contend with other consumers that are receiving data from the stream. our data cap is up to 500kb, meaning we have the ability to pull at least 4 messages per shard per second. The 10 parralel invocations would mean that we can call up to 10 shards in parallel and thus consume a minimum of 40 messages at the maximum message size as defined in the question. Thus, one lambda enhanced fanout consumer should be enough to route to 20 destinations concurrently and 20 lambdas would be waaay too many, hence B"
      },
      {
        "date": "2023-10-11T08:23:00.000Z",
        "voteCount": 1,
        "content": "What about ordering? With option B will be very difficult to maintain the order in case of a failure, timeout I think D is a better option"
      },
      {
        "date": "2024-04-07T08:06:00.000Z",
        "voteCount": 1,
        "content": "When using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer. Each parallelized invocation contains messages with the same partition key (HotelCode) and maintains order . https://aws.amazon.com/vi/blogs/architecture/field-notes-how-to-scale-opentravel-messaging-architecture-with-amazon-kinesis-data-streams/"
      },
      {
        "date": "2024-03-25T00:42:00.000Z",
        "voteCount": 2,
        "content": "Bing said answer is D :)"
      },
      {
        "date": "2023-11-22T02:25:00.000Z",
        "voteCount": 1,
        "content": "I've heard that the enhanced fan out feature only works for http/2."
      },
      {
        "date": "2023-11-08T09:24:00.000Z",
        "voteCount": 1,
        "content": "I am confused with lambda payload limit that is 256kb thhat is greater than message size"
      },
      {
        "date": "2023-10-11T08:28:00.000Z",
        "voteCount": 2,
        "content": "In my opinion this two requirements make me select D\n\"The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations.\"\n\nThis requirement underscores the need for a design that isolates the message delivery paths to different endpoints. If a single Lambda function were responsible for all destinations, any slow response or failure at one endpoint could cause delays or retries that would affect the entire function, impacting the delivery performance for other endpoints. This is undesirable because the behavior or performance of one endpoint should not influence the others and Message Ordering,  Each function maintains the order for its own endpoint, reducing complexity and the risk of out-of-order delivery that might arise when one function handles multiple endpoints."
      },
      {
        "date": "2023-07-03T21:27:00.000Z",
        "voteCount": 2,
        "content": "B does not address the requirement: The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations. As one single lambda would send http put requests one after the other.\n\n20 independent destinations = 20 consumers\n\nso I say D"
      },
      {
        "date": "2023-07-03T21:30:00.000Z",
        "voteCount": 1,
        "content": "Also with one lambda you get 10 concurrent lambdas to consume from the same shard,\nwith 20 lambdas you get 200 concurrent lambdas for the entire stream this would process the stream way faster and reduce latency. Question does not mention costs."
      },
      {
        "date": "2023-05-01T13:43:00.000Z",
        "voteCount": 2,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-18T18:19:00.000Z",
        "voteCount": 7,
        "content": "The key requirement is \"The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations.\""
      },
      {
        "date": "2023-03-08T12:04:00.000Z",
        "voteCount": 2,
        "content": "Answer: D\nI think it's D because\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/aws-lambda-supports-kinesis-data-streams-enhanced-fan-out-and-http2/\n\nenhanced-fan-out with one lambda doesn't makes sense and logically incorrect statement."
      },
      {
        "date": "2023-01-21T14:54:00.000Z",
        "voteCount": 1,
        "content": "KDS for near real time ? for low size .. KDS is for real time and min 10GB of data streaming ?? \nconsidering its near real time - shouldnt the answer be firehose ?"
      },
      {
        "date": "2023-01-12T11:49:00.000Z",
        "voteCount": 4,
        "content": "The question is: does 20 HTTP Endpoint destinations considered as one consumer or 20 consumers? \n\nFan-out lambda can support up to 5 consumers so 20 HTTPS endpoints consider 20 consumers, you will need to establish at least 4 Fan-out lambda functions, which rule out B.\n\nHowever, these 20 HTTP endpoints should consider as one consumer, they are going to the same place and this place has 20 endpoints. So one Fan-out should be good enough instead of 20.\nLink: https://aws.amazon.com/blogs/compute/increasing-real-time-stream-processing-performance-with-amazon-kinesis-data-streams-enhanced-fan-out-and-aws-lambda/"
      },
      {
        "date": "2022-12-09T21:32:00.000Z",
        "voteCount": 3,
        "content": "Ans- B\nLambda enhanced fan-out consumers\nEnhanced fan-out consumers can increase the per shard read consumption throughput through event-based consumption, reduce latency with parallelization, and support error handling. The enhanced fan-out consumer increases the read capacity of consumers from a shared 2 MB per second, to a dedicated 2 MB per second for each consumer.\n\nWhen using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer. Each parallelized invocation contains messages with the same partition key (HotelCode) and maintains order. The invocations complete each message before processing with the next parallel invocation.\n\n\nFigure 8: Lambda fan-out consumers with parallel invocations, maintaining order"
      },
      {
        "date": "2022-08-11T00:06:00.000Z",
        "voteCount": 4,
        "content": "Answer B - https://aws.amazon.com/blogs/compute/increasing-real-time-stream-processing-performance-with-amazon-kinesis-data-streams-enhanced-fan-out-and-aws-lambda/\nrefer \"Enhanced fan-out with Lambda functions\""
      },
      {
        "date": "2023-03-24T08:46:00.000Z",
        "voteCount": 2,
        "content": "The blog has a comparison that uses 3 lambda function in EFO.\n\nComparing methods\nTo demonstrate the advantage of Kinesis Data Streams enhanced fan-out, I built an application with a single shard stream. It has three Lambda functions connected using the standard method and three Lambda functions connected using enhanced fan-out for consumers. I created roughly 76 KB of dummy data and inserted it into the stream at 1,000 records per second. After four seconds, I stopped the process, leaving a total of 4,000 records to be processed.\n\nMeans we need 20 EOF, max limit for EOF is 20 so that satisfies the max\nhttps://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html"
      },
      {
        "date": "2022-07-31T07:38:00.000Z",
        "voteCount": 3,
        "content": "I have concerns regarding B,  1 lamda function processing data and sending to 20 HTTP endpoints seems bit complicated and fan out for 1 lambda seems none necessary I think. From the other side in D haveing 20 lambdas that send to each point puts unnecesary load on kinesis, I think good case would be to have severl lambdas , for example 5 , each for 4 endpoints"
      },
      {
        "date": "2022-07-21T19:49:00.000Z",
        "voteCount": 2,
        "content": "Answer-B"
      },
      {
        "date": "2022-07-03T06:01:00.000Z",
        "voteCount": 1,
        "content": "Why would you want to use enhanced fan out consumer in answer B? there is only a single consumer and there are no benefits in using this feature.\nIn answer D, using 20 lambda functions is a waste, but with enhanced consumer you can be sure that you will have not throttling problems."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/amazon/view/74005-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A retail company stores order invoices in an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster Indices on the cluster are created monthly.<br>Once a new month begins, no new writes are made to any of the indices from the previous months. The company has been expanding the storage on the Amazon<br>OpenSearch Service (Amazon Elasticsearch Service) cluster to avoid running out of space, but the company wants to reduce costs. Most searches on the cluster are on the most recent 3 months of data, while the audit team requires infrequent access to older data to generate periodic reports. The most recent 3 months of data must be quickly available for queries, but the audit team can tolerate slower queries if the solution saves on cluster costs<br>Which of the following is the MOST operationally efficient solution to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArchive indices that are older than 3 months by using Index State Management (ISM) to create a policy to store the indices in Amazon S3 Glacier. When the audit team requires the archived data, restore the archived indices back to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArchive indices that are older than 3 months by taking manual snapshots and storing the snapshots in Amazon S3. When the audit team requires the archived data, restore the archived indices back to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArchive indices that are older than 3 months by using Index State Management (ISM) to create a policy to migrate the indices to Amazon OpenSearch Service (Amazon Elasticsearch Service) UltraWarm storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArchive indices that are older than 3 months by using Index State Management (ISM) to create a policy to migrate the indices to Amazon OpenSearch Service (Amazon Elasticsearch Service) UltraWarm storage. When the audit team requires the older data, migrate the indices in UltraWarm storage back to hot storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-23T10:08:00.000Z",
        "voteCount": 6,
        "content": "C is okay. No need to return back to hot storage\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/ultrawarm.html\n\n\"UltraWarm nodes use Amazon S3 and a sophisticated caching solution to improve performance. For indexes that you are not actively writing to, query less frequently, and don't need the same performance from, UltraWarm offers significantly lower costs per GiB of data. Because warm indexes are read-only unless you return them to hot storage, UltraWarm is best-suited to immutable data, such as logs.\n\nIn OpenSearch, warm indexes behave just like any other index. You can query them using the same APIs or use them to create visualizations in OpenSearch Dashboards.\""
      },
      {
        "date": "2022-04-24T03:08:00.000Z",
        "voteCount": 5,
        "content": "C - As ISM can move it from Hot to Ultrawarm and uses S3 in the background for Storage(hence not A to reduce operational overhead), you don't need to move it back to Hot as it is used by the Audit team for auditing meaning only read hence the reason it's not D."
      },
      {
        "date": "2023-05-01T13:44:00.000Z",
        "voteCount": 4,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-08-06T14:33:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-04-21T04:20:00.000Z",
        "voteCount": 2,
        "content": "C - data is always available in ultra warm storage, just queries are slower, which matches the statement requirements"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/amazon/view/74004-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A financial services company is building a data lake solution on Amazon S3. The company plans to use analytics offerings from AWS to meet user needs for one- time querying and business intelligence reports. A portion of the columns will contain personally identifiable information (PII) Only authorized users should be able to see plaintext PII data.<br>What is the MOST operationally efficient solution that meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a bucket policy for each S3 bucket of the data lake to allow access to users who have authorization to see PII data. Catalog the data by using AWS Glue. Create two IAM roles. Attach a permissions policy with access to PII columns to one role. Attach a policy without these permissions to the other role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the S3 locations with AWS Lake Formation. Create two IAM roles. Use Lake Formation data permissions to grant Select permissions to all of the columns for one role. Grant Select permissions to only columns that contain non-PII data for the other role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the S3 locations with AWS Lake Formation. Create an AWS Glue job to create an ETL workflow that removes the PII columns from the data and creates a separate copy of the data in another data lake S3 bucket. Register the new S3 locations with Lake Formation. Grant users the permissions to each data lake data based on whether the users are authorized to see PII data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the S3 locations with AWS Lake Formation. Create two IAM roles. Attach a permissions policy with access to PII columns to one role. Attach a policy without these permissions to the other role. For each downstream analytics service, use its native security functionality and the IAM roles to secure the PII data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T04:17:00.000Z",
        "voteCount": 9,
        "content": "https://docs.aws.amazon.com/lake-formation/latest/dg/lake-formation-permissions.html"
      },
      {
        "date": "2023-02-13T00:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lake-formation/latest/dg/managing-permissions.html\nLake Formation provides central access controls for data in your data lake. You can define security policy-based rules for your users and applications by role in Lake Formation, and integration with AWS Identity and Access Management authenticates those users and roles. Once the rules are defined, Lake Formation enforces your access controls at table and column-level granularity for users of Amazon Redshift Spectrum and Amazon Athena."
      },
      {
        "date": "2022-04-23T10:59:00.000Z",
        "voteCount": 6,
        "content": "B is good"
      },
      {
        "date": "2023-05-01T13:45:00.000Z",
        "voteCount": 3,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-03-18T06:03:00.000Z",
        "voteCount": 1,
        "content": "I agree B, but we actually use C in my current workplace datalake. Reason being retention/purging/privacy requests - need to isolate the data not just manage permissions to view it."
      },
      {
        "date": "2022-11-08T22:21:00.000Z",
        "voteCount": 2,
        "content": "Answer B AWS Lake Formation column-level permissions can be used to restrict access to specific columns in a table. When a user retrieves metadata about the table using the console or an API like glue:GetTable , the column list in the table object contains only the fields to which they have access."
      },
      {
        "date": "2022-07-20T23:31:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      },
      {
        "date": "2022-07-14T22:53:00.000Z",
        "voteCount": 2,
        "content": "Column Level permissions is possible only with Lake Formation Fine grained access controls"
      },
      {
        "date": "2022-04-30T14:27:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/amazon/view/74257-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A gaming company is building a serverless data lake. The company is ingesting streaming data into Amazon Kinesis Data Streams and is writing the data to<br>Amazon S3 through Amazon Kinesis Data Firehose. The company is using 10 MB as the S3 buffer size and is using 90 seconds as the buffer interval. The company runs an AWS Glue ETL job to merge and transform the data to a different format before writing the data back to Amazon S3.<br>Recently, the company has experienced substantial growth in its data volume. The AWS Glue ETL jobs are frequently showing an OutOfMemoryError error.<br>Which solutions will resolve this issue without incurring additional costs? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the small files into one S3 folder. Define one single table for the small S3 files in AWS Glue Data Catalog. Rerun the AWS Glue ETL jobs against this AWS Glue table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to merge small S3 files and invoke them periodically. Run the AWS Glue ETL jobs after successful completion of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the S3DistCp utility in Amazon EMR to merge a large number of small S3 files before running the AWS Glue ETL jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the groupFiles setting in the AWS Glue ETL job to merge small S3 files and rerun AWS Glue ETL jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Kinesis Data Firehose S3 buffer size to 128 MB. Update the buffer interval to 900 seconds.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-01T13:47:00.000Z",
        "voteCount": 6,
        "content": "DE: I passed the test"
      },
      {
        "date": "2023-01-12T12:08:00.000Z",
        "voteCount": 1,
        "content": "The buffer sizes hints range from 1 MbB to 128 MbB for Amazon S3 delivery. For Amazon OpenSearch Service (OpenSearch Service) delivery, they range from 1 MB to 100 MB. For AWS Lambda processing, you can set a buffering hint between 0.2 MB and up to 3 MB using the BufferSizeInMBs processor parameter. The size threshold is applied to the buffer before compression. These options are treated as hints. Kinesis Data Firehose might choose to use different values when it is optimal.\n\nThe buffer interval hints range from 60 seconds to 900 seconds."
      },
      {
        "date": "2022-10-20T11:12:00.000Z",
        "voteCount": 1,
        "content": "DE see extract from AWS \"You can configure the values for Amazon S3 Buffer size (1\u2013128 MB) or Buffer interval (60\u2013900 seconds). The condition satisfied first triggers data delivery to Amazon S3. When data delivery to the destination falls behind data writing to the delivery stream, Kinesis Data Firehose raises the buffer size dynamically.\""
      },
      {
        "date": "2022-10-26T08:22:00.000Z",
        "voteCount": 1,
        "content": "Link: https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html"
      },
      {
        "date": "2022-10-08T19:46:00.000Z",
        "voteCount": 3,
        "content": "DE. \nA \u2013 Won\u2019t resolve this issue.\nB \u2013 Lambda has additional cost.\nC \u2013 S3DistCp has an additional cost.\nD \u2013 It\u2019s covered in Glue ETL cost.\nE \u2013 Setting won\u2019t have an additional cost."
      },
      {
        "date": "2022-08-05T21:30:00.000Z",
        "voteCount": 1,
        "content": "I doubt \"E\" is an answer, because max buffer time of KDF is 5 mins. I would go with BD"
      },
      {
        "date": "2023-03-21T11:48:00.000Z",
        "voteCount": 2,
        "content": "DE - KDF max buffer time is 900 seconds (15 mins)\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/#:~:text=Kinesis%20Data%20Firehose%20buffers%20incoming,data%20delivery%20to%20Amazon%20S3."
      },
      {
        "date": "2022-07-21T19:46:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: DE"
      },
      {
        "date": "2022-07-15T04:54:00.000Z",
        "voteCount": 2,
        "content": "Grouping files together reduces the memory footprint on the Spark driver as well as simplifying file split orchestration.\nBuffer size increase avoids creating small size files"
      },
      {
        "date": "2022-06-16T05:34:00.000Z",
        "voteCount": 1,
        "content": "Another good article discussing Out of memory issue; https://aws.amazon.com/premiumsupport/knowledge-center/glue-oom-java-heap-space-error/"
      },
      {
        "date": "2022-06-06T02:02:00.000Z",
        "voteCount": 2,
        "content": "DE.\nGrouping files together reduces the memory footprint on the Spark driver as well as simplifying file split orchestration. Without grouping, a Spark application must process each file using a different Spark task. Each task must then send mapStatus object containing the location information to the Spark driver. \nhttps://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/"
      },
      {
        "date": "2022-04-30T15:45:00.000Z",
        "voteCount": 1,
        "content": "Answer-D,E"
      },
      {
        "date": "2022-04-29T00:24:00.000Z",
        "voteCount": 3,
        "content": "buffer increase increases file size\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/?nc1=h_ls\ngroupFiles helps reading small files\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"
      },
      {
        "date": "2022-04-23T11:06:00.000Z",
        "voteCount": 1,
        "content": "DE seems good"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/amazon/view/74002-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A healthcare company ingests patient data from multiple data sources and stores it in an Amazon S3 staging bucket. An AWS Glue ETL job transforms the data, which is written to an S3-based data lake to be queried using Amazon Athena. The company wants to match patient records even when the records do not have a common unique identifier.<br>Which solution meets this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Macie pattern matching as part of the ETLjob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain and use the AWS Glue PySpark filter class in the ETLjob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition tables and use the ETL job to partition the data on patient name",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain and use the AWS Glue FindMatches ML transform in the ETLjob\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T04:01:00.000Z",
        "voteCount": 15,
        "content": "The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly.\nReference:\nhttps://docs.aws.amazon.com/glue/latest/dg/machine-learning.html"
      },
      {
        "date": "2023-05-01T13:48:00.000Z",
        "voteCount": 1,
        "content": "D: I passed the test"
      },
      {
        "date": "2023-03-08T08:23:00.000Z",
        "voteCount": 1,
        "content": "why not C?"
      },
      {
        "date": "2023-03-18T18:53:00.000Z",
        "voteCount": 3,
        "content": "partitioning based on name may not be a great idea. It will result in too many partitions"
      },
      {
        "date": "2022-07-25T21:52:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: D"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/amazon/view/74001-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A social media company is using business intelligence tools to analyze its data for forecasting. The company is using Apache Kafka to ingest the low-velocity data in near-real time. The company wants to build dynamic dashboards with machine learning (ML) insights to forecast key business trends. The dashboards must provide hourly updates from data in Amazon S3. Various teams at the company want to view the dashboards by using Amazon QuickSight with ML insights. The solution also must correct the scalability problems that the company experiences when it uses its current architecture to ingest data.<br>Which solution will MOST cost-effectively meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace Kafka with Amazon Managed Streaming for Apache Kafka. Ingest the data by using AWS Lambda, and store the data in Amazon S3. Use QuickSight Standard edition to refresh the data in SPICE from Amazon S3 hourly and create a dynamic dashboard with forecasting and ML insights.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace Kafka with an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to consume the data and store the data in Amazon S3. Use QuickSight Enterprise edition to refresh the data in SPICE from Amazon S3 hourly and create a dynamic dashboard with forecasting and ML insights.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Kafka-Kinesis-Connector to publish the data to an Amazon Kinesis Data Firehose delivery stream that is configured to store the data in Amazon S3. Use QuickSight Enterprise edition to refresh the data in SPICE from Amazon S3 hourly and create a dynamic dashboard with forecasting and ML insights.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Kafka-Kinesis-Connector to publish the data to an Amazon Kinesis Data Firehose delivery stream that is configured to store the data in Amazon S3. Configure an AWS Glue crawler to crawl the data. Use an Amazon Athena data source with QuickSight Standard edition to refresh the data in SPICE hourly and create a dynamic dashboard with forecasting and ML insights."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T11:32:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect because QuickSight Standard Edition does not support ML insights.\nC is incorrect because this does not solve the scalability problems since it keeps part of the current architecture.\nD is incorrect for the same reasons as A and C.\nAlso KDS is cheaper than MSK."
      },
      {
        "date": "2023-11-09T22:41:00.000Z",
        "voteCount": 1,
        "content": "kafka-connecter is very expensive."
      },
      {
        "date": "2023-11-05T19:13:00.000Z",
        "voteCount": 1,
        "content": "A and B is not correct . \nQuickSight SE is not supported ML insights.\nML insights needs QuickSIght EE.\nC is not correct.\nKafka-connecter is high cost solution."
      },
      {
        "date": "2023-07-08T01:02:00.000Z",
        "voteCount": 1,
        "content": "\"The solution also must correct the scalability problems\": so, can't be C"
      },
      {
        "date": "2023-05-01T13:49:00.000Z",
        "voteCount": 1,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-01-12T12:29:00.000Z",
        "voteCount": 3,
        "content": "C is wrong as it stated: The solution also must correct the scalability problems that the company experiences when it uses its current architecture to ingest data.\n\nWe gonna using pure AWS solution. Using connector cannot solve Apache Kafka scale problem unless they using MSK"
      },
      {
        "date": "2022-12-14T01:10:00.000Z",
        "voteCount": 1,
        "content": "Kafka-Kinesis-Connector for Firehose is used to publish messages from Kafka to one of the following destinations: Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service and in turn enabling near real time analytics with existing business intelligence tools and dashboards. Amazon Kinesis Firehose has ability to transform, batch, archive message onto S3 and retry if destination is unavailable. https://github.com/awslabs/kinesis-kafka-connector\n\n\nML insights available in enterprise edition"
      },
      {
        "date": "2022-10-28T02:24:00.000Z",
        "voteCount": 1,
        "content": "cost-effectively is the key\nC will save the cost and B need change the codes from kafka to KDS."
      },
      {
        "date": "2022-08-24T12:16:00.000Z",
        "voteCount": 1,
        "content": "I think it is C. Because we don't need to use KDS to get streams. \nKafka can publish the data to KDF directly with the connector.\n\nhttps://github.com/awslabs/kinesis-kafka-connector"
      },
      {
        "date": "2022-08-24T12:17:00.000Z",
        "voteCount": 2,
        "content": "pardon, it is replacing kafka with KDS. so it is b"
      },
      {
        "date": "2022-07-21T17:08:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B"
      },
      {
        "date": "2022-07-08T03:18:00.000Z",
        "voteCount": 3,
        "content": "ML insights available in enterprise edition. And hourly refresh of SPICE is also available in enterprise edition. So is gotta be B or C. Since the current solution is not scalable it need to be replaced. Hence B is the answer."
      },
      {
        "date": "2022-04-30T14:40:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2022-04-28T07:37:00.000Z",
        "voteCount": 4,
        "content": "Enterprise edition needed because of ML capabilities. Current solution not scalable, thus needs to be replaced, i.e. B"
      },
      {
        "date": "2022-04-23T11:22:00.000Z",
        "voteCount": 2,
        "content": "B seems okay. ML insight is in enterprise edition"
      },
      {
        "date": "2022-04-21T04:00:00.000Z",
        "voteCount": 2,
        "content": "B - Kinesis is cheaper than Kafka"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/amazon/view/74259-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A manufacturing company is storing data from its operational systems in Amazon S3. The company's business analysts need to perform one-time queries of the data in Amazon S3 with Amazon Athena. The company needs to access the Athena network from the on-premises network by using a JDBC connection. The company has created a VPC Security policies mandate that requests to AWS services cannot traverse the Internet.<br>Which combination of steps should a data analytics specialist take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish an AWS Direct Connect connection between the on-premises network and the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the JDBC connection to connect to Athena through Amazon API Gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the JDBC connection to use a gateway VPC endpoint for Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the JDBC connection to use an interface VPC endpoint for Athena.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Athena within a private subnet."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-23T11:45:00.000Z",
        "voteCount": 9,
        "content": "AD looks right\n\nhttps://docs.aws.amazon.com/athena/latest/ug/interface-vpc-endpoint.html"
      },
      {
        "date": "2023-08-20T08:32:00.000Z",
        "voteCount": 3,
        "content": "The correct answers are A and D.\n\n    Option B is incorrect because Amazon API Gateway is a public facing service and requests to it will traverse the internet.\n    Option C is incorrect because a gateway VPC endpoint for Amazon S3 allows you to connect to Amazon S3 from within your VPC, but it does not allow you to connect to Athena.\n\nThe following are the explanations for the correct answers:\n\n    Option A: An AWS Direct Connect connection is a dedicated network connection between your on-premises network and AWS. This will allow you to connect to Athena from your on-premises network without traversing the internet.\n    Option D: An interface VPC endpoint for Athena is a private connection to Athena that is created within your VPC. This will allow you to connect to Athena from your VPC without traversing the internet."
      },
      {
        "date": "2023-05-01T13:50:00.000Z",
        "voteCount": 1,
        "content": "AD: I passed the test"
      },
      {
        "date": "2023-03-21T07:50:00.000Z",
        "voteCount": 2,
        "content": "A. Establish an AWS Direct Connect connection between the on-premises network and the VPC, which provides a dedicated network connection between the on-premises network and the VPC, avoiding the need to traverse the public internet.\n\nD. Configure the JDBC connection to use an interface VPC endpoint for Athena, which enables private connectivity from the VPC to Athena without traversing the internet. The VPC endpoint can be configured to use a security group to control access to the endpoint."
      },
      {
        "date": "2023-01-28T08:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/interface-vpc-endpoint.html"
      },
      {
        "date": "2022-07-25T21:44:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: AD"
      },
      {
        "date": "2022-06-16T16:24:00.000Z",
        "voteCount": 2,
        "content": "Establish an AWS Direct Connect connection between the on-premises network and the VPC.\n Configure the JDBC connection to use an interface VPC endpoint for Athena."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/amazon/view/73999-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company stores revenue data in Amazon Redshift. A data analyst needs to create a dashboard so that the company's sales team can visualize historical revenue and accurately forecast revenue for the upcoming months.<br>Which solution will MOST cost-effectively meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon QuickSight analysis by using the data in Amazon Redshift. Add a custom field in QuickSight that applies a linear regression function to the data. Publish the analysis as a dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a JavaScript dashboard by using D3.js charts and the data in Amazon Redshift. Export the data to Amazon SageMaker. Run a Python script to run a regression model to forecast revenue. Import the data back into Amazon Redshift. Add the new forecast information to the dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon QuickSight analysis by using the data in Amazon Redshift. Add a forecasting widget Publish the analysis as a dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon SageMaker model for forecasting. Integrate the model with an Amazon QuickSight dataset. Create a widget for the dataset.    Publish the analysis as a dashboard."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-23T11:52:00.000Z",
        "voteCount": 9,
        "content": "C \nhttps://docs.aws.amazon.com/quicksight/latest/user/forecasts-and-whatifs.html"
      },
      {
        "date": "2023-05-01T13:52:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-07-19T22:33:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2022-05-22T01:56:00.000Z",
        "voteCount": 2,
        "content": "C should be Correct"
      },
      {
        "date": "2022-04-21T03:15:00.000Z",
        "voteCount": 3,
        "content": "C, per https://docs.aws.amazon.com/quicksight/latest/user/forecasts-and-whatifs.html"
      },
      {
        "date": "2022-04-24T03:45:00.000Z",
        "voteCount": 1,
        "content": "Agree C is the most-cost effective."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/amazon/view/73997-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A company is using an AWS Lambda function to run Amazon Athena queries against a cross-account AWS Glue Data Catalog. A query returns the following error:<br><br>HIVE_METASTORE_ERROR -<br>The error message states that the response payload size exceeds the maximum allowed size. The queried table is already partitioned, and the data is stored in an<br>Amazon S3 bucket in the Apache Hive partition format.<br>Which solution will resolve this error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function to upload the query response payload as an object into the S3 bucket. Include an S3 object presigned URL as the payload in the Lambda function response.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the MSCK REPAIR TABLE command on the queried table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate folder in the S3 bucket. Move the data files that need to be queried into that folder. Create an AWS Glue crawler that points to the folder instead of the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the schema of the queried table for any characters that Athena does not support. Replace any unsupported characters with characters that Athena supports."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T03:04:00.000Z",
        "voteCount": 12,
        "content": "A from https://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-metastore-error/"
      },
      {
        "date": "2022-05-27T10:24:00.000Z",
        "voteCount": 6,
        "content": "I think it is B,  per the link https://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-metastore-error/\nIt says,  If your table is already partitioned, and the data is loaded in Amazon Simple Storage Service (Amazon S3) Hive partition format, then load the partitions by running a command MSCK REPAIR TABLE  table_name"
      },
      {
        "date": "2023-03-26T09:17:00.000Z",
        "voteCount": 1,
        "content": "Article says, \"If your table is partitioned, and your use case permits, query only the specific partition.\" This is not one of the 4 options. So the simple thing is maybe the use case does not permit querying the specific partition since it is not explicitly or implicitly mentioned in the question. The article also does not mention running MSCK."
      },
      {
        "date": "2023-11-05T19:34:00.000Z",
        "voteCount": 1,
        "content": "Lambda is limitted 6MB response.\nso this error occuerd.\nCounterplan is using pre shared URL (S3).\nLambda dosent back data directly to frontend. \nFrontend can access to data by pre shared URL(S3)."
      },
      {
        "date": "2023-05-01T13:53:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-01-12T12:43:00.000Z",
        "voteCount": 2,
        "content": "HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: java.io.IOException: Response payload size (11112222 bytes) exceeded maximum allowed payload size (6291556 bytes)\nTo resolve this error, choose one or more of the following solutions:\n\nUpload the Lambda function's response payload as an object into an Amazon S3 bucket. Then, include this object as payload in the Lambda function response. For information on generating a presigned URL for your object, see Sharing an object with a presigned URL.\nIf your table is partitioned, and your use case permits, query only the specific partition.\nSpecify the spill location in Amazon S3 when you create the Lambda function. Responses that are larger than the threshold spill into the specified S3 location."
      },
      {
        "date": "2022-10-13T17:58:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-metastore-error/"
      },
      {
        "date": "2022-08-06T14:25:00.000Z",
        "voteCount": 4,
        "content": "Answer is A."
      },
      {
        "date": "2022-05-27T10:32:00.000Z",
        "voteCount": 5,
        "content": "If the full error is HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Required Table Storage Descriptor is not populated: Then it is B.\nIf the full error is \nHIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: java.io.IOException: Response payload size (11112222 bytes) exceeded maximum allowed payload size\nThen the answer is A."
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/amazon/view/73996-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A machinery company wants to collect data from sensors. A data analytics specialist needs to implement a solution that aggregates the data in near-real time and saves the data to a persistent data store. The data must be stored in nested JSON format and must be queried from the data store with a latency of single-digit milliseconds.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to receive the data from the sensors. Use Amazon Kinesis Data Analytics to read the stream, aggregate the data, and send the data to an AWS Lambda function. Configure the Lambda function to store the data in Amazon DynamoDB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to receive the data from the sensors. Use Amazon Kinesis Data Analytics to aggregate the data. Use an AWS Lambda function to read the data from Kinesis Data Analytics and store the data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to receive the data from the sensors. Use an AWS Lambda function to aggregate the data during capture. Store the data from Kinesis Data Firehose in Amazon DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to receive the data from the sensors. Use an AWS Lambda function to aggregate the data during capture. Store the data in Amazon S3."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-28T22:23:00.000Z",
        "voteCount": 11,
        "content": "single-digit milliseconds --&gt; DynamoDB\nFirehose cannot store directly to DynamoDB, Lambda can\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      },
      {
        "date": "2023-05-01T13:54:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2023-03-30T17:36:00.000Z",
        "voteCount": 2,
        "content": "Answer: B ; device --&gt; PutRecord --&gt; Firehose --&gt; KDA --&gt; Lambda --&gt;S3. Kinesis data analytics can only write to lambda, data streams or firehose."
      },
      {
        "date": "2023-02-14T21:31:00.000Z",
        "voteCount": 1,
        "content": "A over C - Lambda function not to aggregate but to store the data in Amazon DynamoDB."
      },
      {
        "date": "2023-02-11T11:29:00.000Z",
        "voteCount": 1,
        "content": "The answer is C - KDS cant directly connect to source ,, answer C is firehose - near real time and DyanmoDB to save nested json strcture with single digit millisecond requirmeent."
      },
      {
        "date": "2023-02-27T07:09:00.000Z",
        "voteCount": 3,
        "content": "Ans is A. Firehose can't write directly to DynamoDb"
      },
      {
        "date": "2022-12-31T17:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-12-15T07:33:00.000Z",
        "voteCount": 1,
        "content": "A is The only answer which combines KDA (analytics) for aggregation and DynamoDB for single-digit latency. Lambda is the way to persist and not to compute. \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html"
      },
      {
        "date": "2022-07-21T18:59:00.000Z",
        "voteCount": 1,
        "content": "Answer - A"
      },
      {
        "date": "2022-07-05T15:45:00.000Z",
        "voteCount": 1,
        "content": "The answer is C, kinesis data streams is for realtime, it should be kinesis firehose"
      },
      {
        "date": "2022-07-23T14:04:00.000Z",
        "voteCount": 4,
        "content": "Firehose cannot store directly to DynamoDB"
      },
      {
        "date": "2022-04-30T14:56:00.000Z",
        "voteCount": 1,
        "content": "Answer - A"
      },
      {
        "date": "2022-04-25T17:01:00.000Z",
        "voteCount": 3,
        "content": "I vote for C"
      },
      {
        "date": "2022-04-28T08:09:00.000Z",
        "voteCount": 1,
        "content": "You need lambda"
      },
      {
        "date": "2022-08-02T02:43:00.000Z",
        "voteCount": 4,
        "content": "Firehose cant send data to DynamoDB"
      },
      {
        "date": "2022-04-21T03:02:00.000Z",
        "voteCount": 3,
        "content": "single-digit performance =&gt; DynamoDb. You need a lambda to store data to S3, Firehose doesn't store it natively"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/amazon/view/73992-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An ecommerce company ingests a large set of clickstream data in JSON format and stores the data in Amazon S3. Business analysts from multiple product divisions need to use Amazon Athena to analyze the data. The company's analytics team must design a solution to monitor the daily data usage for Athena by each product division. The solution also must produce a warning when a division exceeds its quota.<br>Which solution will meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a CREATE TABLE AS SELECT (CTAS) statement to create separate tables for each product division. Use AWS Budgets to track Athena usage. Configure a threshold for the budget. Use Amazon Simple Notification Service (Amazon SNS) to send notifications when thresholds are breached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS account for each division. Provide cross-account access to an AWS Glue Data Catalog to all the accounts. Set an Amazon CloudWatch alarm to monitor Athena usage. Use Amazon Simple Notification Service (Amazon SNS) to send notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Athena workgroup for each division. Configure a data usage control for each workgroup and a time period of 1 day. Configure an action to send notifications to an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS account for each division. Configure an AWS Glue Data Catalog in each account. Set an Amazon CloudWatch alarm to monitor Athena usage. Use Amazon Simple Notification Service (Amazon SNS) to send notifications."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-21T01:58:00.000Z",
        "voteCount": 14,
        "content": "Use workgroups to separate users, teams, applications, or workloads, to set limits on amount of data each query or the entire workgroup can process, and to track costs."
      },
      {
        "date": "2023-06-28T18:00:00.000Z",
        "voteCount": 2,
        "content": "it's C. No doubts at all. Workgroups offers data usage control and management of query quotas."
      },
      {
        "date": "2023-05-01T13:55:00.000Z",
        "voteCount": 1,
        "content": "C: I passed the test"
      },
      {
        "date": "2022-10-12T23:24:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html"
      },
      {
        "date": "2022-07-27T23:41:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2022-05-21T05:22:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C"
      },
      {
        "date": "2022-04-23T12:24:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/amazon/view/73990-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "A banking company is currently using Amazon Redshift for sensitive data. An audit found that the current cluster is unencrypted. Compliance requires that a database with sensitive data must be encrypted using a hardware security module (HSM) with customer managed keys.<br>Which modifications are required in the cluster to ensure compliance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new HSM-encrypted Amazon Redshift cluster and migrate the data to the new cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB parameter group with the appropriate encryption settings and then restart the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable HSM encryption in Amazon Redshift using the command line.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Amazon Redshift cluster from the console and enable encryption using the HSM option."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T13:04:00.000Z",
        "voteCount": 9,
        "content": "To migrate an unencrypted cluster to a cluster encrypted using a hardware security module (HSM), you create a new encrypted cluster and move your data to the new cluster. You can't migrate to an HSM-encrypted cluster by modifying the cluster.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html"
      },
      {
        "date": "2023-05-01T13:55:00.000Z",
        "voteCount": 2,
        "content": "A: I passed the test"
      },
      {
        "date": "2022-12-15T07:46:00.000Z",
        "voteCount": 3,
        "content": "You can't enable hardware security module (HSM) encryption by modifying the cluster. Instead, create a new, HSM-encrypted cluster and migrate your data to the new cluster. \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html"
      },
      {
        "date": "2022-07-21T19:59:00.000Z",
        "voteCount": 1,
        "content": "Answer-A"
      },
      {
        "date": "2022-04-30T16:10:00.000Z",
        "voteCount": 1,
        "content": "Answer-A"
      },
      {
        "date": "2022-04-24T08:20:00.000Z",
        "voteCount": 3,
        "content": "A - New Cluster needs to be created as Encryption only available during Cluster creation."
      },
      {
        "date": "2022-04-21T01:55:00.000Z",
        "voteCount": 2,
        "content": "Encryption can only be enabled when creating the cluster"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/amazon/view/74619-exam-aws-certified-data-analytics-specialty-topic-1-question/",
    "body": "An advertising company has a data lake that is built on Amazon S3. The company uses AWS Glue Data Catalog to maintain the metadata. The data lake is several years old and its overall size has increased exponentially as additional data sources and metadata are stored in the data lake. The data lake administrator wants to implement a mechanism to simplify permissions management between Amazon S3 and the Data Catalog to keep them in sync.<br>Which solution will simplify permissions management with minimal development effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet AWS Identity and Access Management (IAM) permissions for AWS Glue",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lake Formation permissions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManage AWS Glue and S3 permissions by using bucket policies",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Cognito user pools"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-01T13:56:00.000Z",
        "voteCount": 3,
        "content": "B: I passed the test"
      },
      {
        "date": "2023-02-14T22:11:00.000Z",
        "voteCount": 1,
        "content": "B over A - a simpler GRANT/REVOKE permissions model"
      },
      {
        "date": "2022-07-20T23:25:00.000Z",
        "voteCount": 4,
        "content": "B is right answer"
      },
      {
        "date": "2022-07-06T06:34:00.000Z",
        "voteCount": 1,
        "content": "Isn't it meant by saying \"Data lake is many year old\" that it was not produced by Lake Formation (introduced on 2018). And trick us to select our choice by forgeting this fact?"
      },
      {
        "date": "2022-12-05T18:45:00.000Z",
        "voteCount": 3,
        "content": "It doesn't matter. You can add the S3 bucket at a later time to Lake Formation."
      },
      {
        "date": "2022-04-30T14:19:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-04-28T06:03:00.000Z",
        "voteCount": 3,
        "content": "You can grant access to your data by using AWS Glue methods or by using AWS Lake Formation grants. The AWS Glue methods use AWS Identity and Access Management (IAM) policies to achieve fine-grained access control. Lake Formation uses a simpler GRANT/REVOKE permissions model similar to the GRANT/REVOKE commands in a relational database system.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/access-control-overview.html"
      },
      {
        "date": "2022-04-28T05:33:00.000Z",
        "voteCount": 4,
        "content": "Textbook use case for  AWS Lake Formation"
      },
      {
        "date": "2022-04-26T12:56:00.000Z",
        "voteCount": 1,
        "content": "permission"
      },
      {
        "date": "2022-04-26T11:15:00.000Z",
        "voteCount": 1,
        "content": "B is right answer"
      }
    ],
    "examNameCode": "aws-certified-data-analytics-specialty",
    "topicNumber": "1"
  }
]