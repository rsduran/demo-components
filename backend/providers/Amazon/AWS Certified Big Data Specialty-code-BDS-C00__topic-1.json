[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/5297-exam-aws-certified-big-data-specialty-topic-1-question-1/",
    "body": "A data engineer in a manufacturing company is designing a data processing platform that receives a large volume of unstructured data. The data engineer must populate a well-structured star schema in Amazon<br>Redshift.<br>What is the most efficient architecture strategy for this purpose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform the unstructured data using Amazon EMR and generate CSV data. COPY the CSV data into the analysis schema within Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the unstructured data into Redshift, and use string parsing functions to extract structured data for inserting into the analysis schema.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the data is saved to Amazon S3, use S3 Event Notifications and AWS Lambda to transform the file contents. Insert the data into the analysis schema on Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the data using an AWS Marketplace ETL tool, persist the results to Amazon S3, and use AWS Lambda to INSERT the data into Redshift."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-31T06:35:00.000Z",
        "voteCount": 9,
        "content": "A is correct.\nNot B - never load unstructured data to Redshift\nNot C - s3 event + lambda would be more suitable for incremental, continuous S3-Redshift integration. Here, we have one large bulk load, so event notifications don't make sense and lambda may not be able to handle all transformation in one call due to service limits.\nNot D - Normalization is the act of adjusting values on a scale, usually subtracting mean and dividing by standard deviation. That doesn't make sense here."
      },
      {
        "date": "2021-11-06T05:13:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-10-31T23:46:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2021-10-31T01:34:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2021-10-28T04:04:00.000Z",
        "voteCount": 1,
        "content": "Selected A."
      },
      {
        "date": "2021-10-16T15:52:00.000Z",
        "voteCount": 3,
        "content": "option a is correct. Using EMR, we can process un-structured data, and put schema on top of it before saving it to s3"
      },
      {
        "date": "2021-10-06T14:57:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2021-09-22T17:00:00.000Z",
        "voteCount": 2,
        "content": "Answer is A."
      },
      {
        "date": "2021-09-19T13:21:00.000Z",
        "voteCount": 2,
        "content": "answer a is correct"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/3484-exam-aws-certified-big-data-specialty-topic-1-question-2/",
    "body": "A new algorithm has been written in Python to identify SPAM e-mails. The algorithm analyzes the free text contained within a sample set of 1 million e-mails stored on Amazon S3. The algorithm must be scaled across a production dataset of 5 PB, which also resides in Amazon S3 storage.<br>Which AWS service strategy is best for this use case?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the data into Amazon ElastiCache to perform text analysis on the in-memory data and export the results of the model into Amazon Machine Learning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to parallelize the text analysis tasks across the cluster using a streaming program step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Elasticsearch Service to store the text and then use the Python Elasticsearch Client to run analysis against the text index.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInitiate a Python job from AWS Data Pipeline to run directly against the Amazon S3 text files."
    ],
    "answer": "C",
    "answerDescription": "Reference: https://aws.amazon.com/blogs/database/indexing-metadata-in-amazon-elasticsearch-service- using-aws-lambda-and-python/",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-02T00:05:00.000Z",
        "voteCount": 6,
        "content": "Answer is B: Hadoop Streaming is a utility that comes with Hadoop that enables you to develop MapReduce executables in languages other than Java. Streaming is implemented in the form of a JAR file, so you can run it from the Amazon EMR API or command line just like a standard JAR file. \n https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UseCase_Streaming.html"
      },
      {
        "date": "2024-01-09T00:52:00.000Z",
        "voteCount": 1,
        "content": "Option B (Use Amazon EMR) is the best service strategy for this use case. EMR can handle the scale of the data and provides the necessary computational resources to run complex text analysis algorithms in parallel across a large cluster, making it a suitable choice for processing 5 PB of data."
      },
      {
        "date": "2021-11-06T23:32:00.000Z",
        "voteCount": 1,
        "content": "Option B.\nEMR supports on S3 supports 5PB. Hadoop or spark steaming supports Python algo. \nElastisearch has limit of 30TB for a single domain. Even after request to Amazon to increase, it can go max 300TB, so not possible to store 3TB.\nSecondly, using lambda in this case might not be good choice with such huge volume of data flowing in."
      },
      {
        "date": "2021-11-03T21:20:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/CLI_CreateStreaming.html"
      },
      {
        "date": "2021-10-30T09:42:00.000Z",
        "voteCount": 2,
        "content": "C cannot be a right answer considering that the question does not mention about a Lambda function to copy from S3, secondly the question is talking about 5PB of data. Elasticsearch can not support 5 PB. Option B is the correct answer as the data is huge EMR can be used to parallelly analyse the data using Streaming program which supports python."
      },
      {
        "date": "2021-10-29T22:11:00.000Z",
        "voteCount": 1,
        "content": "C is reasonable"
      },
      {
        "date": "2021-10-29T13:28:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2021-10-27T01:30:00.000Z",
        "voteCount": 1,
        "content": "Selected B"
      },
      {
        "date": "2021-10-15T07:38:00.000Z",
        "voteCount": 1,
        "content": "The question is lying here:\nThe algorithm analyzes the free text contained within a sample set of 1 million e-mails stored on Amazon S3.\nThis secondary information is for overthinking:\nThe algorithm must be scaled across a production dataset of 5 PB, which also resides in Amazon S3 storage. (Must notice...also resides in Amazon S3 storage.)\nSo clearly the correct answer is C"
      },
      {
        "date": "2021-10-09T19:31:00.000Z",
        "voteCount": 2,
        "content": "It can NOT be B, cause it says that 'using a streaming program step', how can a streaming program analysis text content and figure out if the content is spam then give feedback the content belongs to which piece of mail?"
      },
      {
        "date": "2021-10-09T20:42:00.000Z",
        "voteCount": 1,
        "content": "For me, C is the right one"
      },
      {
        "date": "2021-10-12T10:54:00.000Z",
        "voteCount": 2,
        "content": "AWS ES cant support more than 3 PB data"
      },
      {
        "date": "2021-11-02T04:41:00.000Z",
        "voteCount": 1,
        "content": "streaming could mean using hadoop streaming/spark streaming to process files in S3. Each row in csv is sent over to the streaming python function to be process. B is could be a good option to parallelize a function thats working good on a sample."
      },
      {
        "date": "2021-10-08T23:58:00.000Z",
        "voteCount": 1,
        "content": "I dont understand, I would have said also that answer is B but it seems it is not, is it possible that in the exam not only one answer is allowed?"
      },
      {
        "date": "2021-11-02T13:27:00.000Z",
        "voteCount": 1,
        "content": "thing is, the answers provided by examtopics are often wrong for some reason (at least for this exam)"
      },
      {
        "date": "2021-10-04T08:31:00.000Z",
        "voteCount": 2,
        "content": "for data size in PB, EMR and answer B should be"
      },
      {
        "date": "2021-10-02T09:57:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-09-29T13:29:00.000Z",
        "voteCount": 3,
        "content": "Spam filtering is a machine learning algorithm. It works with EMR and S3 which are most suitable scenario. b is the correct answer"
      },
      {
        "date": "2021-09-26T09:24:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer , since Amazon Elasticsearch Servic can not suppot 5 PB"
      },
      {
        "date": "2021-10-18T11:26:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/ru/elasticsearch-service/faqs/\n\nQ: Is there a limit on the amount of EBS storage that can be allocated to an Amazon Elasticsearch Service domain?\n\nYes. Amazon Elasticsearch Service supports one EBS volume (max size of 1.5 TB) per instance associated with a domain. With the default maximum of 20 data nodes allowed per Amazon Elasticsearch Service domain, you can allocate about 30 TB of EBS storage to a single domain. You can request a service limit increase up to 200 instances per domain by creating a case with the AWS Support Center. With 200 instances, you can allocate about 300 TB of EBS storage to a single domain."
      },
      {
        "date": "2021-09-21T16:52:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-09-22T04:36:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/indexing-metadata-in-amazon-elasticsearch-service- using-aws-lambda-and-python/"
      },
      {
        "date": "2021-09-24T13:22:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UseCase_Streaming.html)"
      },
      {
        "date": "2021-09-29T19:29:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-10-18T22:11:00.000Z",
        "voteCount": 1,
        "content": "Hi, if B is correct, what does this URL you pasted, explains?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/5298-exam-aws-certified-big-data-specialty-topic-1-question-3/",
    "body": "A data engineer chooses Amazon DynamoDB as a data store for a regulated application. This application must be submitted to regulators for review. The data engineer needs to provide a control framework that lists the security controls from the process to follow to add new users down to the physical controls of the data center, including items like security guards and cameras.<br>How should this control mapping be achieved using AWS?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest AWS third-party audit reports and/or the AWS quality addendum and map the AWS responsibilities to the controls that must be provided.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest data center Temporary Auditor access to an AWS data center to verify the control mapping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest relevant SLAs and security guidelines for Amazon DynamoDB and define these guidelines within the applications architecture to map to the control framework.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest Amazon DynamoDB system architecture designs to determine how to map the AWS responsibilities to the control that must be provided."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-02T02:20:00.000Z",
        "voteCount": 1,
        "content": "A. Should be the right answer.\nhttps://aws.amazon.com/pt/compliance/soc-faqs/\nhttps://aws.amazon.com/pt/artifact/faq/"
      },
      {
        "date": "2021-10-28T02:29:00.000Z",
        "voteCount": 3,
        "content": "A :: AWS System and Organization Controls (SOC) Reports are independent third-party examination reports that demonstrate how AWS achieves key compliance controls and objectives. The purpose of these reports is to help you and your auditors understand the AWS controls established to support operations and compliance."
      },
      {
        "date": "2021-10-24T18:34:00.000Z",
        "voteCount": 2,
        "content": "Selected A"
      },
      {
        "date": "2021-10-05T20:50:00.000Z",
        "voteCount": 3,
        "content": "My answer is A because it's the only way to get the control framework details and map the responsibilities. B, C and D are distractors."
      },
      {
        "date": "2021-10-05T17:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2021-10-04T10:54:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/3422-exam-aws-certified-big-data-specialty-topic-1-question-4/",
    "body": "An administrator needs to design a distribution strategy for a star schema in a Redshift cluster. The administrator needs to determine the optimal distribution style for the tables in the Redshift schema.<br>In which three circumstances would choosing Key-based distribution be most appropriate? (Select three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the administrator needs to optimize a large, slowly changing dimension table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the administrator needs to reduce cross-node traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the administrator needs to optimize the fact table for parity with the number of slices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the administrator needs to balance data distribution and collocation data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the administrator needs to take advantage of data locality on a local node for joins and aggregates."
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-25T23:21:00.000Z",
        "voteCount": 7,
        "content": "bde? any thoughts?"
      },
      {
        "date": "2021-09-26T06:13:00.000Z",
        "voteCount": 3,
        "content": "agreed"
      },
      {
        "date": "2021-09-28T01:23:00.000Z",
        "voteCount": 3,
        "content": "BDE seems good"
      },
      {
        "date": "2021-10-07T09:45:00.000Z",
        "voteCount": 2,
        "content": "I also choose BDE"
      },
      {
        "date": "2021-10-30T02:17:00.000Z",
        "voteCount": 1,
        "content": "For slowly changing dimensions of reasonable size, DISTSTYLE ALL is a good choice for the dimension (reasonable size in this case means up to a few million rows, and that the number of rows in the dimension table is fewer than the filtered fact table for a typical join)"
      },
      {
        "date": "2021-10-27T10:17:00.000Z",
        "voteCount": 7,
        "content": "B,D and E are the right choices. Please read the article at the below link in its entirety and watch out for phrases used as options in this question and you will also arrive at B, D and E.\nhttps://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/"
      },
      {
        "date": "2021-11-03T12:55:00.000Z",
        "voteCount": 1,
        "content": "ABD is correct. Reference https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/"
      },
      {
        "date": "2021-10-31T18:26:00.000Z",
        "voteCount": 2,
        "content": "B cannot be the right option cause it will not reduce the cross-node traffic. I would choose C,D,E."
      },
      {
        "date": "2021-10-25T20:03:00.000Z",
        "voteCount": 1,
        "content": "BCE. why C because fact table's corresponding foreign key as DISTKEY which is a pattern for Key Distribution Type"
      },
      {
        "date": "2021-10-25T17:33:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      },
      {
        "date": "2021-10-23T05:39:00.000Z",
        "voteCount": 3,
        "content": "Selected BDE"
      },
      {
        "date": "2021-10-22T23:34:00.000Z",
        "voteCount": 2,
        "content": "B,D,E are my options"
      },
      {
        "date": "2021-10-22T22:20:00.000Z",
        "voteCount": 1,
        "content": "From my point of view:\nA - is an option for ALL  distribution.\nC - is an option for even distribution\n\nBDE - is a right answers"
      },
      {
        "date": "2021-10-18T13:58:00.000Z",
        "voteCount": 1,
        "content": "A,C,D looks correct \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html"
      },
      {
        "date": "2021-10-15T19:44:00.000Z",
        "voteCount": 1,
        "content": "Options B and E are for the outlook of KEY distribution.\nQuestion is for the determining factors for KEY distribution (DISTKEY), So Options A, C and D are the correct answers.\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html\nhttps://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-distribution-styles-and-distribution-keys/"
      },
      {
        "date": "2021-10-15T11:04:00.000Z",
        "voteCount": 1,
        "content": "BCE looks good, key distribution is for fact table in star schema."
      },
      {
        "date": "2021-10-15T09:20:00.000Z",
        "voteCount": 2,
        "content": "BDE seems right"
      },
      {
        "date": "2021-10-06T07:44:00.000Z",
        "voteCount": 2,
        "content": "BCE looks correct to me. D is not because even distribution does the balancing."
      },
      {
        "date": "2021-10-28T02:14:00.000Z",
        "voteCount": 3,
        "content": "No, because collocation is not possible with even distribution. So in order to achieve collocation and balancing, Key-distribution style is needed. Also, C is really a great use case for Even-distribution style, so C is wrong."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/3512-exam-aws-certified-big-data-specialty-topic-1-question-5/",
    "body": "Company A operates in Country X. Company A maintains a large dataset of historical purchase orders that contains personal data of their customers in the form of full names and telephone numbers. The dataset consists of 5 text files, 1TB each. Currently the dataset resides on-premises due to legal requirements of storing personal data in-country. The research and development department needs to run a clustering algorithm on the dataset and wants to use Elastic Map Reduce service in the closest AWS region. Due to geographic distance, the minimum latency between the on-premises system and the closet AWS region is 200 ms.<br>Which option allows Company A to do clustering in the AWS Cloud and meet the legal requirement of maintaining personal data in-country?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnonymize the personal data portions of the dataset and transfer the data files into Amazon S3 in the AWS region. Have the EMR cluster read the dataset using EMRFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a Direct Connect link between the on-premises system and the AWS region to reduce latency. Have the EMR cluster read the data directly from the on-premises storage system over Direct Connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the data files according to encryption standards of Country X and store them on AWS region in Amazon S3. Have the EMR cluster read the dataset using EMRFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Import/Export Snowball device to securely transfer the data to the AWS region and copy the files onto an EBS volume. Have the EMR cluster read the dataset using EMRFS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-20T09:51:00.000Z",
        "voteCount": 7,
        "content": "A. EMR cant ingest data from on on premise and also there would be alot of latency."
      },
      {
        "date": "2021-09-20T10:53:00.000Z",
        "voteCount": 2,
        "content": "I think A is not an option. When the data is being anonymous, you can not categorize the users because there is no user info at the dataset."
      },
      {
        "date": "2021-09-20T11:52:00.000Z",
        "voteCount": 6,
        "content": "What about with the key statement being \"legal requirements of storing personal data in-country.\" If the AWS region is out of country (which is implied by the distance but not directly stated), then isnt A is the answer. All other options put customer data in AWS (either through encryption or copying), which does not meet the legal requirement. \n\nData anonymization before analysis is the demand for a number of compliance standards like HIPAA, GDPR etc. In the question it's underlined - \"meet the legal requirement of maintaining personal data in-country\". Data should be anonymized , transferred to S3. After use EMR with EMRFS to directly process data from S3."
      },
      {
        "date": "2021-09-20T13:31:00.000Z",
        "voteCount": 2,
        "content": "I go with A"
      },
      {
        "date": "2021-09-25T09:51:00.000Z",
        "voteCount": 1,
        "content": "I also choose A. When you anonymous data, you can still main the actual-anonymous map onpremis, run ML in aws and map it back to onpremis actual."
      },
      {
        "date": "2021-10-02T03:49:00.000Z",
        "voteCount": 2,
        "content": "You can anonymous data and still run cluster algorithm."
      },
      {
        "date": "2021-10-21T05:23:00.000Z",
        "voteCount": 3,
        "content": "you do not need personal information like name and telephone numbers for clustering... I would go with option A"
      },
      {
        "date": "2021-09-28T10:35:00.000Z",
        "voteCount": 2,
        "content": "Amazon EMR provides several ways to get data onto a cluster. If you have large amounts of on-premises data to process, you may find the AWS Direct Connect service useful\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html"
      },
      {
        "date": "2021-10-19T07:30:00.000Z",
        "voteCount": 1,
        "content": "Exactly, Option B is the correct answer."
      },
      {
        "date": "2021-10-18T17:26:00.000Z",
        "voteCount": 6,
        "content": "Option A is the right answer: I have worked with PII and PHI data in US. Anonymization/de-identification/tokenization of personalized data is how most of the companies deal with moving and operating on sensitive data without violating laws and regulations around privacy. Once anonymized, it is does not matter where the data lives. \n\nOption B is not right because 1) The problem statement does not clearly say whether the nearest AWS region is within country X. Even if, EMR can only operate on files that are either in S3 or HDFS (instance store/EBS). It is not possible to work with remote data directly with EMR (SAN/FTP/etc). Option A will work much better and cost effective (direct connect still costs you money to move data. However S3 to EMR is free)"
      },
      {
        "date": "2021-11-06T08:50:00.000Z",
        "voteCount": 1,
        "content": "agree with option A"
      },
      {
        "date": "2021-10-24T10:12:00.000Z",
        "voteCount": 1,
        "content": "B is clearly wrong. EMR can't use on premise data directly using connect direct."
      },
      {
        "date": "2021-10-25T20:43:00.000Z",
        "voteCount": 3,
        "content": "You can, https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html, and another thing, people are confusing the fact that the data CAN'T LEAVE the onpremise, so doest not matter if it's obfuscated, anonymized, encrypted, whatever... it CAN'T, so the only option is to build an interconnection and extend your network."
      },
      {
        "date": "2021-10-27T08:25:00.000Z",
        "voteCount": 1,
        "content": "read the links ppl have posted emr can be used with on prem"
      },
      {
        "date": "2021-10-24T09:54:00.000Z",
        "voteCount": 1,
        "content": "IMO it should be A. Main emphasize is on maintaining legal requirements for handling PII data. So option A is most accurate."
      },
      {
        "date": "2021-10-24T05:16:00.000Z",
        "voteCount": 1,
        "content": "Answer B - Beginning with Amazon EMR version 5.28.0, you can create and run EMR clusters on AWS Outposts. AWS Outposts enables native AWS services, infrastructure, and operating models in on-premises facilities https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-outposts.html"
      },
      {
        "date": "2021-10-22T17:56:00.000Z",
        "voteCount": 2,
        "content": "Option B.\nIf you have large amounts of on-premises data to process, you may find the AWS Direct Connect service useful. (Read Paragraph 1)\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html"
      },
      {
        "date": "2021-10-22T09:18:00.000Z",
        "voteCount": 5,
        "content": "Absolutely B,\nScenario 2. Use AWS Direct Connect to connect your data center with AWS resources. Once connected, you can use Amazon EMR to process your data stored in your own data center and store the results on AWS or back in your data center. This approach gives you 1 or 10 gigabit-per-second link connectivity to AWS at all time. And directconnect outbound bandwidth costs less than public Internet outbound cost. So in cases where you expect great amount of traffic exported to your own data center, having direct connect in place can reduce your bandwidth charges.\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf"
      },
      {
        "date": "2021-10-21T11:27:00.000Z",
        "voteCount": 2,
        "content": "Selected A"
      },
      {
        "date": "2021-10-19T14:54:00.000Z",
        "voteCount": 1,
        "content": "answer B.\nhttps://aws.amazon.com/getting-started/projects/connect-data-center-to-aws/"
      },
      {
        "date": "2021-10-19T09:20:00.000Z",
        "voteCount": 1,
        "content": "i will go with  B.\nhe most common way is to upload the data to Amazon S3 and use the built-in features of Amazon EMR to load the data onto your cluster. You can also use the Distributed Cache feature of Hadoop to transfer files from a distributed file system to the local file system. The implementation of Hive provided by Amazon EMR (Hive version 0.7.1.1 and later) includes functionality that you can use to import and export data between DynamoDB and an Amazon EMR cluster. If you have large amounts of on-premises data to process, you may find the AWS Direct Connect service useful.\n https://aws.amazon.com/emr/features/outposts/"
      },
      {
        "date": "2021-10-14T08:23:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A as the latency is high it would be ideal to transfer the data to AWS and process using EMR and EMRFS. Also, anonymizing the data would help meet the legal requirement."
      },
      {
        "date": "2021-10-10T14:13:00.000Z",
        "voteCount": 4,
        "content": "Ladies and gentlemen I have found the answer I believe and it is B. Since we all know a direct connection will reduce latency the next question is if EMR can handle data directly from on prem and here is a quote that will clear this up.\n\nAWS Outposts bring AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility. Amazon EMR is available on AWS Outposts, allowing you to set up, deploy, manage, and scale Apache Hadoop, Apache Hive, Apache Spark, and Presto clusters in your on-premises environments, just as you would in the cloud\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-input-directconnect.html\nhttps://aws.amazon.com/emr/features/outposts/"
      },
      {
        "date": "2021-11-05T22:07:00.000Z",
        "voteCount": 1,
        "content": "AWS Outposts is different from DirectConnect. It does not mention anything about Outposts in the question"
      },
      {
        "date": "2021-10-02T04:53:00.000Z",
        "voteCount": 2,
        "content": "I work with data redisency use cases (China Cybersecurity Law, EUs GDPR etc.) - option B would be absolute violation of data-in-country requirements - of course reding data from EMR cluster would move data personal data our of country even for temporary processing! I don't know who chose the answer, event if it's from AWS it's just not right."
      },
      {
        "date": "2021-10-10T01:32:00.000Z",
        "voteCount": 3,
        "content": "I must disagree with you, B would not be an absolute violation of data-in-country laws. The United States has multiple regions inside of it. A direct connection from a business located in the U.S. to a region in the U.S. would absolutely be allowed. Processing the data in and EMR cluster would also not be a violation as you can configure your settings to ensure the data remains within the same region. B is the answer."
      },
      {
        "date": "2021-09-28T02:46:00.000Z",
        "voteCount": 3,
        "content": "Answer B looks reasonable.\n\nJustification points :\na) Options A,C and E talk about moving data out of the premises. The mentioned dataset resides on-premises due to legal requirements of storing personal data in-country.\nb) Given that the minimum latency between the on-premises system and the closet AWS region is 200 ms. Time that would be taken to move 5 Text Files of 1TB should also be considered."
      },
      {
        "date": "2021-09-29T08:09:00.000Z",
        "voteCount": 2,
        "content": "Agree, i will go with B"
      },
      {
        "date": "2021-10-14T07:44:00.000Z",
        "voteCount": 2,
        "content": "emr cannot \"read data from direct connect\""
      },
      {
        "date": "2021-09-23T01:56:00.000Z",
        "voteCount": 1,
        "content": "these question is related to data security. A looks more accurate bcoz in options B it says direct connection to reduce latency, which will not be the case as it is reading from onpremise it will be slow and does not address the confidentiality issue of maintaining personal data in country"
      },
      {
        "date": "2021-09-22T11:41:00.000Z",
        "voteCount": 3,
        "content": "looks B but not sure. anyone can explain further ?"
      },
      {
        "date": "2021-09-21T23:34:00.000Z",
        "voteCount": 2,
        "content": "Why not B?\nData can be transfer on direct connect\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-input-directconnect.html"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/3261-exam-aws-certified-big-data-specialty-topic-1-question-6/",
    "body": "An administrator needs to design a strategy for the schema in a Redshift cluster. The administrator needs to determine the optimal distribution style for the tables in the Redshift schema.<br>In which two circumstances would choosing EVEN distribution be most appropriate? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the tables are highly denormalized and do NOT participate in frequent joins.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen data must be grouped based on a specific key on a defined slice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen data transfer between nodes must be eliminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen a new table has been loaded and it is unclear how it will be joined to dimension."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-22T12:44:00.000Z",
        "voteCount": 17,
        "content": "I think it should be A &amp; D. B is describing Key style"
      },
      {
        "date": "2021-11-07T08:27:00.000Z",
        "voteCount": 1,
        "content": "A &amp;D . Not sure how B is correct"
      },
      {
        "date": "2021-10-31T00:20:00.000Z",
        "voteCount": 2,
        "content": "my selection A,D"
      },
      {
        "date": "2021-10-29T23:32:00.000Z",
        "voteCount": 1,
        "content": "A,D is correct \nreference: https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"
      },
      {
        "date": "2021-10-25T10:50:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D--EVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution."
      },
      {
        "date": "2021-10-21T21:23:00.000Z",
        "voteCount": 1,
        "content": "A&amp;D. This site contains lot of bad default answers.  Thanks for these discussions. They provide much-needed corrections."
      },
      {
        "date": "2021-10-14T15:45:00.000Z",
        "voteCount": 1,
        "content": "A,D for sure."
      },
      {
        "date": "2021-10-11T22:34:00.000Z",
        "voteCount": 1,
        "content": "AD seems right"
      },
      {
        "date": "2021-10-09T15:56:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D are correct.\n\nAmazon Redshift EVEN distribution : \nThe leader node distributes the rows across the slices in a round-robin fashion, regardless of the values in any particular column. \nEVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.\n\nRefer : https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"
      },
      {
        "date": "2021-10-09T01:43:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D is correct as the rest 2 doesnt apply to Even distribution"
      },
      {
        "date": "2021-10-03T15:43:00.000Z",
        "voteCount": 1,
        "content": "A and D are the right answers for this question . because when you are not sure about data you can choose even and when no more joins are required"
      },
      {
        "date": "2021-10-01T22:33:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D plase!"
      },
      {
        "date": "2021-10-02T00:08:00.000Z",
        "voteCount": 1,
        "content": "I also think AD"
      },
      {
        "date": "2021-10-01T12:54:00.000Z",
        "voteCount": 1,
        "content": "a,d is the correct answer"
      },
      {
        "date": "2021-09-29T05:08:00.000Z",
        "voteCount": 3,
        "content": "a d are right"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/5299-exam-aws-certified-big-data-specialty-topic-1-question-7/",
    "body": "A large grocery distributor receives daily depletion reports from the field in the form of gzip archives od CSV files uploaded to Amazon S3. The files range from 500MB to 5GB. These files are processed daily by an EMR job.<br>Recently it has been observed that the file sizes vary, and the EMR jobs take too long. The distributor needs to tune and optimize the data processing workflow with this limited information to improve the performance of the<br>EMR job.<br>Which recommendation should an administrator provide?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the HDFS block size to increase the number of task processors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse bzip2 or Snappy rather than gzip for the archives.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecompress the gzip archives and store the data as CSV files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Avro rather than gzip for the archives."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-08T09:34:00.000Z",
        "voteCount": 8,
        "content": "B looks to be the right answer.\n\nNotes: \n(*) GZIP compression uses more CPU resources than Snappy or LZO, but provides a higher compression ratio. GZip is often a good choice for cold data, which is accessed infrequently. \n(*) Snappy or LZO are a better choice for hot data, which is accessed frequently.\n(*) BZip2 can also produce more compression than GZip for some types of files, at the cost of some speed when compressing and decompressing. \n(*) For MapReduce, if you need your compressed data to be splittable, BZip2, LZO, and Snappy formats are splittable, but GZip is not. \n\nRefer : https://docs.cloudera.com/documentation/enterprise/5-3-x/topics/admin_data_compression_performance.html"
      },
      {
        "date": "2021-11-03T05:17:00.000Z",
        "voteCount": 1,
        "content": "Ans is D. A 5GB GZIP file will turn out be bigger when compressed with SNAPPY. And snappy is not splittable"
      },
      {
        "date": "2021-10-20T18:36:00.000Z",
        "voteCount": 1,
        "content": "D was my choice. Bzip2 is splittable, but snappy is not. So B seems odd. Avro is splittable, so this conversion should help."
      },
      {
        "date": "2021-10-22T10:18:00.000Z",
        "voteCount": 1,
        "content": "B is still valid:\n\nBzip2 if splitting.\nSnappy if compressing."
      },
      {
        "date": "2021-10-20T12:43:00.000Z",
        "voteCount": 1,
        "content": "For\ninstance, if you are aggregating your data (using the ingest tool of your choice) and the aggregated data files are\nLarge File\nHTTP Range Request: 64MB\nHTTP Range Request: 64MB\nHTTP Range Request: 64MB\nHTTP Range Request: 64MB\nMap Task\nMap Task\nMap Task\nMap Task\nEMR Cluster\nS3 Bucket\nAmazon Web Services \u2013 Best Practices for Amazon EMR August 2013\nPage 16 of 38\nbetween 500 MB to 1 GB, GZIP compression is an acceptable data compression type. However, if your data aggregation\ncreates files larger than 1 GB, its best to pick a compression algorithm that supports splitting.\n\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf"
      },
      {
        "date": "2021-10-18T14:23:00.000Z",
        "voteCount": 2,
        "content": "my selection B"
      },
      {
        "date": "2021-10-13T10:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf"
      },
      {
        "date": "2021-10-05T17:59:00.000Z",
        "voteCount": 3,
        "content": "B looks correct. bzip2 splittable or snappy compress-decompress speed very fast"
      },
      {
        "date": "2021-09-24T06:06:00.000Z",
        "voteCount": 1,
        "content": "Snappy is not split table as well however it does compression and decompression quickly as compare to gzip so Answer would still be B."
      },
      {
        "date": "2021-09-22T19:08:00.000Z",
        "voteCount": 3,
        "content": "B is right answer : reason Bzip2 and snappy can split files and gzip can't split"
      },
      {
        "date": "2021-09-19T19:38:00.000Z",
        "voteCount": 3,
        "content": "B looks good"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/2485-exam-aws-certified-big-data-specialty-topic-1-question-8/",
    "body": "A web-hosting company is building a web analytics tool to capture clickstream data from all of the websites hosted within its platform and to provide near-real-time business intelligence. This entire system is built on<br>AWS services. The web-hosting company is interested in using Amazon Kinesis to collect this data and perform sliding window analytics.<br>What is the most reliable and fault-tolerant technique to get each website to send data to Amazon Kinesis with every click?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the sessionID as a partition key and set up a loop to retry until a success response is received.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis Producer Library .addRecords method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach web server buffers the requests until the count reaches 500 and sends them to Amazon Kinesis using the Amazon Kinesis PutRecord API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the exponential back-off algorithm for retries until a successful response is received."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-13T05:16:00.000Z",
        "voteCount": 6,
        "content": "The KPL provides the following features out of the box:\n\n    Batching of puts using PutRecords (the Collector in the architecture diagram)\n    Tracking of record age and enforcement of maximum buffering times (all components)\n    Per-shard record aggregation (the Aggregator)\n    Retries in case of errors, with ability to distinguish between retryable and non-retryable errors (the Retrier)\n    Per-shard rate limiting to prevent excessive and pointless spamming (the Limiter)\n    Useful metrics and a highly efficient CloudWatch client (not shown in diagram)\n\nhttps://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\n\nthe answer should be B"
      },
      {
        "date": "2021-10-21T02:27:00.000Z",
        "voteCount": 2,
        "content": "PutRecords is right. Not sure if there a .addrecords"
      },
      {
        "date": "2021-11-04T14:21:00.000Z",
        "voteCount": 1,
        "content": "Keywords : near-real-time, most reliable and fault-tolerant technique\nAnswer is B : Kinesis Producer Library"
      },
      {
        "date": "2021-11-04T05:24:00.000Z",
        "voteCount": 1,
        "content": "B is right answer.\n\nThere seems to be typo in question - it must be addUserRecord method.  https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/"
      },
      {
        "date": "2021-11-02T19:25:00.000Z",
        "voteCount": 2,
        "content": "D\nhttps://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html"
      },
      {
        "date": "2021-10-29T07:44:00.000Z",
        "voteCount": 4,
        "content": "The answer should be B however KPL does not have a method named .addRecords. It looks like typo, as the KPL method is addUserRecord."
      },
      {
        "date": "2021-10-28T18:40:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer \nA,is worng   need to setup back of retry .\nB. dont' exist addrecords api\nC.it is anti-patern"
      },
      {
        "date": "2021-10-27T10:37:00.000Z",
        "voteCount": 1,
        "content": "A is correct. B - there is no addrecords in KPL. D - There is no backoff algorithm in Kinesis Agent."
      },
      {
        "date": "2021-10-20T00:17:00.000Z",
        "voteCount": 2,
        "content": "After researching further, I think B is the right answer. KPL has built in fault tolerance with configurable retry mechanism and there is no need to write custom fault-tolerant logic to do so. Besides KPL allows us to batch the records ( aggregation and collection) out of the box without having to write custom code to achieve scalability."
      },
      {
        "date": "2021-11-02T12:01:00.000Z",
        "voteCount": 2,
        "content": "There is no addRecords method in KPL, so B is wrong."
      },
      {
        "date": "2021-10-19T00:51:00.000Z",
        "voteCount": 4,
        "content": "D is the right answer. You need to do a back off with retry. You can do it exponentially. This usually happens due to a hot partition. To ensure fault tolerance if using an PutRecord API you will need to handle ProvisionedThroughputExceeded  exceptions and the way to do that is to use backoff with retry mechanism. \n\nOption A seems to suggest we are doing a retry but in a loop. However if we don't do a back-off (meaning wait for a certain duration) before we retry, the system would continue to fail."
      },
      {
        "date": "2021-11-04T21:47:00.000Z",
        "voteCount": 1,
        "content": "Agree with D. Reference: https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/"
      },
      {
        "date": "2021-10-17T05:49:00.000Z",
        "voteCount": 2,
        "content": "B is the Correct Answer"
      },
      {
        "date": "2021-11-02T03:03:00.000Z",
        "voteCount": 1,
        "content": "There is no .addRecords method in KPL."
      },
      {
        "date": "2021-11-06T03:40:00.000Z",
        "voteCount": 1,
        "content": "Is it that option B implies . addUserRecord() method when it refers to .addRecord() method, because if that is the case, the correct answer should be B."
      },
      {
        "date": "2021-10-17T05:37:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. KPL is used when\nHigh performance, long-running producers\nAutomated and configurable retry mechanism\nSync and Async API(better perf for Async)\n100B records (high volume of data)"
      },
      {
        "date": "2021-10-14T19:13:00.000Z",
        "voteCount": 1,
        "content": "my selection A"
      },
      {
        "date": "2021-10-10T14:38:00.000Z",
        "voteCount": 2,
        "content": "why not B? seems to me you need to reinvent the wheel in A while kpl can do all that already"
      },
      {
        "date": "2021-11-01T00:51:00.000Z",
        "voteCount": 2,
        "content": "There is no .addRecords method in KPL."
      },
      {
        "date": "2021-10-09T10:39:00.000Z",
        "voteCount": 3,
        "content": "exponential back-off algorithm appears in answer D -&gt; this seems to be the correct one."
      },
      {
        "date": "2021-10-09T20:59:00.000Z",
        "voteCount": 5,
        "content": "replying to myself -&gt; there is no exponential back-off algorithm for KPL. It uses a more aggressive strategy to do retries. A is correct."
      },
      {
        "date": "2021-09-28T07:16:00.000Z",
        "voteCount": 2,
        "content": "A looks correct."
      },
      {
        "date": "2021-09-27T12:03:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2021-11-02T14:39:00.000Z",
        "voteCount": 1,
        "content": "The loop in A might create spamming due to excessive retries,. For example, KPL has a Rate limiting feature to deal with this, but our manual soultion here does not. Therefore A looks wrong to me."
      },
      {
        "date": "2021-09-20T00:58:00.000Z",
        "voteCount": 4,
        "content": "Option A is correct because there is concept of back-off algorithm in Kinesis\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-producer-adv-retries-rate-limiting.html"
      },
      {
        "date": "2021-09-26T23:37:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/3513-exam-aws-certified-big-data-specialty-topic-1-question-9/",
    "body": "A customer has an Amazon S3 bucket. Objects are uploaded simultaneously by a cluster of servers from multiple streams of data. The customer maintains a catalog of objects uploaded in Amazon S3 using an<br>Amazon DynamoDB table. This catalog has the following fileds: StreamName, TimeStamp, and ServerName, from which ObjectName can be obtained.<br>The customer needs to define the catalog to support querying for a given stream or server within a defined time range.<br>Which DynamoDB table scheme is most efficient to support these queries?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a Primary Key with ServerName as Partition Key and TimeStamp as Sort Key. Do NOT define a Local Secondary Index or Global Secondary Index.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a Primary Key with StreamName as Partition Key and TimeStamp followed by ServerName as Sort Key. Define a Global Secondary Index with ServerName as partition key and TimeStamp followed by StreamName.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a Primary Key with ServerName as Partition Key. Define a Local Secondary Index with StreamName as Partition Key. Define a Global Secondary Index with TimeStamp as Partition Key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a Primary Key with ServerName as Partition Key. Define a Local Secondary Index with TimeStamp as Partition Key. Define a Global Secondary Index with StreamName as Partition Key and TimeStamp as Sort Key."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-20T22:02:00.000Z",
        "voteCount": 7,
        "content": "Option B. You can use composite primary keys using a combination of (StreamName as the partition key and TimeStamp as the sort key) and (ServerName as the partition key and TimeStamp as the sort key) which would allow you to meet the requirements of the question."
      },
      {
        "date": "2021-09-25T20:52:00.000Z",
        "voteCount": 1,
        "content": "How can you create this? Define a Primary Key with StreamName as Partition Key and TimeStamp followed by ServerName as Sort Key. You can create streamname as partition key and timestamp as sortkey and cannot be timestamp and servername together as sort key. The question tells customer needs to define the catalog to support querying for a given stream or Server. Here it is OR not mandatory to be both so better option i would think is A."
      },
      {
        "date": "2021-10-02T19:40:00.000Z",
        "voteCount": 2,
        "content": "I think b has typo . no need to have a sort key like in B. \nin my opinion, or means that both is needed but not at the same time."
      },
      {
        "date": "2021-10-04T15:01:00.000Z",
        "voteCount": 1,
        "content": "With all the confusion, I still think B is the answer"
      },
      {
        "date": "2021-10-01T18:00:00.000Z",
        "voteCount": 1,
        "content": "Can't you create a primary key on two fields so combine streamname + timestamp as hash/primary key then create the sort key on server name. Then you can create your GSI table with no problem. Just my thoughts, as the question is written quite badly to deceive you. When i attempted the test last time i did choose A but didnt score too well for Collection or Storage section so i was thinking B as it also you to look up the streamname."
      },
      {
        "date": "2021-10-18T11:26:00.000Z",
        "voteCount": 1,
        "content": "You are right in terms of the best approaches. Anyway,  pay attention to this \"and TimeStamp followed by ServerName as Sort Key\" - this is absolutely wrong."
      },
      {
        "date": "2021-11-06T09:49:00.000Z",
        "voteCount": 1,
        "content": "C and D - Wrong, because LSI wrongly mentioned with different partition key. LSI has same partition key.\nA - Wrong because it does not allow query based on stream name. GSI is must for this\nB - Correct by compromise, but phrasing is so messed up. I read it as below. \nPrimary table : Partition Key : Stream Name, Sort Key : Timestamp, Attribute : Serrvername\nGSI : Partition Key: Server name, Sort Key: Timestamp, Attribute: Stream\nI read it like this because, \"timestamp, followed by servername, as Sort key\". Two comma missing, so Sort Key was mentioned for Timestamp, and followed by is a just a an additional attribute on same table , after sort key"
      },
      {
        "date": "2021-11-03T10:27:00.000Z",
        "voteCount": 3,
        "content": "Option B.\nThe sort key can be composite. Although it looks ugly.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html"
      },
      {
        "date": "2021-11-03T03:09:00.000Z",
        "voteCount": 1,
        "content": "None of them are exact solution. A sounds more appropriate...but still doesnt address query using StreamName"
      },
      {
        "date": "2021-11-03T00:58:00.000Z",
        "voteCount": 1,
        "content": "Refer to this link to confirm that you can create multiple LSIs on a table.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html"
      },
      {
        "date": "2021-11-02T18:36:00.000Z",
        "voteCount": 1,
        "content": "Option B. None of the option suggest the use of ServerName as a GSI which is requirement except for Option B. What is throwing people off is the phrase \" TimeStamp followed by ServerName as Sort Key\" and \" TimeStamp followed by StreamName\" in Option B. My interpretation is that it is suggesting we create 2 LSIs 1) TimeStamp 2) ServerName when StreamName is the partitionKey and similarly create 2 LSIs 1) TimeStamp and 2) StreamName when ServerName is the partitionKey. So I will go with Option B"
      },
      {
        "date": "2021-11-04T16:33:00.000Z",
        "voteCount": 1,
        "content": "Wording should be corrected: You don't create 2 LSI's, but one sort key (timestamp) and one LSI (ServerName or StreamName, resp.)"
      },
      {
        "date": "2021-11-02T15:56:00.000Z",
        "voteCount": 1,
        "content": "B is the Correct Answer"
      },
      {
        "date": "2021-10-30T11:00:00.000Z",
        "voteCount": 1,
        "content": "Option A.\n(Read this section: Example 1: Working with log data)\nhttps://aws.amazon.com/blogs/database/using-sort-keys-to-organize-data-in-amazon-dynamodb/"
      },
      {
        "date": "2021-10-31T04:01:00.000Z",
        "voteCount": 1,
        "content": "But it mentions \"compound sort key\" concept in Example 2: Working with chat messages, which seems to refer to option B."
      },
      {
        "date": "2021-10-25T16:55:00.000Z",
        "voteCount": 3,
        "content": "B!\nComposite Primary Key = partitionKey+compositeSortKey\nStreamNmae + ServerName#TImeStamp, ServerName + StreamName#TimeStamp"
      },
      {
        "date": "2021-10-23T22:13:00.000Z",
        "voteCount": 2,
        "content": "my selection B"
      },
      {
        "date": "2021-10-18T01:09:00.000Z",
        "voteCount": 1,
        "content": "Eliminate wrong answers.\nSo A is the only visible and correct answer."
      },
      {
        "date": "2021-10-13T02:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct since the requirement states Server OR Stream. Answer accommodates the requirement."
      },
      {
        "date": "2021-10-22T07:03:00.000Z",
        "voteCount": 1,
        "content": "Good catch so is the final answer A ?"
      },
      {
        "date": "2021-10-13T07:54:00.000Z",
        "voteCount": 2,
        "content": "\"given stream or server within a defined time range\" can be understood as query in different queries, not at the same time, especially in the context of DynamoDB with multiple possible query patters would make absolute sense"
      },
      {
        "date": "2021-10-12T16:09:00.000Z",
        "voteCount": 1,
        "content": "It is A"
      },
      {
        "date": "2021-10-11T05:13:00.000Z",
        "voteCount": 1,
        "content": "for c and d, LSI &amp; GSI both are not required\nfor a, there is no way of querying streamname \n b is correct answer."
      },
      {
        "date": "2021-09-25T13:17:00.000Z",
        "voteCount": 2,
        "content": "A is correct."
      },
      {
        "date": "2021-09-22T02:26:00.000Z",
        "voteCount": 3,
        "content": "no way of querying StreamName at option A. A is wrong"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/3514-exam-aws-certified-big-data-specialty-topic-1-question-10/",
    "body": "A company has several teams of analysts. Each team of analysts has their own cluster. The teams need to run<br>SQL queries using Hive, Spark-SQL, and Presto with Amazon EMR. The company needs to enable a centralized metadata layer to expose the Amazon S3 objects as tables to the analysts.<br>Which approach meets the requirement for a centralized metadata layer?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEMRFS consistent view with a common Amazon DynamoDB table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBootstrap action to change the Hive Metastore to an Amazon RDS database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ts3distcp with the outputManifest option to generate RDS DDL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNaming scheme support with automatic partition discovery from Amazon S3"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-22T00:31:00.000Z",
        "voteCount": 8,
        "content": "correct answer is A. You can check it from https://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html"
      },
      {
        "date": "2021-10-27T21:08:00.000Z",
        "voteCount": 6,
        "content": "Its B. There are basically three choices of having external Hive style data catalog - Hive catalog hosted in RDS, Glue catalog and pure Hive catalog hosted in EMR. Option A here talks about how EMRFS consistant view uses DynamoDB table as its checkpointing mechanism to track objects to make sure updates/deletes to S3 objects are not served in their default eventual consistancy mode. This dynamodb metadata tables has nothing to do with external Hive metastore/catalog."
      },
      {
        "date": "2021-10-28T01:14:00.000Z",
        "voteCount": 1,
        "content": "Thanks, i am unable to determine the answer as well, I'm going with B as well looking your clarification.....But, is there anyone here who can confirm if this question is coming in AWS Data analytics specialty exam?"
      },
      {
        "date": "2021-11-05T09:30:00.000Z",
        "voteCount": 1,
        "content": "The key sentence is \"to expose the Amazon S3 objects as tables\". So the answer is \"B\". If the sentence is changed to \"to ensure the Amazon S3 objects consistency\", the answer would be \"A\". Trick question."
      },
      {
        "date": "2021-11-02T11:43:00.000Z",
        "voteCount": 1,
        "content": "Does this pointer on using Hive elimiate B? \n\"Hive neither supports nor prevents concurrent write access to metastore tables. If you share metastore information between two clusters, you must ensure that you do not write to the same metastore table concurrently, unless you are writing to different partitions of the same metastore table.\""
      },
      {
        "date": "2021-11-01T01:02:00.000Z",
        "voteCount": 1,
        "content": "I think B, A is metadata about S3 path, and they are asking \"as tables\" then B"
      },
      {
        "date": "2021-10-27T04:29:00.000Z",
        "voteCount": 1,
        "content": "Answer B. EMRFS consistent view is for different purpose, not for centralised metadata."
      },
      {
        "date": "2021-10-26T23:08:00.000Z",
        "voteCount": 3,
        "content": "I think it's B. Consistent view using DDB is used to address S3 read after write consistency issue. And we all know best place to store hive meta store is at MySQL which is an RDS. So I think B is the correct answer."
      },
      {
        "date": "2021-10-26T09:00:00.000Z",
        "voteCount": 1,
        "content": "I'm still not sure it its A or B"
      },
      {
        "date": "2021-10-26T07:03:00.000Z",
        "voteCount": 5,
        "content": "Th correct answer is B. The question is asking about exposing a HCatalog to other clusters. It is not about the consistency issue while reading object manipulated by other clusters. Therefore externalizing the Hive Catalog into RDS or creating a Amazon Glue catalog table that all clusters in the region can access is the right answer.\n\nWhen you create a cluster with consistent view enabled, Amazon EMR uses an Amazon DynamoDB database to store object metadata and track consistency with Amazon S3. If consistent view determines that Amazon S3 is inconsistent during a file system operation, it retries that operation according to rules that you can define."
      },
      {
        "date": "2021-10-24T22:05:00.000Z",
        "voteCount": 1,
        "content": "Answer : A - EMRFS consistent view tracks consistency using a DynamoDB table to track objects in Amazon S3 that have been synced with or created by EMRFS. All EMRFS clusters can use the same Dynamo DB table for each  object in S3 whose metadata needs to be made available centrally across all analyst clusters."
      },
      {
        "date": "2021-10-23T10:04:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is B as two options for metastore one is AWS GLUE Data Catalog and second is RDS for External Metastore for Hive"
      },
      {
        "date": "2021-10-21T21:30:00.000Z",
        "voteCount": 3,
        "content": "I think i will go with B\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html"
      },
      {
        "date": "2021-10-19T03:28:00.000Z",
        "voteCount": 1,
        "content": "my selection B"
      },
      {
        "date": "2021-10-17T21:32:00.000Z",
        "voteCount": 1,
        "content": "A is correct anser\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html"
      },
      {
        "date": "2021-10-14T01:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is B. \n\nConsistent view addresses an issue that can arise due to the Amazon S3 Data Consistency Model. \n\nFor example, if you add objects to Amazon S3 in one operation and then immediately list objects in a subsequent operation, the list and the set of objects processed may be incomplete."
      },
      {
        "date": "2021-10-16T00:17:00.000Z",
        "voteCount": 1,
        "content": "Agreed, the answer is B. Hive, SparkSQL and Presto all need a consistent metadata store to be able to query. Hive Metastore can be configured to be be in RDS (not DynamoDB)."
      },
      {
        "date": "2021-10-11T11:21:00.000Z",
        "voteCount": 3,
        "content": "The EMRFS consisten view table does not hold any metadata about the structure inside the files or tables. A proper metadata store is required here as per the question. B should be the correct one."
      },
      {
        "date": "2021-10-10T02:12:00.000Z",
        "voteCount": 3,
        "content": "A looks correct. When you create a cluster with consistent view enabled, Amazon EMR uses an Amazon DynamoDB database to store object metadata and track consistency with Amazon S3."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/4340-exam-aws-certified-big-data-specialty-topic-1-question-11/",
    "body": "An administrator needs to manage a large catalog of items from various external sellers. The administrator needs to determine if the items should be identified as minimally dangerous, dangerous, or highly dangerous based on their textual descriptions. The administrator already has some items with the danger attribute, but receives hundreds of new item descriptions every day without such classification.<br>The administrator has a system that captures dangerous goods reports from customer support team of from user feedback.<br>What is a cost-effective architecture to solve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a set of regular expression rules that are based on the existing examples, and run them on the DynamoDB Streams as every new item description is added to the system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Kinesis Streams process that captures and marks the relevant items in the dangerous goods reports using a Lambda function once more than two reports have been filed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a machine learning model to properly classify dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a machine learning model with binary classification for dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-27T17:16:00.000Z",
        "voteCount": 5,
        "content": "C is correct. Binary only allows for two possible classes."
      },
      {
        "date": "2021-10-31T06:30:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't it be D? D specifically talks about 'binary classification' but C talks about 'proper' classification."
      },
      {
        "date": "2021-11-01T07:49:00.000Z",
        "voteCount": 1,
        "content": "Yeah, so if C were true, D would definitely also be true - another reason why D is wrong. Anyways, the technical reason for D being wrong is that binary classification only allows two possible classes (e.g. not dangerous and dangerous), but here we have three classes (\"minimally dangerous\", \"dangerous\" and \"highly dangerous\")."
      },
      {
        "date": "2021-10-30T16:46:00.000Z",
        "voteCount": 1,
        "content": "Either C or D, but it's not a binary classification problem so C is more appropriate."
      },
      {
        "date": "2021-10-28T06:18:00.000Z",
        "voteCount": 2,
        "content": "my selection C"
      },
      {
        "date": "2021-10-22T12:42:00.000Z",
        "voteCount": 2,
        "content": "C is correct as the problem statement itself saying its classification problem not prediction\n(Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.)"
      },
      {
        "date": "2021-10-16T03:37:00.000Z",
        "voteCount": 2,
        "content": "C looks correct as others are not making much sense"
      },
      {
        "date": "2021-10-08T00:39:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2021-09-29T01:11:00.000Z",
        "voteCount": 1,
        "content": "AML does not support dynamodb so C and D are incorrect. I am thinking B"
      },
      {
        "date": "2021-10-01T20:29:00.000Z",
        "voteCount": 1,
        "content": "It did not mention AML, C should be correct. similar case: https://aws.amazon.com/cn/blogs/machine-learning/anomaly-detection-on-amazon-dynamodb-streams-using-the-amazon-sagemaker-random-cut-forest-algorithm/"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/3423-exam-aws-certified-big-data-specialty-topic-1-question-12/",
    "body": "A company receives data sets coming from external providers on Amazon S3. Data sets from different providers are dependent on one another. Data sets will arrive at different times and in no particular order.<br>A data architect needs to design a solution that enables the company to do the following:<br>\u2711 Rapidly perform cross data set analysis as soon as the data becomes available<br>\u2711 Manage dependencies between data sets that arrive at different times<br>Which architecture strategy offers a scalable and cost-effective solution that meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain data dependency information in Amazon RDS for MySQL. Use an AWS Data Pipeline job to load an Amazon EMR Hive table based on task dependencies and event notification triggers in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain data dependency information in an Amazon DynamoDB table. Use Amazon SNS and event notifications to publish data to fleet of Amazon EC2 workers. Once the task dependencies have been resolved, process the data with Amazon EMR.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain data dependency information in an Amazon ElastiCache Redis cluster. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to Redis. Once the task dependencies have been resolved, process the data with Amazon EMR.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain data dependency information in an Amazon DynamoDB table. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to the task associated with it in DynamoDB. Once all task dependencies have been resolved, process the data with Amazon EMR."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-31T20:47:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer which is also regular practice."
      },
      {
        "date": "2021-10-28T06:17:00.000Z",
        "voteCount": 2,
        "content": "I choose D"
      },
      {
        "date": "2021-10-28T01:02:00.000Z",
        "voteCount": 3,
        "content": "I'll go with D. DDB is more apt for storing configuration data instead of storing in a cache. Also Redis would have limitation is size so it's not a scale-able approach."
      },
      {
        "date": "2021-10-26T05:44:00.000Z",
        "voteCount": 1,
        "content": "D is the Correct Answer"
      },
      {
        "date": "2021-10-24T03:24:00.000Z",
        "voteCount": 1,
        "content": "my selection D"
      },
      {
        "date": "2021-10-22T16:27:00.000Z",
        "voteCount": 1,
        "content": "agree, it should be D"
      },
      {
        "date": "2021-10-19T19:07:00.000Z",
        "voteCount": 3,
        "content": "AWS Big Data Blog post describes exactly option D\nhttps://aws.amazon.com/blogs/big-data/how-expedia-implemented-near-real-time-analysis-of-interdependent-datasets/\nRedis is costlier, need to run an instance continuously for the caching.\nhttps://aws.amazon.com/elasticache/pricing/\nFinally, Option D is the correct answer"
      },
      {
        "date": "2021-10-18T06:09:00.000Z",
        "voteCount": 1,
        "content": "Compared with D,  option C redis is much cheaper than dynamodb."
      },
      {
        "date": "2021-10-15T05:04:00.000Z",
        "voteCount": 1,
        "content": "A, RDS is less expensive than Dynamo"
      },
      {
        "date": "2021-10-11T02:17:00.000Z",
        "voteCount": 2,
        "content": "D is right as others are not cost effective compared to D"
      },
      {
        "date": "2021-10-06T10:26:00.000Z",
        "voteCount": 2,
        "content": "I think D"
      },
      {
        "date": "2021-09-29T11:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is D: Since the question was s scalable and cost effective solution."
      },
      {
        "date": "2021-09-23T15:09:00.000Z",
        "voteCount": 3,
        "content": "agreed. d"
      },
      {
        "date": "2021-09-28T04:26:00.000Z",
        "voteCount": 1,
        "content": "Because of the cost-effective ?"
      },
      {
        "date": "2021-09-28T11:23:00.000Z",
        "voteCount": 2,
        "content": "Yes, due to the cost effectiveness"
      },
      {
        "date": "2021-09-23T10:50:00.000Z",
        "voteCount": 1,
        "content": "d? any thoughts?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/4879-exam-aws-certified-big-data-specialty-topic-1-question-13/",
    "body": "A media advertising company handles a large number of real-time messages sourced from over 200 websites in real time. Processing latency must be kept low. Based on calculations, a 60-shard Amazon Kinesis stream is more than sufficient to handle the maximum data throughput, even with traffic spikes. The company also uses an Amazon Kinesis Client Library (KCL) application running on Amazon Elastic Compute Cloud (EC2) managed by an Auto Scaling group. Amazon CloudWatch indicates an average of 25% CPU and a modest level of network traffic across all running servers.<br>The company reports a 150% to 200% increase in latency of processing messages from Amazon Kinesis during peak times. There are NO reports of delay from the sites publishing to Amazon Kinesis.<br>What is the appropriate solution to address the latency?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in the Amazon Kinesis stream to 80 for greater concurrency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the Amazon EC2 instances to increase network throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the minimum number of instances in the Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease Amazon DynamoDB throughput on the checkpoint table."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-27T04:12:00.000Z",
        "voteCount": 1,
        "content": "my selection D"
      },
      {
        "date": "2021-10-13T04:15:00.000Z",
        "voteCount": 2,
        "content": "looks D is appropriate answer\nhttps://aws.amazon.com/blogs/big-data/processing-amazon-dynamodb-streams-using-the-amazon-kinesis-client-library/"
      },
      {
        "date": "2021-10-03T09:45:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2021-09-27T11:08:00.000Z",
        "voteCount": 2,
        "content": "Checkpoint helps to increase performance... D looks correct"
      },
      {
        "date": "2021-09-20T20:58:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-ddb.html"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/4773-exam-aws-certified-big-data-specialty-topic-1-question-14/",
    "body": "A Redshift data warehouse has different user teams that need to query the same table with very different query types. These user teams are experiencing poor performance.<br>Which action improves performance for the user teams in this situation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate custom table views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd interleaved sort keys per team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain team-specific copies of the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd support for workload management queue hopping."
    ],
    "answer": "D",
    "answerDescription": "Reference: https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-25T00:45:00.000Z",
        "voteCount": 5,
        "content": "D is correct"
      },
      {
        "date": "2021-11-03T19:25:00.000Z",
        "voteCount": 1,
        "content": "D is my answer. Different user have different query types, could be long or short. WLM should be use to allocate different type of queue to run the jobs, either automatically or manually. Hopping queue is one of the manual option to tune the queue."
      },
      {
        "date": "2021-11-03T16:33:00.000Z",
        "voteCount": 2,
        "content": "C in my opinion.\nNot A - Non-materialized views won't magically make your query faster.\nNot B - \"per team\" doesn't make sense, there could be only one interleaved sort key that all use. Also, this is only feasible in case there are few teams as adding many columns to an interleaved sort key degrades performance.\nNot D - query hopping only affects execution order of querys, not their performance.\nC - admittedly, this is brute force and I wouldn't recommend it, but you can't argue that it actually improves performance for the user teams."
      },
      {
        "date": "2021-11-01T07:21:00.000Z",
        "voteCount": 3,
        "content": "answer is D. It cannot be B because, adding interleaved sort key per team is not scalable with operational overhead. A new team comes with a new query type, interleaved sort key cannot be altered once created. https://docs.aws.amazon.com/redshift/latest/dg/r_ALTER_TABLE.html\nWLM does not have such an issue."
      },
      {
        "date": "2021-11-02T03:14:00.000Z",
        "voteCount": 1,
        "content": "Also, too many interleaved sort keys will be a drag on performance. They need frequent VACCUM. Adding too many interleaved keys is an anti pattern."
      },
      {
        "date": "2021-10-31T10:32:00.000Z",
        "voteCount": 3,
        "content": "Answer : B\nNot A \u2013 Its impractical to create as many view as the query types.\nNot C \u2013 Its impractical to create as many copies of the table  as the user teams\nNot D \u2013 Unless the table is tuned for performance, hoping queues will become more frequent resulting in degraded performance. Associating one queue per user group also won\u2019t guarantee improved performance if the table is filtered on a column that it is cannot be sorted on\nAnswer is B- An interleaved sort gives equal weight to each column, or subset of columns, in the sort key. If multiple queries use different columns for filters, then you can often improve performance for those queries by using an interleaved sort style. When a query uses restrictive predicates on secondary sort columns, interleaved sorting significantly improves query performance as compared to compound sorting."
      },
      {
        "date": "2021-10-27T14:02:00.000Z",
        "voteCount": 2,
        "content": "D is more correct. \nB is correct if different queries have their WHERE clause on different columns that are being defined on global index. However, the problem statement did not mention that."
      },
      {
        "date": "2021-10-24T04:50:00.000Z",
        "voteCount": 1,
        "content": "It is D. see https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html. The question is trying to reinforce 'different users'"
      },
      {
        "date": "2021-10-24T11:08:00.000Z",
        "voteCount": 2,
        "content": "Correct, it has to be D. \n\nThe question states, that  \"user teams are experiencing poor performance.\nWhich action improves performance for the user teams in this situation? \".  \n\nWLM is the one that can be used to route queries based on user groups (user teams) and alloted priorities."
      },
      {
        "date": "2021-10-29T18:08:00.000Z",
        "voteCount": 1,
        "content": "I changed my opinion, it is B."
      },
      {
        "date": "2021-10-22T23:00:00.000Z",
        "voteCount": 1,
        "content": "my selection B"
      },
      {
        "date": "2021-10-21T19:30:00.000Z",
        "voteCount": 4,
        "content": "D is correct \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html"
      },
      {
        "date": "2021-10-07T20:33:00.000Z",
        "voteCount": 1,
        "content": "On the first side, it seems B. Add interleaved sort keys per team. is correct. But how it can be \"per team\", this made it a wrong answer.\nThe nearest alternative option is D. Add support for workload management queue hopping.\nit can share resources between the parallel query. And in that way, it improves performance."
      },
      {
        "date": "2021-10-08T11:23:00.000Z",
        "voteCount": 2,
        "content": "This is confusing a lot \"the same table with very different query types\".  I think it means a lot of types of queries (short living time,long living, etc). It doesn't mention about key inside tables and there're no any ways to fix it with keys (all teams share the same data)"
      },
      {
        "date": "2021-10-22T03:17:00.000Z",
        "voteCount": 1,
        "content": "I changed my opinion. B is the right answer.\n\n\"But how it can be \"per team\", this made it a wrong answer.\"\nEasy. interleaved sort keys could include different columns for different team.  As example you can create key with (n1,n2,n3) columns where different team can use their filter conditions."
      },
      {
        "date": "2021-10-06T21:39:00.000Z",
        "voteCount": 1,
        "content": "i think it is B, it mentions, the same table different types of queries."
      },
      {
        "date": "2021-10-01T00:40:00.000Z",
        "voteCount": 2,
        "content": "Ans is B\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html#t_Sorting_data-interleaved"
      },
      {
        "date": "2021-09-30T14:51:00.000Z",
        "voteCount": 2,
        "content": "answer is B"
      },
      {
        "date": "2021-09-23T08:43:00.000Z",
        "voteCount": 2,
        "content": "I support B as well"
      },
      {
        "date": "2021-09-21T01:25:00.000Z",
        "voteCount": 4,
        "content": "Answer is B. \nWLM primary use case is for queuing long or short queries."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/4774-exam-aws-certified-big-data-specialty-topic-1-question-15/",
    "body": "A company operates an international business served from a single AWS region. The company wants to expand into a new country. The regulator for that country requires the Data Architect to maintain a log of financial transactions in the country within 24 hours of the product transaction. The production application is latency insensitive. The new country contains another AWS region.<br>What is the most cost-effective way to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CloudFormation to replicate the production application to the new region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudFront to serve application content locally in the country; Amazon CloudFront logs will satisfy the requirement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContinue to serve customers from the existing region while using Amazon Kinesis to stream transaction data to the regulator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 cross-region replication to copy and persist production transaction logs to a bucket in the new countrys region."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-08T08:09:00.000Z",
        "voteCount": 5,
        "content": "D is the right answer. S3 is very cost effective. CRR happens from few seconds to few minutes  and will satisfy within 24 hrs requirement. \n\nB is an absurd - CloudFront is for content serving and as such logs content access. It does not store transaction logs.\n\nC is NOT the right answer. Kinesis stores data only for a period of time (24 Hrs by default) and then the data will expire. Any data regulatory requirement would also expect the data to be durable along with being available within a period of time."
      },
      {
        "date": "2021-10-09T09:10:00.000Z",
        "voteCount": 2,
        "content": "agree, \nEven with Kinesis Firehose, you can't directly stream data into S3 bucket in another region.\nSo S3 CRR would solve the problem and is also cost-effective."
      },
      {
        "date": "2023-12-19T04:18:00.000Z",
        "voteCount": 1,
        "content": "I will go with D. CRR seems correct to me."
      },
      {
        "date": "2021-11-04T17:54:00.000Z",
        "voteCount": 1,
        "content": "The  consensus seems do be C or D but the posted answer is B, Cloudfront. Does anyone have an explanation or is the answer wrong?"
      },
      {
        "date": "2021-11-04T06:08:00.000Z",
        "voteCount": 1,
        "content": "I will probably choose D"
      },
      {
        "date": "2021-10-31T19:37:00.000Z",
        "voteCount": 1,
        "content": "For me the key in this questions is \"The new country contains another AWS region\" so hinting at CRR"
      },
      {
        "date": "2021-10-26T11:04:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D, as its cost effective."
      },
      {
        "date": "2021-10-24T15:21:00.000Z",
        "voteCount": 1,
        "content": "I think D is the right answer coz of cost effectiveness. C seems to be a dis-tractor to me."
      },
      {
        "date": "2021-10-22T12:10:00.000Z",
        "voteCount": 1,
        "content": "answer id D. Enabling S3 cross region replication is cheaper. the sentence latency insensitive means no Kinesis and Cloudfront, both can improve performance but at additional service cost."
      },
      {
        "date": "2021-10-20T09:25:00.000Z",
        "voteCount": 1,
        "content": "B\n'Cost efficiency'-A is out\n\u2018latency intensive \u2019 C&amp;D are out"
      },
      {
        "date": "2021-10-25T03:01:00.000Z",
        "voteCount": 1,
        "content": "It's latency insensitive. That's why it's D."
      },
      {
        "date": "2021-10-29T10:00:00.000Z",
        "voteCount": 1,
        "content": "Who says anything about latency intensity? D is correct. &gt;Also, s3 cross region replication may take quite a few hours in rare cases."
      },
      {
        "date": "2021-10-13T04:40:00.000Z",
        "voteCount": 2,
        "content": "Data Architect to maintain a log of financial transactions in the country within 24 hours of the product transaction - If C, Kinesis records expire after 24 hours, if the logs are to be maintained, then we need a persistent storage in S3. D - Seems correct."
      },
      {
        "date": "2021-10-12T19:13:00.000Z",
        "voteCount": 1,
        "content": "my selection D"
      },
      {
        "date": "2021-10-08T13:53:00.000Z",
        "voteCount": 1,
        "content": "I chose D. As only the logs need to be maintained in the new country, S3 cross region replication can be used to copy the data to the AWS region within the new Country."
      },
      {
        "date": "2021-10-06T08:15:00.000Z",
        "voteCount": 4,
        "content": "D -- based on this link...  \nIt says \"CRR is a bucket-level configuration, and it can help you meet compliance requirements and minimize latency by keeping copies of your data in different Regions.\"\n\nhttps://aws.amazon.com/blogs/big-data/trigger-cross-region-replication-of-pre-existing-objects-using-amazon-s3-inventory-amazon-emr-and-amazon-athena/\n\nPlease let me know if you agree"
      },
      {
        "date": "2021-10-01T23:47:00.000Z",
        "voteCount": 1,
        "content": "what's the final answer here ... C or D?"
      },
      {
        "date": "2021-09-30T08:47:00.000Z",
        "voteCount": 2,
        "content": "C looks reasonable.\n\n1. The primary requirement in this scenario is to maintain a log of the financial transaction in the country (existing) within 24 hours of the product transaction.\n2. Given that the product application is latency insenstive. \nBased on #2 above, they can continue to serve from the existing region.\nAnd use Kinesis to stream transactions to the Regulator."
      },
      {
        "date": "2021-09-28T20:12:00.000Z",
        "voteCount": 3,
        "content": "Ans is C. By default, Records of a Kinesis stream are accessible for up to 24 hours from the time they are added to the stream"
      },
      {
        "date": "2021-10-29T17:13:00.000Z",
        "voteCount": 4,
        "content": "kinesis maintains data for 24 hours, not logs"
      },
      {
        "date": "2021-09-28T18:39:00.000Z",
        "voteCount": 1,
        "content": "The regulator for that country requires the Data Architect to maintain a log of financial transactions in the country \"within 24 hours of the product transaction\" : Option C.\nSending each product transaction to Kinesis will make each of them persist there for 24 hours(default) before they expire."
      },
      {
        "date": "2021-10-06T07:41:00.000Z",
        "voteCount": 4,
        "content": "\"within 24 hours of the product transaction\" means the logs must be in the country in 24hrs or less of the transaction, and doesn't imply keeping the logs for only 24hrs. D seems the best solution here"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/3489-exam-aws-certified-big-data-specialty-topic-1-question-16/",
    "body": "An administrator needs to design the event log storage architecture for events from mobile devices. The event data will be processed by an Amazon EMR cluster daily for aggregated reporting and analytics before being archived.<br>How should the administrator recommend storing the log data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket and write log data into folders by device. Execute the EMR job on the device folders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table partitioned on the device and sorted on date, write log data to table. Execute the EMR job on the Amazon DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket and write data into folders by day. Execute the EMR job on the daily folder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table partitioned on EventID, write log data to table. Execute the EMR job on the table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-19T19:41:00.000Z",
        "voteCount": 6,
        "content": "Thoughts on C?"
      },
      {
        "date": "2023-12-19T04:21:00.000Z",
        "voteCount": 1,
        "content": "I think C is correct. We are running EMR daily, so partitioning by day will give data for all devices for each day."
      },
      {
        "date": "2021-10-27T17:29:00.000Z",
        "voteCount": 2,
        "content": "I think it is A because the EMR job is run daily, so partitioning by day will lead to only one partition.   Partitioning by device, on the other hand, allows for parallel jobs to be run for each device or group of devices."
      },
      {
        "date": "2021-10-27T23:20:00.000Z",
        "voteCount": 1,
        "content": "On second thought, it could still be C because one can sub-partition the logs by device under date.  E.g. date/device which still allows for parallelization.   Whereas device/date will be the read query more complex as the job needs to be run daily."
      },
      {
        "date": "2021-10-31T18:42:00.000Z",
        "voteCount": 1,
        "content": "Actually, you don't need supartitioning. Partitioning isn't even a thing in S3. Instead, EMR is quite capable of reading many objects from one s3 folder in parallel. Also, if possible, it splits up the objects into 64MB chunks when reading so it can even read one object in parallel.\nBottom line, C is true and thinking about partitioning does not make sense in this context. hope it helps :)"
      },
      {
        "date": "2021-10-26T20:21:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C. It's a daily aggregation not by device."
      },
      {
        "date": "2021-10-26T06:29:00.000Z",
        "voteCount": 2,
        "content": "of course is C\nA is clearly wrong because the api call fee is TOOOOOOOOO expensive"
      },
      {
        "date": "2021-10-26T06:21:00.000Z",
        "voteCount": 1,
        "content": "my selection C"
      },
      {
        "date": "2021-10-19T11:25:00.000Z",
        "voteCount": 1,
        "content": "Why not A"
      },
      {
        "date": "2021-10-23T11:46:00.000Z",
        "voteCount": 1,
        "content": "Because of the partition scheme. We should imagine about simple access method. In this case \"yyyy/mm/dd\" or similar are the best choice for partitioning."
      },
      {
        "date": "2021-10-19T08:33:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice. Nothing else"
      },
      {
        "date": "2021-10-17T01:36:00.000Z",
        "voteCount": 4,
        "content": "This is not for real time analysis, but intends to process log data in batch, hence S3 is better than dynamo db."
      },
      {
        "date": "2021-10-16T15:19:00.000Z",
        "voteCount": 2,
        "content": "Why can't we use DynamoDB?"
      },
      {
        "date": "2021-10-12T04:14:00.000Z",
        "voteCount": 1,
        "content": "C looks correct to me"
      },
      {
        "date": "2021-10-04T08:12:00.000Z",
        "voteCount": 3,
        "content": "Yeah C looks correct. day/time mechanism is always better for storing logs"
      },
      {
        "date": "2021-10-01T05:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is C:"
      },
      {
        "date": "2021-09-24T14:35:00.000Z",
        "voteCount": 3,
        "content": "It is C. Daily EMR job and based on time not device."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/3594-exam-aws-certified-big-data-specialty-topic-1-question-17/",
    "body": "A data engineer wants to use an Amazon Elastic Map Reduce for an application. The data engineer needs to make sure it complies with regulatory requirements. The auditor must be able to confirm at any point which servers are running and which network access controls are deployed.<br>Which action should the data engineer take to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the auditor IAM accounts with the SecurityAudit policy attached to their group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the auditor with SSH keys for access to the Amazon EMR cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the auditor with CloudFormation templates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the auditor with access to AWS DirectConnect to use their existing tools."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-21T15:34:00.000Z",
        "voteCount": 6,
        "content": "It is A. you can check it from https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html#jf_security-auditor \nat the option C, there is not any information about cloudformation templates."
      },
      {
        "date": "2021-09-25T02:01:00.000Z",
        "voteCount": 5,
        "content": "cloudformation templates only provide the information about what was deployed not about what is currently running, so best answer is A"
      },
      {
        "date": "2021-09-25T04:53:00.000Z",
        "voteCount": 2,
        "content": "A look most appropriate"
      },
      {
        "date": "2021-11-01T18:39:00.000Z",
        "voteCount": 1,
        "content": "answer is A"
      },
      {
        "date": "2021-10-30T13:36:00.000Z",
        "voteCount": 1,
        "content": "C is wrong, template doesn't provide whats implemented. Its A"
      },
      {
        "date": "2021-10-28T18:09:00.000Z",
        "voteCount": 1,
        "content": "A , obviously"
      },
      {
        "date": "2021-10-28T06:38:00.000Z",
        "voteCount": 1,
        "content": "answer is C. auditor wants to know about the servers and (roles) associated with them, not how people in the group have their security policies like in \"IAM accounts with the SecurityAudit policy attached to their group\". If you have to know how servers are deployed cloudformation is the way."
      },
      {
        "date": "2021-10-19T23:00:00.000Z",
        "voteCount": 1,
        "content": "Between A and C, A is more correct way to get a security auditor to start the process. \nhttps://kevinslin.com/aws/aws_account_access_policies/#"
      },
      {
        "date": "2021-10-12T01:54:00.000Z",
        "voteCount": 1,
        "content": "It is A."
      },
      {
        "date": "2021-10-08T00:10:00.000Z",
        "voteCount": 3,
        "content": "The SecurityAudit policy has a permission about checking cloudformation documents already.\nSo, answer is A ^^"
      },
      {
        "date": "2021-10-05T14:50:00.000Z",
        "voteCount": 2,
        "content": "my selection A"
      },
      {
        "date": "2021-10-04T19:05:00.000Z",
        "voteCount": 1,
        "content": "Option C is valid\nThe template is a blueprint that provides intend servers and network access controls.\nAnd by checking resource drift status can find current status against intend status."
      },
      {
        "date": "2021-10-01T16:13:00.000Z",
        "voteCount": 1,
        "content": "A looks right bcoz c will not tell you about running servers."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/5343-exam-aws-certified-big-data-specialty-topic-1-question-18/",
    "body": "A social media customer has data from different data sources including RDS running MySQL, Redshift, and<br>Hive on EMR. To support better analysis, the customer needs to be able to analyze data from different data sources and to combine the results.<br>What is the most cost-effective solution to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all data from a different database/warehouse to S3. Use Redshift COPY command to copy data to Redshift for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Presto on the EMR cluster where Hive sits. Configure MySQL and PostgreSQL connector to select from different data sources in a single query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpin up an Elasticsearch cluster. Load data from all three data sources and use Kibana to analyze.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a program running on a separate EC2 instance to run queries to three different systems. Aggregate the results after getting the responses from all three systems."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-18T01:04:00.000Z",
        "voteCount": 5,
        "content": "my selection B"
      },
      {
        "date": "2021-11-01T00:24:00.000Z",
        "voteCount": 1,
        "content": "Technically B is wrong, as it mentions nothing about redshift."
      },
      {
        "date": "2021-11-06T16:27:00.000Z",
        "voteCount": 2,
        "content": "It's be, Redshift connector is the sames as Postrges connector, you only need to set one\nconnector.name=redshift\nconnection-url=jdbc:postgresql://example.net:5439/database\nconnection-user=root\nconnection-password=secret"
      },
      {
        "date": "2021-10-21T23:26:00.000Z",
        "voteCount": 1,
        "content": "I got with B"
      },
      {
        "date": "2021-10-19T17:50:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is B as Presto (or PrestoDB) is an open source, distributed SQL query engine, designed from the ground up for fast analytic queries against data of any size. It supports both non-relational sources, such as the Hadoop Distributed File System (HDFS), Amazon S3, Cassandra, MongoDB, and HBase, and relational data sources such as MySQL, PostgreSQL, Amazon Redshift, Microsoft SQL Server, and Teradata.\nhttps://aws.amazon.com/big-data/what-is-presto/"
      },
      {
        "date": "2021-09-29T18:03:00.000Z",
        "voteCount": 1,
        "content": "Why not C? It seems a lot more easier to ingest data to Elasticsearch"
      },
      {
        "date": "2021-10-02T21:55:00.000Z",
        "voteCount": 2,
        "content": "The question says most cost-effective way, and Elasticsearch is an expensive option. I feel its B."
      },
      {
        "date": "2021-10-12T22:07:00.000Z",
        "voteCount": 2,
        "content": "Because 'Presto is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources' (source: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-presto.html), and EMR had been already provisioned."
      },
      {
        "date": "2021-10-13T20:01:00.000Z",
        "voteCount": 2,
        "content": "I would agree if only mention \"Presto\" but why configure MySQL and PostgreSQL? this makes me go with C"
      },
      {
        "date": "2021-10-14T02:31:00.000Z",
        "voteCount": 5,
        "content": "because PostgreSQL is used for the redshift and MySQL for the RDS, Answer is B"
      },
      {
        "date": "2021-10-15T22:50:00.000Z",
        "voteCount": 1,
        "content": "i agree, i think answer is B."
      },
      {
        "date": "2021-09-23T13:50:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D are for sure not the answer. question is only between b &amp; c"
      },
      {
        "date": "2021-09-22T22:18:00.000Z",
        "voteCount": 3,
        "content": "B is correct I think"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/3407-exam-aws-certified-big-data-specialty-topic-1-question-19/",
    "body": "An Amazon EMR cluster using EMRFS has access to petabytes of data on Amazon S3, originating from multiple unique data sources. The customer needs to query common fields across some of the data sets to be able to perform interactive joins and then display results quickly.<br>Which technology is most appropriate to enable this capability?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPresto",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicroStrategy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPig",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tR Studio"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-19T04:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct for this use case."
      },
      {
        "date": "2021-11-02T23:13:00.000Z",
        "voteCount": 1,
        "content": "Yes, it is Presto."
      },
      {
        "date": "2021-10-31T15:30:00.000Z",
        "voteCount": 2,
        "content": "Like everyone, I went with option A"
      },
      {
        "date": "2021-10-25T14:57:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2021-10-28T22:17:00.000Z",
        "voteCount": 2,
        "content": "Presto would be most appropriate answer due to following phrase in the question \u201cinteractive joins and then display results quickly\u201d\nPig is more suitable for batch processing and Presto for interactive queries."
      },
      {
        "date": "2021-10-19T03:30:00.000Z",
        "voteCount": 1,
        "content": "my selection A"
      },
      {
        "date": "2021-10-18T04:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\nhttps://aws.amazon.com/emr/features/presto/"
      },
      {
        "date": "2021-10-14T02:42:00.000Z",
        "voteCount": 1,
        "content": "Presto is the right answer here"
      },
      {
        "date": "2021-10-11T03:12:00.000Z",
        "voteCount": 2,
        "content": "Presto is good for Peta bytes of data and for interactive queries while pig is mostly used for etl processing so correct answer is A i.e. Presto"
      },
      {
        "date": "2021-10-08T22:22:00.000Z",
        "voteCount": 3,
        "content": "A is correct. Presto is more suited for interactive joins whereas pig is for batch processing"
      },
      {
        "date": "2021-10-09T01:27:00.000Z",
        "voteCount": 2,
        "content": "A .. Presto is used for fast interactive joins"
      },
      {
        "date": "2021-10-05T10:18:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-09-30T18:06:00.000Z",
        "voteCount": 3,
        "content": "A is the answer"
      },
      {
        "date": "2021-09-24T23:35:00.000Z",
        "voteCount": 4,
        "content": "It's A. Supports PBs https://dzone.com/articles/getting-introduced-with-presto"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/3424-exam-aws-certified-big-data-specialty-topic-1-question-20/",
    "body": "A game company needs to properly scale its game application, which is backed by DynamoDB. Amazon<br>Redshift has the past two years of historical data. Game traffic varies throughout the year based on various factors such as season, movie release, and holiday season. An administrator needs to calculate how much read and write throughput should be provisioned for DynamoDB table for each week in advance.<br>How should the administrator accomplish this task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFeed the data into Amazon Machine Learning and build a regression model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFeed the data into Spark Mlib and build a random forest modest.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFeed the data into Apache Mahout and build a multi-classification model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFeed the data into Amazon Machine Learning and build a binary classification model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-04T09:00:00.000Z",
        "voteCount": 4,
        "content": "Answer is A. Key is \"Redshift has the past two years of historical data\". This means we have labelled data that we can use to train a linear regression model to predict  RCU and WCU."
      },
      {
        "date": "2021-10-29T13:26:00.000Z",
        "voteCount": 3,
        "content": "my selection A"
      },
      {
        "date": "2021-10-25T09:43:00.000Z",
        "voteCount": 4,
        "content": "\"needs to calculate how much read and write throughput should be provisioned for DynamoDB table for each week in advance\"\n\nA regression model is more suitable for this job, you can just run 2 models, one for WCU, one for RCU.\nB is for detecting anomalies."
      },
      {
        "date": "2021-10-22T12:50:00.000Z",
        "voteCount": 2,
        "content": "Normally, when predict some numbers like salary, price we use the regression model based on the histrory data. So personally, i agree with the B as it is used to predicted the number of the read and write throughput"
      },
      {
        "date": "2021-10-20T03:39:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B\n... An anomaly score with low values indicates that the data point is considered \u201cnormal\u201d whereas high values indicate the presence of an anomaly. The definitions of \u201clow\u201d and \u201chigh\u201d depend on the application, but common practice suggests that scores beyond three standard deviations from the mean score are considered anomalous.\nhttps://aws.amazon.com/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/"
      },
      {
        "date": "2021-10-11T14:32:00.000Z",
        "voteCount": 2,
        "content": "Not sure about A, mainly because regression is to precit just one numeric value, not two (RCU/WCU). Regarding Random forest (B) is also used for reggresion tasks as suggest here: https://www.newgenapps.com/blog/random-forest-analysis-in-ml-and-when-to-use-it"
      },
      {
        "date": "2021-10-06T19:18:00.000Z",
        "voteCount": 4,
        "content": "A. Feed the data into Amazon Machine Learning and build a regression model is the right answer because regression model is used for numeric value and here we are looking for RCU/WCU which is numeric as well. though B can be an option but it's not cost effective as it needs EMR cluster"
      },
      {
        "date": "2021-10-05T17:23:00.000Z",
        "voteCount": 2,
        "content": "it should be A .. Random forest in B is for classification model."
      },
      {
        "date": "2021-10-17T03:36:00.000Z",
        "voteCount": 2,
        "content": "Random forest modest can do regression"
      },
      {
        "date": "2021-10-05T16:00:00.000Z",
        "voteCount": 2,
        "content": "A I think"
      },
      {
        "date": "2021-09-30T13:33:00.000Z",
        "voteCount": 2,
        "content": "A, because historiccal using regression ml model"
      },
      {
        "date": "2021-09-28T23:15:00.000Z",
        "voteCount": 1,
        "content": "Why not B?"
      },
      {
        "date": "2021-11-07T06:59:00.000Z",
        "voteCount": 1,
        "content": "because a random forest decides between two options and is therefore not suited for choosing WCU and RCU, which could be any positive number."
      },
      {
        "date": "2021-09-28T20:39:00.000Z",
        "voteCount": 2,
        "content": "A is correct one"
      },
      {
        "date": "2021-09-26T20:17:00.000Z",
        "voteCount": 3,
        "content": "Correct. A"
      },
      {
        "date": "2021-09-25T21:57:00.000Z",
        "voteCount": 3,
        "content": "a? anyone?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/5344-exam-aws-certified-big-data-specialty-topic-1-question-21/",
    "body": "A data engineer is about to perform a major upgrade to the DDL contained within an Amazon Redshift cluster to support a new data warehouse application. The upgrade scripts will include user permission updates, view and table structure changes as well as additional loading and data manipulation tasks.<br>The data engineer must be able to restore the database to its existing state in the event of issues.<br>Which action should be taken prior to performing this upgrade task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an UNLOAD command for all data in the warehouse and save it to S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual snapshot of the Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a copy of the automated snapshot on the Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the waitForSnapshotAvailable command from either the AWS CLI or an AWS SDK."
    ],
    "answer": "B",
    "answerDescription": "Reference: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with- snapshot-restore-table-from-snapshot",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-15T00:09:00.000Z",
        "voteCount": 5,
        "content": "my selection B"
      },
      {
        "date": "2021-11-06T11:56:00.000Z",
        "voteCount": 1,
        "content": "The FIRST one with most straightforward answer, without any controversy, confusion, doubt or anxiety ;-) !!"
      },
      {
        "date": "2021-09-30T20:59:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-restore"
      },
      {
        "date": "2021-09-27T15:06:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-09-23T21:25:00.000Z",
        "voteCount": 2,
        "content": "B.. Manual snapshot"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/3408-exam-aws-certified-big-data-specialty-topic-1-question-22/",
    "body": "A large oil and gas company needs to provide near real-time alerts when peak thresholds are exceeded in its pipeline system. The company has developed a system to capture pipeline metrics such as flow rate, pressure, and temperature using millions of sensors. The sensors deliver to AWS IoT.<br>What is a cost-effective way to provide near real-time alerts on the pipeline metrics?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS IoT rule to generate an Amazon SNS notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data points in an Amazon DynamoDB table and poll if for peak metrics data from an Amazon EC2 application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Machine Learning model and invoke it with AWS Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Streams and a KCL-based application deployed on AWS Elastic Beanstalk."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-02T16:57:00.000Z",
        "voteCount": 5,
        "content": "It's A . why so many questions have wrong answer ? who provide these answers?"
      },
      {
        "date": "2021-11-01T22:33:00.000Z",
        "voteCount": 5,
        "content": "my selection A"
      },
      {
        "date": "2021-10-26T15:40:00.000Z",
        "voteCount": 1,
        "content": "It's \"A\" as IOT also includes IOT Analytics..https://d1.awsstatic.com/IoT/AWS%20Industrial%20-%20Predictive%20Maintenance%20Reference%20Architecture.pdf"
      },
      {
        "date": "2021-10-30T04:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/iot/latest/developerguide/iot-create-rule.html"
      },
      {
        "date": "2021-10-25T19:13:00.000Z",
        "voteCount": 1,
        "content": "It's A . IOT rules engine have capability to send the messages to the target topic"
      },
      {
        "date": "2021-10-19T13:59:00.000Z",
        "voteCount": 1,
        "content": "To determine \"peak thresholds\" need the option C. Create an Amazon Machine Learning model and invoke it with AWS Lambda.\n\nFor each ML model, you need to determine the threshold for inference confidence that equates to a predicted failure condition. For example, if an inference for a machine you are monitoring indicates with high confidence (let\u2019s say a level of 90%), then you would take appropriate action. \nhttps://aws.amazon.com/blogs/iot/using-aws-iot-for-predictive-maintenance/"
      },
      {
        "date": "2021-11-04T13:46:00.000Z",
        "voteCount": 4,
        "content": "thresholds of pipeline system are defined when building . It is a physical problem . predicting peak thresholds of gas system would cause disasert ."
      },
      {
        "date": "2021-10-14T19:29:00.000Z",
        "voteCount": 2,
        "content": "actually, I know a lot of people choose A, but C still makes sense. In this case, the AWS IoT rule still uses machine learning + lambda step function to trigger SNS. the real question is what kind of IoT rule you are about to create."
      },
      {
        "date": "2021-10-08T14:58:00.000Z",
        "voteCount": 1,
        "content": "A. IOT rule is the right answer for this question"
      },
      {
        "date": "2021-10-07T12:12:00.000Z",
        "voteCount": 2,
        "content": "it is a .. but why some of the answers are wrong? :("
      },
      {
        "date": "2021-10-09T13:07:00.000Z",
        "voteCount": 1,
        "content": "Near real-time alerts is a requirement, thus the SNS part"
      },
      {
        "date": "2021-10-03T12:09:00.000Z",
        "voteCount": 2,
        "content": "IOT rule is fast and cost-effective"
      },
      {
        "date": "2021-10-03T08:59:00.000Z",
        "voteCount": 1,
        "content": "A, itis"
      },
      {
        "date": "2021-09-25T09:23:00.000Z",
        "voteCount": 2,
        "content": "it is A"
      },
      {
        "date": "2021-09-25T02:37:00.000Z",
        "voteCount": 2,
        "content": "It's A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/5345-exam-aws-certified-big-data-specialty-topic-1-question-23/",
    "body": "A company is using Amazon Machine Learning as part of a medical software application. The application will predict the most likely blood type for a patient based on a variety of other clinical tests that are available when blood type knowledge is unavailable.<br>What is the appropriate model choice and target attribute combination for this problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMulti-class classification model with a categorical target attribute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegression model with a numeric target attribute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBinary Classification with a categorical target attribute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tK-Nearest Neighbors model with a multi-class target attribute."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-18T07:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. The target label is unknown from other clinical trial data. KNN is the best model to predict that multi-classfication targert label based on other clinical trial data."
      },
      {
        "date": "2021-10-31T12:40:00.000Z",
        "voteCount": 1,
        "content": "in my opinion, the \"multi class target attribute\" in D means the prediction result can be multi-class, eg, the blood type can be B and O, which didn't make sense here. That's why I rule out D. KNN can be used to do multi-class classification, thus A is a general but still correct answer."
      },
      {
        "date": "2021-10-22T19:40:00.000Z",
        "voteCount": 6,
        "content": "Amazon Machine Learning does not have KNN. KNN is available in sagemaker. Its A"
      },
      {
        "date": "2021-10-15T19:32:00.000Z",
        "voteCount": 2,
        "content": "my selection A"
      },
      {
        "date": "2021-10-01T02:57:00.000Z",
        "voteCount": 2,
        "content": "KNN is simplest ML, blood type classification is serious. so the answer should be A."
      },
      {
        "date": "2021-09-26T16:19:00.000Z",
        "voteCount": 2,
        "content": "A - Multi-class looks appropriate"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/3519-exam-aws-certified-big-data-specialty-topic-1-question-24/",
    "body": "A data engineer is running a DWH on a 25-node Redshift cluster of a SaaS service. The data engineer needs to build a dashboard that will be used by customers. Five big customers represent 80% of usage, and there is a long tail of dozens of smaller customers. The data engineer has selected the dashboarding tool.<br>How should the data engineer make sure that the larger customer workloads do NOT interfere with the smaller customer workloads?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply query filters based on customer-id that can NOT be changed by the user and apply distribution keys on customer-id.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the largest customers into a single user group with a dedicated query queue and place the rest of the customers into a different query queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush aggregations into an RDS for Aurora instance. Connect the dashboard application to Aurora rather than Redshift for faster queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRoute the largest customers to a dedicated Redshift cluster. Raise the concurrency of the multi-tenant Redshift cluster to accommodate the remaining customers."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-31T19:41:00.000Z",
        "voteCount": 8,
        "content": "Answer : B\nNot A- query filters on customer ID and hence the distribution key will not segregate the workload between  small and large customers.\nNot C- RDS is not a good choice for aggregation.\nNot D \u2013 You could route the largest customers to a dedicated RedShift cluster. But I am not sure why I would raise the concurrency of the multi-tenant cluster to accommodate the remaining customers when these customers are already on the cluster and consists of 20% of the volume. For that reason I wouldn\u2019t chose D.\nB is the correct answer- Depending upon the company the logged in user belongs to, you can assign the query to a specific query group. Each of the large customer can be assigned to one query group each and all small customers can be assigned to another query group. This allow the large customers to get their own query queue with a dedicated share of the 25 node hardware to run their queries while smaller customers share a single queue and therefore a portion of the 25 node hardware."
      },
      {
        "date": "2021-10-31T11:00:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2021-10-31T01:09:00.000Z",
        "voteCount": 2,
        "content": "Redshift enforces a query concurrency limit of 15 on a cluster.Queries in are executed in a queue, by default there is one queue per query cluster which can run up to five concurrent queries. Users can modify the configuration to allow up to 15 queries per queue and a maximum of 8 queues.The concurrent queries for a cluster across queues is limited to a maximum of 15. Users cannot modify this configuration.\n\nEven after you configure WLM queues , and no matter how big you cluster is, concurrent queries that can be run are limited to 15.\n\nD is the best sofar."
      },
      {
        "date": "2021-11-06T12:16:00.000Z",
        "voteCount": 1,
        "content": "If you create new queue, then you can increase concurrency level while default queue can have only 5 concurrency queries. Across the queue, you can have 50 concurrent queue. That's why the answer is B."
      },
      {
        "date": "2021-10-29T16:59:00.000Z",
        "voteCount": 1,
        "content": "my selection B"
      },
      {
        "date": "2021-10-28T14:50:00.000Z",
        "voteCount": 1,
        "content": "I'd say D - not sure but I think WLM approach wouldn't work here in multi-tenant architecture since our users are not IAM users and can not be used in queues. Futhermore it's not about long-running and short-running queues but about \"big\" and \"small\" cutomers."
      },
      {
        "date": "2021-10-26T21:50:00.000Z",
        "voteCount": 1,
        "content": "B seems correct. https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html"
      },
      {
        "date": "2021-10-22T08:20:00.000Z",
        "voteCount": 1,
        "content": "B is correct. The question asks for a solution in which large customer workload \"do NOT interfere\" with smaller customer. It doesn't tell multi-tenant separation. Performance is important in serving the large customers."
      },
      {
        "date": "2021-10-20T01:19:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-10-06T03:59:00.000Z",
        "voteCount": 2,
        "content": "the answer is b. to manage workload at redshift to create queu for different query type."
      },
      {
        "date": "2021-10-06T19:30:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the explanation this now makes more sense than creating an additional cluster."
      },
      {
        "date": "2021-10-07T07:08:00.000Z",
        "voteCount": 2,
        "content": "Read from pg 27. https://d1.awsstatic.com/whitepapers/Multi_Tenant_SaaS_Storage_Strategies.pdf\nFrom my understanding of the article it is best for SaaS providers to use the silo approach. Meaning Answer D is correct. Anyone disagree?"
      },
      {
        "date": "2021-10-15T09:37:00.000Z",
        "voteCount": 1,
        "content": "@muhsin why b over d?"
      },
      {
        "date": "2021-10-15T11:54:00.000Z",
        "voteCount": 1,
        "content": "I selected B in the exam. When i reread the question, it states SaaS service. Also the WLM use case works here as mention by @muhsin as we are trying to separate long and short queries."
      },
      {
        "date": "2021-10-16T02:54:00.000Z",
        "voteCount": 2,
        "content": "I agree with B"
      },
      {
        "date": "2021-09-21T06:15:00.000Z",
        "voteCount": 1,
        "content": "Thoughts on B?"
      },
      {
        "date": "2021-10-03T12:39:00.000Z",
        "voteCount": 2,
        "content": "the problem is that they don't share the same databases. If they have same DB, then we need to create user group and query group to prevent long run queries from consuming all resources."
      },
      {
        "date": "2021-09-27T22:31:00.000Z",
        "voteCount": 2,
        "content": "larger customer workloads do NOT interfere with the smaller customer workloads .  B still share the workload"
      },
      {
        "date": "2021-09-30T15:50:00.000Z",
        "voteCount": 1,
        "content": "thanks"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/3520-exam-aws-certified-big-data-specialty-topic-1-question-25/",
    "body": "An Amazon Kinesis stream needs to be encrypted.<br>Which approach should be used to accomplish this task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a client-side encryption of the data before it enters the Amazon Kinesis stream on the producer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a partition key to segment the data by MD5 hash function, which makes it undecipherable while in transit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a client-side encryption of the data before it enters the Amazon Kinesis stream on the consumer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a shard to segment the data, which has built-in functionality to make it indecipherable while in transit."
    ],
    "answer": "A",
    "answerDescription": "Reference: https://docs.aws.amazon.com/firehose/latest/dev/encryption.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-04T13:51:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-11-02T20:45:00.000Z",
        "voteCount": 4,
        "content": "my selection A"
      },
      {
        "date": "2021-10-20T04:15:00.000Z",
        "voteCount": 2,
        "content": "Agree with A\n https://docs.aws.amazon.com/firehose/latest/dev/encryption.html"
      },
      {
        "date": "2021-10-18T04:34:00.000Z",
        "voteCount": 2,
        "content": "Agree with A"
      },
      {
        "date": "2021-10-16T01:24:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer"
      },
      {
        "date": "2021-10-10T09:50:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer"
      },
      {
        "date": "2021-10-09T22:04:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/big-data/encrypt-and-decrypt-amazon-kinesis-records-using-aws-kms/"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/4906-exam-aws-certified-big-data-specialty-topic-1-question-26/",
    "body": "An online photo album app has a key design feature to support multiple screens (e.g, desktop, mobile phone, and tablet) with high-quality displays. Multiple versions of the image must be saved in different resolutions and layouts.<br>The image-processing Java program takes an average of five seconds per upload, depending on the image size and format. Each image upload captures the following image metadata: user, album, photo label, upload timestamp.<br>The app should support the following requirements:<br>\u2711 Hundreds of user image uploads per second<br>\u2711 Maximum image upload size of 10 MB<br>\u2711 Maximum image metadata size of 1 KB<br>\u2711 Image displayed in optimized resolution in all supported screens no later than one minute after image upload<br>Which strategy should be used to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite images and metadata to Amazon Kinesis. Use a Kinesis Client Library (KCL) application to run the image processing and save the image output to Amazon S3 and metadata to the app repository DB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite image and metadata RDS with BLOB data type. Use AWS Data Pipeline to run the image processing and save the image output to Amazon S3 and metadata to the app repository DB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload image with metadata to Amazon S3, use Lambda function to run the image processing and save the images output to Amazon S3 and metadata to the app repository DB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite image and metadata to Amazon Kinesis. Use Amazon Elastic MapReduce (EMR) with Spark Streaming to run image processing and save the images output to Amazon S3 and metadata to app repository DB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-22T01:42:00.000Z",
        "voteCount": 6,
        "content": "Answer : C \u2013 record size limitation of 1MB in Kinesis takes that option A out. Moreover using Lambda to do image processing on S3 file upload event and writing those formats back into S3 bucket and the metadata into a repository DB like Dynamo is a standard practice."
      },
      {
        "date": "2021-09-28T15:38:00.000Z",
        "voteCount": 5,
        "content": "@antoneti -- Why not Kinesis -- Kinesis is for real-time streaming. You need to deal with Shards. Single shard has 1Mb limit. This question is about image upload and transformation.  Maximum image size is 10Gb. It is easier to deal with S3 for image upload, lambda for transformation and put the metadata into something like DynamoDB. So, I think the answer is C."
      },
      {
        "date": "2021-10-20T19:49:00.000Z",
        "voteCount": 2,
        "content": "my selection C"
      },
      {
        "date": "2021-10-08T02:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is B,\nUploading images to S3 then use Lambda to resize is a popular solution.\nhttps://aws.amazon.com/jp/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/"
      },
      {
        "date": "2021-10-19T00:27:00.000Z",
        "voteCount": 3,
        "content": "Well so you mean to say Answer is \"C\"(I feel the same) as \"B\" does not seem to resonate with what you mentioned above .?"
      },
      {
        "date": "2021-09-30T03:14:00.000Z",
        "voteCount": 2,
        "content": "Kinesis is out because maximum record size of 1 MB but Lambda has also limitation of payload (6 MB) and big payloads on lambda can cause timeout problems. Any ideas? Split images before sending them to Lambda?"
      },
      {
        "date": "2021-11-04T16:03:00.000Z",
        "voteCount": 1,
        "content": "the payload limit is just on the invocation. Since you already uploaded the file on s3, you just need to tell lambda where to find it. No need to send it again to lambda. C is correct."
      },
      {
        "date": "2021-09-27T12:38:00.000Z",
        "voteCount": 1,
        "content": "and why not used Kinesis?"
      },
      {
        "date": "2021-09-24T09:34:00.000Z",
        "voteCount": 1,
        "content": "c it is"
      },
      {
        "date": "2021-09-20T03:18:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/4907-exam-aws-certified-big-data-specialty-topic-1-question-27/",
    "body": "A customer needs to determine the optimal distribution strategy for the ORDERS fact table in its Redshift schema. The ORDERS table has foreign key relationships with multiple dimension tables in this schema.<br>How should the company determine the most appropriate distribution key for the ORDERS table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the largest and most frequently joined dimension table and ensure that it and the ORDERS table both have EVEN distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the largest dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the smallest dimension table and designate the key of this dimension table as the distribution key of the ORDERS table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the largest and the most frequently joined dimension table and designate the key of this dimension table as the distribution key of the ORDERS table."
    ],
    "answer": "D",
    "answerDescription": "Reference:<br>https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on- amazon-redshift/",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-31T05:22:00.000Z",
        "voteCount": 5,
        "content": "my selection D"
      },
      {
        "date": "2021-10-20T10:31:00.000Z",
        "voteCount": 1,
        "content": "I'm sure D is a right choice"
      },
      {
        "date": "2021-10-18T10:23:00.000Z",
        "voteCount": 2,
        "content": "it is D"
      },
      {
        "date": "2021-10-01T21:29:00.000Z",
        "voteCount": 3,
        "content": "A or D?\n\nIn a typical star schema, the fact table has foreign key relationships with multiple dimension tables, so you need to choose one of the dimensions. You would choose the foreign key for the largest frequently joined dimension as a distribution key in the fact table and the primary key in the dimension table. Make sure that the distribution keys chosen result in relatively even distribution for both tables, and if the distribution is skewed, use a different dimension. Then analyze the remaining dimensions to determine if a distribution style of ALL, KEY, or EVEN is appropriate."
      },
      {
        "date": "2021-10-14T16:27:00.000Z",
        "voteCount": 1,
        "content": "I think D"
      },
      {
        "date": "2021-09-27T08:02:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/3425-exam-aws-certified-big-data-specialty-topic-1-question-28/",
    "body": "A customer is collecting clickstream data using Amazon Kinesis and is grouping the events by IP address into<br>5-minute chunks stored in Amazon S3.<br>Many analysts in the company use Hive on Amazon EMR to analyze this data. Their queries always reference a single IP address. Data must be optimized for querying based on IP address using Hive running on Amazon<br>EMR.<br>What is the most efficient method to query the data with Hive?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore an index of the files by IP address in the Amazon DynamoDB metadata store for EMRFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the Amazon S3 objects with the following naming scheme: bucket_name/source=ip_address/ year=yy/month=mm/day=dd/hour=hh/filename.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in an HBase table with the IP address as the row key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the events for an IP address as a single file in Amazon S3 and add metadata with keys: Hive_Partitioned_IPAddress."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-29T02:41:00.000Z",
        "voteCount": 15,
        "content": "Hi,\nit is not A. Hive on EMR can just use Aruro-RDS or Glue as external metastore\nit is not C. HBase no-sql database.\nit is not D. Copying all files into one single file does not help partioning.\nIt is B. partitioning in Hive supported in S3 based on typically date (this scenario ip address)"
      },
      {
        "date": "2021-09-29T18:16:00.000Z",
        "voteCount": 1,
        "content": "thanks @muhsin"
      },
      {
        "date": "2021-10-02T06:30:00.000Z",
        "voteCount": 3,
        "content": "I support B"
      },
      {
        "date": "2021-10-29T18:21:00.000Z",
        "voteCount": 11,
        "content": "After researching more I think the answer is B. When we create the hive metadata table for the bucket where these objects are stored, we will be able to query that data in S3 using a Single IP address as the table structure will have an IP address column and columns associated with month, year, day, hour."
      },
      {
        "date": "2021-11-05T19:02:00.000Z",
        "voteCount": 1,
        "content": "A is correct. \"for querying based on IP address using Hive running on Amazon\nEMR\" can be the clue. C can not be the answer because Hbase itself is not related to AWS services."
      },
      {
        "date": "2021-11-06T19:28:00.000Z",
        "voteCount": 1,
        "content": "After researching more, I would choose B. C also can be the alternative but B is simpler to implement."
      },
      {
        "date": "2021-10-31T03:50:00.000Z",
        "voteCount": 2,
        "content": "Answer: A \nQuery on a HIVE table that covers an index, avoids table scan. HIVE checks the index first and then goes to the particular column and performs the operation.\nUsing option B will require queries to specify all the column partitions in every where clause"
      },
      {
        "date": "2021-10-24T13:11:00.000Z",
        "voteCount": 1,
        "content": "Answer : D\nThis is a tough one. \nNot  A- Its not efficient although this might not be even doable. \nNot B- I didn\u2019t select \u201cB\u201d only because the question clearly states that \u201cTheir queries always reference a single IP address\u201d. So why create more partitions ( year, month, day, hour) and introduce latency when reading the metadata catalog in Hive created from S3 while executing the Hive QL when the intent is to just query using IP address alone.\nNot C \u2013 This doesn\u2019t make sense. Why would you use HBase table when your data is in S3 and Hive can query S3 objects directly using the metadata stored in EMR\nD is the correct answer- Query will be performant if there are fewer partitions in S3. Also as per the question there is a need to query the data only using IP address and therefore creating a metadata with Partition ID as the key is the right option."
      },
      {
        "date": "2021-10-20T23:40:00.000Z",
        "voteCount": 5,
        "content": "my selection B"
      },
      {
        "date": "2021-10-12T08:28:00.000Z",
        "voteCount": 1,
        "content": "This AWS Big Data Blog proves option A is correct.\nhttps://aws.amazon.com/blogs/big-data/data-lake-ingestion-automatically-partition-hive-external-tables-with-aws/"
      },
      {
        "date": "2021-10-17T12:21:00.000Z",
        "voteCount": 1,
        "content": "No. In our case, we use Kynesis for storing data and we have an opportunity to store with necessary structure. In your link lambda+ dynamoDB use for creating partition for hive before loading. It's not our case"
      },
      {
        "date": "2021-10-10T12:38:00.000Z",
        "voteCount": 3,
        "content": "I would choose B over C because analysts are, currently, using HIve and storing the data in organized manner on S3 would be efficient solution."
      },
      {
        "date": "2021-10-10T11:58:00.000Z",
        "voteCount": 1,
        "content": "Q: What is the most efficient method to query the data with Hive?  We can't design new solution with HBase .. Just have to use Hive only. B is the answer."
      },
      {
        "date": "2021-10-04T09:44:00.000Z",
        "voteCount": 1,
        "content": "I support C, In my first attempt I choose B and was wong"
      },
      {
        "date": "2021-10-21T23:59:00.000Z",
        "voteCount": 7,
        "content": "How did you know that b was wrong?"
      },
      {
        "date": "2021-09-28T07:55:00.000Z",
        "voteCount": 1,
        "content": "which answer is correct A OR C?"
      },
      {
        "date": "2021-09-28T12:49:00.000Z",
        "voteCount": 6,
        "content": "it should be C. As per the link: Apache Hadoop is not a perfect big data framework for real-time analytics and this is when HBase can be used i.e.  For real-time querying of data. HBase is an ideal big data solution if the application requires random read or random write operations or both. If the application requires to access some data in real-time then it can be stored in a NoSQL database. HBase has its own set of wonderful API\u2019s that can be used to pull or push data. HBase can also be integrated perfectly with Hadoop MapReduce for bulk operations like analytics, indexing, etc. The best way to use HBase is to make Hadoop the repository for static data and HBase the data store for data that is going to change in real-time after some processing.\n\nHBase should be used when \u2013\n\nThere is large amount of data.\nACID properties are not mandatory but just required.\nData model schema is sparse.\nWhen your applications needs to scale gracefully."
      },
      {
        "date": "2021-09-23T11:35:00.000Z",
        "voteCount": 3,
        "content": "hbase? selected C is answered key?"
      },
      {
        "date": "2021-09-27T08:00:00.000Z",
        "voteCount": 2,
        "content": "https://www.dezyre.com/article/hive-vs-hbase-different-technologies-that-work-better-together/322"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/3521-exam-aws-certified-big-data-specialty-topic-1-question-29/",
    "body": "An online retailer is using Amazon DynamoDB to store data related to customer transactions. The items in the table contains several string attributes describing the transaction as well as a JSON attribute containing the shopping cart and other details corresponding to the transaction. Average item size is  250KB, most of which is associated with the JSON attribute. The average customer generates  3GB of data per month.<br>Customers access the table to display their transaction history and review transaction details as needed.<br>Ninety percent of the queries against the table are executed when building the transaction history view, with the other 10% retrieving transaction details. The table is partitioned on CustomerID and sorted on transaction date.<br>The client has very high read capacity provisioned for the table and experiences very even utilization, but complains about the cost of Amazon DynamoDB compared to other NoSQL solutions.<br>Which strategy will reduce the cost associated with the clients read queries while not degrading quality?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify all database calls to use eventually consistent reads and advise customers that transaction history may be one second out-of-date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the primary table to partition on TransactionID, create a GSI partitioned on customer and sorted on date, project small attributes into GSI, and then query GSI for summary data and the primary table for JSON details.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertically partition the table, store base attributes on the primary table, and create a foreign key reference to a secondary table containing the JSON data. Query the primary table for summary data and the secondary table for JSON details.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an LSI sorted on date, project the JSON attribute into the index, and then query the primary table for summary data and the LSI for JSON details."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-01T16:40:00.000Z",
        "voteCount": 6,
        "content": "Answer : A\nNot B- Cannot change the primary table\u2019s partition key once created. If  GSI was created on transaction ID then we could use the base table for summary transactions and the GSI for transaction details. But even then its hard to reduce the total RCU and WCU from what they currently have as the RCU will now then distributed 90%/10% between the base table and the GSI table.\nNot C -  No concept of a foreign key in Dynamo DB.\nNot D- Cannot create an LSI after the table is created. Also the table already has date as the sort key already. Besides there are limitation on the table size containing an LSI which is 10GB.\nAnswer is A- by forcing the consumers to use eventual consistency the cost of RCU can be reduced into half."
      },
      {
        "date": "2021-11-05T21:14:00.000Z",
        "voteCount": 1,
        "content": "That's true: No concept of a foreign key in Dynamo DB. So it must be A"
      },
      {
        "date": "2021-10-03T07:59:00.000Z",
        "voteCount": 5,
        "content": "I think the answer is A as it will reduce cost and history use case don't need strongly consistent reads. The table's partition key and sort key are already correct as well.\nAre these questions really on actual exam. @mattyB123 did you appear for exam? Please let us know how was it?"
      },
      {
        "date": "2021-10-06T21:06:00.000Z",
        "voteCount": 7,
        "content": "Yes, @ranabhay majority of these questions were on my exam. But as you have noticed some of the selected answers are incorrect which is why i have been so active to discuss the reasons why for certain answers. As you can tell with these questions they aren't worded very well on purpose to make you either over or under think the solution."
      },
      {
        "date": "2021-10-09T00:30:00.000Z",
        "voteCount": 1,
        "content": "@ranabhay i think your right must be A"
      },
      {
        "date": "2021-10-09T19:33:00.000Z",
        "voteCount": 1,
        "content": "Thanks"
      },
      {
        "date": "2021-10-12T22:36:00.000Z",
        "voteCount": 3,
        "content": "But.. the question says ..\"Which strategy will reduce the cost associated with the clients read queries while not degrading quality?\" ... when you change from STRONG to EVENTUAL consistency, are we not degrading the quality?"
      },
      {
        "date": "2021-10-28T17:49:00.000Z",
        "voteCount": 2,
        "content": "yes, and not only for history. And nothing tells us that we are not already eventual consistency"
      },
      {
        "date": "2021-10-12T00:09:00.000Z",
        "voteCount": 2,
        "content": "for A, I think it degrade the quality. It can be D as it does not mentioned that you cannot re-creae the table"
      },
      {
        "date": "2021-10-12T11:47:00.000Z",
        "voteCount": 1,
        "content": "Agree with A"
      },
      {
        "date": "2021-11-06T21:13:00.000Z",
        "voteCount": 2,
        "content": "It has to be B- Question clearly says JSON has majority chunk of data which is being returned in result even though its not needed. Infrequent accessed bulk data has to be separated from small sized data to reduce cost of RCU which is best practice. One such practice is to store data in S3 and just put refrence in DynamoDB.  \nAs someone pointed vertical partitioning is way to do this as mentioned in slide page 44.\nhttps://es.slideshare.net/AmazonWebServices/advanced-design-patterns-for-amazon-dynamodb-dat403-reinvent-2017"
      },
      {
        "date": "2021-11-06T20:36:00.000Z",
        "voteCount": 3,
        "content": "B. Look at page 44:\n\nhttps://es.slideshare.net/AmazonWebServices/advanced-design-patterns-for-amazon-dynamodb-dat403-reinvent-2017"
      },
      {
        "date": "2021-11-03T23:45:00.000Z",
        "voteCount": 1,
        "content": "90% of queries against the table are executed when building transaction history view. In that case option A makes more sense. It will not impact quality as much and reduce cost immediately. Option B and D requires rebuilding of table. This also requires further evaluation that what is projected in the index will meet most of the time query requirements or not."
      },
      {
        "date": "2021-11-03T06:57:00.000Z",
        "voteCount": 1,
        "content": "A compromises quality\nB cannot change partition key for existing table\nD cannnot add a LSI after table is created\nthis leaves C, even though this is not ideal it is possible. So, C is the answer."
      },
      {
        "date": "2021-10-29T10:52:00.000Z",
        "voteCount": 1,
        "content": "A is correct because B and D are not practically doable. Cannot change the partition key and cannot create an LSI after the table is created."
      },
      {
        "date": "2021-10-26T04:24:00.000Z",
        "voteCount": 1,
        "content": "I choose A.\nB. GSI only provide eventual consistency(same as A), but it also needs to provision capacity.\nD. LSI max size for each partition key is 10GB, each customer generates 3GB data per month, so it will exceed the size limit."
      },
      {
        "date": "2021-10-25T07:48:00.000Z",
        "voteCount": 1,
        "content": "my selection A"
      },
      {
        "date": "2021-10-22T06:24:00.000Z",
        "voteCount": 2,
        "content": "For D, both the primary table and LSI contain the JSON attribute. When you query the primary table, it still returns the JSON attribute"
      },
      {
        "date": "2021-10-19T12:11:00.000Z",
        "voteCount": 3,
        "content": "I think B - yes we can not change partition key after creation but the option says \"Change the primary table to partition on TransactionID\" so we can migrate without downtime. For D - we can not create LSI after creation of table.\nFuthermore local secondary index shares provisioned throughput settings for read and write activity with the table it is indexing so there will be no improvement. GSI can be added later and has its own provisioned throughput settings for read and write activity that are separate from those of the table so would have positive impact on costs."
      },
      {
        "date": "2021-10-21T10:45:00.000Z",
        "voteCount": 2,
        "content": "One more thought concerning D - actually our table is already sorted by CustomerID and Date, so the only additional value is projecting Details to LSI. But it doesn't make sense - we can add a ProjectionExpression parameter to return only attributes excluding Details even without LSI, so additional LSI (and GSI) doesn't give us any advantage here. Really very irritating, so A seems only possible solution here but would degrage quality...."
      },
      {
        "date": "2021-10-17T00:52:00.000Z",
        "voteCount": 2,
        "content": "I will choose B since there is a limitation of max. 10GB data size per partition key for tables with LSI index and it is given that avg customer generates approx. 3GB data per month which will make  the data size exceed 10GB limit in few months per a customer.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html#LSI.ItemCollections.SizeLimit"
      },
      {
        "date": "2021-10-15T15:42:00.000Z",
        "voteCount": 2,
        "content": "C no doubts"
      },
      {
        "date": "2021-10-15T10:13:00.000Z",
        "voteCount": 1,
        "content": "\"Which strategy will reduce the cost associated with the clients read queries while not degrading quality?\" - Question is reduce the associated and latency is not quality so eventual consistent should be okay but A doesn't seem right. So C?"
      },
      {
        "date": "2021-10-15T09:56:00.000Z",
        "voteCount": 1,
        "content": "why not C? This item size is 250KB and DDB max item size is 200KB. If we cut down the usual read in 200KB. The cost will drop down."
      },
      {
        "date": "2021-10-27T09:22:00.000Z",
        "voteCount": 1,
        "content": "C looks to me like B but with relational terms (foreign key) not NoSQL ones"
      },
      {
        "date": "2021-10-14T14:50:00.000Z",
        "voteCount": 1,
        "content": "Attribute Projections is important skill of tuning Dynamo DB. so answer has to be one of projecting item.  B partition using transaction ID looks weird. so D."
      },
      {
        "date": "2021-10-26T13:31:00.000Z",
        "voteCount": 1,
        "content": "partition on transaction ID helps when going from history to one transaction detail."
      },
      {
        "date": "2021-10-13T05:09:00.000Z",
        "voteCount": 1,
        "content": "@mattyb123 you agree with A but what about performance if we choose strongly consistent , also share your exam score if possible"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/3434-exam-aws-certified-big-data-specialty-topic-1-question-30/",
    "body": "A company that manufactures and sells smart air conditioning units also offers add-on services so that customers can see real-time dashboards in a mobile application or a web browser. Each unit sends its sensor information in JSON format every two seconds for processing and analysis. The company also needs to consume this data to predict possible equipment problems before they occur. A few thousand pre-purchased units will be delivered in the next couple of months. The company expects high market growth in the next year and needs to handle a massive amount of data and scale without interruption.<br>Which ingestion solution should the company use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite sensor data records to Amazon Kinesis Streams. Process the data using KCL applications for the end-consumer dashboard and anomaly detection workflows.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch sensor data to Amazon Simple Storage Service (S3) every 15 minutes. Flow the data downstream to the end-consumer dashboard and to the anomaly detection application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite sensor data records to Amazon Kinesis Firehose with Amazon Simple Storage Service (S3) as the destination. Consume the data with a KCL application for the end-consumer dashboard and anomaly detection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite sensor data records to Amazon Relational Database Service (RDS). Build both the end-consumer dashboard and anomaly detection application on top of Amazon RDS."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-07T03:27:00.000Z",
        "voteCount": 1,
        "content": "A. Should be the right answer, considering that the Kinesis stream have automatic scale, and considering that KCL is not used for Kinesis firehose.\nhttps://aws.amazon.com/pt/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/"
      },
      {
        "date": "2021-11-03T18:29:00.000Z",
        "voteCount": 1,
        "content": "is this an actual exam question? B and D are obviously incorrect, but then we have answer C which can be excluded due to KCL not being an available consumer for FIrehose (and the dashboard being real-time), but at the same time answer A doesn't really sit well with \"scaling without interruption\" (resharding takes time and if I'm not mistaken the shards are unavailable for some time during the operation). Of the 4, A seems the only viable solution, but far from ideal."
      },
      {
        "date": "2021-11-03T12:56:00.000Z",
        "voteCount": 1,
        "content": "based on the context of the question \"massive amount of data and scale without interruption.\nWhich ingestion solution should the company use?\" it is FH. Would like this to be A but question is on on scale. Adding shard is not done in parallel. It would take long time to add 100 shards, in this aspect there could be a service interruption. C is the answer."
      },
      {
        "date": "2021-10-31T03:31:00.000Z",
        "voteCount": 4,
        "content": "Answer A: Spark /KCL does not read from KDF."
      },
      {
        "date": "2021-10-18T20:12:00.000Z",
        "voteCount": 2,
        "content": "my selection A"
      },
      {
        "date": "2021-10-17T22:29:00.000Z",
        "voteCount": 1,
        "content": "The wording of this question is really weird, real-time analysis implies Kinesis Streams (but you have to manage sharding), while massive scaling without interruption implies Kinesis Firehose since it is a managed service (but the minimum delay is 60 seconds)."
      },
      {
        "date": "2021-10-27T05:47:00.000Z",
        "voteCount": 1,
        "content": "What did you select ?"
      },
      {
        "date": "2021-10-15T18:31:00.000Z",
        "voteCount": 1,
        "content": "\"that customers can see real-time dashboards\" - this is a key for right answer.\nI think \"A\", but C would be more appropriate without this phrase"
      },
      {
        "date": "2021-10-13T16:32:00.000Z",
        "voteCount": 2,
        "content": "the difference between fh and kinesis streams is whether managed services or not. fh support autoscaling but kinesis streams need manual processing for autoscaling. 2 seconds interval is not for analytics. it is just for sending data from sensors to kinesis streams. plz do not confuse sending time with processing time. the delay for buffers (60seconds) to S3 is reasonable. it does not hurt a real time processing..."
      },
      {
        "date": "2021-10-14T10:58:00.000Z",
        "voteCount": 3,
        "content": "you are right about auto scaling, however you can split shards for Kinesis. Another point, you can not consume the data directly from Firehose using KCL (you can aggregate data using KPL but only using Kinesis Data Streams before - https://docs.aws.amazon.com/streams/latest/dev/kpl-with-firehose.html). Another point - what you are going to do after you wrote your data to S3, how to provide real-time capabilities?"
      },
      {
        "date": "2021-10-12T05:42:00.000Z",
        "voteCount": 1,
        "content": "I would choose A over C because Firehose has minimum delay of 60 seconds which is not ideal in this case (2 seconds frequency)\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/"
      },
      {
        "date": "2021-10-07T04:19:00.000Z",
        "voteCount": 1,
        "content": "What about Scale without interruption?? Can kinesis stream scale automatically? I don't think so"
      },
      {
        "date": "2021-10-05T04:00:00.000Z",
        "voteCount": 1,
        "content": "But Kinesis Streams would be handled for scaling for spikes, FH handles it automatically"
      },
      {
        "date": "2021-09-27T15:58:00.000Z",
        "voteCount": 1,
        "content": "A - https://docs.aws.amazon.com/streams/latest/dev/introduction.html"
      },
      {
        "date": "2021-10-01T16:18:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2021-09-26T22:22:00.000Z",
        "voteCount": 1,
        "content": "A is the right one"
      },
      {
        "date": "2021-09-25T22:04:00.000Z",
        "voteCount": 1,
        "content": "answer is A. Firehose is not real-time ingestion method."
      },
      {
        "date": "2021-09-20T16:52:00.000Z",
        "voteCount": 1,
        "content": "Thoughts on A? no FH delay with kinesis streams"
      },
      {
        "date": "2021-09-23T07:24:00.000Z",
        "voteCount": 1,
        "content": "answer is A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/3347-exam-aws-certified-big-data-specialty-topic-1-question-31/",
    "body": "An organization needs a data store to handle the following data types and access patterns:<br>\u2711 Faceting<br>\u2711 Search<br>\u2711 Flexible schema (JSON) and fixed schema<br>\u2711 Noise word elimination<br>Which data store should the organization choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Relational Database Service (RDS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Redshift",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Elasticsearch Service"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-03T06:10:00.000Z",
        "voteCount": 1,
        "content": "I think D, ES. I think noise word elimination can be done in ES not in DDB."
      },
      {
        "date": "2021-10-27T22:46:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer. I just search for the term facting in Big Data and Apache Solr showed up which is the underlying technology used by Elasticsearch."
      },
      {
        "date": "2021-10-24T19:32:00.000Z",
        "voteCount": 1,
        "content": "my selection D"
      },
      {
        "date": "2021-10-13T21:25:00.000Z",
        "voteCount": 4,
        "content": "Answer C is correct\nUltimately DynamoDB is a data store\nhttps://aws.amazon.com/blogs/compute/indexing-amazon-dynamodb-content-with-amazon-elasticsearch-service-using-aws-lambda/"
      },
      {
        "date": "2021-10-26T09:10:00.000Z",
        "voteCount": 2,
        "content": "Dynamodb doesn't support schema so answer is D"
      },
      {
        "date": "2021-10-11T15:16:00.000Z",
        "voteCount": 3,
        "content": "i cannot believe most of user answers are wrong. Elasticsearch is a search engine with the document data store as a secondary database; it doesn't support key-value type data, so the right answer is C"
      },
      {
        "date": "2021-10-06T10:36:00.000Z",
        "voteCount": 1,
        "content": "I would say D as well"
      },
      {
        "date": "2021-10-02T01:23:00.000Z",
        "voteCount": 1,
        "content": "Yes, answer is D"
      },
      {
        "date": "2021-10-01T19:41:00.000Z",
        "voteCount": 1,
        "content": "Are you guys 100% sure it is Elasticsearch? ..."
      },
      {
        "date": "2021-09-25T11:39:00.000Z",
        "voteCount": 1,
        "content": "d is the correct answer"
      },
      {
        "date": "2021-10-01T01:16:00.000Z",
        "voteCount": 3,
        "content": "Yep. D.. Elasticsearch supports all 4 points"
      },
      {
        "date": "2021-09-24T23:47:00.000Z",
        "voteCount": 1,
        "content": "i think so as well"
      },
      {
        "date": "2021-09-20T16:57:00.000Z",
        "voteCount": 1,
        "content": "isn't this answer D"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/5348-exam-aws-certified-big-data-specialty-topic-1-question-32/",
    "body": "A travel website needs to present a graphical quantitative summary of its daily bookings to website visitors for marketing purposes. The website has millions of visitors per day, but wants to control costs by implementing the least-expensive solution for this visualization.<br>What is the most cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a static graph with a transient EMR cluster daily, and store it an Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a graph using MicroStrategy backed by a transient EMR cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Jupyter front-end provided by a continuously running EMR cluster leveraging spot instances for task nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Zeppelin application that runs on a long-running EMR cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-30T01:38:00.000Z",
        "voteCount": 6,
        "content": "A. transient cluster is cost-effective solution"
      },
      {
        "date": "2021-10-17T07:05:00.000Z",
        "voteCount": 3,
        "content": "my selection A"
      },
      {
        "date": "2021-10-05T03:47:00.000Z",
        "voteCount": 1,
        "content": "OPTION C and D are ruled out . AS it is for marketing purpose only they can just generate graph daily once and store it in S3. webiste can access the graph using S3 static web hosting. So Option A is best answer"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/3413-exam-aws-certified-big-data-specialty-topic-1-question-33/",
    "body": "A system engineer for a company proposes digitalization and backup of large archives for customers. The systems engineer needs to provide users with a secure storage that makes sure that data will never be tampered with once it has been uploaded.<br>How should this be accomplished?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Glacier Vault. Specify a \"Deny\" Vault Lock policy on this Vault to block \"glacier:DeleteArchive\".",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Specify a \"Deny\" bucket policy on this bucket to block \"s3:DeleteObject\".",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Glacier Vault. Specify a \"Deny\" vault access policy on this Vault to block \"glacier:DeleteArchive\".",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate secondary AWS Account containing an Amazon S3 bucket. Grant \"s3:PutObject\" to the primary account."
    ],
    "answer": "C",
    "answerDescription": "Reference: https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-02T05:25:00.000Z",
        "voteCount": 5,
        "content": "Should be A. Vault access policy can be modified, so it mean the data can be tampered when someone change the vault access policy. Vault lock policy cannot be modified, so it can say 'never be tampered'"
      },
      {
        "date": "2021-11-01T00:18:00.000Z",
        "voteCount": 2,
        "content": "A for sure.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html\n\"As an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test this policy before locking it down.\"\nYou can also find a policy with a Deny effect on \"glacier:DeleteArchive\" action on the same link."
      },
      {
        "date": "2021-10-30T15:42:00.000Z",
        "voteCount": 1,
        "content": "A. Because the data requires NEVER be tampered, after uploaded."
      },
      {
        "date": "2021-10-23T07:51:00.000Z",
        "voteCount": 1,
        "content": "Should be A :\nAWS Docs : https://d1.awsstatic.com/Projects/P4113791/aws-project_set-up-compliant-archive.pdf\nPage 4 : A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. \nHowever, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. \nYou can use the vault lock policy to deploy regulatory and compliance\ncontrols, which typically require tight controls on data access. \nIn contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification. Vault lock and vault access policies can be used together. For example, you can implement time-based data retention rules in the vault lock policy (deny deletes), and grant read access to designated third parties or your business partners (allow reads)."
      },
      {
        "date": "2021-10-22T00:11:00.000Z",
        "voteCount": 2,
        "content": "Answer is A based on AWS Documentation below.\n\nAn Amazon S3 Glacier (S3 Glacier) vault can have one resource-based vault access policy and one Vault Lock policy attached to it. A Vault Lock policy is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements. Amazon S3 Glacier provides a set of API operations for you to manage the Vault Lock policies, see Locking a Vault by Using the Amazon S3 Glacier API.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html\nAs an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test this policy before locking it down. After you lock the policy, the policy becomes immutable. For more information about the locking process, see Amazon S3 Glacier Vault Lock. If you want to manage other user permissions that can be changed, you can use the vault access policy (see Amazon S3 Glacier Access Control with Vault Access Policies)."
      },
      {
        "date": "2021-10-20T06:07:00.000Z",
        "voteCount": 3,
        "content": "A\n\"A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. However, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. You can use the vault lock policy to deploy regulatory and compliance controls, which typically require tight controls on data access. In contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification\"\n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html"
      },
      {
        "date": "2021-10-21T14:36:00.000Z",
        "voteCount": 1,
        "content": "Agree\n\nSuppose that you have a regulatory requirement to retain archives for up to one year before you can delete them. You can enforce that requirement by implementing the following Vault Lock policy. The policy denies the glacier:DeleteArchive action on the examplevault vault if the archive being deleted is less than one year old. The policy uses the S3 Glacier-specific condition key ArchiveAgeInDays to enforce the one-year retention requirement."
      },
      {
        "date": "2021-10-18T19:35:00.000Z",
        "voteCount": 2,
        "content": "i think it should be A.  \"You can use the Vault Lock policy to deploy regulatory and compliance controls that are typically restrictive and are \u201cset and forget\u201d in nature.\" Here also they use the word 'Never"
      },
      {
        "date": "2021-10-18T14:15:00.000Z",
        "voteCount": 1,
        "content": "my selection A"
      },
      {
        "date": "2021-10-16T05:28:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\nhttps://aws.amazon.com/glacier/faqs/  -- refer Vault Lock section"
      },
      {
        "date": "2021-10-15T22:47:00.000Z",
        "voteCount": 1,
        "content": "You can now create a Vault Lock policy on a vault, and after it is locked, the policy cannot be overwritten or deleted. For SaaS, one should have chance to delete customers' data. Any suggestions?"
      },
      {
        "date": "2021-10-14T03:39:00.000Z",
        "voteCount": 1,
        "content": "It would be A over C since Vault Lock policy is immutable and it satisfies the requirement\nthat data will never be tampered once uploaded."
      },
      {
        "date": "2021-10-13T09:15:00.000Z",
        "voteCount": 4,
        "content": "Answer is A\n\nA Glacier \"vault access policy\" is a resource based policy that you can use to manage permissions to your vault.You can modify permissions in a Vault access policy at any time. \n\nA Glacier \"vault lock policy\" is vault access policy that can be locked. After you lock a vault lock policy, the policy cannot be changed. You can use a vault lock policy to enforce compliance controls. \n\nYou can enforce the requirement by implementing the following vault lock policy: \"glacier:DeleteArchieve\" action on the vault."
      },
      {
        "date": "2021-10-12T13:54:00.000Z",
        "voteCount": 2,
        "content": "I will go with A. As there is no \"glacier:DeleteArchive\" option in Vault Access Policy"
      },
      {
        "date": "2021-10-08T04:46:00.000Z",
        "voteCount": 3,
        "content": "Should be A\nhttps://aws.amazon.com/glacier/faqs/\na Vault Lock policy can be made immutable ...."
      },
      {
        "date": "2021-10-10T11:42:00.000Z",
        "voteCount": 1,
        "content": "It is A, read FAQ for the difference between Vault Lock vs Vault Access"
      },
      {
        "date": "2021-10-07T10:19:00.000Z",
        "voteCount": 1,
        "content": "It is C"
      },
      {
        "date": "2021-10-02T22:03:00.000Z",
        "voteCount": 1,
        "content": "I think it is \"A\" (vault lock) ..because the question says \"..makes sure that data will never be tampered with once it has been uploaded...\" . If it is vault-access, you can change it after anytime and this is not permitted according the question."
      },
      {
        "date": "2021-09-25T15:30:00.000Z",
        "voteCount": 3,
        "content": "policy name is vault lock policy. but the configuration is being done with vaul-access-policy\nc is the answer."
      },
      {
        "date": "2021-09-25T19:30:00.000Z",
        "voteCount": 3,
        "content": "Doesn't the question ask never be tampered with, meaning no user access it at all? From the link it mentions the below: A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. However, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. You can use the vault lock policy to deploy regulatory and compliance controls, which typically require tight controls on data access. In contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification. Vault lock and vault access policies can be used together. For example, you can implement time-based data retention rules in the vault lock policy (deny deletes), and grant read access to designated third parties or your business partners (allow reads)."
      },
      {
        "date": "2021-09-27T04:52:00.000Z",
        "voteCount": 1,
        "content": "Apologies it is C. \nhttps://aws.amazon.com/glacier/faqs/\nVault access policies can make certain use cases simpler. For example, to protect information in a business-critical vault from unintended deletion, you can create a vault access policy that denies delete attempts from all users."
      },
      {
        "date": "2021-10-26T09:44:00.000Z",
        "voteCount": 1,
        "content": "It should be A. If someone was able to remove the vault access policy then the data can be tampered with. The keyword is \"never\". Therefore the vault lock policy will 100% gaurantee the data will not be tampered as you cannot change the vault lock policy once it is created"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/3598-exam-aws-certified-big-data-specialty-topic-1-question-34/",
    "body": "An organization needs to design and deploy a large-scale data storage solution that will be highly durable and highly flexible with respect to the type and structure of data being stored. The data to be stored will be sent or generated from a variety of sources and must be persistently available for access and processing by multiple applications.<br>What is the most cost-effective technique to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Simple Storage Service (S3) as the actual data storage system, coupled with appropriate tools for ingestion/acquisition of data and for subsequent processing and querying.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a long-running Amazon Elastic MapReduce (EMR) cluster with Amazon Elastic Block Store (EBS) volumes for persistent HDFS storage and appropriate Hadoop ecosystem tools for processing and querying.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift with data replication to Amazon Simple Storage Service (S3) for comprehensive durable data storage, processing, and querying.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon Relational Database Service (RDS), and use the enterprise grade and capacity of the Amazon Aurora engine for storage, processing, and querying."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-06T18:00:00.000Z",
        "voteCount": 2,
        "content": "A is obvious but when you say Reveal solution why it is showing C. very misleading answers"
      },
      {
        "date": "2021-11-06T01:52:00.000Z",
        "voteCount": 4,
        "content": "my selection A"
      },
      {
        "date": "2021-11-02T11:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2021-11-01T22:41:00.000Z",
        "voteCount": 1,
        "content": "same over here, mine would be A for sure"
      },
      {
        "date": "2021-10-18T01:45:00.000Z",
        "voteCount": 1,
        "content": "Agreed.A would be cost effective solution."
      },
      {
        "date": "2021-10-13T23:24:00.000Z",
        "voteCount": 2,
        "content": "A is cost-effective"
      },
      {
        "date": "2021-09-20T23:36:00.000Z",
        "voteCount": 4,
        "content": "Thoughts on A. Highly flexible storage system makes me think s3"
      },
      {
        "date": "2021-09-24T04:09:00.000Z",
        "voteCount": 1,
        "content": "Anyone agree or disagree?"
      },
      {
        "date": "2021-09-27T07:43:00.000Z",
        "voteCount": 1,
        "content": "Agreed: Answer should be A"
      },
      {
        "date": "2021-09-29T05:55:00.000Z",
        "voteCount": 1,
        "content": "I also agree with A from Cost-effective"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/5349-exam-aws-certified-big-data-specialty-topic-1-question-35/",
    "body": "A customer has a machine learning workflow that consists of multiple quick cycles of reads-writes-reads on<br>Amazon S3. The customer needs to run the workflow on EMR but is concerned that the reads in subsequent cycles will miss new data critical to the machine learning from the prior cycles.<br>How should the customer accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on EMRFS consistent view when configuring the EMR cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Data Pipeline to orchestrate the data processing cycles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet hadoop.data.consistency = true in the core-site.xml file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet hadoop.s3.consistency = true in the core-site.xml file."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-28T14:56:00.000Z",
        "voteCount": 3,
        "content": "EMRFS consistent view is made for this, so A."
      },
      {
        "date": "2021-09-28T08:51:00.000Z",
        "voteCount": 2,
        "content": "my selection A"
      },
      {
        "date": "2021-09-19T19:54:00.000Z",
        "voteCount": 3,
        "content": "I agree with A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/3362-exam-aws-certified-big-data-specialty-topic-1-question-36/",
    "body": "An Amazon Redshift Database is encrypted using KMS. A data engineer needs to use the AWS CLI to create a KMS encrypted snapshot of the database in another AWS region.<br>Which three steps should the data engineer take to accomplish this task? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new KMS key in the destination region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the existing KMS key to the destination region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source region, enable cross-region replication and specify the name of the copy grant created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the destination region, enable cross-region replication and specify the name of the copy grant created."
    ],
    "answer": "ABD",
    "answerDescription": "Reference: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with- aws-kms",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-01T14:17:00.000Z",
        "voteCount": 1,
        "content": "Question 66 is the same"
      },
      {
        "date": "2021-11-01T07:11:00.000Z",
        "voteCount": 1,
        "content": "ACD is correct. Direct question from \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with-aws-kms"
      },
      {
        "date": "2021-10-27T08:01:00.000Z",
        "voteCount": 2,
        "content": "my selection ACD"
      },
      {
        "date": "2021-10-26T03:42:00.000Z",
        "voteCount": 2,
        "content": "Here one option is missing\nC. Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the destination region. (Correct)\nhttps://chercher.tech/aws-certification/aws-certified-big-data-speciality-practice-exams-set-1\n\nfind here step by step AWS Redshift cross-region copy snapshot through the console (Question ask for CLI but both has same steps)\nhttps://www.youtube.com/watch?v=9DepoiBOe6o"
      },
      {
        "date": "2021-10-24T13:25:00.000Z",
        "voteCount": 1,
        "content": "i think ACD."
      },
      {
        "date": "2021-10-19T15:50:00.000Z",
        "voteCount": 3,
        "content": "Before the snapshot is copied to the destination AWS Region, Amazon Redshift decrypts the snapshot using the master key in the source AWS Region and re-encrypts it temporarily using a randomly generated RSA key that Amazon Redshift manages internally. Amazon Redshift then copies the snapshot over a secure channel to the destination AWS Region, decrypts the snapshot using the internally managed RSA key, and then re-encrypts the snapshot using the master key in the destination AWS Region."
      },
      {
        "date": "2021-10-13T03:52:00.000Z",
        "voteCount": 2,
        "content": "I would choose ACD though C is not accurate answer as suggested by Mattyb123"
      },
      {
        "date": "2021-10-10T23:40:00.000Z",
        "voteCount": 2,
        "content": "I agree with ACD.there was one more option as below. though that is wrong option. \nUse CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region."
      },
      {
        "date": "2021-10-01T15:57:00.000Z",
        "voteCount": 1,
        "content": "ACD is correct"
      },
      {
        "date": "2021-09-30T03:06:00.000Z",
        "voteCount": 3,
        "content": "C option should be like Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key created in the destination region NOT source region"
      },
      {
        "date": "2021-10-18T05:52:00.000Z",
        "voteCount": 1,
        "content": "why destination? the cross-region replica is made from the source so the source is the one to have access to the KMS created"
      },
      {
        "date": "2021-11-01T02:07:00.000Z",
        "voteCount": 1,
        "content": "If you want to enable cross-Region snapshot copy for an AWS KMS\u2013encrypted cluster, you must configure a snapshot copy grant for a master key in the destination AWS Region. By doing this, you enable Amazon Redshift to perform encryption operations in the destination AWS Region\n\nLink: https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot"
      },
      {
        "date": "2021-09-30T10:08:00.000Z",
        "voteCount": 2,
        "content": "Same question is on page 14. https://www.examtopics.com/exams/amazon/aws-certified-big-data-specialty/view/14/ ADF is correct answer. as F includes Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key created in the destination region"
      },
      {
        "date": "2021-09-28T02:27:00.000Z",
        "voteCount": 2,
        "content": "ACD is the correct answer"
      },
      {
        "date": "2021-09-28T06:17:00.000Z",
        "voteCount": 1,
        "content": "ACD is correct"
      },
      {
        "date": "2021-09-22T01:51:00.000Z",
        "voteCount": 2,
        "content": "Agreed ACD."
      },
      {
        "date": "2021-09-27T16:01:00.000Z",
        "voteCount": 1,
        "content": "https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      },
      {
        "date": "2021-09-21T17:54:00.000Z",
        "voteCount": 2,
        "content": "acd ? anyone?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/3522-exam-aws-certified-big-data-specialty-topic-1-question-37/",
    "body": "Managers in a company need access to the human resources database that runs on Amazon Redshift, to run reports about their employees. Managers must only see information about their direct reports.<br>Which technique should be used to address this requirement with Amazon Redshift?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an IAM group for each manager with each employee as an IAM user in that group, and use that to limit the access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift snapshot to create one cluster per manager. Allow the manager to access only their designated clusters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a key for each manager in AWS KMS and encrypt the data for their employees with their private keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a view that uses the employee\u2019s manager name to filter the records based on current user names."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-20T11:18:00.000Z",
        "voteCount": 8,
        "content": "Answer is D. One of the reasons for not going with A is, max number of IAM groups in an AWS account is 300. So, A is not scalable solution. If the company has more than 300 managers, A won't work."
      },
      {
        "date": "2021-09-20T12:05:00.000Z",
        "voteCount": 1,
        "content": "I would go with A. You can create as many groups as you can."
      },
      {
        "date": "2021-10-17T22:59:00.000Z",
        "voteCount": 2,
        "content": "But you can have up-to 5000 IAM users in one AWS account"
      },
      {
        "date": "2021-10-26T16:37:00.000Z",
        "voteCount": 5,
        "content": "All in all, option D is the correct answer\n\n\"The second advantage of views is that you can assign a different set of permissions to the view. A user might be able to query the view, but not the underlying table. Creating the view excluding the sensitive columns (or rows) should be useful in this scenario.\"\nhttp://www.silota.com/blog/rethink-database-views-redshift/\n\n\"For example, the following command enables the user HR both to perform SELECT commands on the employees table and to grant and revoke the same privilege for other users.\ngrant select on table employees to HR with grant option;\"\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html"
      },
      {
        "date": "2021-11-06T22:48:00.000Z",
        "voteCount": 1,
        "content": "D is probably the best option. A means creating a IAM user for each employee. While we have no details of the industry or the number of employees, in general that doesn't seem a great idea."
      },
      {
        "date": "2021-11-05T20:52:00.000Z",
        "voteCount": 3,
        "content": "Answer : D \u2013 \nNot A \u2013 Lot of maintenance to create one group per manager.\nNot B \u2013 Cost overhead- cost will multiple manifold.\nNot C \u2013 Doesn\u2019t make sense. How will RedShift talk to KMS? Who will manage the key-pair for each manager in KMS.\nD \u2013 is the correct answer because you can create view to provide row-level access based on the attribute values in the underlying table."
      },
      {
        "date": "2021-11-04T23:06:00.000Z",
        "voteCount": 5,
        "content": "my selection D"
      },
      {
        "date": "2021-10-30T12:30:00.000Z",
        "voteCount": 2,
        "content": "D is the right choice. There is only one option for RLS (row-level security)"
      },
      {
        "date": "2021-10-25T10:31:00.000Z",
        "voteCount": 1,
        "content": "I would choose D over A since IAM access let you control access or deny at table level but not at records level as per my understanding."
      },
      {
        "date": "2021-10-13T16:59:00.000Z",
        "voteCount": 1,
        "content": "D, using view you can restrict data that is being retrieved from Redshift. It is a common practice in traditional Relational DBs"
      },
      {
        "date": "2021-10-13T08:28:00.000Z",
        "voteCount": 2,
        "content": "I went with A"
      },
      {
        "date": "2021-10-12T17:40:00.000Z",
        "voteCount": 1,
        "content": "D is a common approach in this case. A should be incorrect since, e.j.\nCreate view my_employees as  select * from employees where manager = db_user_who_is_a _manager. \nOption A is incorrect as per its wording. You don\u2019t create IAM groups for a manager with each employee as an IAM user in that group...\nThis question is not related to security."
      },
      {
        "date": "2021-10-10T16:59:00.000Z",
        "voteCount": 3,
        "content": "create as many groups to control access right, I don't hire pra276 even he has tons of Certs."
      },
      {
        "date": "2021-09-24T18:07:00.000Z",
        "voteCount": 1,
        "content": "Whoever says D is answer. Please read about these \nhttps://www.intermix.io/blog/iam-to-generate-temporary-amazon-redshift-passwords/ \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/generating-iam-credentials-steps.html"
      },
      {
        "date": "2021-10-01T02:01:00.000Z",
        "voteCount": 3,
        "content": "I completely understand where you are coming from but the question asks 'address this requirement with Amazon Redshift'. So the simple way to do that in redshift is through views. Also its hinted quite heavily on the acloudguru and aws big data exam prep course about using redshift views."
      },
      {
        "date": "2021-10-09T07:11:00.000Z",
        "voteCount": 1,
        "content": "Please view the big data exam preparation course on aws. Views are mentioned https://www.aws.training/Details/Curriculum?id=21332"
      },
      {
        "date": "2021-09-20T12:25:00.000Z",
        "voteCount": 4,
        "content": "D, redshift view is allowed filter out the user base access"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/4724-exam-aws-certified-big-data-specialty-topic-1-question-38/",
    "body": "A company is building a new application in AWS. The architect needs to design a system to collect application log events. The design should be a repeatable pattern that minimizes data loss if an application instance fails, and keeps a durable copy of a log data for at least 30 days.<br>What is the simplest architecture that will allow the architect to analyze the logs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite them directly to a Kinesis Firehose. Configure Kinesis Firehose to load the events into an Amazon Redshift cluster for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite them to a file on Amazon Simple Storage Service (S3). Write an AWS Lambda function that runs in response to the S3 event to load the events into Amazon Elasticsearch Service for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite them to the local disk and configure the Amazon CloudWatch Logs agent to load the data into CloudWatch Logs and subsequently into Amazon Elasticsearch Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite them to CloudWatch Logs and use an AWS Lambda function to load them into HDFS on an Amazon Elastic MapReduce (EMR) cluster for analysis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-18T16:19:00.000Z",
        "voteCount": 11,
        "content": "Correct answer is C: \nNot A - because writing logs to RedShift doesn't make sense.\nNot B - because to write logs to S3 you will have to still configure Cloudwatch logs on the server or write code in your application to use S3 SDK to write the log data to S3 directly which is not the best or simplest solution.\nNot D -because EMR is not a storage service.\nOnly option that fits is C with no additional effort except for configuring Cloudwatch log agent to ship the logs to Cloudwatch and then configure CloudWatch to send the logs directly to Elasticsearch which doesn't require Lambda to glue them together."
      },
      {
        "date": "2021-11-06T22:14:00.000Z",
        "voteCount": 1,
        "content": "For me the right option is B considering that \"Write them to the local disk\" will not be compliance with the requirement because if the application fail the logs will be lost."
      },
      {
        "date": "2021-11-05T22:45:00.000Z",
        "voteCount": 2,
        "content": "C:\nYou can configure a CloudWatch Logs log group to stream data it receives to your Amazon Elasticsearch Service (Amazon ES) cluster in near real-time through a CloudWatch Logs subscription."
      },
      {
        "date": "2021-11-05T02:57:00.000Z",
        "voteCount": 2,
        "content": "\"Simplest architecture\" .... Answer is C. No code , just configuration"
      },
      {
        "date": "2021-10-29T18:44:00.000Z",
        "voteCount": 1,
        "content": "A is perfect answer actually, kinesis firehose first writes data to s3, this meets saving logs upto 30 days with lifecycle policy on bucket, while redshift can be used for analysis"
      },
      {
        "date": "2021-10-24T06:04:00.000Z",
        "voteCount": 1,
        "content": "B is wrong. If you are a developer ,you will know application can't write log to s3 directly . \napplication must write log to buffer or file ,and then put to s3\nso C is correct answer"
      },
      {
        "date": "2021-10-26T10:03:00.000Z",
        "voteCount": 1,
        "content": "good point, but i think \"write them to a file on S3\" leaves room for creating this file first locally and then store it to S3. no need to subsequentially write to an existing S3 object. but yeah, i still think C is correct due to Bulit's comments."
      },
      {
        "date": "2021-10-19T07:18:00.000Z",
        "voteCount": 4,
        "content": "to support answer C- Here is from Cloudwatch FAQ\nCloudWatch Logs uses your log data for monitoring; so, no code changes are required. Long term log retention: You can use CloudWatch Logs to store your log data indefinitely in highly durable and cost effective storage without worrying about hard drives running out of space."
      },
      {
        "date": "2021-10-26T13:53:00.000Z",
        "voteCount": 1,
        "content": "to make this even more clear, Cloudwatch Log groups can also be easily streamed to ElasticSearch https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html"
      },
      {
        "date": "2021-10-29T13:23:00.000Z",
        "voteCount": 1,
        "content": "This is C, look at this page: https://www.oreilly.com/library/view/aws-automation-cookbook/9781788394925/30fd87cf-3d67-4363-a95a-5208296d32cb.xhtml"
      },
      {
        "date": "2021-10-17T20:39:00.000Z",
        "voteCount": 1,
        "content": "can anyone pls explain why not A?"
      },
      {
        "date": "2021-10-23T01:30:00.000Z",
        "voteCount": 1,
        "content": "to me it looks like technically this could work. however, Elasticsearch is really suited for log file analysis and both Kinesis and Redshift can be expensive."
      },
      {
        "date": "2021-10-17T10:01:00.000Z",
        "voteCount": 3,
        "content": "my selection B"
      },
      {
        "date": "2021-10-27T16:13:00.000Z",
        "voteCount": 1,
        "content": "Itc C... But a large portion of ur answers are right :p"
      },
      {
        "date": "2021-10-17T04:48:00.000Z",
        "voteCount": 2,
        "content": "The big problem with option C is how to load events into ES?\nSo one option left is B"
      },
      {
        "date": "2021-10-22T02:10:00.000Z",
        "voteCount": 1,
        "content": "Simple way to do it through console. I guess the answer is C.\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-aws-integrations.html#es-aws-integrations-cloudwatch-es"
      },
      {
        "date": "2021-10-12T21:50:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-10-12T15:54:00.000Z",
        "voteCount": 2,
        "content": "B is Correct. \n\nLocal disk or EBS durability is 5 9's, while S3 durability is 11 9's.\n\nAs the requirement is to keep a durable copy of the log data, S3 is the the best option."
      },
      {
        "date": "2021-10-04T23:59:00.000Z",
        "voteCount": 1,
        "content": "I also pick B, S3 is simple and durable and Elasticsearch is for log analysis."
      },
      {
        "date": "2021-09-29T16:55:00.000Z",
        "voteCount": 1,
        "content": "the \"minimizes data loss if an application instance fails\" phrase hints me to option C.\nOption B looks to simple, but you have to re-invent the services (lambda probably) to transfer the files to S3 and schedule it."
      },
      {
        "date": "2021-09-29T13:31:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-09-28T23:26:00.000Z",
        "voteCount": 2,
        "content": "mattyb123 are you sure ?"
      },
      {
        "date": "2021-09-26T10:16:00.000Z",
        "voteCount": 2,
        "content": "B. Question states minimizes data loss  S3 should be used."
      },
      {
        "date": "2021-09-28T14:22:00.000Z",
        "voteCount": 2,
        "content": "B. most durable"
      },
      {
        "date": "2021-10-27T04:21:00.000Z",
        "voteCount": 1,
        "content": "Slightly more durable but not nearly as simple"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/3586-exam-aws-certified-big-data-specialty-topic-1-question-39/",
    "body": "An organization uses a custom map reduce application to build monthly reports based on many small data files in an Amazon S3 bucket. The data is submitted from various business units on a frequent but unpredictable schedule. As the dataset continues to grow, it becomes increasingly difficult to process all of the data in one day. The organization has scaled up its Amazon EMR cluster, but other optimizations could improve performance.<br>The organization needs to improve performance with minimal changes to existing processes and applications.<br>What action should the organization take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Event Notifications and AWS Lambda to create a quick search file index in DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd Spark to the Amazon EMR cluster and utilize Resilient Distributed Datasets in-memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Event Notifications and AWS Lambda to index each file into an Amazon Elasticsearch Service cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a daily AWS Data Pipeline process that aggregates content into larger files using S3DistCp.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave business units submit data via Amazon Kinesis Firehose to aggregate data hourly into Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-18T22:24:00.000Z",
        "voteCount": 10,
        "content": "The Answer is D.\n\nS3Distcp has native capability to combine multiple small files into large files and does not require any coding. So, minimal change to existing processes. \n\nhttps://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/#5 \n\nAlso through a process of elimination, we can exclude \n\nA) DynamoDB is not a good service for large scale data analytics.\nC) Moving from EMR to ElasticSearch requires significant changes to existing processes and applications. \nB) Moving from MR to Spark requires significant changes to existing processes and applications. \nE) Moving from S3 to firehose requires significant changes to existing processes and applications."
      },
      {
        "date": "2021-10-19T23:04:00.000Z",
        "voteCount": 3,
        "content": "Option E is wrong because of how FH to aggregate data hourly into Amazon S3?"
      },
      {
        "date": "2021-10-30T07:16:00.000Z",
        "voteCount": 8,
        "content": "D. Hadoop is optimized for reading a fewer number of large files rather than many small files, whether from S3 or HDFS. You can use S3DistCp to aggregate small files into fewer large files of a size that you choose, which can optimize your analysis for both performance and cost. https://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/#5"
      },
      {
        "date": "2021-11-06T00:40:00.000Z",
        "voteCount": 1,
        "content": "D is correct. You can aggregate files using DistCp command. Hadoop is optimized for reading a fewer number of large files rather than many small files"
      },
      {
        "date": "2021-10-23T12:39:00.000Z",
        "voteCount": 2,
        "content": "The choice is really between B and D. \nWith B- Spark has the capability to improve performance over the existing AMR setup if lets says its using Hive or Pig instead. However it still needs to deal with large number of small data sets to process which is really the issue. So at the end of the day it won't solve the root cause of the issue which is about I/O not about compute. \nWith D- This is the simplest solution that doesn't need the businesses to change the way they ingest those files into the organization today. It's a change that solves the root cause of the problem which is I/O yet keeps the overall process and flow unchanged.\n\nSo the correct answer is D."
      },
      {
        "date": "2021-10-22T00:04:00.000Z",
        "voteCount": 2,
        "content": "It is B\n\nWrong for : \nA- is wrong because new function\nC- Lambda with elastic search - why something new \nD- data is already there, its just gorwing the issue is not how to store or move data - but how to process\nE- a big change for all"
      },
      {
        "date": "2021-10-26T09:39:00.000Z",
        "voteCount": 2,
        "content": "Hadoop has a default chunk size of 64MB and is not efficient at processing many small files, that's why D actually does help."
      },
      {
        "date": "2021-10-21T18:16:00.000Z",
        "voteCount": 3,
        "content": "my selection D"
      },
      {
        "date": "2021-10-17T18:47:00.000Z",
        "voteCount": 3,
        "content": "Answer us D https://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/"
      },
      {
        "date": "2021-10-15T13:46:00.000Z",
        "voteCount": 1,
        "content": "I support B since you can add a custom Jar so I believe you could use your own map-reduce app there"
      },
      {
        "date": "2021-10-15T03:08:00.000Z",
        "voteCount": 1,
        "content": "I would choose D as it is simple solution to improve performance with minimal changes\n to the existing process."
      },
      {
        "date": "2021-10-13T02:02:00.000Z",
        "voteCount": 4,
        "content": "D looks reasonable.\n\nUsing S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster."
      },
      {
        "date": "2021-10-11T19:06:00.000Z",
        "voteCount": 1,
        "content": "What is the right answer for this? A or D?"
      },
      {
        "date": "2021-10-05T21:11:00.000Z",
        "voteCount": 3,
        "content": "Option A, require EMR to access DynamoDB to get the index files which needs change in current setup. Using Option D, you can add a S3DistCp  step in EMR job which is minimal change. Option D is correct"
      },
      {
        "date": "2021-10-05T13:35:00.000Z",
        "voteCount": 3,
        "content": "A,E &amp; C are not minimal changes. the question says minimal changes to existing processes and application. Answer should be  D or B."
      },
      {
        "date": "2021-09-26T16:07:00.000Z",
        "voteCount": 1,
        "content": "Any thought on B?"
      },
      {
        "date": "2021-09-22T05:23:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-09-22T04:43:00.000Z",
        "voteCount": 3,
        "content": "a require some additional services. the question asks with minimal changes."
      },
      {
        "date": "2021-09-26T14:11:00.000Z",
        "voteCount": 2,
        "content": "Since the app is written for map reduce wouldn't adding spark mean the app would have to be re-written for spark?"
      },
      {
        "date": "2021-09-19T21:09:00.000Z",
        "voteCount": 1,
        "content": "Thoughts on A?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/3592-exam-aws-certified-big-data-specialty-topic-1-question-40/",
    "body": "An administrator is processing events in near real-time using Kinesis streams and Lambda. Lambda intermittently fails to process batches from one of the shards due to a 5-munite time limit.<br>What is a possible solution for this problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more Lambda functions to improve concurrent batch processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the batch size that Lambda is reading from the stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIgnore and skip events that are older than 5 minutes and put them to Dead Letter Queue (DLQ).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Lambda to read from fewer shards in parallel."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-20T05:35:00.000Z",
        "voteCount": 6,
        "content": "I think it is B. with Lambda, we can finetune \nMemory\nBatch-size\nTimeout. There is no option for lambda to tune shards reading. \nhttps://tech.trivago.com/2018/07/13/aws-kinesis-with-lambdas-lessons-learned/"
      },
      {
        "date": "2021-09-24T06:42:00.000Z",
        "voteCount": 7,
        "content": "More resources:\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\nTest with different batch and record sizes so that the polling frequency of each event source is tuned to how quickly your function is able to complete its task. BatchSize controls the maximum number of records that can be sent to your function with each invoke. A larger batch size can often more efficiently absorb the invoke overhead across a larger set of records, increasing your throughput. By default, Lambda invokes your function as soon as records are available in the stream. If the batch it reads from the stream only has one record in it, Lambda only sends one record to the function. To avoid invoking the function with a small number of records, you can tell the event source to buffer records for up to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to read records from the stream until it has gathered a full batch, or until the batch window expires."
      },
      {
        "date": "2021-09-23T11:49:00.000Z",
        "voteCount": 6,
        "content": "each Kinesis shard invoke a separate Lambda."
      },
      {
        "date": "2021-09-24T01:09:00.000Z",
        "voteCount": 3,
        "content": "agreed"
      },
      {
        "date": "2021-10-03T00:58:00.000Z",
        "voteCount": 4,
        "content": "B. Agreed"
      },
      {
        "date": "2021-10-08T09:36:00.000Z",
        "voteCount": 3,
        "content": "Yes, it is B"
      },
      {
        "date": "2021-11-05T12:24:00.000Z",
        "voteCount": 3,
        "content": "my selection B"
      },
      {
        "date": "2021-10-17T09:09:00.000Z",
        "voteCount": 2,
        "content": "B. is correct\nQuestion asking for \"from one of the shards\"\nhere we go...\nError Handling\nFor function errors, you can also configure the event source mapping to split a failed batch into two batches. Retrying with smaller batches isolates bad records and works around timeout issues.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2021-10-16T10:14:00.000Z",
        "voteCount": 3,
        "content": "I would select B as suggested by muhsin and mattyb123."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/3524-exam-aws-certified-big-data-specialty-topic-1-question-41/",
    "body": "An organization uses Amazon Elastic MapReduce(EMR) to process a series of extract-transform-load (ETL) steps that run in sequence. The output of each step must be fully processed in subsequent steps but will not be retained.<br>Which of the following techniques will meet this requirement most efficiently?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the EMR File System (EMRFS) to store the outputs from each step as objects in Amazon Simple Storage Service (S3).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the s3n URI to store the data to be processed as objects in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine the ETL steps as separate AWS Data Pipeline activities.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data to be processed into HDFS, and then write the final output to Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-25T18:36:00.000Z",
        "voteCount": 9,
        "content": "Answer is C. AWS Data Pipepline works well for sequence of ETL processing.\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\n\nAWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up."
      },
      {
        "date": "2021-10-10T16:28:00.000Z",
        "voteCount": 8,
        "content": "C should be the correct answer. The question never mentioned anything about keeping the final output in s3. ETL might be to-and-from any other database. And D only says load data to be processed in HDFS, and not really the output of each process."
      },
      {
        "date": "2021-10-21T00:52:00.000Z",
        "voteCount": 1,
        "content": "With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html"
      },
      {
        "date": "2021-11-01T12:33:00.000Z",
        "voteCount": 2,
        "content": "only 2 logical answers are C &amp; D. Out if which Data Pipeline does not have the ability to share data on its own in between steps (it has to be stored somewhere). My choice is D as HDFS is ephemeral, data is lost of cluster termination"
      },
      {
        "date": "2021-11-03T22:53:00.000Z",
        "voteCount": 1,
        "content": "Exactly what I was going to say, \"Data Pipeline does not have the ability to share data on its own in between steps (it has to be stored somewhere)\". Spot on, D from my perspective."
      },
      {
        "date": "2021-10-26T16:18:00.000Z",
        "voteCount": 5,
        "content": "C &amp; D will both do the job. However I think D is more efficient than C so you do not have to deal with starting and terminating a transient EMR cluster on each intermediate step. AWS Data pipeline is being introduced to confuse us because it is the service used to execute a series of job in a sequence. However I think D is the right answer as its more efficient. The reason why the final output is persisted to S3 is because we cannot lose it as is it the result of all the map/reduce processing we did on the cluster. So when the cluster is terminate we don't want to lose the results of our Map/reduce processing we did on the data fed to the cluster for processing."
      },
      {
        "date": "2021-10-26T02:09:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D you can efficiently copy large amounts of data from Amazon S3 into HDFS where subsequent steps in your EMR clusters can process it. You can also use S3DistCp to copy data between Amazon S3 buckets or from HDFS to Amazon S3"
      },
      {
        "date": "2021-10-20T16:51:00.000Z",
        "voteCount": 1,
        "content": "my selection C"
      },
      {
        "date": "2021-10-20T03:18:00.000Z",
        "voteCount": 1,
        "content": "A.B and D support retain.However, C doesn't. So far me C is correct"
      },
      {
        "date": "2021-10-15T00:52:00.000Z",
        "voteCount": 2,
        "content": "Moreover, \"Datapipe line launches an Amazon EMR cluster for each scheduled interval, submits jobs as steps to the cluster, and terminates the cluster after tasks have completed.\" By this, data is not retained. So C looks good for me.\nDo you think the \"final output\" in D, means final output of each step. final output = output here."
      },
      {
        "date": "2021-10-14T12:39:00.000Z",
        "voteCount": 1,
        "content": "\"D. Load the data to be processed into HDFS\" = retained?. So D is not right? Moreover, the question looks is only about ETL, did not mention what we should do regarding \"final output\"\nC. Define the ETL steps as SEPARATE AWS Data Pipeline activities. SEPARATE means \"not ratained\", doesn't it?"
      },
      {
        "date": "2021-10-12T16:58:00.000Z",
        "voteCount": 1,
        "content": "@mattyb123 or anyone here- did you cleared the exam recently? need reviews on most of the contradictory answers"
      },
      {
        "date": "2021-10-11T22:31:00.000Z",
        "voteCount": 1,
        "content": "output will not be retained. do we still require S3? not sure of the correct answer here"
      },
      {
        "date": "2021-10-11T16:54:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is D\n\nThe main ask is efficiency. \n\nA) EMRFS provides consistency. However copying intermediate results into S3 is not an efficient approach. \nB) s3n is a old protocol so it is not efficient\nC) Using data pipelines activity for each step is just orchestration and may not guarantee efficiency. \nD) Using HDFS for intermediate steps ensures that the data is replicated and stored within EMR core nodes and is the most efficient way to store data for processing in EMR (even compared to S3). Storing the final result in S3 provides durability and may or may not matter from the context of this question."
      },
      {
        "date": "2021-10-11T03:36:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\nHere we go...\nScalability and Flexibility\nAdditionally, Amazon EMR provides the flexibility to use several file systems for your input, output, and intermediate data. For example, you might choose the Hadoop Distributed File System (HDFS) which runs on the master and core nodes of your cluster for processing data that you do not need to store beyond your cluster\u2019s lifecycle. You might choose the EMR File System (EMRFS) to use Amazon S3 as a data layer for applications running on your cluster so that you can separate your compute and storage, and persist data outside of the lifecycle of your cluster. \nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-benefits.html"
      },
      {
        "date": "2021-10-09T09:13:00.000Z",
        "voteCount": 1,
        "content": "I would choose D since intermittent output saving it in HDFS is best option for chain of ETL jobs."
      },
      {
        "date": "2021-10-06T10:03:00.000Z",
        "voteCount": 2,
        "content": "My answer is D"
      },
      {
        "date": "2021-10-05T14:59:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-10-05T05:08:00.000Z",
        "voteCount": 2,
        "content": "answer should D as it is writing only final output only in s3"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/3420-exam-aws-certified-big-data-specialty-topic-1-question-42/",
    "body": "The department of transportation for a major metropolitan area has placed sensors on roads at key locations around the city. The goal is to analyze the flow of traffic and notifications from emergency services to identify potential issues and to help planners correct trouble spots.<br>A data engineer needs a scalable and fault-tolerant solution that allows planners to respond to issues within<br>30 seconds of their occurrence.<br>Which solution should the data engineer choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect the sensor data with Amazon Kinesis Firehose and store it in Amazon Redshift for analysis. Collect emergency services events with Amazon SQS and store in Amazon DynampDB for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect the sensor data with Amazon SQS and store in Amazon DynamoDB for analysis. Collect emergency services events with Amazon Kinesis Firehose and store in Amazon Redshift for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect both sensor data and emergency services events with Amazon Kinesis Streams and use DynamoDB for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect both sensor data and emergency services events with Amazon Kinesis Firehose and use Amazon Redshift for analysis."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-18T09:48:00.000Z",
        "voteCount": 9,
        "content": "Answer : A \nA-\tIs the correct answer because alerting in 30 seconds cannot happen on data collected from sensors without further analysis which needs to be done using some transformation in KFH/Lambda and some aggregation in RedShift. Alerting in 30 seconds can happen on notification from emergency services only and for that a real-time collection solution such as SQS will be fine. It is also scalable and fault-tolerant as well.\nB-\tNot a good choice because emergency events needs to be notified within 30 seconds and KFH/Redshift combo doesn\u2019t do that.\nC-\tKS is not auto scalable / fully managed plus I don\u2019t see how data collected from the sensors can be analyzed and reported on within 30 seconds using a storage service such as Dynamo DB.\nD-\tKFH is not a real-time solution to store and alert on emergency services in 30 sec."
      },
      {
        "date": "2021-10-18T16:46:00.000Z",
        "voteCount": 2,
        "content": "I like the points you make and I also wonder how data collected from sensors can be analyzed so quickly, but to quote from jay1ram2's comment: \"The main asks are scalability, fault tolerance, and 30 seconds SLA for all data (It does not say just emergency services data).\" So in my opinion it is C and the sensors just provide really useful data that almost literally say \"problem\" or \"no problem\" out of the box.\nLastly, auto-scalability and being fully managed was not asked for; only scalability, and KS is manually scalable."
      },
      {
        "date": "2021-09-21T00:48:00.000Z",
        "voteCount": 7,
        "content": "is it C. This will ensure the real time aspect? FH has a 60 second delay"
      },
      {
        "date": "2021-09-26T18:46:00.000Z",
        "voteCount": 1,
        "content": "Only reason why i think it could be A also is the mention of SQS for emergency services. Just with the FH delay there is 30 seconds un accounted for."
      },
      {
        "date": "2021-09-27T09:42:00.000Z",
        "voteCount": 1,
        "content": "Anyone agree or disagree with A. Or is it C?"
      },
      {
        "date": "2021-10-03T06:32:00.000Z",
        "voteCount": 1,
        "content": "I think C"
      },
      {
        "date": "2021-10-27T04:00:00.000Z",
        "voteCount": 1,
        "content": "it cannot be A because AF cannot store data sirectly to redshift. It needs to use S3 copy command. Please read the docs. Its gottabe c."
      },
      {
        "date": "2021-10-27T02:13:00.000Z",
        "voteCount": 1,
        "content": "it is A. C is not suitable because DS is not auto scalable and the text says that only the alerts must be lower than 30 seconds."
      },
      {
        "date": "2021-10-28T11:52:00.000Z",
        "voteCount": 1,
        "content": "The questions asks for scalable not autp scalable"
      },
      {
        "date": "2021-10-22T10:23:00.000Z",
        "voteCount": 1,
        "content": "I think It could be A, look at this movie: https://www.youtube.com/watch?v=J8GJ4b4jLGI"
      },
      {
        "date": "2021-10-23T20:58:00.000Z",
        "voteCount": 1,
        "content": "he said every 3 minutes data is sent, ?"
      },
      {
        "date": "2021-10-16T21:41:00.000Z",
        "voteCount": 2,
        "content": "Realtime to Kinesis Streams - Correct Answer is C"
      },
      {
        "date": "2021-10-15T19:47:00.000Z",
        "voteCount": 4,
        "content": "my selection C"
      },
      {
        "date": "2021-10-15T19:28:00.000Z",
        "voteCount": 4,
        "content": "Firehose has the minimum delay of 60 seconds so C is the only correct choice."
      },
      {
        "date": "2021-10-10T17:57:00.000Z",
        "voteCount": 1,
        "content": "The IoT Core rules engine is not currently compatible with SQS FIFO queues: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-compatibility"
      },
      {
        "date": "2021-10-09T15:30:00.000Z",
        "voteCount": 5,
        "content": "My answer is C\n\nThe main asks are scalability, fault tolerance, and 30 seconds SLA for all data (It does not say just emergency services data). \n\nA, B, D) Uses firehose which has 60 seconds minimum buffer.  \nC) Both Kinesis and DynamoDB have sub-second SLAs. As for the scalability of kinesis, even though it does auto scale, it can be manually scaled with no downtime. Both are managed serverless and have built-in fault tolerance."
      },
      {
        "date": "2021-10-06T23:42:00.000Z",
        "voteCount": 1,
        "content": "I will go with C. \nhttps://sookocheff.com/post/aws/comparing-kinesis-and-sqs/"
      },
      {
        "date": "2021-10-04T17:29:00.000Z",
        "voteCount": 1,
        "content": "I agree with C"
      },
      {
        "date": "2021-10-03T09:07:00.000Z",
        "voteCount": 5,
        "content": "\"..A data engineer needs a scalable and fault-tolerant solution...\" .. Is Auto Scaling enabled in Kinesis Streams in this scenario? ..I don't think so.. and we know Kinesis Firehose is scalable because \"..It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration...\" and it also has 60 sec minimum delay and can not be used for Emergency Services. So, in my opinion, A is correct. Firehose for non-emergency and SQS for emergency because both are managed services."
      },
      {
        "date": "2021-10-05T01:25:00.000Z",
        "voteCount": 2,
        "content": "I also agree with A. DynamoDB is more suitable for real-time analysis (OLTP) and Redshift for  data warehouse (OLAP) analysis."
      },
      {
        "date": "2021-09-29T19:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-09-30T15:49:00.000Z",
        "voteCount": 2,
        "content": "Why do you think its A?"
      },
      {
        "date": "2021-09-29T02:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/3525-exam-aws-certified-big-data-specialty-topic-1-question-43/",
    "body": "A telecommunications company needs to predict customer churn (i.e., customers who decide to switch to a competitor). The company has historic records of each customer, including monthly consumption patterns, calls to customer service, and whether the customer ultimately quit the service. All of this data is stored in<br>Amazon S3. The company needs to know which customers are likely going to churn soon so that they can win back their loyalty.<br>What is the optimal approach to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Machine Learning service to build the binary classification model based on the dataset stored in Amazon S3. The model will be used regularly to predict churn attribute for existing customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS QuickSight to connect it to data stored in Amazon S3 to obtain the necessary business insight. Plot the churn trend graph to extrapolate churn likelihood for existing customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EMR to run the Hive queries to build a profile of a churning customer. Apply a profile to existing customers to determine the likelihood of churn.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Redshift cluster to COPY the data from Amazon S3. Create a User Defined Function in Redshift that computes the likelihood of churn."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-21T19:41:00.000Z",
        "voteCount": 11,
        "content": "it is absolutely A. churn prediction is a machine learning algorithm. Quicksight provide a visual analysis."
      },
      {
        "date": "2021-09-21T02:50:00.000Z",
        "voteCount": 6,
        "content": "Thoughts on A?"
      },
      {
        "date": "2021-10-31T23:19:00.000Z",
        "voteCount": 1,
        "content": "Prediction it has to be Machine Learning. So i will go with A"
      },
      {
        "date": "2021-10-31T18:53:00.000Z",
        "voteCount": 1,
        "content": "I will go with A"
      },
      {
        "date": "2021-10-30T05:06:00.000Z",
        "voteCount": 3,
        "content": "my selection A"
      },
      {
        "date": "2021-10-22T11:25:00.000Z",
        "voteCount": 3,
        "content": "This is obvious A is the correct answer.\nhttps://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/"
      },
      {
        "date": "2021-10-13T06:37:00.000Z",
        "voteCount": 1,
        "content": "I would go with A."
      },
      {
        "date": "2021-10-05T18:29:00.000Z",
        "voteCount": 1,
        "content": "It is A. You may use QuickSight to visually analyze the data points using Scatter Plot and find out if a customer is going to leave or not.  It is not an \"Optimal Solution\". Binary classification ML is appropriate for it."
      },
      {
        "date": "2021-10-02T16:00:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/quicksight/features-ml/?nc=sn&amp;loc=2&amp;dn=2\nIt talks about using quicksight to discover trends, like whether someone is going to flip or not."
      },
      {
        "date": "2021-10-05T09:47:00.000Z",
        "voteCount": 1,
        "content": "answers only referenced Quicksight not Quicksight ML so will stick with A ..u have a point though"
      },
      {
        "date": "2021-11-06T17:18:00.000Z",
        "voteCount": 1,
        "content": "I dont believe you can still use Quicksight ML to attain your use case, going through the video you can only attain the following use cases via Quicksight ML:- \n-\tAnomaly Detection (Random Forest)\n-\tForecasting --&gt; Regression over time --&gt;Time-series Analysis --&gt; Estimating values of certain attributes over time. \n-        What if Analysis --&gt; If You change the value of a certain attribute you can see its effect on your analysis being reflected over your dataset\n-\tAuto-Narratives --&gt; Summarizing your Graphs in words by parsing through your dataset.\nThus, I believe A is the correct answer herein!"
      },
      {
        "date": "2021-10-02T00:54:00.000Z",
        "voteCount": 2,
        "content": "who put answer B as correct answer here ? any admin of this website can correct the obvious wrong answers provided ?"
      },
      {
        "date": "2021-11-01T11:54:00.000Z",
        "voteCount": 2,
        "content": "also curious who did this :D to honor san2020 let me write:\n\nmy selection A"
      },
      {
        "date": "2021-10-01T15:54:00.000Z",
        "voteCount": 1,
        "content": "For A, the company needs to have all data including previous customers, i.e. they should have both old and new customers in S3.. then they can build a binary classification (yes/no) to decide."
      },
      {
        "date": "2021-09-27T04:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-09-23T01:55:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/3426-exam-aws-certified-big-data-specialty-topic-1-question-44/",
    "body": "A system needs to collect on-premises application spool files into a persistent storage layer in AWS. Each spool file is 2 KB. The application generates 1 M files per hour. Each source file is automatically deleted from the local server after an hour.<br>What is the most cost-efficient option to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite file contents to an Amazon DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy files to Amazon S3 Standard Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite file contents to Amazon ElastiCache.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy files to Amazon S3 infrequent Access Storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-16T13:54:00.000Z",
        "voteCount": 23,
        "content": "Correct Answer is A\n\nThe main asks are persistence and cost-efficiency. \n\nLet us calculate the storage and R/W numbers\n2KB/File  * 1 Million files/Hr * 24 Hrs * 30 Days = 1 Month of data\n2/(1024 * 1024) * 1,000,000 * 24 * 30 = 1,373 GB of data/month\n1,000,000 * 24 * 30  = 720 Million writes/month or ~278 writes/second.\n\nA) The cost of storing 1.37 TB of data in DynamoDB is $370. On-demand cost of writing ~278/sec into DynamoDB is ~600 a month with a total cost of ~$1000. It is persistence storage.  \nB) The cost of storing 1.37 TB of data in S3 is ~$31. Put request cost of 720 Million objects  ~$3600  a month with aa total cost of ~$3631.\nC) Storing 720 Million records with total size 1.3 TB in Elasticache memory will cost more than $10K/month along with snapshot cost for persistence. \nD) S3 IA costs are even higher than standard costs given that the minimum size requirement/object is 128KB.\n\nA is the most cost-efficient of all along with persistence SLA."
      },
      {
        "date": "2021-10-25T13:24:00.000Z",
        "voteCount": 1,
        "content": "Yes correct,\nfrom S3 FAQ\nRequest Example:\nAssume you transfer 10,000 files into Amazon S3 and transfer 20,000 files out of Amazon S3 each day during the month of March. Then, you delete 5,000 files on March 31st.\nTotal PUT requests = 10,000 requests x 31 days = 310,000 requests\nTotal GET requests = 20,000 requests x 31 days = 620,000 requests\nTotal DELETE requests = 5,000\u00d71 day = 5,000 requests\n\nAssuming your bucket is in the US East (Northern Virginia) Region, the Request fees are calculated below:\n310,000 PUT Requests: 310,000 requests x $0.005/1,000 = $1.55\n620,000 GET Requests: 620,000 requests x $0.004/10,000 = $0.25\n5,000 DELETE requests = 5,000 requests x $0.00 (no charge) = $0.00"
      },
      {
        "date": "2021-09-25T16:13:00.000Z",
        "voteCount": 10,
        "content": "for N.virginia\nDynamoDB per GB 0.25/month + RCU \nS3 Standard per GB 0.023/month\nS3 Standard IA per GB /0.0125 but bill minimum 128KB for each object\nS3 Standard is cheaper"
      },
      {
        "date": "2021-11-05T17:13:00.000Z",
        "voteCount": 1,
        "content": "This article can be a clue. The answer could be A due to too many small files. https://www.reddit.com/r/aws/comments/5haamf/has_anyone_done_the_costs_math_on_s3_vs_dynamodb/"
      },
      {
        "date": "2021-11-04T20:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct : Too many small files.. S3 is not a good choice.  DynamoDB is the right option."
      },
      {
        "date": "2021-11-04T06:49:00.000Z",
        "voteCount": 1,
        "content": "D. You can run a batch job e.g. every 30 minutes to gzip new files into one gz file and upload to S3 IA. All requirements met, only 48 API S3 PUT calls per day, and the file will for sure be bigger than 128kb."
      },
      {
        "date": "2021-10-26T03:08:00.000Z",
        "voteCount": 3,
        "content": "B is the correct, for dynamo you also need to pay by request.\nAlso you can call once per minute"
      },
      {
        "date": "2021-10-22T02:12:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2021-10-20T12:09:00.000Z",
        "voteCount": 2,
        "content": "As the question is asking to store files on AWS, I will go with Option - B"
      },
      {
        "date": "2021-10-16T21:39:00.000Z",
        "voteCount": 2,
        "content": "my selection A"
      },
      {
        "date": "2021-10-14T22:05:00.000Z",
        "voteCount": 1,
        "content": "D would give you 30 days of storage per file, it doesn't say you have to keep the files, dynamoDB with TTL is a better option?"
      },
      {
        "date": "2021-10-14T02:47:00.000Z",
        "voteCount": 2,
        "content": "S3 Standard is most cost efficient.\n2M Kb files per hour which leads to 48GB per day and 1152 GB per month\nFor this S3 cost 28 USD\nDynamoDB cost 308 USD"
      },
      {
        "date": "2021-10-16T00:31:00.000Z",
        "voteCount": 3,
        "content": "need to consider put api cost here. the cost of putting 1M small object per hour to s3 will kill your customer at the end of the month"
      },
      {
        "date": "2021-10-13T18:26:00.000Z",
        "voteCount": 1,
        "content": "I would agree with @pra276 and select A because too many small object put requests to S3 increases cost compared to DynamoDB."
      },
      {
        "date": "2021-10-09T22:31:00.000Z",
        "voteCount": 3,
        "content": "I choose D\nThe question is asking for \"persistent storage layer\", but DynamoDB is naturally  a database. So I think S3 should be option. However, the question does not clearly mention how frequently the stored data need be accessed, so I will go with S3 IA which is less expensive than S3 standard."
      },
      {
        "date": "2021-10-10T04:09:00.000Z",
        "voteCount": 8,
        "content": "S3 IA charges on items min size 128kb. So even if the objects in the question have 2kb they will be charged for 128kb each. This results in higher costs compared to standard S3."
      },
      {
        "date": "2021-10-09T11:17:00.000Z",
        "voteCount": 5,
        "content": "D, Persistence and Cheap solution. The question didn't tell how frequent the files will be accessed. So D instead of B."
      },
      {
        "date": "2021-10-03T03:21:00.000Z",
        "voteCount": 1,
        "content": "B correct"
      },
      {
        "date": "2021-10-01T21:13:00.000Z",
        "voteCount": 3,
        "content": "Answer is B as it is frequently accessed storage and cheaper"
      },
      {
        "date": "2021-09-29T02:22:00.000Z",
        "voteCount": 4,
        "content": "I'd look at the two key phrases in this question: a persistent storage layer AND   most cost-efficient option. That leads me to D"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/5350-exam-aws-certified-big-data-specialty-topic-1-question-45/",
    "body": "An administrator receives about 100 files per hour into Amazon S3 and will be loading the files into Amazon<br>Redshift. Customers who analyze the data within Redshift gain significant value when they receive data as quickly as possible. The customers have agreed to a maximum loading interval of 5 minutes.<br>Which loading approach should the administrator use to meet this objective?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad each file as it arrives because getting data into the cluster as quickly as possibly is the priority.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the cluster as soon as the administrator has the same number of files as nodes in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the cluster when the administrator has an event multiple of files relative to Cluster Slice Count, or 5 minutes, whichever comes first.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the cluster when the number of files is less than the Cluster Slice Count."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-31T22:44:00.000Z",
        "voteCount": 5,
        "content": "To verify  that C is the right answer refer to https://aws.amazon.com/blogs/big-data/best-practices-for-micro-batch-loading-on-amazon-redshift/"
      },
      {
        "date": "2021-11-07T00:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-21T07:36:00.000Z",
        "voteCount": 3,
        "content": "my selection C"
      },
      {
        "date": "2021-10-11T04:58:00.000Z",
        "voteCount": 3,
        "content": "It's easy and obvious and answer is C"
      },
      {
        "date": "2021-10-04T01:45:00.000Z",
        "voteCount": 3,
        "content": "I agree with C"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/4587-exam-aws-certified-big-data-specialty-topic-1-question-46/",
    "body": "An enterprise customer is migrating to Redshift and is considering using dense storage nodes in its Redshift cluster. The customer wants to migrate 50 TB of data. The customers query patterns involve performing many joins with thousands of rows.<br>The customer needs to know how many nodes are needed in its target Redshift cluster. The customer has a limited budget and needs to avoid performing tests unless absolutely needed.<br>Which approach should this customer use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart with many small nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart with fewer large nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave two separate clusters with a mix of a small and large nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsist on performing multiple tests to determine the optimal configuration."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-20T17:07:00.000Z",
        "voteCount": 17,
        "content": "A is correct.\nhttps://d1.awsstatic.com/whitepapers/Size-Cloud-Data-Warehouse-on-AWS.pdf\nUsing compression ratio of 3 as per the link. The 50TB/3= 16TB. \nThe calculation 50TB/3=16.66 * (1.25) =20.83 ~21TB. 21TB/2 =10.5 ~11 ds2.xlarge nodes\nThe calculation 50TB/3=16.66 * (1.25) =20.83 ~21TB. 21TB/16 =1.3125 ~2 ds2.8xlarge nodes."
      },
      {
        "date": "2021-09-23T19:40:00.000Z",
        "voteCount": 9,
        "content": "With the required storage size you can either go with RA2 or DS2 node types and not with DC2. If we go with DS2.xlarge  type nodes, we will need 11 of them to hold 50 TB of data giving us a total of 44 CPU cores for compute. if we go with  DS2.8xlarge type nodes, we will need 2 of them giving us a total of 72 CPU ores for compute. Starting with many small nodes will help distribute the data and query processing across 11 nodes with a sizable compute power considering there are many joins in the query and will cost way less than using 2 Ds2.8xlarge nodes.  It will be about $2500 /month cheaper to operate 11 xlarge nodes than operating 2  8xlarge nodes."
      },
      {
        "date": "2021-10-26T04:58:00.000Z",
        "voteCount": 1,
        "content": "B. best practice is fewer large nodes. Reason is changing Node types for scale in/out will result in classic/manual resize"
      },
      {
        "date": "2021-10-22T12:58:00.000Z",
        "voteCount": 1,
        "content": "is the paragraph about \"many large joins\" a confounder? Because one best practice that's often mentioned is to avoid traffic between nodes as much as possible (often mentioned with resources about distribution styles). That would point to B (few large nodes)."
      },
      {
        "date": "2021-11-01T21:02:00.000Z",
        "voteCount": 1,
        "content": "\"many joins with thousands of rows\", joins are on the small tables, you can dist all those tables to avoid traffic between nodes"
      },
      {
        "date": "2021-10-06T00:51:00.000Z",
        "voteCount": 2,
        "content": "I think it should be B.\nif they are performing many joins, many data will be passing back and forth between nodes during the query. For better performance, it's better to use larger nodes. The question states they don't have budget for testing, but not saying they cann't afford larger nodes..."
      },
      {
        "date": "2021-09-23T07:17:00.000Z",
        "voteCount": 3,
        "content": "my selection A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/5352-exam-aws-certified-big-data-specialty-topic-1-question-47/",
    "body": "A company is centralizing a large number of unencrypted small files from multiple Amazon S3 buckets. The company needs to verify that the files contain the same data after centralization.<br>Which method meets the requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the S3 Etags from the source and destination objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the S3 CompareObjects API for the source and destination objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace a HEAD request against the source and destination objects comparing SIG v4.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the size of the source and destination objects."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-05T00:11:00.000Z",
        "voteCount": 13,
        "content": "A. https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html"
      },
      {
        "date": "2021-10-10T07:28:00.000Z",
        "voteCount": 5,
        "content": "my selection A"
      },
      {
        "date": "2021-11-04T00:27:00.000Z",
        "voteCount": 1,
        "content": "A:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/data-integrity-s3/"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/5216-exam-aws-certified-big-data-specialty-topic-1-question-48/",
    "body": "An online gaming company uses DynamoDB to store user activity logs and is experiencing throttled writes on the companys DynamoDB table. The company is NOT consuming close to the provisioned capacity. The table contains a large number of items and is partitioned on user and sorted by date. The table is 200GB and is currently provisioned at 10K WCU and 20K RCU.<br>Which two additional pieces of information are required to determine the cause of the throttling? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe structure of any GSIs that have been defined on the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloudWatch data showing consumed and provisioned write capacity when writes are being throttled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApplication-level metrics showing the average item size and peak update rates for each attribute",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe structure of any LSIs that have been defined on the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe maximum historical WCU and RCU for the table"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-21T15:32:00.000Z",
        "voteCount": 9,
        "content": "AC in my opinion. The key of this question is to figure out the cause of throttling under capacity. I take the \"capacity\" as the wcu and rcu you defined when creating a new dynamodb table. the capacity is the TOTAL wcu and rcu in ALL partitions together. the max wcu per partition is 1000, the max size per partition is 10G. However, we have 200G in total and the total wcu requested is 10k. This means, dynamoDB has to give us 20 partitions, with 500 wcu for each partition. therefore, a throttling will occur if for a single partiton the wcu goes over 500. option c gives us information of the most frequently updated attributes and the size, thus can help us calculate whether a wcu for a single partition is over 500. On the other hand, since GSI uses its own capacity, so even the traffic is under \"general capacity\", it might exceed the GSI capacity, thus we need to know if there is any GSI, and the capacity corresponding to it."
      },
      {
        "date": "2021-11-04T04:28:00.000Z",
        "voteCount": 1,
        "content": "Spot on, this is exactly my reasoning as well, 100% agree with this."
      },
      {
        "date": "2021-11-07T09:03:00.000Z",
        "voteCount": 1,
        "content": "A &amp; B are the right answers"
      },
      {
        "date": "2021-10-27T01:20:00.000Z",
        "voteCount": 1,
        "content": "Hot partition issue, A &amp; D, GSI has partition key &amp; LSI will have the table partition key as it leftmost key"
      },
      {
        "date": "2021-10-26T11:07:00.000Z",
        "voteCount": 2,
        "content": "A and C in my opinion:\nA - GSI definitely impacts base table writes if GSI WCU is not provisioned correct. \nC - for understanding if the columns in GSI get updated frequently and compute required WCUs. best practice is to set the WCU same as base table.\n\nThrottling on a GSI affects the base table in different ways, depending on whether the throttling is for read or write activity:\n\nWhen a GSI has insufficient read capacity, the base table isn't affected.\nWhen a GSI has insufficient write capacity, write operations won't succeed on the base table or any of its GSIs."
      },
      {
        "date": "2021-10-23T02:54:00.000Z",
        "voteCount": 2,
        "content": "My guess is A and C. After reading all comments."
      },
      {
        "date": "2021-10-24T18:32:00.000Z",
        "voteCount": 1,
        "content": "Changed my mind to A and E. \n\nPrevious comments:\nA is right because:\n' If your table uses a global secondary index, then any write to the table also writes to the index. If the many writes are occuring on a single partition key for the index, regardless of how well the table partition key is distributed, the write to the table will be throttled too.'\nhttps://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling\n\nE: Consider the following scenario:\n\nyou dial up the throughput for a table because there\u2019s a sudden spike in traffic or you need the extra throughput to run an expensive scan\nthe extra throughputs cause DynamoDB to increase the no. of partitions\nyou dial down the throughput to previous levels, but now you notice that some requests are throttled even when you have not exceeded the provisioned throughput on the table\nThis happens because there are less read and write throughput units per partition than before due to the increased no. of partitions."
      },
      {
        "date": "2021-10-22T11:10:00.000Z",
        "voteCount": 2,
        "content": "B and D. This is likely to be a hot partition issue, need to find out how to confirm it is due to hot partition.\nA. GSI has its own RCU and WCU, will NOT impact the table's RCU and WCU.\nC. Average item couldn't tell whether hit hot partition or not.\nE. Historical doesn't mean you won't have hot partition now."
      },
      {
        "date": "2021-10-14T18:46:00.000Z",
        "voteCount": 1,
        "content": "Excessive throttling is caused by:\n\nHot partitions: throttles are caused by a few partitions in the table that receive more requests than the average partition\nNot enough capacity: throttles are caused by the table itself not having enough capacity to service requests on many partitions\n the question ask we to forcus on finding hot partitions . \n\nA:  GSI  has different partition key . \nD:  update rates  for the key"
      },
      {
        "date": "2021-10-19T12:37:00.000Z",
        "voteCount": 1,
        "content": "GSI has different capcity"
      },
      {
        "date": "2021-11-03T06:36:00.000Z",
        "voteCount": 1,
        "content": "Update rates is C, not D. I agree with AC"
      },
      {
        "date": "2021-10-14T18:04:00.000Z",
        "voteCount": 1,
        "content": "I think A is incorrect. GSI will have its own capacity units for both Read &amp; Write. Also, the question is to determine the root cause for WRITE throttling and NOT about under-used provisioned capacity. Which means, E is incorrect as well. In my view, it has to be between B, C &amp; D. I will go with B &amp; C."
      },
      {
        "date": "2021-10-29T20:15:00.000Z",
        "voteCount": 1,
        "content": "Exactly, because GSI has its own WCU, it could be throttled, don't you agree?"
      },
      {
        "date": "2021-10-06T22:22:00.000Z",
        "voteCount": 1,
        "content": "It is given that the company is not consuming close to the provisioned capacity. Therefore, option B and E is not the choice. This requires to know the assess pattern of table, which can determine hot partition issue. Being said that, option C gives clue about assess pattern and option A gives clue if GSI is there and how it is partitioned. D can give some information but it is more aligned with table partition which we know is user with sorted by date."
      },
      {
        "date": "2021-10-28T22:32:00.000Z",
        "voteCount": 1,
        "content": "So are you agreeing with A&amp;C ? :)"
      },
      {
        "date": "2021-10-02T08:09:00.000Z",
        "voteCount": 4,
        "content": "Answer is A and E-  For A read this link- &gt; https://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling and for E -&gt; https://theburningmonk.com/2017/05/beware-of-dilution-of-dynamodb-throughput-due-to-excessive-scaling/"
      },
      {
        "date": "2021-10-14T17:02:00.000Z",
        "voteCount": 1,
        "content": "@Bulit - I like most of your explanation. However for this one, I just thought of sharing my inputs. \nI think A is incorrect. GSI will have its own capacity units for both Read &amp; Write. Also, the question is to determine the root cause for WRITE throttling and NOT about under-used provisioned capacity. Which means, E is incorrect as well. In my view, it has to be between B, C &amp; D. I will go with B &amp; C."
      },
      {
        "date": "2021-10-27T03:06:00.000Z",
        "voteCount": 1,
        "content": "A is correct because GSI uses its own WCU/RCU independent of the table's WCU/RCU which can be throttled since the question says that the table's provisioned throughput is not execeeded (at least it is what I understood from it). C is my 2nd option."
      },
      {
        "date": "2021-09-28T18:30:00.000Z",
        "voteCount": 3,
        "content": "I would go for B and D"
      },
      {
        "date": "2021-10-01T22:13:00.000Z",
        "voteCount": 1,
        "content": "why would you go for D?"
      },
      {
        "date": "2021-09-27T15:08:00.000Z",
        "voteCount": 2,
        "content": "my selection AD"
      },
      {
        "date": "2021-09-27T14:58:00.000Z",
        "voteCount": 4,
        "content": "A is right because: \n' If your table uses a global secondary index, then any write to the table also writes to the index. If the many writes are occuring on a single partition key for the index, regardless of how well the table partition key is distributed, the write to the table will be throttled too.'\nhttps://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling"
      },
      {
        "date": "2021-09-25T01:27:00.000Z",
        "voteCount": 2,
        "content": "Answer D and E\nWhy option E ? find the explanation\nDynamoDB provisioned capacity up and down induces less read and write throughput units per partition than before due to the increased no. of partitions after scale down.\nhttps://theburningmonk.com/2017/05/beware-of-dilution-of-dynamodb-throughput-due-to-excessive-scaling/"
      },
      {
        "date": "2021-09-25T00:37:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoring-cloudwatch.html\n\nYou do need cloudwatch logs...."
      },
      {
        "date": "2021-09-22T10:42:00.000Z",
        "voteCount": 1,
        "content": "It should be C and D. C because if we get information if a specific attribute only is getting updated which is causing throttling and D because of hot key partition."
      },
      {
        "date": "2021-10-07T12:57:00.000Z",
        "voteCount": 1,
        "content": "Application level metrics are not useful here since we care about summed reads and writes, hence C is wrong.\nLSI is only about sort keys not partition keys, therefore D is wrong."
      },
      {
        "date": "2021-09-22T07:35:00.000Z",
        "voteCount": 1,
        "content": "It should be caused by Hot Key (Partition), D should be one of the answers"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/3564-exam-aws-certified-big-data-specialty-topic-1-question-49/",
    "body": "A city has been collecting data on its public bicycle share program for the past three years. The 5PB dataset currently resides on Amazon S3. The data contains the following datapoints:<br>\u2711 Bicycle origination points<br>\u2711 Bicycle destination points<br>\u2711 Mileage between the points<br>\u2711 Number of bicycle slots available at the station (which is variable based on the station location)<br>\u2711 Number of slots available and taken at a given time<br>The program has received additional funds to increase the number of bicycle stations available. All data is regularly archived to Amazon Glacier.<br>The new bicycle stations must be located to provide the most riders access to bicycles.<br>How should this task be performed?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data from Amazon S3 into Amazon EBS-backed volumes and use an EC-2 based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Redshift COPY command to move the data from Amazon S3 into Redshift and perform a SQL query that outputs the most popular bicycle stations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPersist the data on Amazon S3 and use a transient EMR cluster with spot instances to run a Spark streaming job that will move the data into Amazon Kinesis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the data on Amazon S3 and use an Amazon EMR-based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization over EMRFS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-12T12:24:00.000Z",
        "voteCount": 6,
        "content": "my selection D"
      },
      {
        "date": "2021-10-31T22:52:00.000Z",
        "voteCount": 5,
        "content": "Answer : D because you would want to use the EMR cluster with EMRFS as opposed to Ec2 based Hadoop cluster using HDFS to run the Spark Job without moving the data from S3"
      },
      {
        "date": "2021-10-07T20:56:00.000Z",
        "voteCount": 2,
        "content": "I go with B, gradient descent is an optimization technique used in many ML algorithms. The answer didn't tell what ML algorithm is used. Moreover the problem doesn't require machine learning predictive model to solve it. They've data and need to be analyzed to find the answer which can be done using SQL queries using Redshift."
      },
      {
        "date": "2021-10-11T04:42:00.000Z",
        "voteCount": 11,
        "content": "How would you identify new spots for bicycle stations from the current data? You only have current stations, their usage and the trips of the people that use the bikes. You need ML here to identify ideal locations for new stations. Answer D should be correct."
      },
      {
        "date": "2021-11-02T04:39:00.000Z",
        "voteCount": 1,
        "content": "I am confused between B and D,  as I am still not completely convinced for a use case of employing ML as stated in option D.\nThe question states that the information of \"Number of slots available and taken at a given time\" is present, which indicates  how busy/popular the station is. As more the number of taken slots --&gt; more popular the station --&gt;  which becomes the ideal location for increasing the Bicycle station. \nAny thoughts ?"
      },
      {
        "date": "2021-10-06T01:44:00.000Z",
        "voteCount": 2,
        "content": "I'll go with D"
      },
      {
        "date": "2021-09-19T12:00:00.000Z",
        "voteCount": 3,
        "content": "thoughts on D?"
      },
      {
        "date": "2021-09-22T05:03:00.000Z",
        "voteCount": 2,
        "content": "Just thinking can a Redshift SQL query on the current information even outputs the most popular bicycle stations? Compared to the ML stochastic gradient descent algorithm?"
      },
      {
        "date": "2021-09-26T09:22:00.000Z",
        "voteCount": 1,
        "content": "Any further ideas anyone?"
      },
      {
        "date": "2021-09-26T23:40:00.000Z",
        "voteCount": 1,
        "content": "tricky, i chose B and failed, so D is making sense for now?"
      },
      {
        "date": "2021-09-27T13:19:00.000Z",
        "voteCount": 1,
        "content": "That's my logic as well"
      },
      {
        "date": "2021-09-28T05:54:00.000Z",
        "voteCount": 6,
        "content": "I think , for B , the query result is the existed bicycle stations who are most popular.  But this question is to add more bicycle stations which is no way to build the same place .  So it is a prediction problem , D is the right answer,"
      },
      {
        "date": "2021-10-23T20:45:00.000Z",
        "voteCount": 4,
        "content": "This question is not about can \"Redshift SQL query on the current information\" v/s \"ML stochastic gradient descent algorithm\". \nData is in S3. What is the easiest way tand most cost effecctive way to analyze data one time? Use EMR spot instance (cheap) Use EMRFS to read data directly from S3. No additional charges for spinning up a redshift cluster"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/4275-exam-aws-certified-big-data-specialty-topic-1-question-50/",
    "body": "An administrator tries to use the Amazon Machine Learning service to classify social media posts that mention the administrators company into posts that require a response and posts that do not. The training dataset of<br>10,000 posts contains the details of each post including the timestamp, author, and full text of the post. The administrator is missing the target labels that are required for training.<br>Which Amazon Machine Learning model is the most appropriate for the task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBinary classification model, where the target class is the require-response post",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBinary classification model, where the two classes are the require-response post and does-not-require- response",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMulti-class prediction model, with two classes: require-response post and does-not-require-response",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegression model where the predicted value is the probability that the post requires a response"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-24T20:24:00.000Z",
        "voteCount": 9,
        "content": "B is correct. binary classification has two classess."
      },
      {
        "date": "2021-09-26T15:11:00.000Z",
        "voteCount": 6,
        "content": "Thanks for the correction i have overlooked this one sadly.\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/types-of-ml-models.html \nBinary Classification Model\nML models for binary classification problems predict a binary outcome (one of two possible classes). To train binary classification models, Amazon ML uses the industry-standard learning algorithm known as logistic regression."
      },
      {
        "date": "2021-10-11T11:32:00.000Z",
        "voteCount": 1,
        "content": "Agree with B.Also, the administrator is missing the target labels that are required for training."
      },
      {
        "date": "2021-10-01T14:29:00.000Z",
        "voteCount": 9,
        "content": "\"The administrator is missing the target labels that are required for training.\", so you can't train binary classification model.\nThe only option left is D, here is some Use Case:\nhttps://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-part-5-4c00f2366b90"
      },
      {
        "date": "2021-10-06T06:10:00.000Z",
        "voteCount": 4,
        "content": "Exactly, The answer is D.\nhttps://stats.stackexchange.com/questions/55357/binary-classifier-with-training-data-for-one-label-only"
      },
      {
        "date": "2021-10-08T18:22:00.000Z",
        "voteCount": 2,
        "content": "https://hbr.org/2015/11/a-refresher-on-regression-analysis"
      },
      {
        "date": "2021-10-19T06:03:00.000Z",
        "voteCount": 1,
        "content": "Binary Classification uses Logistic Regression under the hood. So the answer is B."
      },
      {
        "date": "2021-10-28T04:43:00.000Z",
        "voteCount": 1,
        "content": "Just attempted exam - Option A was\nUnary classification model, where the target class is the require-response post. \n\nI selected B ."
      },
      {
        "date": "2021-10-26T04:45:00.000Z",
        "voteCount": 2,
        "content": "find the exact sceniro in AWS official sample questions but with a different question:\n\nWhich two options will create valid target label data?\nA) Ask the social media handling team to review each post and provide the label.\nB) Use the sentiment analysis NLP library to determine whether a post requires a response.\nC) Use the Amazon Mechanical Turk web service to publish Human Intelligence Tasks that ask Turk workers to label the posts.\nD) Using the a priori probability distribution of the two classes, use Monte-Carlo simulation to generate the labels. \n\nwhich makes more sense then this one..."
      },
      {
        "date": "2021-10-24T01:36:00.000Z",
        "voteCount": 1,
        "content": "The key point here is missing the target labels. There are two kinds of machine learning: supervised/unsupervised. if the target labels are missing, then we can't perform supervised learning, because we don't know the right answer. Therefore, A, D are wrong: we don't know the target class or the prediction value of the training, so we can't train. For B, binary classification is one kind of supervised training, which you must provide the ground truth answer of whether the post is response-needed or not. Therefore we left with C. We use unsupervised learning to tell the model, here we have two classes in all these posts, classify them for me."
      },
      {
        "date": "2021-10-18T17:26:00.000Z",
        "voteCount": 1,
        "content": "Amazon ML only supports supervised models, meaning it must have a sample of data with the target value info otherwise the model can not be used for training. Not having the target value invalidates all options in this question. That is why i think its A."
      },
      {
        "date": "2021-10-10T19:44:00.000Z",
        "voteCount": 2,
        "content": "my selection D"
      },
      {
        "date": "2021-10-20T23:13:00.000Z",
        "voteCount": 2,
        "content": "Regression is not suited for classification, you'd at the minimum still transform the predicted probabilities to 0 or 1 - B is better."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/3597-exam-aws-certified-big-data-specialty-topic-1-question-51/",
    "body": "A medical record filing system for a government medical fund is using an Amazon S3 bucket to archive documents related to patients. Every patient visit to a physician creates a new file, which can add up millions of files each month. Collection of these files from each physician is handled via a batch process that runs ever night using AWS Data Pipeline. This is sensitive data, so the data and any associated metadata must be encrypted at rest.<br>Auditors review some files on a quarterly basis to see whether the records are maintained according to regulations. Auditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician. Auditors spend a significant amount of time location such files.<br>What is the most cost- and time-efficient collection methodology in this situation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis to get the data feeds directly from physicians, batch them using a Spark application on Amazon Elastic MapReduce (EMR), and then store them in Amazon S3 with folders separated per physician.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon API Gateway to get the data feeds directly from physicians, batch them using a Spark application on Amazon Elastic MapReduce (EMR), and then store them in Amazon S3 with folders separated per physician.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 event notification to populate an Amazon DynamoDB table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 event notification to populate an Amazon Redshift table with metadata about every file loaded to Amazon S3, and partition them based on the month and year of the file."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-19T13:24:00.000Z",
        "voteCount": 6,
        "content": "C is the answer, it allows for easy search of metadata using DynamoDB queries. \n\nEMR methods do not provide options to search for data by patient or date.   \n\nRedshift does not support partition"
      },
      {
        "date": "2021-10-31T20:43:00.000Z",
        "voteCount": 1,
        "content": "Remember no of files and batch with AWS Data Pipeline\nA wrong: will be very expensive - shrads/24h + EMR why you need it?\nB wrong: API Gateway + EMR? you need to architecture everything from the beginning\nC OK: cheapest simplest and working\nD wrong: no partition in Redshift"
      },
      {
        "date": "2021-10-30T17:26:00.000Z",
        "voteCount": 1,
        "content": "C is good answer.data stored in S3 and dynamoDB stored meta data to make it easy search"
      },
      {
        "date": "2021-10-26T19:10:00.000Z",
        "voteCount": 2,
        "content": "I think the focus of this question is on the most cost-efficient and time-efficient way of collecting the data and not how to analyze or locate the data. Based on that- I would go with C instead of A because it's definitely much simpler to collect and store the data. As regards analyzing the data per the requirements ( to locate the file) , we could create GSI on the physician + patient to quickly access the metadata containing the link to the S3 file."
      },
      {
        "date": "2021-10-25T00:09:00.000Z",
        "voteCount": 1,
        "content": "For Option C- It may work well if we create a GSI on physician + patient. However I think that would result in a significantly increased cost due to the creation of a separate table with its own RCU + WCU."
      },
      {
        "date": "2021-10-24T12:37:00.000Z",
        "voteCount": 1,
        "content": "Option A- I would run a transient EMR cluster every night to produce the file structure in the S3 bucket like this-&gt; bucket/physician/date/patient/ patient.txt. With this structure in place I can use S3 select to locate a file for a specific patient file very quickly. \nOption B- This is a big data exam and using API Gateway can be discarded. Integrating API Gateway with EMR using Lambda or EC2 doesn't seem cost efficient either.\nOption C: The only reason I wouldn't select this option is because of the partition + sort key. It will fetch millions of records to look through to find a specific patient's file for a specific physician. Ideal partition + sort key would have been physician + patient.\nOption D:- The data that needs to be located is too granular to consider RedShift which is a Data Warehouse solution.\nTherefore the choice is Option A"
      },
      {
        "date": "2021-10-22T20:03:00.000Z",
        "voteCount": 1,
        "content": "my selection C"
      },
      {
        "date": "2021-10-21T16:35:00.000Z",
        "voteCount": 2,
        "content": "The record are already getting stored in S3, we only need to keep track of where to find them.\nS3 event push&gt; Lambda &gt;DynamoDB is the easiest and cost-efficient solution."
      },
      {
        "date": "2021-10-19T01:31:00.000Z",
        "voteCount": 3,
        "content": "All user data stored in Amazon DynamoDB is fully encrypted at rest. DynamoDB encryption at rest provides enhanced security by encrypting all your data at rest using encryption keys stored in AWS Key Management Service (AWS KMS). This functionality helps reduce the operational burden and complexity involved in protecting sensitive data. With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements.\nNote - only metadata of each file has to be queried...\nSo going with C. Millions of data each month does not seem very high..so partition based on month and year will work."
      },
      {
        "date": "2021-10-16T19:25:00.000Z",
        "voteCount": 2,
        "content": "I choose B, it is most cost-effective compared to other solutions."
      },
      {
        "date": "2021-10-19T01:46:00.000Z",
        "voteCount": 1,
        "content": "Agreed. The answer is in the question: \u201cAuditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician.\u201d So it\u2019s A or B. B make more sense for an enterprise solution."
      },
      {
        "date": "2021-10-16T15:20:00.000Z",
        "voteCount": 2,
        "content": "I think it is A"
      },
      {
        "date": "2021-10-15T19:18:00.000Z",
        "voteCount": 4,
        "content": "What is the most cost- and time-efficient collection methodology in this situation? C. Only lambda doing the job and both S3 for files and dynamo for metadata support encryption at rest. How do we hash/sort the dynamo table here is irrelevant to the answer--my opinion"
      },
      {
        "date": "2021-10-18T05:23:00.000Z",
        "voteCount": 1,
        "content": "Seems right."
      },
      {
        "date": "2021-10-14T17:26:00.000Z",
        "voteCount": 1,
        "content": "Any Thoughts on C?"
      },
      {
        "date": "2021-10-11T23:32:00.000Z",
        "voteCount": 2,
        "content": "Do we think D via redshift spectrum as per https://www.quora.com/Does-Amazon-Redshift-support-partitioning\nOr do we think dynamodb still in a timeseries format item2019Sept?"
      },
      {
        "date": "2021-10-12T11:01:00.000Z",
        "voteCount": 1,
        "content": "so D? final?"
      },
      {
        "date": "2021-09-24T18:36:00.000Z",
        "voteCount": 3,
        "content": "C? Possible answer?"
      },
      {
        "date": "2021-09-29T11:05:00.000Z",
        "voteCount": 5,
        "content": "I'm going to go with C."
      },
      {
        "date": "2021-10-11T12:51:00.000Z",
        "voteCount": 1,
        "content": "As @muhsin mentioned if we use dynamodb, the partition key should be physican and sort key is patient. The partition/sort key on month and year is highly ineffective."
      },
      {
        "date": "2021-09-23T19:40:00.000Z",
        "voteCount": 1,
        "content": "for option A, how is it possible to find easily patient information."
      },
      {
        "date": "2021-09-24T05:15:00.000Z",
        "voteCount": 3,
        "content": "Good point. what are your thoughts on C? Thinking since they refer to collection and dynamodb refers to collection items could be linked. Also this will also a look up or to find patient information quickly compared to the redshift answer."
      },
      {
        "date": "2021-09-30T11:29:00.000Z",
        "voteCount": 1,
        "content": "in S3, the file has already timestamp information. if we use dynamodb, the partition key should be physican and sort key is patient."
      },
      {
        "date": "2021-10-09T13:40:00.000Z",
        "voteCount": 1,
        "content": "Are you thinking either A or B due to the partitioning/sorting  being done on physician"
      },
      {
        "date": "2021-10-11T14:12:00.000Z",
        "voteCount": 3,
        "content": "as the question says auditors must be able to locate any physical file in the S3 bucket for a given date, patient, or physician. Wouldnt A be correct?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/3419-exam-aws-certified-big-data-specialty-topic-1-question-52/",
    "body": "A clinical trial will rely on medical sensors to remotely assess patient health. Each physician who participates in the trial requires visual reports each morning. The reports are built from aggregations of all the sensor data taken each minute.<br>What is the most cost-effective solution for creating this visualization each day?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Aggregators Library to generate reports for reviewing the patient sensor data and generate a QuickSight visualization on the new data each morning for the physician to review.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a transient EMR cluster that shuts down after use to aggregate the patient sensor data each night and generate a QuickSight visualization on the new data each morning for the physician to review.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spark streaming on EMR to aggregate the patient sensor data in every 15 minutes and generate a QuickSight visualization on the new data each morning for the physician to review.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an EMR cluster to aggregate the patient sensor data each night and provide Zeppelin notebooks that look at the new data residing on the cluster each morning for the physician to review."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-16T14:22:00.000Z",
        "voteCount": 9,
        "content": "The asks are cost-effective visual reports.\n\nB is the right answer as transient clusters each night will run only during ETL and quicksight is capable of producing cost-effective daily reports for multiple individuals (physicians)\n\nA - Kinesis Aggregators Library does not exists\nC - Running spark streaming server is not cost effective\nD - Zeppelin on EMR does not have capability to provide reports/access for multiple physicians."
      },
      {
        "date": "2021-10-06T08:49:00.000Z",
        "voteCount": 5,
        "content": "correct. B"
      },
      {
        "date": "2021-10-26T19:01:00.000Z",
        "voteCount": 5,
        "content": "Not A: No such library called Kinesis Aggregators Library.\nNot C: This is not cost-effective\nNot D: This is not cost-effective as the EMR cluster will be utilized not only to aggregate the data but also run Zeppelin command to provide reports on a daily basis.\nAnswer is B: Transient EMR cluster will reduce cost and QuickSight could use the S3 dataset to generate insight using Athena or directly."
      },
      {
        "date": "2021-10-29T16:01:00.000Z",
        "voteCount": 1,
        "content": "D: we can run EMR cluster at night and store data in S3. Shutdown the cluster once processing is completed. Use zeppelin for reporting. what about this solution? correct me if I am wrong"
      },
      {
        "date": "2021-11-06T08:16:00.000Z",
        "voteCount": 1,
        "content": "Option B - correct.\nOption D states that we will provide Zeppelin notebooks that look at the new data residing on the cluster - in this case we will have to keep cluster in running mode.\nAlso Zeppelin will not run without EMR - so fails at cost-effective condition."
      },
      {
        "date": "2021-10-25T20:10:00.000Z",
        "voteCount": 1,
        "content": "D because 24*7 patient care"
      },
      {
        "date": "2021-10-23T10:42:00.000Z",
        "voteCount": 2,
        "content": "my selection B"
      },
      {
        "date": "2021-10-20T06:23:00.000Z",
        "voteCount": 1,
        "content": "D anyone? We may even use Zappelin for visualisation:\nhttps://scalegrid.io/blog/data-visualization-using-apache-zeppelin/"
      },
      {
        "date": "2021-10-17T15:41:00.000Z",
        "voteCount": 1,
        "content": "transient EMR cluster is not suitable for patient care business, so I choose D"
      },
      {
        "date": "2021-11-05T01:46:00.000Z",
        "voteCount": 1,
        "content": "It is asked for reports only, having (near-)realtime notebooks is overkill. Even patient care does not always require (near-)realtime data."
      },
      {
        "date": "2021-10-12T05:58:00.000Z",
        "voteCount": 1,
        "content": "Option B doesn't take into consideration the data generated from sensor at night. On the other hand Kinesis can aggregate this real time data and store it for 24 hours, which can be used for visualisation each morning"
      },
      {
        "date": "2021-10-10T04:05:00.000Z",
        "voteCount": 1,
        "content": "Zeppelin notebooks are cheaper than QuickSights?"
      },
      {
        "date": "2021-10-31T10:50:00.000Z",
        "voteCount": 3,
        "content": "Zepplin Notebooks requires your cluster to run non-stop, which is not cost-effective, hence D is wrong."
      },
      {
        "date": "2021-10-09T23:03:00.000Z",
        "voteCount": 1,
        "content": "agree with B"
      },
      {
        "date": "2021-10-08T08:57:00.000Z",
        "voteCount": 1,
        "content": "B is most cost effective"
      },
      {
        "date": "2021-09-20T14:48:00.000Z",
        "voteCount": 1,
        "content": "is it C. Most cost effective solution?"
      },
      {
        "date": "2021-09-23T03:36:00.000Z",
        "voteCount": 9,
        "content": "B? Cost effecctive? transients emr cluster?"
      },
      {
        "date": "2021-10-06T18:32:00.000Z",
        "voteCount": 4,
        "content": "Yes , B is correct ."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/4231-exam-aws-certified-big-data-specialty-topic-1-question-53/",
    "body": "A company uses Amazon Redshift for its enterprise data warehouse. A new on-premises PostgreSQL OLTP<br>DB must be integrated into the data warehouse. Each table in the PostgreSQL DB has an indexed timestamp column. The data warehouse has a staging layer to load source data into the data warehouse environment for further processing.<br>The data lag between the source PostgreSQL DB and the Amazon Redshift staging layer should NOT exceed four hours.<br>What is the most efficient technique to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a DBLINK on the source DB to connect to Amazon Redshift. Use a PostgreSQL trigger on the source table to capture the new insert/update/delete event and execute the event on the Amazon Redshift staging table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a PostgreSQL trigger on the source table to capture the new insert/update/delete event and write it to Amazon Kinesis Streams. Use a KCL application to execute the event on the Amazon Redshift staging table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtract the incremental changes periodically using a SQL query. Upload the changes to multiple Amazon Simple Storage Service (S3) objects, and run the COPY command to load to the Amazon Redshift staging layer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtract the incremental changes periodically using a SQL query. Upload the changes to a single Amazon Simple Storage Service (S3) object, and run the COPY command to load to the Amazon Redshift staging layer."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-28T07:07:00.000Z",
        "voteCount": 6,
        "content": "A - Incorrect due to the DBLink  constraint around using RDS PostgreSQL as opposed to on-prem PostgreSQL.\nB. Using Kinesis KCL to write to RedShift although can be done using JBDC/ODBC driver doesn't seem efficient. \nC. This makes the best sense. Using a cron job, we could extract new/changed records based on the timestamp from each table and create an S3 object for that table. Then use a Copy command to copy the data from the S3 object to a RedShift Table.\nD. It doesn't make any practical sense to extract changes across all tables into a single S3 object because it's not possible to then split the records into multiple RedShift tables.\nCorrect option is C."
      },
      {
        "date": "2021-10-14T01:24:00.000Z",
        "voteCount": 5,
        "content": "correct... C?"
      },
      {
        "date": "2021-10-23T10:43:00.000Z",
        "voteCount": 2,
        "content": "Agreed. COPY command is the fastest way to load in Redshit, if you have ++files that is..."
      },
      {
        "date": "2021-10-29T06:57:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/55262639/automatically-load-data-into-redshift-with-the-copy-function"
      },
      {
        "date": "2021-10-28T07:40:00.000Z",
        "voteCount": 3,
        "content": "I found this post which suggests that a cron job or a Lambda function triggered by an S3 event upon creating a new file in an S3 bucket could invoke the COPY command to load the data from each file into the corresponding RedShift table. My answer is C with more confidence this time."
      },
      {
        "date": "2021-10-26T17:24:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2021-10-25T13:28:00.000Z",
        "voteCount": 1,
        "content": "C. per best practices, COPY is the best way to update data in Redshift not insert. D is incorrect as Redshift can process multiple S3 in parallel"
      },
      {
        "date": "2021-10-24T04:08:00.000Z",
        "voteCount": 2,
        "content": "my selection C"
      },
      {
        "date": "2021-10-15T20:58:00.000Z",
        "voteCount": 1,
        "content": "@shwang, the parallelisation increases."
      },
      {
        "date": "2021-10-15T03:47:00.000Z",
        "voteCount": 3,
        "content": "why not D?  what is the difference to store the increment data to single or multiple S3?"
      },
      {
        "date": "2021-11-05T17:34:00.000Z",
        "voteCount": 2,
        "content": "Because of a Redshift best practice: Copy data split into a multiple of the number of slices in the Redshift cluster. It will be faster."
      },
      {
        "date": "2021-11-06T10:12:00.000Z",
        "voteCount": 1,
        "content": "Even worse, as Bulti correctly pointed out below, you might have multiple tables that you incrementally update, and their data must not be in a common file."
      },
      {
        "date": "2021-09-26T16:09:00.000Z",
        "voteCount": 2,
        "content": "A! i think!"
      },
      {
        "date": "2021-10-09T15:44:00.000Z",
        "voteCount": 4,
        "content": "from the last post at this link https://forums.aws.amazon.com/message.jspa?messageID=807709. It doesn't seem to be time effective to do DBLINK. In addition as @muhsin said DBLINK to redshift ONLY works with RDS postgresql instances. Answer must be C."
      },
      {
        "date": "2021-10-29T13:42:00.000Z",
        "voteCount": 1,
        "content": "Additionally, it appears that DBLINK is used in RDS to query from Redshift, not the other way round. https://stackoverflow.com/questions/45516920/does-amazon-redshift-support-extension-dblink"
      },
      {
        "date": "2021-09-19T10:23:00.000Z",
        "voteCount": 1,
        "content": "Is it A?\n1. https://aws.amazon.com/blogs/big-data/join-amazon-redshift-and-amazon-rds-postgresql-with-dblink/\n2. https://forums.aws.amazon.com/message.jspa?messageID=807709"
      },
      {
        "date": "2021-09-24T17:45:00.000Z",
        "voteCount": 1,
        "content": "this is aws RDS. but in the question, db is on-prem. so we need to move the data to aws."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/5354-exam-aws-certified-big-data-specialty-topic-1-question-54/",
    "body": "An administrator is deploying Spark on Amazon EMR for two distinct use cases: machine learning algorithms and ad-hoc querying. All data will be stored in Amazon S3. Two separate clusters for each use case will be deployed. The data volumes on Amazon S3 are less than 10 GB.<br>How should the administrator align instance types with the clusters purpose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMachine Learning on C instance types and ad-hoc queries on R instance types",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMachine Learning on R instance types and ad-hoc queries on G2 instance types",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMachine Learning on T instance types and ad-hoc queries on M instance types",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMachine Learning on D instance types and ad-hoc queries on I instance types"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-26T12:41:00.000Z",
        "voteCount": 6,
        "content": "A is correct"
      },
      {
        "date": "2021-10-12T10:51:00.000Z",
        "voteCount": 3,
        "content": "A) is correct\nDetails:\nC- Compute optimized\nR \u2013 Memory optimized\nG \u2013 GPU omptimized \nI, D \u2013 Storage optimized\nT \u2013 not exists\n\nC3, C4, and C5 are compute optimized instances featuring high performance processors and with a lowest price/compute performance in EC2 compared to R3, R4 or R5 although it's recommended use cases are distributed memory caches and in-memory analytics. But C5 will do the job for you for a lower price.\nhttps://stackoverflow.com/questions/30435610/spark-which-instance-type-is-preferred-for-aws-emr-cluster"
      },
      {
        "date": "2021-10-24T10:59:00.000Z",
        "voteCount": 2,
        "content": "T and M are actually general purpose, but yeah, it's A)"
      },
      {
        "date": "2021-10-07T21:46:00.000Z",
        "voteCount": 4,
        "content": "my selection A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/5355-exam-aws-certified-big-data-specialty-topic-1-question-55/",
    "body": "An organization is designing an application architecture. The application will have over 100 TB of data and will support transactions that arrive at rates from hundreds per second to tens of thousands per second, depending on the day of the week and time of the day. All transaction data, must be durably and reliably stored. Certain read operations must be performed with strong consistency.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB as the data store and use strongly consistent reads when necessary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Relational Database Service (RDS) instance sized to meet the maximum anticipated transaction rate and with the High Availability option enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a NoSQL data store on top of an Amazon Elastic MapReduce (EMR) cluster, and select the HDFS High Durability option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift with synchronous replication to Amazon Simple Storage Service (S3) and row-level locking for strong consistency."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-25T09:35:00.000Z",
        "voteCount": 1,
        "content": "A it is"
      },
      {
        "date": "2021-10-20T02:34:00.000Z",
        "voteCount": 4,
        "content": "my selection A"
      },
      {
        "date": "2021-10-22T16:07:00.000Z",
        "voteCount": 4,
        "content": "a discussion section without a san2020 selection would not be the same :D"
      },
      {
        "date": "2021-10-08T18:25:00.000Z",
        "voteCount": 1,
        "content": "Why not B?"
      },
      {
        "date": "2021-10-16T16:54:00.000Z",
        "voteCount": 2,
        "content": "Limit of RDS is 100TB, the data is over 100TB so DynamoDB"
      },
      {
        "date": "2021-10-08T22:59:00.000Z",
        "voteCount": 3,
        "content": "Proobably becouse is not a best practice \"sized to meet the maximum anticipated transaction rate\"...and also becouse if I read \"Strongly Consistent or Eventually Consistent\" my mind goes directlytoi DynamoDB :)"
      },
      {
        "date": "2021-10-11T03:34:00.000Z",
        "voteCount": 6,
        "content": "100TB is too much for RDS. 64TB limit for Aurora. DynamoDB is limitless and is OLTP"
      },
      {
        "date": "2021-10-08T16:34:00.000Z",
        "voteCount": 4,
        "content": "yes.. A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/5023-exam-aws-certified-big-data-specialty-topic-1-question-56/",
    "body": "A company generates a large number of files each month and needs to use AWS import/export to move these files into Amazon S3 storage. To satisfy the auditors, the company needs to keep a record of which files were imported into Amazon S3.<br>What is a low-cost way to create a unique log for each import job?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the same log file prefix in the import/export manifest files to create a versioned log file in Amazon S3 for all imports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the log file prefix in the import/export manifest files to create a unique log file in Amazon S3 for each import.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the log file checksum in the import/export manifest files to create a unique log file in Amazon S3 for each import.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a script to iterate over files in Amazon S3 to generate a log after each import/export job."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-14T16:05:00.000Z",
        "voteCount": 9,
        "content": "B is correct. Refer to https://aws.amazon.com/cn/blogs/aws/send-us-that-data/ - check the screenshots listed in the article. During manifest file creation we can specify \"log file prefix\""
      },
      {
        "date": "2021-10-18T22:54:00.000Z",
        "voteCount": 7,
        "content": "my selection B"
      },
      {
        "date": "2021-10-03T17:24:00.000Z",
        "voteCount": 1,
        "content": "Thoughts b?"
      },
      {
        "date": "2021-09-23T05:06:00.000Z",
        "voteCount": 1,
        "content": "Any thoughts on C?"
      },
      {
        "date": "2021-09-23T12:51:00.000Z",
        "voteCount": 1,
        "content": "checksum is already included in the log file : https://aws.amazon.com/cn/blogs/aws/send-us-that-data/"
      },
      {
        "date": "2021-10-24T13:48:00.000Z",
        "voteCount": 2,
        "content": "Think C should actually work in practice. Howeer, two theoritcal issues come to my mind:\n1. MD5 hashes could coincide accidentally, but chances for that are really, reaaaaally low.\n2. MD5 hashes would also coincide if you sent the same log files twice (why would you tho).\nOverall, why provide cryptographic file names when you can just specify a log file prefix instead. B is better."
      },
      {
        "date": "2021-10-27T16:04:00.000Z",
        "voteCount": 1,
        "content": "(checksums = MD5 hashes)"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/3834-exam-aws-certified-big-data-specialty-topic-1-question-57/",
    "body": "A company needs a churn prevention model to predict which customers will NOT renew their yearly subscription to the companys service. The company plans to provide these customers with a promotional offer. A binary classification model that uses Amazon Machine Learning is required.<br>On which basis should this binary classification model be built?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUser profiles (age, gender, income, occupation)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLast user session",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach user time series events in the past 3 months",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuarterly results"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-01T05:04:00.000Z",
        "voteCount": 11,
        "content": "my selection C"
      },
      {
        "date": "2021-11-06T19:27:00.000Z",
        "voteCount": 1,
        "content": "it's yearly subscription; and the time series data is only for 3 months. Is this not a mis-match ? We should at least be reviewing a years data to make prediction ?"
      },
      {
        "date": "2021-10-24T05:54:00.000Z",
        "voteCount": 2,
        "content": "The ask is \"On which basis should this binary classification model be built\". To build a model we need recent utilization data. I will go with C"
      },
      {
        "date": "2021-10-23T15:48:00.000Z",
        "voteCount": 4,
        "content": "A. User profiles.\n\nhttps://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/\nMobile operators have historical records on which customers ultimately ended up churning and which continued using the service. \nWe can use this historical information to construct an ML model of one mobile operator\u2019s churn using a process called training. \n\nAfter training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn."
      },
      {
        "date": "2021-10-25T15:26:00.000Z",
        "voteCount": 18,
        "content": "listen to this guy and say bye bye to your $300"
      },
      {
        "date": "2021-11-02T23:12:00.000Z",
        "voteCount": 2,
        "content": "better listen to san2020, his selections are almost always spot-on"
      },
      {
        "date": "2021-10-03T11:17:00.000Z",
        "voteCount": 2,
        "content": "to predict churn, we need to have user pattern for the last period. profile info can be added also"
      },
      {
        "date": "2021-10-13T19:33:00.000Z",
        "voteCount": 2,
        "content": "Would you agree C still?"
      },
      {
        "date": "2021-10-15T14:57:00.000Z",
        "voteCount": 7,
        "content": "C is correct"
      },
      {
        "date": "2021-09-23T18:35:00.000Z",
        "voteCount": 1,
        "content": "a? thoughts?"
      },
      {
        "date": "2021-09-24T07:30:00.000Z",
        "voteCount": 2,
        "content": "Since it's a binary model responses can only be true or false. Due to that reason i thought time series is correct. For example sample data could be number of days offline, number of support call, number of support emails etc instead of user information"
      },
      {
        "date": "2021-09-28T10:51:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/5359-exam-aws-certified-big-data-specialty-topic-1-question-58/",
    "body": "A company with a support organization needs support engineers to be able to search historic cases to provide fast responses on new issues raised. The company has forwarded all support messages into an Amazon<br>Kinesis Stream. This meets a company objective of using only managed services to reduce operational overhead.<br>The company needs an appropriate architecture that allows support engineers to search on historic cases and find similar issues and their associated responses.<br>Which AWS Lambda action is most appropriate?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest and index the content into an Amazon Elasticsearch domain.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStem and tokenize the input and store the results into Amazon ElastiCache.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite data as JSON into Amazon DynamoDB with primary and secondary indexes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAggregate feedback in Amazon S3 using a columnar format with partitioning."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-24T15:39:00.000Z",
        "voteCount": 7,
        "content": "Agree with A"
      },
      {
        "date": "2021-10-06T01:21:00.000Z",
        "voteCount": 6,
        "content": "my selection A"
      },
      {
        "date": "2021-10-11T18:24:00.000Z",
        "voteCount": 1,
        "content": "Agree with A"
      },
      {
        "date": "2021-10-03T23:38:00.000Z",
        "voteCount": 2,
        "content": "This question seems to be a straight forward. Will these kind of questions appear in real test ?"
      },
      {
        "date": "2021-10-13T07:33:00.000Z",
        "voteCount": 1,
        "content": "Nope. Mostly not possible. This is just for practice."
      },
      {
        "date": "2021-10-18T05:38:00.000Z",
        "voteCount": 1,
        "content": "Just attempted exam - same question :D"
      },
      {
        "date": "2021-10-20T08:53:00.000Z",
        "voteCount": 1,
        "content": "I selected A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/3563-exam-aws-certified-big-data-specialty-topic-1-question-59/",
    "body": "A solutions architect works for a company that has a data lake based on a central Amazon S3 bucket. The data contains sensitive information. The architect must be able to specify exactly which files each user can access. Users access the platform through a SAML federation Single Sign On platform.<br>The architect needs to build a solution that allows fine grained access control, traceability of access to the objects, and usage of the standard tools (AWS Console, AWS CLI) to access the data.<br>Which solution should the architect build?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Server-Side Encryption with AWS KMS-Managed Keys for storing data. Use AWS KMS Grants to allow access to specific elements of the platform. Use AWS CloudTrail for auditing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Server-Side Encryption with Amazon S3-Managed Keys. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Client-Side Encryption with Client-Side Master Key. Set Amazon S3 ACLs to allow access to specific elements of the platform. Use Amazon S3 to access logs for auditing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Client-Side Encryption with AWS KMS-Managed Keys for storing data. Use AWS KMS Grants to allow access to specific elements of the platform. Use AWS CloudTrail for auditing."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-13T19:29:00.000Z",
        "voteCount": 18,
        "content": "Correct Answer is B as S3 Server Side Encryption with S3 Managed Keys provide encryption. S3 ACLs allows fine grained control access and S3 to access logs would help provide traceability across all tools.\n\nUse Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) \u2013 Each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\n\nOption C is wrong as with Client-Side Encryption, the users must have the keys to decrypt the data.\n\nWhen downloading an object\u2014The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.\n\nOptions A &amp; D are wrong as KMS Grants are mainly to provide access to the KMS keys. There is not mention of fine grained control over the S3 objects"
      },
      {
        "date": "2021-10-13T13:29:00.000Z",
        "voteCount": 10,
        "content": "Why not B?\nAccess from AWS Console: SSE only, A or B\nFine grained access control: S3 ACL or Bucket List\nAccess Log: S3 Access Log\nanswer is B?"
      },
      {
        "date": "2021-11-06T03:38:00.000Z",
        "voteCount": 1,
        "content": "Wrong A&amp;D: AWS KMS Grants are for accessing keys.\nNot C: There is no request for any additional protection\nOK B: Amazon S3 to access log should have enough information\nexample:\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 3E57427F3EXAMPLE REST.GET.VERSIONING - \"GET /awsexamplebucket1?versioning HTTP/1.1\" 200 - 113 - 7 - \"-\" \"S3Console/0.4\" - s9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234= SigV2 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1"
      },
      {
        "date": "2021-11-04T18:29:00.000Z",
        "voteCount": 1,
        "content": "B my thinking\nhttps://docs.amazonaws.cn/en_us/sdk-for-java/v1/developer-guide/examples-s3-access-permissions.html"
      },
      {
        "date": "2021-10-29T03:43:00.000Z",
        "voteCount": 2,
        "content": "b is correct"
      },
      {
        "date": "2021-10-26T04:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. The user is not uploading, coming through SAML, that means from some other system with not necessarily aws. Server side encryption makes sense."
      },
      {
        "date": "2021-10-24T23:49:00.000Z",
        "voteCount": 2,
        "content": "Moreover if you encrypt the objects using SSE-C and Client side encryption, the data key will not be stored along with the object in S3 as its metadata. As a result you will not be able to access these object from AWS Console as AWS will not be able to decrypt these objects before returning to the console."
      },
      {
        "date": "2021-10-24T13:13:00.000Z",
        "voteCount": 3,
        "content": "A and D will record access log to the objects in S3 bucket using CloudTrail but it won't allow creation of object level permissions because it uses KMS Grant which only provide granular access to CMS key used for KMS encryption. \nOption C- Client side encryption is more expensive to setup than using Out of the box SSE-S3 encryption and therefore the correct answer is B."
      },
      {
        "date": "2021-10-22T04:31:00.000Z",
        "voteCount": 2,
        "content": "D. It is recommended to use CloudTrail https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html.\nClient-side encryption will ensure data is encrypted before upload, better than A"
      },
      {
        "date": "2021-10-19T09:05:00.000Z",
        "voteCount": 1,
        "content": "It should be A or D.\nB and C are out because access logs won't tell you whether you are trying to access a object via cli or console. you have to use cloudtrail to do that.\nI am just not sure sse or cse, any thoughts?"
      },
      {
        "date": "2021-10-20T00:53:00.000Z",
        "voteCount": 2,
        "content": "Sorry for the earlier post.  It seems that access logs do tell you \"console/cli\", so it should be B."
      },
      {
        "date": "2021-10-16T15:28:00.000Z",
        "voteCount": 4,
        "content": "my selection B"
      },
      {
        "date": "2021-10-14T19:58:00.000Z",
        "voteCount": 2,
        "content": "A is the answer \nhttps://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/securing-protecting-managing-data.html"
      },
      {
        "date": "2021-10-07T04:06:00.000Z",
        "voteCount": 2,
        "content": "A is the Answer.\n\nServer side encryption is at object level, while client side encryption is at data level. \n\nRefer : https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\n\nServer-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.\n\nClient side encryption is the act of encrypting data before sending it to S3."
      },
      {
        "date": "2021-10-05T14:27:00.000Z",
        "voteCount": 1,
        "content": "Should be C"
      },
      {
        "date": "2021-10-04T21:40:00.000Z",
        "voteCount": 1,
        "content": "Should be C."
      },
      {
        "date": "2021-10-01T17:46:00.000Z",
        "voteCount": 3,
        "content": "Should be A. If client-side encryption then you cannot access the data in AWS Console?"
      },
      {
        "date": "2021-09-20T19:55:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html D is correct. Creating your own CMK gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data."
      },
      {
        "date": "2021-10-27T18:03:00.000Z",
        "voteCount": 6,
        "content": "Your explanation is good but still allows for A. And Server Side Encryption is to be preferred, since it is simpler and there is no good reason not to use it.\n\nmy selection A"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/5360-exam-aws-certified-big-data-specialty-topic-1-question-60/",
    "body": "A company that provides economics data dashboards needs to be able to develop software to display rich, interactive, data-driven graphics that run in web browsers and leverages the full stack of web standards<br>(HTML, SVG, and CSS).<br>Which technology provides the most appropriate support for this requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tD3.js",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIPython/Jupyter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tR Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHue"
    ],
    "answer": "A",
    "answerDescription": "Reference: https://sa.udacity.com/course/data-visualization-and-d3js--ud507",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-08T09:18:00.000Z",
        "voteCount": 5,
        "content": "A is the Answer.\n\nRefer : https://d3js.org/.\n\nD3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3\u2019s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation."
      },
      {
        "date": "2021-10-12T13:45:00.000Z",
        "voteCount": 5,
        "content": "my selection A"
      },
      {
        "date": "2021-09-28T11:13:00.000Z",
        "voteCount": 1,
        "content": "Should be D"
      },
      {
        "date": "2021-10-28T07:21:00.000Z",
        "voteCount": 2,
        "content": "From Hue's website: Hue is an open source SQL Assistant for Databases &amp; Data Warehouses. \n\nHue's visualization power is at best limited if at all existing. A is more appropriate here."
      },
      {
        "date": "2021-09-23T22:11:00.000Z",
        "voteCount": 4,
        "content": "A it is"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/2499-exam-aws-certified-big-data-specialty-topic-1-question-61/",
    "body": "A company hosts a portfolio of e-commerce websites across the Oregon, N. Virginia, Ireland, and Sydney<br>AWS regions. Each site keeps log files that capture user behavior. The company has built an application that generates batches of product recommendations with collaborative filtering in Oregon. Oregon was selected because the flagship site is hosted there and provides the largest collection of data to train machine learning models against. The other regions do NOT have enough historic data to train accurate machine learning models.<br>Which set of data processing steps improves recommendations for each region?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the e-commerce application in Oregon to write replica log files in each other region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 bucket replication to consolidate log entries and build a single model in Oregon.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis as a buffer for web logs and replicate logs to the Kinesis stream of a neighboring region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CloudWatch Logs agent to consolidate logs into a single CloudWatch Logs group."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-09T15:14:00.000Z",
        "voteCount": 11,
        "content": "I feel like we went down a rabbit hole here. We are looking for what improves recommendations for each region. The company seems to want to build regional models so B would not improve Oregon's recommendations, C seems rather off base to me, and D agains goes for consolidating logs. I feel the answer is A as you can transfer logs from Oregon to other regions which will then give them enough data to begin training their models for their regions."
      },
      {
        "date": "2021-11-05T03:19:00.000Z",
        "voteCount": 1,
        "content": "B. Because other regions don't have enough data to train the their own model, then funnel them all to a single region to train one model to apply to all to improve their recommendation.\nCloudWatch cross region is only available from Nov 2019, but this question is available 9 months ago, which is Sep, 2019. Therefore, D is wrong. https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/"
      },
      {
        "date": "2021-11-03T03:15:00.000Z",
        "voteCount": 1,
        "content": "I will go for C. imo the key of improving the RM engine is to give other regions the access to Oregon logs."
      },
      {
        "date": "2021-10-29T12:38:00.000Z",
        "voteCount": 1,
        "content": "Answer is D \nRefer : https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/"
      },
      {
        "date": "2021-10-24T12:48:00.000Z",
        "voteCount": 1,
        "content": "the answer is D \nhttps://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/"
      },
      {
        "date": "2021-10-22T09:56:00.000Z",
        "voteCount": 2,
        "content": "I think A is the right answer. None of the option clearly talk about replicating the user behavior log captured in Oregon to all the other region to be able to retain the model there and thereby improve the prediction in those regions except for A. Although A might look a bit more involved and less efficient it is the only way to successfully improve the recommendation in other regions."
      },
      {
        "date": "2021-10-13T08:11:00.000Z",
        "voteCount": 2,
        "content": "Two keywords in this question are \"data manipulation\" and \"interactive\". Only  Option B- Pig with Tachyon satisfies both requirements. Pig with Tachyon will be able to handle very high throughput working with large datasets and also be able to manipulate the data.  Also You can execute Pig commands interactively or in batch mode. To use Pig interactively, create an SSH connection to the master node and submit commands using the Grunt shell.  Refer to this link-&gt; https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-pig.html"
      },
      {
        "date": "2021-10-20T14:58:00.000Z",
        "voteCount": 3,
        "content": "This is a comment against next question."
      },
      {
        "date": "2021-11-06T22:41:00.000Z",
        "voteCount": 1,
        "content": "This is not what the question is about.\nWhich set of data processing steps improves recommendations for each region"
      },
      {
        "date": "2021-10-12T11:20:00.000Z",
        "voteCount": 1,
        "content": "Option B- Doesn't make sense as it doesn't talk about how to improve the recommendation in each region.\nOption D: CloudWatch can consolidate logs into a log group within the same region and not replicate these logs ( from Oregon for e.g.) into other regions of interest.\nSo its between Option A and Option C. I would go with Option C over Option A because it involves using AWS services to replicate the logs from one region to another using a relay mechanism as opposed to having to write code in your ecommerce application to replicate the logs across all regions. Once the logs are replicated using the relay mechanism from one region to the next , the application can use these logs in its respective region to improve recommendation."
      },
      {
        "date": "2021-10-12T02:38:00.000Z",
        "voteCount": 4,
        "content": "my selection D"
      },
      {
        "date": "2021-10-11T20:00:00.000Z",
        "voteCount": 2,
        "content": "Oregon was selected because the flagship site is hosted there and provides the largest collection of data to train machine learning models against.\nit means ML model is already and the application is ready.\nThe other regions do NOT have enough historic data to train accurate machine learning models.\nSo the other sides are going to use the application based on the Oregon ML model and for that they need...\nD. \nUse the CloudWatch Logs agent to consolidate logs into a single CloudWatch Logs group."
      },
      {
        "date": "2021-10-10T22:07:00.000Z",
        "voteCount": 1,
        "content": "The application built can consume log files..and generates recommendations only. It does not mention that the application can write log files...for that you need Kinesis."
      },
      {
        "date": "2021-10-10T14:15:00.000Z",
        "voteCount": 1,
        "content": "The recommendations of a cold place like oregon (in nov) will be different than sydney (summer in nov). So tehre cannot eb any consolidation. As per elimination, only C seems correct...."
      },
      {
        "date": "2021-10-07T09:16:00.000Z",
        "voteCount": 2,
        "content": "the question is \"recommendation for EACH region\", nothing says aggregate across regions. D seems like a good answer"
      },
      {
        "date": "2021-10-04T11:07:00.000Z",
        "voteCount": 2,
        "content": "From AWS: \"You can aggregate the metrics for AWS resources across multiple resources. Amazon CloudWatch can't aggregate data across Regions. Metrics are completely separate between Regions.\"\nsooo... B?"
      },
      {
        "date": "2021-10-01T06:36:00.000Z",
        "voteCount": 1,
        "content": "Check this out.\nSeems like D is the closet answer\n\nhttps://aws.amazon.com/solutions/centralized-logging/"
      },
      {
        "date": "2021-10-02T18:35:00.000Z",
        "voteCount": 3,
        "content": "Actually, I think @Hitu is correct. Cloudwatch agent could not aggregate cross-region log into one group. After reading this article, I am voting for B now.\nAny comments gents?\n\nLogs are generated regionally by AWS services so the best practice is to funnel all regional logs into one region in order to analyze the data across regions. There are three options to centralize your AWS logs.\nUse CloudWatch for your centralized log collection and then push them to a log analysis solution via Lambda or Kinesis.\nSend all logs directly to S3 and further process them with Lambda functions.\nConfigure agents like Beats on EC2 instances and FunctionBeat on Lambdas to push logs to a logging solution.\nhttps://coralogix.com/log-analytics-blog/aws-centralized-logging-guide/"
      },
      {
        "date": "2021-10-12T06:15:00.000Z",
        "voteCount": 1,
        "content": "actually Cloudwatch can support cross regions monitoring as of Nov 8,2019\nhttps://aws.amazon.com/tw/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/"
      },
      {
        "date": "2021-09-29T10:03:00.000Z",
        "voteCount": 1,
        "content": "Centralized Log Management with AWS CloudWatch.\nLog groups are used to classify log streams together\nhttps://cloudacademy.com/blog/centralized-log-management-with-aws-cloudwatch-part-1-of-3/\nso D"
      },
      {
        "date": "2021-09-27T03:19:00.000Z",
        "voteCount": 3,
        "content": "I support D"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/3494-exam-aws-certified-big-data-specialty-topic-1-question-62/",
    "body": "There are thousands of text files on Amazon S3. The total size of the files is 1 PB. The files contain retail order information for the past 2 years. A data engineer needs to run multiple interactive queries to manipulate the data. The Data Engineer has AWS access to spin up an Amazon EMR cluster. The data engineer needs to use an application on the cluster to process this data and return the results in interactive time frame.<br>Which application on the cluster should the data engineer use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOozie",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Pig with Tachyon",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Hive",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPresto"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-22T04:21:00.000Z",
        "voteCount": 5,
        "content": "Is there any question where we don't choose Presto? It is as if AWS is trying to beat it into our heads lol"
      },
      {
        "date": "2021-10-31T08:28:00.000Z",
        "voteCount": 2,
        "content": "B. \"to manipulate the data\". Pig is for ETL so you can transform the data. Tachyon is an in memory engine so you get quick results.\nNowadays, you would want to use Spark for this task."
      },
      {
        "date": "2021-10-28T07:46:00.000Z",
        "voteCount": 1,
        "content": "D fit best !\nhttps://aws.amazon.com/fr/big-data/what-is-presto/"
      },
      {
        "date": "2021-10-26T13:09:00.000Z",
        "voteCount": 2,
        "content": "On second thought I think the answer is D because the ask is to just query the data interactively and then later use the results from the query to manipulate the data. So since \"interactive\" is the  only requirement I think Presto fits the best."
      },
      {
        "date": "2021-10-25T15:32:00.000Z",
        "voteCount": 2,
        "content": "C. data size is PB, Hive is better. Hive is optimized for query throughput, while Presto is optimized for latency. Presto has a limitation on the maximum amount of memory that each task in a query can store, so if a query requires a large amount of memory, the query simply fails. Such error handling logic (or a lack thereof) is acceptable for interactive queries; however, for daily/weekly reports that must run reliably, it is ill-suited. For such tasks, Hive is a better alternative."
      },
      {
        "date": "2021-10-20T10:47:00.000Z",
        "voteCount": 3,
        "content": "my selection D"
      },
      {
        "date": "2021-10-18T14:37:00.000Z",
        "voteCount": 1,
        "content": "\"Presto is an open source, distributed SQL query engine designed for fast, interactive queries on data in HDFS\"\nI think this is D, but without \"interactive\" hive could be better choice"
      },
      {
        "date": "2021-10-16T02:08:00.000Z",
        "voteCount": 1,
        "content": "Answer C\n\"A data engineer needs to run multiple interactive queries to manipulate the data.\"\n\nHive is optimized for query throughput, while Presto is optimized for latency.\nHive translates SQL queries into multiple stages of MapReduce and it is powerful enough to handle huge numbers of jobs.\n\nhttps://blog.treasuredata.com/blog/2015/03/20/presto-versus-hive/"
      },
      {
        "date": "2021-10-18T17:29:00.000Z",
        "voteCount": 2,
        "content": "the 'interactive' queries might point to Presto only"
      },
      {
        "date": "2021-10-13T07:44:00.000Z",
        "voteCount": 4,
        "content": "Presto is used in production at very large scale at many well-known organizations. You\u2019ll find it used at Facebook, Airbnb, Netflix, Atlassian, Nasdaq, and many more. Facebook\u2019s implementation of Presto is used by over a thousand employees, who run more than 30,000 queries, processing one petabyte of data daily.\nD it is...."
      },
      {
        "date": "2021-10-09T16:22:00.000Z",
        "voteCount": 2,
        "content": "D\nPresto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes."
      },
      {
        "date": "2021-10-07T09:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is D:"
      },
      {
        "date": "2021-09-22T00:53:00.000Z",
        "voteCount": 1,
        "content": "I thought c due to multiple interactive queries as presto can only do one at a time."
      },
      {
        "date": "2021-09-30T09:55:00.000Z",
        "voteCount": 1,
        "content": "I have this service confused with Athena"
      },
      {
        "date": "2021-10-06T03:37:00.000Z",
        "voteCount": 1,
        "content": "Athena built on Presto w/ SQL Support"
      },
      {
        "date": "2021-09-20T12:51:00.000Z",
        "voteCount": 4,
        "content": "The correct answer may is D"
      },
      {
        "date": "2021-09-26T19:28:00.000Z",
        "voteCount": 1,
        "content": "I think your right D"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/3435-exam-aws-certified-big-data-specialty-topic-1-question-63/",
    "body": "A media advertising company handles a large number of real-time messages sourced from over 200 websites.<br>The companys data engineer needs to collect and process records in real time for analysis using Spark<br>Streaming on Amazon Elastic MapReduce (EMR). The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority.<br>Which Amazon Kinesis configuration meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish messages to Amazon Kinesis Firehose backed by Amazon Simple Storage Service (S3). Pull messages off Firehose with Spark Streaming in parallel to persistence to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish messages to Amazon Kinesis Streams. Pull messages off Streams with Spark Streaming in parallel to AWS Lambda pushing messages from Streams to Firehose backed by Amazon Simple Storage Service (S3).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish messages to Amazon Kinesis Firehose backed by Amazon Simple Storage Service (S3). Use AWS Lambda to pull messages from Firehose to Streams for processing with Spark Streaming.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish messages to Amazon Kinesis Streams, pull messages off with Spark Streaming, and write row data to Amazon Simple Storage Service (S3) before and after processing."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-23T15:31:00.000Z",
        "voteCount": 13,
        "content": "Not A - Because Spark Streams or any Spark component for that matter cannot read directly from KFH. Also not real-time solution.\nNot C - Not a real-time solution although doable if real-time is not an issue.\nNot D- It doesn't make sense to write the original row data one at a time to S3 when its possible to configure Kinesis stream destination as S3 and split data into multiple files organized by date as prefix.\nB- Is the right answer. Spark Streams can read directly from Kinesis streams and so can a Lambda function which will then insert each record into KFH to be delivered to S3."
      },
      {
        "date": "2021-10-31T23:14:00.000Z",
        "voteCount": 1,
        "content": "The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority."
      },
      {
        "date": "2021-11-05T22:08:00.000Z",
        "voteCount": 1,
        "content": "A is wrong - Spark Streaming can only read from Kinesis Data Streams\nB doesn't make sense - Kinesis Data Firehose has direct integration with Kinesis Data Streams\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-kinesis-streams.html\nC doesn't make sense - Lambda to integrate Firehose with Data Streams. I don't belive people design such crazy things\nD is correct and looks best compared to the others"
      },
      {
        "date": "2021-10-28T09:23:00.000Z",
        "voteCount": 1,
        "content": "D.\nReal-time message ruled out A and C.\nFor B, my concern is that streaming data in parallel to both Lambda and Spark, will that reduce the performance of KDS, or requires more shards? Hence, I pick D."
      },
      {
        "date": "2021-11-03T23:36:00.000Z",
        "voteCount": 1,
        "content": "I agree with D. While we are not degrading any performance in copying the stream message twice, But the no.of services(Firehouse,Lambda) we are bringing. All this is for saving RAW data to S3. Not justifying\n\nSo I believe D is correct answer"
      },
      {
        "date": "2021-10-27T23:02:00.000Z",
        "voteCount": 1,
        "content": "Option D. \"The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority\" It does mention anything regarding batching, so its ok to write single rows to S3"
      },
      {
        "date": "2021-10-27T20:29:00.000Z",
        "voteCount": 1,
        "content": "It has to be B."
      },
      {
        "date": "2021-10-24T04:24:00.000Z",
        "voteCount": 3,
        "content": "It could have been B, but we don't need a lambda to push messages, send the stream straight to FH. Because of this, going with D to keep the solution simple."
      },
      {
        "date": "2021-10-23T23:52:00.000Z",
        "voteCount": 2,
        "content": "It could have been B, but we don't need a lambda to push messages, send the stream straight to FH. Because of this, going with D to keep the solution simple."
      },
      {
        "date": "2021-10-26T15:13:00.000Z",
        "voteCount": 1,
        "content": "i like Bultis point arguing against D (storing row-wise to S3 appears awful), but your point on B was also what i thought. so weird..."
      },
      {
        "date": "2021-10-22T07:11:00.000Z",
        "voteCount": 2,
        "content": "my selection B"
      },
      {
        "date": "2021-10-11T03:02:00.000Z",
        "voteCount": 2,
        "content": "I vote for D. It looks simple and working solution"
      },
      {
        "date": "2021-10-14T02:34:00.000Z",
        "voteCount": 4,
        "content": "After researching I changed for B. Why? Because we should collect batch for storing in S3 (FH is good for it) instead of storing single row events (it will be awful)"
      },
      {
        "date": "2021-10-09T01:19:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nSpark Streaming can't pull messages from Firehose or Streams, so options A, B and D invalid.\nhttps://docs.aws.amazon.com/solutions/latest/real-time-analytics-spark-streaming/architecture.html"
      },
      {
        "date": "2021-10-10T11:29:00.000Z",
        "voteCount": 1,
        "content": "https://spark.apache.org/docs/2.3.0/streaming-kinesis-integration.html\nSpark streaming can pull messages from Kynesis, but it cann't dot it from firehose"
      },
      {
        "date": "2021-10-11T07:53:00.000Z",
        "voteCount": 1,
        "content": "Amazon Kinesis Firehose is not real-time"
      },
      {
        "date": "2021-10-13T14:22:00.000Z",
        "voteCount": 2,
        "content": "I didn't mention about FH. Amazon Kinesis Streams is using for real-time and spark streaming have integration with it"
      },
      {
        "date": "2021-10-08T22:00:00.000Z",
        "voteCount": 2,
        "content": ", Apache Spark can be over-burdened with file operations if it is processing a large number of small files versus fewer larger files.  Each of these files has its own overhead of a few milliseconds for opening, reading metadata information, and closing. This overhead of file operations on these large numbers of files results in slow processing. This blog post shows how to use Amazon Kinesis Data Firehose to merge many small messages into larger messages for delivery to Amazon S3.  This results in faster processing with Amazon EMR running Spark.\nOption C."
      },
      {
        "date": "2021-10-06T21:25:00.000Z",
        "voteCount": 1,
        "content": "Nobody has thoughts on A? can spark stream pull out message from kFH directly. A lambda function is necessary for pulling data from KFH to feed spark stream?"
      },
      {
        "date": "2021-10-06T20:45:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct. B looks overkill."
      },
      {
        "date": "2021-10-05T18:34:00.000Z",
        "voteCount": 2,
        "content": "In context, this case doesn't need managed service (FH + fee) in gathering &amp; processing. Cz FH needs buffering time for efficient bulk transfer.. For real time processing.. (not real-time gathering..) it doesn't need FH yet (in gathering &amp; processing.) But in back up to S3.. FH is better."
      },
      {
        "date": "2021-10-04T01:35:00.000Z",
        "voteCount": 2,
        "content": "I choose D, kinesis stream can be used to first store the raw data in s3 and then analyze the data in real time."
      },
      {
        "date": "2021-10-02T09:12:00.000Z",
        "voteCount": 1,
        "content": "funny, nobody bore to search KCL checkpoint table"
      },
      {
        "date": "2021-09-28T14:12:00.000Z",
        "voteCount": 2,
        "content": "FH cannot be an answer it is not a realtime. B is correct"
      },
      {
        "date": "2021-09-29T01:13:00.000Z",
        "voteCount": 3,
        "content": "Sorry. The question is \"The companys data engineer needs to collect and process records in real time for analysis using Spark\"  Real time for processing using spark so the answer in this case is C. Anyone have any other thoughts?"
      },
      {
        "date": "2021-09-29T10:06:00.000Z",
        "voteCount": 5,
        "content": "Only thinking B as when i sat the test previously i used the current answers within this guide and scored very poorly in the collection section. Cause of this i am under the impression C is incorrect and the most likely answer is B for kinesis data streams being realtime and FH having the 60 second delay."
      },
      {
        "date": "2021-10-01T03:03:00.000Z",
        "voteCount": 2,
        "content": "Agreed. I miss read. Its really confusing"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/3565-exam-aws-certified-big-data-specialty-topic-1-question-64/",
    "body": "A solutions architect for a logistics organization ships packages from thousands of suppliers to end customers.<br>The architect is building a platform where suppliers can view the status of one or more of their shipments.<br>Each supplier can have multiple roles that will only allow access to specific fields in the resulting information.<br>Which strategy allows the appropriate level of access control and requires the LEAST amount of management work?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the tracking data to Amazon Kinesis Streams. Use AWS Lambda to store the data in an Amazon DynamoDB Table. Generate temporary AWS credentials for the suppliers users with AWS STS, specifying fine-grained security policies to limit access only to their applicable data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the tracking data to Amazon Kinesis Firehose. Use Amazon S3 notifications and AWS Lambda to prepare files in Amazon S3 with appropriate data for each suppliers roles. Generate temporary AWS credentials for the suppliers users with AWS STS. Limit access to the appropriate files through security policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the tracking data to Amazon Kinesis Streams. Use Amazon EMR with Spark Streaming to store the data in HBase. Create one table per supplier. Use HBase Kerberos integration with the suppliers users. Use HBase ACL-based security to limit access for the roles to their specific table and columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the tracking data to Amazon Kinesis Firehose. Store the data in an Amazon Redshift cluster. Create views for the suppliers users and roles. Allow suppliers access to the Amazon Redshift cluster using a user limited to the applicable view. B"
    ],
    "answer": "Explanation",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-28T04:33:00.000Z",
        "voteCount": 9,
        "content": "A is the solution for the least amount of work. Quick and secure access to Dynamo db using dynamodb:LeadingKeys see: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html"
      },
      {
        "date": "2021-10-28T05:18:00.000Z",
        "voteCount": 6,
        "content": "Upon further research I realized that you can use condition keys on the permission policy attached to the IAM role that is used to access Dynamo DB using temporary IAM user credentials. So A is the right answer not B."
      },
      {
        "date": "2021-11-04T16:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. Dynamodb has fine grain access control (FGAC) which further integrated with IAM"
      },
      {
        "date": "2021-11-01T05:18:00.000Z",
        "voteCount": 3,
        "content": "A is possible, look at this: https://stelligent.com/2016/07/12/cross-account-access-control-with-amazon-sts-for-dynamodb/\nAfter read this article I consider A or D, but this is OLTP solution that A should be the right answer."
      },
      {
        "date": "2021-10-31T10:50:00.000Z",
        "voteCount": 1,
        "content": "D is right answer as it says access to specific fields. S3 access is object level."
      },
      {
        "date": "2021-11-02T16:13:00.000Z",
        "voteCount": 1,
        "content": "requires the LEAST amount of management work"
      },
      {
        "date": "2021-11-07T07:43:00.000Z",
        "voteCount": 1,
        "content": "D is wrong - you have thousands of suppliers - it means that you have to create thousands of views"
      },
      {
        "date": "2021-10-27T23:41:00.000Z",
        "voteCount": 2,
        "content": "Answer : B\nNot A- Using temporary credentials you cannot access a Dynamo DB table using a fine-grained authorization policy. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html\nNot C \u2013 Its not manageable to create one table per supplier as there are thousands of supplier as per the question.\nNot D- This is a possible solution but not with least amount of management. You will need to create as many view as there are supplier roles in RedShift. \nB- This is the best choice- It uses the out-of-the-box STS functionality to associate the appropriate IAM Role to the temp user credentials for the user who logs in  via SAML federation into AWS and then grants access to the files using resource based polices in S3."
      },
      {
        "date": "2021-11-05T03:12:00.000Z",
        "voteCount": 1,
        "content": "Not TRUE -&gt; Assume Role where the role has permission to selected subset of table data\nBTW there is no information about restrictions applied to temporary credentials, this way Federated Users and AWS Services would not be able to work"
      },
      {
        "date": "2021-10-27T19:41:00.000Z",
        "voteCount": 1,
        "content": "B. least amount of work"
      },
      {
        "date": "2021-10-24T15:15:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2021-10-23T08:44:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\nB&amp;C&amp;D are out because it's  a oltp scene\nany thoughts?"
      },
      {
        "date": "2021-10-22T10:29:00.000Z",
        "voteCount": 1,
        "content": "my selection D"
      },
      {
        "date": "2021-10-18T05:15:00.000Z",
        "voteCount": 1,
        "content": "D seems to be best answer -- \n\nHere are some of the things that you can build using fine-grained access control:\n\n    A mobile app that displays information for nearby airports, based on the user\u2019s location. The app can access and display attributes such airline names, arrival times, and flight numbers. However, it cannot access or display pilot names or passenger counts.\n    A mobile game which stores high scores for all users in a single table. Each user can update their own scores, but has no access to the other ones.\nhttps://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/"
      },
      {
        "date": "2021-10-22T02:20:00.000Z",
        "voteCount": 1,
        "content": "Do you mean B?"
      },
      {
        "date": "2021-10-17T00:20:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nthe LEAST amount of management work = Firehose + S3 + Lambda (Automated)\n the appropriate level of access control = AWS STS + security policies"
      },
      {
        "date": "2021-10-09T08:15:00.000Z",
        "voteCount": 5,
        "content": "There is no need to store data..we need only latest data at any point of time.\nOnce delivered - that timestamp has to be persisted. So Firehose and S3 combination can be eliminated. Left with A and C....\nIn DynamoDB, you have the option to specify conditions when granting permissions using an IAM policy (see Access Control). For example, you can:\n\nGrant permissions to allow users read-only access to certain items and attributes in a table or a secondary index.\n\nGrant permissions to allow users write-only access to certain attributes in a table, based upon the identity of that user.\nso A..."
      },
      {
        "date": "2021-10-08T22:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-10-08T20:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct.\nFirehose cannot put records directly into Redshift.\nhttps://docs.aws.amazon.com/ja_jp/firehose/latest/dev/what-is-this-service.html#data-flow-diagrams"
      },
      {
        "date": "2021-10-18T16:46:00.000Z",
        "voteCount": 2,
        "content": "Actually you can load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk"
      },
      {
        "date": "2021-10-01T18:40:00.000Z",
        "voteCount": 3,
        "content": "I vote for D\n\"you can assign a different set of permissions to the view. A user might be able to query the view, but not the underlying table. Creating the view excluding the sensitive columns (or rows) should be useful in this scenario.\"\n\nhttp://www.silota.com/blog/rethink-database-views-redshift/"
      },
      {
        "date": "2021-10-01T12:39:00.000Z",
        "voteCount": 4,
        "content": "D no doubts"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/1995-exam-aws-certified-big-data-specialty-topic-1-question-65/",
    "body": "A companys social media manager requests more staff on the weekends to handle an increase in customer contacts from a particular region. The company needs a report to visualize the trends on weekends over the past 6 months using QuickSight.<br>How should the data be represented?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA line graph plotting customer contacts vs. time, with a line for each region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA pie chart per region plotting customer contacts per day of week",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA map of regions with a heatmap overlay to show the volume of customer contacts",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA bar graph plotting region vs. volume of social media contacts"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-13T05:44:00.000Z",
        "voteCount": 9,
        "content": "Agreed A. Line is the best to show change over a long time period"
      },
      {
        "date": "2021-10-14T02:21:00.000Z",
        "voteCount": 4,
        "content": "Answer is A: Use line charts to compare changes in measure values over period of time \nhttps://docs.aws.amazon.com/quicksight/latest/user/line-charts.html"
      },
      {
        "date": "2021-10-23T03:20:00.000Z",
        "voteCount": 2,
        "content": "C because it focus on customer contacts for one region not all regions (a particular region)"
      },
      {
        "date": "2021-10-28T23:39:00.000Z",
        "voteCount": 3,
        "content": "you mean A?"
      },
      {
        "date": "2021-10-19T21:25:00.000Z",
        "voteCount": 4,
        "content": "my selection A"
      },
      {
        "date": "2021-10-17T18:55:00.000Z",
        "voteCount": 3,
        "content": "Ans is  A\nThe concern is the workload on the weekend, so a time-series chart is required"
      },
      {
        "date": "2021-10-16T14:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, line graphs are for trends"
      },
      {
        "date": "2021-10-04T16:00:00.000Z",
        "voteCount": 4,
        "content": "Why not A? How trend can be shown on geographical map?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "1"
  }
]