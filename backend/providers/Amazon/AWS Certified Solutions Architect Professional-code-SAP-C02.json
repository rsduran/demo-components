[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/90837-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.<br>The company has the following DNS resolution requirements:<br>On-premises systems should be able to resolve and connect to cloud.example.com.<br>All VPCs should be able to resolve cloud.example.com.<br>There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.<br>Which architecture should the company use to meet these requirements with the HIGHEST performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 55,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T21:08:00.000Z",
        "voteCount": 53,
        "content": "A. Correct answer. Source: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/\n\nNOT B. EC2 conditional forwarder will not meet Highest performance requirement.\n\nNOT C. Missing: Need to associate private hosted zone to all VPC.\n\"All VPC\u2019s will need to associate their private hosted zones to all other VPC\u2019s if required to.\"\nSource: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/\n\nNOT D. Missing: Need to associate private hosted zone to all VPC.\n\"All VPC\u2019s will need to associate their private hosted zones to all other VPC\u2019s if required to.\"\nSource: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/"
      },
      {
        "date": "2024-10-10T01:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer "
      },
      {
        "date": "2024-02-24T15:12:00.000Z",
        "voteCount": 1,
        "content": "In your link, you missed this sentence:\n\"The most reliable, performant and low-cost approach is to share and associate private hosted zones directly to all VPCs that need them.\" You share the PHZ via the Shared Services VPC. You use the .2 DNS Resolver Address in each VPC to connect to the PHZ in the shared services VPC for domain resolution."
      },
      {
        "date": "2024-03-27T04:24:00.000Z",
        "voteCount": 1,
        "content": "You forgot an additional condition mentioned in the question: \"All VPCs should be able to resolve cloud.example.com.\" Nobody said there are only shared VPCs there."
      },
      {
        "date": "2022-12-09T11:52:00.000Z",
        "voteCount": 9,
        "content": "A because it requires all VPC can resolve the example.com. All VPCs must be associated with private hosted zone"
      },
      {
        "date": "2024-10-15T22:22:00.000Z",
        "voteCount": 1,
        "content": "1-1. Private hosted zone One Account -&gt; 2 Account PHZ is not Equals.\n1-2. VPCs in Private hosted zone\n2. On-Premise -&gt; AWS Domain Name Query [ Route 53 Resolver ]\n3. Private hosted zone - Route 53 Resolver \n"
      },
      {
        "date": "2024-10-15T22:28:00.000Z",
        "voteCount": 1,
        "content": "Route 53 Resolver : inbound"
      },
      {
        "date": "2024-10-15T22:58:00.000Z",
        "voteCount": 1,
        "content": "When I organized it slowly, I decided that it was \"A\" because it was attributed to an account, not a VPC."
      },
      {
        "date": "2024-10-02T06:54:00.000Z",
        "voteCount": 1,
        "content": "\"All VPCs and only need inbound Resolver\""
      },
      {
        "date": "2024-09-26T13:12:00.000Z",
        "voteCount": 2,
        "content": "All options mention a Shared Services VPC that is not in the question. This is used for Route 53 for awsdumps.com\n\nOption A - Associating all VPCs with the private hosted zone allows resolution of cloud.example.com; an inbound resolves allows on-premise resource to resolve to cloud.example.com; the final bit of connectivity allows on-premise to connect and resolve to cloud.example.com\n\nOption B - An Amazon EC2 Conditional Forwarder does not apply in this situation because an Active Directory is not in play in this situation\n\nOption C - Would not work because it is relying on an Outbound resolver (from cloud to on-premise) \n\nOption D - Would not work because the other VPCs are not connected to the private zone. Moreover, connectivity is not complete because only the Shared Services VPC is connected to the Transit Gateway"
      },
      {
        "date": "2024-09-26T11:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is A: Please see link below for the solution:\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-cloud-dns-options-for-vpc/route-53-resolver-endpoints-and-forwarding-rules.html"
      },
      {
        "date": "2024-09-22T21:10:00.000Z",
        "voteCount": 1,
        "content": "The correct option would be option A:\n\nAssociate the private hosted zone to all the VPCs.\nCreate a Route 53 inbound resolver in the shared services VPC.\nAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.\nThis option will allow the on-premises systems to resolve and connect to cloud.example.com by forwarding the DNS queries to the inbound resolver in the shared services VPC, which will then forward the queries to the private hosted zone. All VPCs will be able to resolve cloud.example.com by resolving the queries through the private hosted zone associated to all VPCs. Additionally, this option takes advantage of the already existing AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway, which will provide the highest performance."
      },
      {
        "date": "2024-09-22T21:10:00.000Z",
        "voteCount": 1,
        "content": "The best architecture to meet the given requirements with the HIGHEST performance would be Option A:\n\nA. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.\n\nThis architecture ensures that all VPCs can resolve the cloud.example.com domain using the private hosted zone. Additionally, it creates a Route 53 inbound resolver in the shared services VPC that can handle DNS resolution requests from on-premises systems through the transit gateway. This setup allows for fast and efficient DNS resolution with minimal latency."
      },
      {
        "date": "2024-09-22T21:09:00.000Z",
        "voteCount": 3,
        "content": "All options mention a Shared Services VPC that is not in the question. This is used for Route 53 for cloud.example.com.\n\nOption A - Associating all VPCs with the private hosted zone allows resolution of cloud.example.com; an inbound resolves allows on-premise resource to resolve to cloud.example.com; the final bit of connectivity allows on-premise to connect and resolve to cloud.example.com\n\nOption B - An Amazon EC2 Conditional Forwarder does not apply in this situation because an Active Directory is not in play in this situation\n\nOption C - Would not work because it is relying on an Outbound resolver (from cloud to on-premise) \n\nOption D - Would not work because the other VPCs are not connected to the private zone. Moreover, connectivity is not complete because only the Shared Services VPC is connected to the Transit Gateway"
      },
      {
        "date": "2024-09-22T21:09:00.000Z",
        "voteCount": 2,
        "content": "To achieve the highest performance hybrid DNS solution, the company should associate a Route 53 private hosted zone with \"cloud.example.com\" to all VPCs, then create a Route 53 inbound resolver in a shared services VPC. This inbound resolver is connected to the on-premises network via AWS Direct Connect and Transit Gateway, allowing on-premises systems to resolve the private hosted zone. Forwarding rules on the on-premises DNS server direct queries for \"cloud.example.com\" to the inbound resolver, ensuring seamless resolution for both on-premises and cloud resources."
      },
      {
        "date": "2024-09-22T21:08:00.000Z",
        "voteCount": 1,
        "content": "A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
      },
      {
        "date": "2024-09-22T21:08:00.000Z",
        "voteCount": 1,
        "content": "\"Inbound DNS resolution \u2013 Create Route 53 Resolver inbound endpoints in a centralized VPC and associate all the private hosted zones in your Landing Zone with this centralized VPC.\" Source: https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/dns.html"
      },
      {
        "date": "2024-08-18T01:44:00.000Z",
        "voteCount": 1,
        "content": "Inbound resolver + private zone"
      },
      {
        "date": "2024-07-24T20:33:00.000Z",
        "voteCount": 1,
        "content": "Correct the answer: A"
      },
      {
        "date": "2024-06-16T06:30:00.000Z",
        "voteCount": 1,
        "content": "The 2nd requirement in the question is \"All VPCs should be able to resolve cloud.example.com.\" So the answer is A, not D which is only one VPC not all VPCs."
      },
      {
        "date": "2024-05-14T07:20:00.000Z",
        "voteCount": 1,
        "content": "You need to associate the private hosted zone to all the VPCs for them to be able to use it for DNS resolution."
      },
      {
        "date": "2024-04-07T02:47:00.000Z",
        "voteCount": 1,
        "content": "i think A."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/90937-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T19:39:00.000Z",
        "voteCount": 14,
        "content": "C.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html"
      },
      {
        "date": "2023-04-24T23:01:00.000Z",
        "voteCount": 2,
        "content": "Step1 - set up resources - Route 53 failover DNS records for the domain names"
      },
      {
        "date": "2024-09-22T21:10:00.000Z",
        "voteCount": 8,
        "content": "The best solution to give the API the ability to fail over to a different AWS Region would be option C:\n\nC. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.\n\nThis solution involves deploying a new API Gateway API and Lambda functions in another region. The company should also convert the DynamoDB tables to global tables to enable cross-region replication of the data. Then, the company should change the Route 53 DNS record to a failover record and enable target health monitoring to automatically route traffic to the new region in the event of a failure or outage in the primary region."
      },
      {
        "date": "2024-09-22T21:11:00.000Z",
        "voteCount": 3,
        "content": "The solution that will meet these requirements is option C:\n\nDeploy a new API Gateway API and Lambda functions in another Region.\nChange the Route 53 DNS record to a failover record.\nEnable target health monitoring.\nConvert the DynamoDB tables to global tables.\n\nThis solution will allow the API to failover to a different region, by using Route 53 failover record. The failover record will direct traffic to the primary API endpoint (the one in the primary region) as long as it is healthy. If the primary endpoint becomes unavailable, traffic will be directed to the secondary endpoint (the one in the secondary region). Additionally, by converting the DynamoDB tables to global tables, the data will be available in both regions, which is required for the failover scenario. Target health monitoring can be used to monitor the health of the API Gateway, and when it is determined that the primary endpoint is unavailable, the traffic will be directed to the secondary endpoint."
      },
      {
        "date": "2024-09-22T21:11:00.000Z",
        "voteCount": 3,
        "content": "I also agree with C. But not sure why not B, B is actually pretty good option. No, that I have experience in this specific case; what I normally see is Active/Standby. But option B sounds good because, in theory, we need to have both regions running the current code (Lambda) and if an outage happens we are sure both work, and we don't have stale config/code in the failover region. Sometimes multi-answer does not return the best endpoint for the use case, so that could be something against this solution."
      },
      {
        "date": "2024-09-22T21:11:00.000Z",
        "voteCount": 1,
        "content": "The answer is B.\nA: There is no Route 53, so it cannot be switched in the event of a failure.\nC: It's good to change to a failover record, but compared to other questions, there is no step to add a DNS record answer, so you can't switch to a new region.\nD: The global function is meaningless.\n\nB: A health check is additionally set, and failover is possible because the corresponding records are not returned in the event of a region failure.\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-configuring.html"
      },
      {
        "date": "2024-09-22T21:11:00.000Z",
        "voteCount": 3,
        "content": "Not A. \"edge-optimized API endpoint\" make use of CloudFront to optimize global each, however API Gateway instance is deployed in a single region thus no ability to fail over to a different AWS Region\nNot B. \"Route 53 DNS record to a multivalue\" implements a active-active scenario, while we are requested to have fail over\nNot D. I am not aware of \"global function\" also \"Route 53 DNS record to a multivalue\" is not the best fit (see above)\n\nThus C. is correct has it come with all the required pieces"
      },
      {
        "date": "2024-09-22T21:11:00.000Z",
        "voteCount": 5,
        "content": "Option A - Does not provide a way to fail over to a new region but rather a way for API gateway to respond from the region closest to the client\n\nOption B - Does not provide a way to fail over to a new region because when the main region is healthy name resolution will provide 2 possible regions to connect to\n\nOption C - Provides a way to fail over to a new region through the use of a Route 53 failover record and health monitoring and deployment in another region\n\nOption D -  Does not provide a way to fail over to a new region because when the main region is healthy name resolution will provide 2 possible regions to connect to"
      },
      {
        "date": "2024-09-22T21:10:00.000Z",
        "voteCount": 3,
        "content": "To achieve automatic failover for the weather API, the company should deploy a duplicate API Gateway and Lambda functions in a secondary AWS region, then configure a Route 53 failover record that points to both endpoints. This failover record, combined with health checks, will automatically redirect traffic to the secondary region if the primary one fails. Additionally, converting DynamoDB tables to global tables ensures data availability in both regions, allowing the secondary API to function seamlessly during a failover."
      },
      {
        "date": "2024-08-31T01:38:00.000Z",
        "voteCount": 1,
        "content": "D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."
      },
      {
        "date": "2024-06-16T06:57:00.000Z",
        "voteCount": 1,
        "content": "The changes of A and C are too much, breaking the original security design.\nB is wrong because answer B doesn't mention deny SCP on root level is changed. Allow on OU will not win because when allow and deny the same service, explicit deny always wins for the sake of security concerns."
      },
      {
        "date": "2024-05-31T18:59:00.000Z",
        "voteCount": 2,
        "content": "C, failover health"
      },
      {
        "date": "2024-03-16T08:04:00.000Z",
        "voteCount": 1,
        "content": "C, failover record, this is the typical failover configuration on route53. Be careful, chatgpt suggests the option B \"multivalue answer\""
      },
      {
        "date": "2024-03-12T05:23:00.000Z",
        "voteCount": 3,
        "content": "Choosing C cause you want the API GW and Lambda functions work as a combination behind the DNS with failover, can think of Route53 here as a CDN provider like Cloudflare"
      },
      {
        "date": "2023-11-26T10:27:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-11T01:15:00.000Z",
        "voteCount": 1,
        "content": "failover is required"
      },
      {
        "date": "2023-10-01T18:12:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-09-27T04:36:00.000Z",
        "voteCount": 1,
        "content": "C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/90824-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.<br>The company recently acquired a new business unit and invited the new unit\u2019s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company\u2019s policies.<br>Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the organization\u2019s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company\u2019s standard AWS Config rules and deploy them throughout the organization, including the new account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the organization\u2019s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization\u2019s root that allows AWS Config actions for principals only in the new account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization\u2019s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-09T08:57:00.000Z",
        "voteCount": 49,
        "content": "Right answer is D.\nAn SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can only filter; they never add permissions.\nSO you need to create a new OU for the new account assign an SCP, and move the root SCP to Production OU. Then move the new account to production OU when AWS config is done."
      },
      {
        "date": "2022-12-10T19:45:00.000Z",
        "voteCount": 17,
        "content": "Answer: D.\n\nNot A: too much overhead and maintenance.\nNot B: SCP at Root will still deny Config to the temporary OU.\nNot C: Too much overhead to create allow list."
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 1,
        "content": "Yes, in option D, the solution is to create a temporary OU named Onboarding for the new account. By creating a new OU for the new account, it allows for a new set of permissions and policies to be applied to this account, separate from the existing Production OU.\n\nOnce the new OU is created, an SCP is applied to it to allow AWS Config actions. This SCP allows the new account to make necessary adjustments to AWS Config without being blocked by the existing policies at the root level of the organization.\n\nThen, the root SCP that is blocking these actions is moved to the Production OU, where it will continue to block these actions for all other accounts that are members of the Production OU.\n\nFinally, once the necessary adjustments are made, the new account can be moved to the Production OU, where it will be subject to the existing policies and restrictions."
      },
      {
        "date": "2023-01-13T09:43:00.000Z",
        "voteCount": 1,
        "content": "This approach is the correct solution because it allows the new account to make necessary adjustments to AWS Config while still adhering to the company's policies, and it does not introduce additional long-term maintenance. The new account will be only in the new OU temporarily, and the SCP blocking AWS Config actions will only be in the root temporarily."
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 1,
        "content": "The best option to allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance would be option D:\n\nD. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization\u2019s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.\n\nThis solution involves creating a temporary OU named Onboarding for the new account and applying an SCP to the Onboarding OU that allows AWS Config actions. The organization's root SCP should be moved to the Production OU, and the new account should be moved to the Production OU when the adjustments to AWS Config are complete. This approach allows the administrators of the new account to make changes to AWS Config rules while maintaining the current policies in the Production OU."
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 1,
        "content": "Please note Question Constraint: Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?\nStrategies for using SCPs\nYou can configure the service control policies (SCPs) in your organization to work as either of the following:\nA deny list \u2013 actions are allowed by default, and you specify what services and actions are prohibited\nAn allow list \u2013 actions are prohibited by default, and you specify what services and actions are allowed."
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 6,
        "content": "D: is not correct, because removing the root SCPs on the production OU means removing all the security rules on the services preventing changes, including changes to the AWS Config rules. and depending on the scenario this will be a security hole for production.\n\nDon't forget that the aim is to introduce the new AWS account into the Production OU with the same configurations and restrictions as the accounts that are already there.\n\nSo thanks to the temporary OU on which we have an SCP that authorises actions on AWS Config, we just need to modify the configuration of the new account so that it matches the production requirements. Once the configuration requirements have been met, we move the new account into the production OU."
      },
      {
        "date": "2023-08-31T04:05:00.000Z",
        "voteCount": 3,
        "content": "\" All accounts are members of the Production OU\", therefore we don't need the SCP in root."
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 2,
        "content": "Chosen D.\nB is not correct because root having explicit deny will override any explicit allow in its child OU even if allowance is given. Unless I keep Onboarding account under a parent where there is not explicit deny for Config service, Onboarding account can not configure. So, need to move the explicit deny from root account to production account and then keep onboarding account under root."
      },
      {
        "date": "2024-09-22T21:12:00.000Z",
        "voteCount": 2,
        "content": "This question is ambiguous. If D was formulated like this:\n\n\"D. Create a temporary OU named Onboarding for the new account. Apply a Config non-blocking SCP to the Onboarding OU to allow AWS Config actions. Apply the organization\u2019s root SCP to the Production OU instead of to the root OU. Move the new account to the Production OU when adjustments to AWS Config are complete.\"\n\nThen D would be a viable option. However, it isn't, and even if it were, it fails to mention the crucial fact that the Root OU always must have an SCP, which in this case must Allow everything. For someone with some experience this is a given, but as it isn't mentioned, I'd go for B.\n\nHowever, AWS should reformulate the question and the answers. They are really subpar."
      },
      {
        "date": "2024-03-06T02:33:00.000Z",
        "voteCount": 3,
        "content": "AWS Config will still be restricted despite the Allow SCP in Onboarding because of the Deny SCP in the root of the organization"
      },
      {
        "date": "2024-04-25T05:17:00.000Z",
        "voteCount": 1,
        "content": "This sentence:\n\n\"Apply the organization\u2019s root SCP to the Production OU instead of to the root OU.\"\n\nsolves the issue you mentioned. You can safely move this SCP as the question states that all AWS accounts are in Production OU."
      },
      {
        "date": "2024-09-22T21:12:00.000Z",
        "voteCount": 1,
        "content": "I don't like any of the answers to be honest. Let's look at D since that's the one most people think is right. The problem with D is that you can't detach the last SCP associated with a root container, OU, or account. There has to be at least one. So, removing the SCP from the root and moving it down to the Production OU is a no-go unless you add a permissable SCP to the root. Check the section on detaching here: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html\n\nThe only way B is correct is if the reason the new admins don't have access to Config is not because Config is in the Deny List, but because the management account doesn't have the appropriate IAM Policy giving PERMISSION to Config. You need both an IAM Policy and a permissable SCP to have permission and access to a service. But, why wasn't IAM Policy mentioned in choice B. Clearly, without that information, choice B also is not right."
      },
      {
        "date": "2024-02-24T17:11:00.000Z",
        "voteCount": 1,
        "content": "Also, even if you could remove a root SCP, you would never do that in production. You would never just flat remove an SCP with a Deny list just to give one account access to some service. Even if it's temporary, that's a fatal mistake as the other accounts will not be restricted from certain services they shouldn't have access to."
      },
      {
        "date": "2024-02-24T17:17:00.000Z",
        "voteCount": 1,
        "content": "The question mentioned a Deny List architecture, but it didn't specifically say Config was in the Deny List. We are assuming that, which could lead to the wrong answer. Unfortunately, I'm not satisfied with any of the answers. Hopefully, this is a question that would be thrown out from the exam. LOL."
      },
      {
        "date": "2024-09-22T21:12:00.000Z",
        "voteCount": 2,
        "content": "This was not easy for me due to wording, however here is my take:\n\nNot A. here we permanently remove SCPs that limit access to AWS Config, while we are requested to continue to enforce the current policies\nNot B. temporary OU and related SCP that allows AWS Config are nested under root where SCPs that limit access to AWS Config are applied. As SCP can only remove permission and not add, this will not work\nNot C. converting deny list into allow list here is not beneficial also temporarily apply SCP allowing AWS Config does not meet the request to avoid additional long-term maintenance.\n\nThus D does the job."
      },
      {
        "date": "2024-09-22T21:12:00.000Z",
        "voteCount": 1,
        "content": "Option A - This option actually rolls out AWS Config across the company which is exactly the opposite of what they are doing\n\nOption B - This option does not work because AWS Config will still be restricted despite the Allow SCP in Onboarding because of the Deny SCP in the root of the organization\n\nOption C - This option allows access to AWS Config in the new business unit and restricts access to everything else. However, the SCP will require regular updates to add new AWS services\n\nOption D - This option applies the correct level of access to each OU without needing updates: Onboarding gets access to AWS Config, Production does not and FullAWSAccess is established at the root after the company's Deny SCP is moved."
      },
      {
        "date": "2024-09-22T21:12:00.000Z",
        "voteCount": 1,
        "content": "The question itself is a bit confusing. It says \"Deny List in the root\", which should be understood as Deny List Architecture, but can be misinterpreted as \"Allow List Architecture with attached Deny List in the root that explicitly deny AWS Config\". Since AWS Config on Production OU is denied, an appropriate SCP is attached to it, which explicitly denies AWS Config. Thus, the root has FullAWSAccess SCP attached to it. That's why we just need to create Onboarding OU with no explicit deny of AWS Config and that's it. So the correct answer is truly correct, but the question is a bit tricky and easy to misunderstand."
      },
      {
        "date": "2024-09-22T21:12:00.000Z",
        "voteCount": 3,
        "content": "Option B. \nIf a \"Deny\" list SCP is applied at the root of the organization to restrict access to a service, and then a new SCP is created at a lower level (e.g., an Organizational Unit or OU) to \"Allow\" access to that restricted service, the permissions are cumulative.\n\nSo, if an account is placed under the Test OU, it will inherit the permissions from both SCPs. Since the \"Allow\" SCP at the Test OU level overrides the \"Deny\" SCP at the root level, the account under the Test OU will effectively have access to the restricted service.\n\nThis is because SCPs are evaluated hierarchically, with SCPs at higher levels in the organizational structure being evaluated first, followed by SCPs at lower levels. When there are conflicting SCPs, the most permissive policy (i.e., the one that allows access) takes precedence."
      },
      {
        "date": "2024-09-17T22:39:00.000Z",
        "voteCount": 1,
        "content": "D is the best match !"
      },
      {
        "date": "2024-08-31T01:40:00.000Z",
        "voteCount": 1,
        "content": "D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization\u2019s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete."
      },
      {
        "date": "2024-08-01T22:53:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2024-04-01T07:22:00.000Z",
        "voteCount": 2,
        "content": "Option D:  The link doesn't give you a full explanation on why \"D\" is correct however it does check all the boxes \n \nhttps://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/transitional-ou.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/90976-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application\u2019s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.<br>Which solution will provide a consistent user experience that will allow the application and database tiers to scale?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T20:19:00.000Z",
        "voteCount": 26,
        "content": "C.\n- Aurora writers is a distractor.\n- Single master mode only has read replica - with Aurora replicas.\n- Multi master mode, not in the options\n- NLB does not support round robin and least outstanding algorithm\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html"
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 15,
        "content": "The best solution to provide a consistent user experience that will allow the application and database tiers to scale would be option C:\n\nC. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.\n\nThis solution involves enabling Aurora Auto Scaling for Aurora Replicas to automatically add and remove read replicas to match the application's workload. The solution also uses an Application Load Balancer to distribute traffic to the application layer, with the round robin routing algorithm to balance the traffic evenly across multiple instances. Sticky sessions should be enabled to maintain session affinity for each user, allowing for a consistent user experience."
      },
      {
        "date": "2024-09-22T21:14:00.000Z",
        "voteCount": 2,
        "content": "C is correct. This solution will provide a consistent user experience by using an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled. This allows the application and database tiers to scale by using Aurora Auto Scaling for Aurora Replicas. This will ensure that the application is able to handle the increased user base while maintaining a consistent user experience. The use of an Application Load Balancer also allows for better routing of traffic to the available Aurora Replicas."
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 2,
        "content": "- Auto Scaling for Aurora writers does not exists (distractor)\n- NLB does not support least outstanding requests routing algorithm (it only supports Flow Hash)\n- NLB does not allow to enable Sticky Sessions, this is always enabled with Flow Hash where each TCP/UDP connection is routed to a single target for the life of the connection\n\nThus C is correct"
      },
      {
        "date": "2024-09-22T21:13:00.000Z",
        "voteCount": 3,
        "content": "Option A - Allows the tiers to grow but NLB does not make load balancing decisions that way\n\nOption B - No such thing as Aurora Autoscaling for Aurora Writers\n\nOption C - Allows the tiers to grow and ALB using sticky sessions provides consistent user experience\n\nOption D - No such thing as Aurora Autoscaling for Aurora Writers\n\nNote: The application is web-based so choosing ALB shouldn't be an issue."
      },
      {
        "date": "2024-08-31T01:41:00.000Z",
        "voteCount": 1,
        "content": "D. Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled."
      },
      {
        "date": "2024-06-13T04:57:00.000Z",
        "voteCount": 1,
        "content": "C, Enable Aurora Auto Scaling for Aurora Replicas"
      },
      {
        "date": "2024-03-16T08:21:00.000Z",
        "voteCount": 1,
        "content": "C, Enable Aurora Auto Scaling for Aurora Replicas"
      },
      {
        "date": "2024-03-12T05:47:00.000Z",
        "voteCount": 4,
        "content": "Single writer: In an Aurora PostgreSQL DB cluster, there is only one writer instance at a time. All write operations, such as INSERT, UPDATE, and DELETE statements, are directed to the writer instance."
      },
      {
        "date": "2024-02-14T21:03:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2023-12-30T14:07:00.000Z",
        "voteCount": 2,
        "content": "B, D are distractor, as there is no writer replica in aurora autoscale.\nNLB does not support sticky session so A is out.  The anwser is C."
      },
      {
        "date": "2024-02-17T01:22:00.000Z",
        "voteCount": 2,
        "content": "NLB Sticky Session: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#sticky-sessions"
      },
      {
        "date": "2023-11-26T10:29:00.000Z",
        "voteCount": 1,
        "content": "C Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky session"
      },
      {
        "date": "2023-11-11T01:44:00.000Z",
        "voteCount": 1,
        "content": "Aurora - AS only for read replicas. NLB doesn't support the least outstanding requests or round-robin algorithms, only flow hash is supported."
      },
      {
        "date": "2023-09-27T13:29:00.000Z",
        "voteCount": 1,
        "content": "C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled."
      },
      {
        "date": "2023-09-07T17:24:00.000Z",
        "voteCount": 2,
        "content": "NLB scales better than ALB. Also least outstandind requests algorithm works better than round robin algorith. Any thougts?"
      },
      {
        "date": "2023-09-09T13:40:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is whatever the examiner says it is. Depending on how you look at it either A or C can be the correct answer. \nNLB scales better and supports LOR algorithm which are both factors in its favor, however stickiness is not supported for TLS connections in NLBs. While this has not been called out explicitly, I doubt anyone in today's world would support non-TLS connections to their applications. If that turns out to be a dealbreaker, then the only option is C, to use ALB, however round-robin doesn't guarantee the best performance especially where stickiness is concerned. \nYour call."
      },
      {
        "date": "2023-08-25T12:50:00.000Z",
        "voteCount": 2,
        "content": "write replica is distractor. NLB does not support round robin"
      },
      {
        "date": "2023-06-26T11:34:00.000Z",
        "voteCount": 1,
        "content": "it's C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/90939-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.<br>The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 78,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 56,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 31,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-15T17:13:00.000Z",
        "voteCount": 61,
        "content": "A. The only difference between A and D is CloudFront function vs Lambda@Edge. In this case the CloudFront function can remove the response header based on request header and much faster/light-weight."
      },
      {
        "date": "2023-08-20T18:35:00.000Z",
        "voteCount": 12,
        "content": "After read, answer A \"Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header\" not really clear and fuzzy, \"The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices\" =&gt; \"Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header\" =&gt; D make sence"
      },
      {
        "date": "2022-12-10T10:17:00.000Z",
        "voteCount": 27,
        "content": "I think this is answer D: Lambda@Edge can modify headers\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html"
      },
      {
        "date": "2023-08-20T18:35:00.000Z",
        "voteCount": 5,
        "content": "Agree D"
      },
      {
        "date": "2023-12-09T03:06:00.000Z",
        "voteCount": 2,
        "content": "Agree on D, but also CloudFront Function can manipulate headers https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html#:~:text=cache%20hit%20ratio.-,Header%20manipulation,-%E2%80%93%20You%20can%20insert"
      },
      {
        "date": "2024-09-29T04:04:00.000Z",
        "voteCount": 1,
        "content": "Lambda@Edge is integrated with CloudFront and allows you to manipulate requests and responses closer to the user, such as removing unsupported headers. This is ideal for addressing the issue with older devices by inspecting the User-Agent header and removing the problematic HTTP headers."
      },
      {
        "date": "2024-09-22T21:17:00.000Z",
        "voteCount": 4,
        "content": "A, because it's faster. \n\nCloudFront Functions vs. Lambda@Edge use cases\n* CF Functions:\n    * cache key normalization: transform request attributes (headers, cookies, query string, URL) to create an optimal Cache Key\n    * header manipulation: insert/modify/delete HTTP headers in the request or response\n    * URL rewrites or redirects\n    * request authentication &amp; authorization: create and validate user-generated tokens (e. g. JWT) to allow/deny requests\n* Lambda@Edge:\n    * longer execution time (several ms)\n    * adjustable CPU or memory\n    * 3rd party dependencies (like AWS SDK)\n    * network access to use external services for processing\n    * file system access or access to the body of HTTP requests"
      },
      {
        "date": "2024-09-22T21:17:00.000Z",
        "voteCount": 1,
        "content": "Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:\n\n- After CloudFront receives a request from a viewer (viewer request)\n\n- Before CloudFront forwards the request to the origin (origin request)\n\n- After CloudFront receives the response from the origin (origin response)\n\n- Before CloudFront forwards the response to the viewer (viewer response)"
      },
      {
        "date": "2023-08-16T04:43:00.000Z",
        "voteCount": 1,
        "content": "CF Functions can do this as well - why use Lambda@Edge when you can do it at 1/6th the price with CF Functions?"
      },
      {
        "date": "2024-09-22T21:17:00.000Z",
        "voteCount": 3,
        "content": "D: This solution involves creating an Amazon CloudFront distribution for the metadata service and configuring it to forward requests to the Application Load Balancer (ALB), which is used to invoke the correct Lambda function for each type of request. A Lambda@Edge function should be created that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header. This approach allows the company to remove the problematic headers while supporting older devices and using serverless technologies."
      },
      {
        "date": "2024-09-22T21:17:00.000Z",
        "voteCount": 3,
        "content": "D is correct \nThis solution uses Amazon CloudFront with an Application Load Balancer (ALB) and AWS Lambda@Edge to remove problematic headers based on the User-Agent header. CloudFront can be used as a content delivery network (CDN) to deliver the metadata service to consumer devices while the ALB is used to invoke the correct Lambda function for each type of request. Lambda@Edge is used to modify the response headers in real-time based on the User-Agent header.\n\nThis solution addresses the requirement to support older devices that do not support certain HTTP headers by removing problematic headers based on the value of the User-Agent header. It also leverages serverless technologies such as AWS Lambda and Lambda@Edge for scalability and cost-effectiveness."
      },
      {
        "date": "2024-09-22T21:16:00.000Z",
        "voteCount": 2,
        "content": "In this solution, you use CloudFront, ALB, Lambda@Edge, and Lambda functions to achieve the desired outcome.\n\nCreate an Amazon CloudFront distribution: CloudFront acts as a content delivery network (CDN) and allows you to distribute your metadata service globally. You can configure it to handle incoming requests and route them to the appropriate backend.\n\nCreate an Application Load Balancer (ALB): ALB is used as a target for CloudFront to forward requests. It provides advanced routing capabilities and can invoke the correct Lambda function based on the type of request.\nCreate a Lambda@Edge function: Lambda@Edge allows you to run Lambda functions at the CloudFront edge locations, closer to your users. Create a Lambda@Edge function that examines the User-Agent header of incoming requests and removes the problematic headers from the response when necessary. This ensures compatibility with older devices."
      },
      {
        "date": "2024-09-22T21:16:00.000Z",
        "voteCount": 3,
        "content": "I think thre are two problems in this situation:\n1- \"Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses.\"\n\n2- \"The company has already migrated the applications into a set of AWS Lambda functions\"\n\nThose two problems are addressed by using an API Gateway which 'selects the appropriate function for each type of request' and a mapping template which 'removes the unsupported headers'."
      },
      {
        "date": "2024-09-22T21:16:00.000Z",
        "voteCount": 6,
        "content": "This is really challenging for me. \n\nHere is my reasoning:\ni) user-agent header is stored in request and not in answer\nii) based on i) we need a mechanism to map sessionid to user-agent in requests and access this mapping when processing answers\n\nNot .A as CF Functions do not interact with other AWS services, they can use key value pairs but in read-only mode. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/functions-tutorial-kvs.html\nNot B. as gateway responses only works with \"supported response type\" listed here https://docs.aws.amazon.com/apigateway/latest/developerguide/supported-gateway-response-types.html (plese note the question mention errors, but they occur on devices)\nNot C. as response mapping template do not interact with other AWS services\n\nD. is correct as Lambda@Edge can access other AWS services (e.g. in this case a DynamoDB for sessionid user-agent mapping)"
      },
      {
        "date": "2024-09-22T21:16:00.000Z",
        "voteCount": 2,
        "content": "To migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices, the company can use AWS Application Load Balancer (ALB). The ALB is a serverless technology that can be used to route incoming traffic to serverless functions such as AWS Lambda. The Serverless Framework makes it possible to set up the connection between Application Load Balancers and Lambda functions with the help of the alb event.\n\nTo support the older devices, the company can configure the ALB to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers. The ALB\u2019s focus on HTTP allows it to use parts of the protocol to make decisions about caching and save you some Lambda executions."
      },
      {
        "date": "2024-09-22T21:16:00.000Z",
        "voteCount": 6,
        "content": "You guys are discussing CloudFront function and Lambda@Edge but the question says: \"adopt serverless technologies\". ALB is not a serverless service.\nI think C is the correct answer."
      },
      {
        "date": "2024-09-22T21:15:00.000Z",
        "voteCount": 2,
        "content": "Answer is A.\n\nCloudFront Functions is ideal for lightweight, short-running functions for use cases like the following:\nHeader manipulation \u2013 You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a True-Client-IP header to every request.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-choosing.html"
      },
      {
        "date": "2024-09-22T21:15:00.000Z",
        "voteCount": 1,
        "content": "The other options have drawbacks:\n\nA. CloudFront function is not available: CloudFront functions are not a supported feature.\nB &amp; C. API Gateway modification: These options require modifying responses at the API Gateway level. While achievable, they wouldn't process requests based on User-Agent headers before reaching the origin, potentially causing errors on older devices.\nBy utilizing Lambda@Edge, the company can:\n\nMaintain a serverless architecture with Lambda functions for core logic.\nFilter out unsupported headers close to the user, preventing errors on older devices.\nLeverage CloudFront's caching and edge locations for improved performance."
      },
      {
        "date": "2024-05-07T22:50:00.000Z",
        "voteCount": 2,
        "content": "\"CloudFront function is not available: CloudFront functions are not a supported feature.\"\nUhm no, CloudFront functions do exist, here's a comparison with Lambda@Edge:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-choosing.html"
      },
      {
        "date": "2024-09-22T21:15:00.000Z",
        "voteCount": 1,
        "content": "I'll take A as the correct answer.\n\nRequirements: Using AWS's Serverless feature, maintaining features that support previous devices, removing headers\n\nServerless: Lambda\nPrevious Device: ALB\nRemove Header: Cloudfront\nhttps://aws.amazon.com/ko/about-aws/whats-new/2023/01/amazon-cloudfront-supports-removal-response-headers/"
      },
      {
        "date": "2024-09-22T21:15:00.000Z",
        "voteCount": 1,
        "content": "Answer A and D are wrong. The question requires the serverless solution. Answer A and D introduce the non-serverless but AWS managed service, ALB. Serverless is a managed service but managed service is not necessary to be serverless. Once ALB is created no matter there is traffic or not, AWS charges ALB. Unlike serverless, lamba as example, it is charged according to the invoke. If serverless is not called then there is no change. \nAnswer C is wrong because the custom template is not supported by HTTP API gateway.\nSo the correct answer is B."
      },
      {
        "date": "2024-09-22T21:14:00.000Z",
        "voteCount": 1,
        "content": "after review details of question, correct answer is C.\nMainpoints are \n1. serverless, API GW with HTTP API is serverless while ALB is not.\n2. CloudFront Function is only for 'viewer request/response', not for 'origin request/response', So, A is not correct. see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-choosing.html\n3. B is also not correct because 'default GW response' is generated by API GW, but here to remove response from origin."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/90940-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).<br>Which combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Account A, set the S3 bucket policy to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image1.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Account A, set the S3 bucket policy to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image2.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Account B, set the permissions of User_DataProcessor to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image1.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Account B, set the permissions of User_DataProcessor to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image4.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T09:06:00.000Z",
        "voteCount": 30,
        "content": "Answer: C &amp; D\n\nSource: \nhttps://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html"
      },
      {
        "date": "2024-09-22T21:17:00.000Z",
        "voteCount": 18,
        "content": "C &amp; D\n\nTo allow User_DataProcessor to access the S3 bucket from Account B, the following steps need to be taken:\n\n    In Account A, set the S3 bucket policy to allow access to the bucket from the IAM user in Account B. This is done by adding a statement to the bucket policy that allows the IAM user in Account B to perform the necessary actions (GetObject and ListBucket) on the bucket and its contents.\n\n    In Account B, create an IAM policy that allows the IAM user (User_DataProcessor) to perform the necessary actions (GetObject and ListBucket) on the S3 bucket and its contents. The policy should reference the ARN of the S3 bucket and the actions that the user is allowed to perform.\n\nNote: turning on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A is not necessary for this scenario as it is typically used for allowing web browsers to access resources from different domains."
      },
      {
        "date": "2024-09-22T21:17:00.000Z",
        "voteCount": 1,
        "content": "Option A - CORS does not address cross-account access to S3 buckets\n\nOption B - This option would not work because the bucket policy is missing the Principal\n\nOption C - This option provides a valid S3 bucket policy that grants access to User_DataProcessor\n\nOption D - These permissions allow User_DataProcessor to get objects out of the bucket\n\nOption E - This option would not work because it is not a valid IAM policy"
      },
      {
        "date": "2024-08-31T01:44:00.000Z",
        "voteCount": 1,
        "content": "C. In Account A, set the S3 bucket policy to the following:\nD. In Account B, set the permissions of User_DataProcessor to the following:"
      },
      {
        "date": "2024-07-19T19:26:00.000Z",
        "voteCount": 1,
        "content": "The question says Choose two. The answer is C &amp; D."
      },
      {
        "date": "2024-06-12T21:36:00.000Z",
        "voteCount": 1,
        "content": "Ans C and D\n2 Options have to be selected"
      },
      {
        "date": "2024-06-12T21:35:00.000Z",
        "voteCount": 1,
        "content": "Ans - C and D\n2 Options have to be selected"
      },
      {
        "date": "2024-03-12T06:02:00.000Z",
        "voteCount": 1,
        "content": "Cross-Origin Resource Sharing (CORS) is a security feature in Amazon S3 that allows you to control access to your S3 resources from a different domain (origin) than the one serving the resources. CORS defines a way for client web applications running in one origin to interact with resources in a different origin, which is otherwise restricted by the same-origin policy enforced by web browsers."
      },
      {
        "date": "2024-03-01T05:13:00.000Z",
        "voteCount": 1,
        "content": "C and D."
      },
      {
        "date": "2024-02-25T11:29:00.000Z",
        "voteCount": 1,
        "content": "The answer is C and D. You need to give the IAM User in Account B an IAM Policy and you need to give a Bucket Policy in Account A.\n\nWho is maintaining this database of questions? Someone needs to seriously set the correct answers before making a lot of people confused and potentially screw up their exam."
      },
      {
        "date": "2024-02-05T02:12:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: C and D\nAdding my vote for D to balance the result\n\nModerator, please fix the vote in this ticket."
      },
      {
        "date": "2024-02-01T05:54:00.000Z",
        "voteCount": 1,
        "content": "why we need two steps? I think that we get only one from resource-based policy or identity-based policy."
      },
      {
        "date": "2024-01-24T14:04:00.000Z",
        "voteCount": 1,
        "content": "Answer C &amp; D"
      },
      {
        "date": "2023-12-03T18:22:00.000Z",
        "voteCount": 2,
        "content": "Answer - C &amp; D"
      },
      {
        "date": "2023-11-11T03:26:00.000Z",
        "voteCount": 4,
        "content": "C, D. D and not E, because it is an identity-based inline policy already attached to the specific principal."
      },
      {
        "date": "2023-11-06T09:25:00.000Z",
        "voteCount": 2,
        "content": "A,C\nAccess setting need to be done only on Account A as it's an owner. So Enabling Cross origin access and access to the bucket for account B IAM user."
      },
      {
        "date": "2023-10-18T22:37:00.000Z",
        "voteCount": 2,
        "content": "Answer : C&amp;D."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/90941-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.<br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 60,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T21:18:00.000Z",
        "voteCount": 20,
        "content": "B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.\nThis option meets the requirement of using a serverless architecture by utilizing the Fargate launch type for the ECS clusters, which allows for automatic scaling of the containers based on the expected load. It also allows for separate deployments for production and testing by configuring separate ECS clusters and Application Load Balancers for each environment. This option also minimizes operational complexity by utilizing ECS and Fargate for the container orchestration and scaling."
      },
      {
        "date": "2022-12-11T18:13:00.000Z",
        "voteCount": 14,
        "content": "Answer is A. ABC all works but A is most COST EFFECTIVE"
      },
      {
        "date": "2022-12-12T05:29:00.000Z",
        "voteCount": 3,
        "content": "Is true but \" you can now package and deploy Lambda functions as container images of up to 10 GB in size.\" the size is not specified, personally I find it too small"
      },
      {
        "date": "2023-02-17T16:18:00.000Z",
        "voteCount": 3,
        "content": "10GB image is too small for what? I'm curious how do you containerise those images?\nI'd say the average image size is ~300-400MB"
      },
      {
        "date": "2022-12-11T18:15:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/"
      },
      {
        "date": "2023-02-17T16:20:00.000Z",
        "voteCount": 5,
        "content": "Yes, would be cheap, but can't run a web app from Lambda"
      },
      {
        "date": "2023-01-01T08:00:00.000Z",
        "voteCount": 4,
        "content": "I do not think A is the right answer. \nBecause image must be upload to the ECR."
      },
      {
        "date": "2024-09-22T21:19:00.000Z",
        "voteCount": 3,
        "content": "Option B is the most cost-effective solution for the following reasons:\n\nThe use of Fargate, a serverless compute engine for containers, eliminates the need for managing and scaling the underlying infrastructure. This minimizes operational complexity and reduces costs as the resources are used only when required.\nAuto scaling ensures that the application scales up and down based on the load, providing the required performance and availability without incurring additional costs.\nAmazon ECS is a simpler and more cost-effective solution than Amazon EKS, which requires more management and additional resources to operate the Kubernetes control plane.\nUsing Application Load Balancers to direct traffic to the ECS clusters ensures high availability and fault tolerance."
      },
      {
        "date": "2023-02-22T10:28:00.000Z",
        "voteCount": 1,
        "content": "Changing to A, B is not serverless and cost-effective."
      },
      {
        "date": "2023-06-18T00:49:00.000Z",
        "voteCount": 2,
        "content": "Fargate is serverless by definition."
      },
      {
        "date": "2024-09-22T21:19:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\n\nAmazon ECS with Fargate: By uploading the container images to Amazon ECR and using Amazon ECS with the Fargate launch type, you can run the microservices in containers without having to manage the underlying infrastructure. Fargate automatically scales the containers based on the load.\nSeparate Production and Testing Environments: With two separate auto-scaled Amazon ECS clusters, you can have dedicated environments for production and testing, ensuring isolation and allowing for separate deployments and configurations.\nApplication Load Balancers (ALB): Configuring two separate ALBs allows you to direct traffic to the appropriate ECS clusters. This ensures proper routing of requests between the production and testing environments.\nOption B provides a cost-effective solution by utilizing the serverless nature of Fargate, which eliminates the need to provision and manage EC2 instances explicitly. It also allows for separate environments, easy scalability, and traffic routing using ALBs, providing flexibility and minimizing operational complexity."
      },
      {
        "date": "2024-09-22T21:19:00.000Z",
        "voteCount": 4,
        "content": "Option A - This option might not work. AWS Lambda provides a cheap option to run containers however nothing is said about execution times could be a concern, i.e. AWS Lambda only provides 15 minutes of execution time \n\nOption B - This option will work. ALB, ECR, ECS and Fargate in combination will deliver a running solution.\n\nOption C - This option will work. ALB, ECR, EKS and Fargate will deliver a running solution.\n\nOption D - This option will work: Beanstalk will rely on ECS to run the containers. See https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\n\nCheapest option is B."
      },
      {
        "date": "2024-08-31T01:46:00.000Z",
        "voteCount": 1,
        "content": "B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters."
      },
      {
        "date": "2024-08-10T03:58:00.000Z",
        "voteCount": 1,
        "content": "A because ALB &amp; Beanstalk are not serverless &amp; Lambda added support to use docker images directly. you can upload container images to AWS Lambda and use them as functions. AWS Lambda introduced support for deploying functions as container images, allowing you to package and deploy Lambda functions with custom runtimes, libraries, and dependencies that might exceed the limitations of traditional Lambda deployment packages (zip files)."
      },
      {
        "date": "2024-07-01T06:16:00.000Z",
        "voteCount": 1,
        "content": "NOT B and C, because ALB is not serverless architecture\nNOT D, because Beantalk is not serverless architecture\n\nA is also most effective"
      },
      {
        "date": "2024-06-22T03:13:00.000Z",
        "voteCount": 1,
        "content": "A - COST EFFECTIVE (lambda is only solution where users pay by invocation)"
      },
      {
        "date": "2024-05-10T19:42:00.000Z",
        "voteCount": 1,
        "content": "Once choose eks,do not config load balance manually"
      },
      {
        "date": "2024-03-29T06:31:00.000Z",
        "voteCount": 2,
        "content": "Option B and NOT Option C: I wasn't able to find a good comparison btw AWS ECS vs AWS EKS pricing in AWS documentation however I found  a few articles saying that AWS EKS has additional cost for using EKS control plane. I will leave it up to you to decide.  \n\nhttps://www.densify.com/eks-best-practices/aws-ecs-vs-eks/"
      },
      {
        "date": "2024-03-12T06:13:00.000Z",
        "voteCount": 1,
        "content": "AWS Elastic Beanstalk is not considered a serverless architecture. While it abstracts away some of the underlying infrastructure management, it still involves running and managing EC2 instances, which are virtual servers."
      },
      {
        "date": "2024-02-05T21:27:00.000Z",
        "voteCount": 1,
        "content": "D - because BCD are right solution , D because - beanstalk runs ECS in backend + Reduce operation complexity which is asked in the question"
      },
      {
        "date": "2023-12-31T13:22:00.000Z",
        "voteCount": 1,
        "content": "The confusion here is choice between B and C. Both ECS and EKS are container orchestration service which supports fargate. But ECS is aws fully managed, better suited for simple application and also more cost effective."
      },
      {
        "date": "2023-12-09T05:33:00.000Z",
        "voteCount": 2,
        "content": "Not A. as Lambda is not good for running a \"traditional web application\", also you can use container with Lambda but ECS is \"ideal for organizations that want a simple and cost-effective way to deploy and manage containerized applications\"\nNot C. as there is o pointer to EKS (e.g. open-source, industry standard, etc.) and also ECS is \"ideal for organizations that want a simple and cost-effective way to deploy and manage containerized applications\"\nNot D. as Beanstalk is not serverless\n\nHence B."
      },
      {
        "date": "2023-11-11T03:39:00.000Z",
        "voteCount": 1,
        "content": "B. Not D as Beanstalk isn't serverless. Not C because there are no pointers to use EKS. Not A, because microservices are requested."
      },
      {
        "date": "2023-09-28T03:29:00.000Z",
        "voteCount": 2,
        "content": "B. Image on ECR and ECS cost effective over EKS."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/90943-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application\u2019s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.<br>The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.<br>What should a solutions architect recommend to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReconfigure the application\u2019s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application\u2019s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application\u2019s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 43,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T10:52:00.000Z",
        "voteCount": 18,
        "content": "I go with B\nhttps://docs.amazonaws.cn/en_us/Route53/latest/DeveloperGuide/welcome-health-checks.html"
      },
      {
        "date": "2023-01-13T09:58:00.000Z",
        "voteCount": 16,
        "content": "B is correct, because it meets the company's requirements for reducing RTO to less than 15 minutes and not having a large budget for an active-active strategy.\n\nIn this solution, the company creates an AWS Lambda function in the backup region which promotes the read replica and modifies the Auto Scaling group values. Route 53 is configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The Route 53 record is also updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs. This way, when the primary region goes down, the failover policy triggers and traffic is directed to the backup region, ensuring a quick recovery time."
      },
      {
        "date": "2024-09-22T21:20:00.000Z",
        "voteCount": 4,
        "content": "I Vote B.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\n\nOption A, C and D are wrong. The latency-based routing and endopoint weights should be used for active/active strategy.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints-endpoint-weights.html"
      },
      {
        "date": "2024-09-22T21:20:00.000Z",
        "voteCount": 1,
        "content": "The best option to meet the requirements and reduce RTO to less than 15 minutes is to choose option B.\n\nOption B involves creating an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values. Additionally, Route 53 can be configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The application's Route 53 record can be updated with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.\n\nThis option is cost-effective as it does not require an active-active strategy, and it uses AWS services to minimize the RTO. The Lambda function can be invoked to promote the read replica in the backup region, and the Auto Scaling group values can be updated to launch EC2 instances in the backup region. Furthermore, the Route 53 health check feature can be used to monitor the web application and initiate the failover process."
      },
      {
        "date": "2024-09-22T21:20:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option will not work as needed: The client will get errors when the closest region is the application's backup region\n\nOption B - This option implements an active-passive strategy as needed: When the health check fails, Route 53 will resolve to the backup region and the Lambda function will ensure the backup region has resources to function\n\nOption C - This option implements an active-active strategy\n\nOption D - This option will not work as needed: The client will get errors 50% of the time"
      },
      {
        "date": "2024-09-11T19:26:00.000Z",
        "voteCount": 1,
        "content": "B is right."
      },
      {
        "date": "2024-08-31T01:47:00.000Z",
        "voteCount": 1,
        "content": "B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application\u2019s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs."
      },
      {
        "date": "2024-06-13T19:23:00.000Z",
        "voteCount": 1,
        "content": "Correct answer B"
      },
      {
        "date": "2024-03-16T08:56:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2024-01-24T13:31:00.000Z",
        "voteCount": 1,
        "content": "This explains Lambda promoting backup read replica in other region - https://medium.com/ankercloud-engineering/aws-lambda-promoting-rds-read-replica-on-cross-region-using-aws-lambda-113db758869"
      },
      {
        "date": "2024-01-13T04:10:00.000Z",
        "voteCount": 1,
        "content": "why we need Lambda Function ? Is it enough a Route 53 failover policy ?"
      },
      {
        "date": "2024-02-17T20:04:00.000Z",
        "voteCount": 1,
        "content": "What about RDS failover?\nYou need lambda to promote read replica."
      },
      {
        "date": "2023-12-09T05:44:00.000Z",
        "voteCount": 1,
        "content": "The problem is not detecting the right answer, but reading quickly enough trough all the words in the question!"
      },
      {
        "date": "2023-11-23T01:49:00.000Z",
        "voteCount": 1,
        "content": "B satisfies all the requirements."
      },
      {
        "date": "2023-11-11T03:56:00.000Z",
        "voteCount": 1,
        "content": "Health check is a metric, hence alarms can be executed, and alarms are integrated with SNS, SNS integrated with lambda. This sounds weird, but it will work."
      },
      {
        "date": "2023-09-28T03:33:00.000Z",
        "voteCount": 2,
        "content": "B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application\u2019s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs."
      },
      {
        "date": "2023-08-26T22:19:00.000Z",
        "voteCount": 1,
        "content": "Health check+SNS. This does not need to have active-active which satisfy the rquirement."
      },
      {
        "date": "2023-06-26T16:11:00.000Z",
        "voteCount": 1,
        "content": "it's a B again"
      },
      {
        "date": "2023-06-24T08:35:00.000Z",
        "voteCount": 1,
        "content": "As company can not afford with active active configuration and with lambda data layer can be promoted to primary"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/90944-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.<br>A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.<br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T10:57:00.000Z",
        "voteCount": 16,
        "content": "I go with ADF\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html"
      },
      {
        "date": "2024-09-22T21:20:00.000Z",
        "voteCount": 11,
        "content": "A. Using an Elastic Load Balancer (ELB) to distribute traffic across multiple EC2 instances can help ensure that the application remains available in the event that one of the instances becomes unavailable. By configuring the instances as part of an Auto Scaling group with a minimum capacity of two instances, you can ensure that there is always at least one healthy instance to handle traffic.\n\nD. Modifying the DB instance to create a Multi-AZ deployment that extends across two availability zones can help ensure that the database remains available in the event of a failure. In the event of a failure, traffic will automatically be directed to the secondary availability zone, reducing the amount of downtime.\n\nF. Creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ can help ensure that the in-memory data store remains available in the event of a failure. This will allow traffic to be automatically directed to the secondary availability zone, reducing the amount of downtime."
      },
      {
        "date": "2022-12-21T02:58:00.000Z",
        "voteCount": 3,
        "content": "Why C is wrong?"
      },
      {
        "date": "2023-08-06T06:30:00.000Z",
        "voteCount": 1,
        "content": "let suppose in case one of used AZ is failed?"
      },
      {
        "date": "2023-01-13T10:02:00.000Z",
        "voteCount": 1,
        "content": "Other options like B. and C. does not meet the requirement because the instances are configured in unlimited mode, it will not be possible to ensure that there is always at least one healthy instance to handle traffic if there is a failure."
      },
      {
        "date": "2023-02-12T23:12:00.000Z",
        "voteCount": 6,
        "content": "Issue with C - Read replica in the same AZ does not sound High availability"
      },
      {
        "date": "2023-05-28T03:28:00.000Z",
        "voteCount": 1,
        "content": "in question \"can automatically recover from failure with the least possible downtime\"\nC is correct but D is least possible downtime"
      },
      {
        "date": "2024-09-22T21:21:00.000Z",
        "voteCount": 1,
        "content": "A, D, E are the correct options to meet the requirements.\n\nOption A is correct because an Auto Scaling group with a minimum capacity of two instances and an Elastic Load Balancer distributing traffic across them can provide high availability and automatic recovery from failure.\n\nOption D is correct because a Multi-AZ deployment for the RDS instance will ensure that there is a synchronized standby copy of the database in a separate Availability Zone that can be used for automatic failover.\n\nOption E is correct because configuring an Auto Scaling group for the ElastiCache for Redis cluster will ensure that there is at least one available node at all times, and automatic recovery can be achieved by launching new instances to replace any failed nodes."
      },
      {
        "date": "2024-01-20T04:42:00.000Z",
        "voteCount": 1,
        "content": "There isn't such a thing like \"Auto Scaling group for the ElastiCache for Redis\", there is a \"Replication Group\""
      },
      {
        "date": "2024-09-22T21:21:00.000Z",
        "voteCount": 4,
        "content": "I wasn't sure if E or F was correct until I read this:\n\"This replacement results in some downtime for the cluster, but if Multi-AZ is enabled, the downtime is minimized. The role of primary node will automatically fail over to one of the read replicas. There is no need to create and provision a new primary node, because ElastiCache will handle this transparently. This failover and replica promotion ensure that you can resume writing to the new primary as soon as promotion is complete.\"\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html"
      },
      {
        "date": "2024-09-22T21:21:00.000Z",
        "voteCount": 2,
        "content": "Option A - Ensures there is always at least a healthy instance responding to requests. Nothing is said about whether the Auto Scaling Group includes multiple AZs (but it must)\n\nOption B - No such thing as EC2 Unlimited Mode\n\nOption C - Does not provide a place to fail over to \n\nOption D - Provides a place to fail over to \n\nOption E - Does not provide a place to fail over to \n\nOption F - Provides a place to fail over to\n\n\nChoose A, D, F"
      },
      {
        "date": "2024-08-31T01:48:00.000Z",
        "voteCount": 1,
        "content": "A. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.\nD. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.\nF. Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster."
      },
      {
        "date": "2024-05-11T06:51:00.000Z",
        "voteCount": 1,
        "content": "ElastiCache for Redis Auto Scaling is limited to the following:\n\n    Redis (cluster mode enabled) clusters running Redis engine version 6.0 onwards\nE is out"
      },
      {
        "date": "2024-05-07T18:26:00.000Z",
        "voteCount": 1,
        "content": "Satisfies the High Availability requirement on the EC2 instance, Amazon RDS for MariaDB DB instance, and ElastiCache for Redis cluster"
      },
      {
        "date": "2024-03-16T08:59:00.000Z",
        "voteCount": 1,
        "content": "ADF, as mentioned in the other comments"
      },
      {
        "date": "2024-01-31T08:34:00.000Z",
        "voteCount": 1,
        "content": "\"The infrastructure can automatically recover from failure with the least possible downtime\",\nto me this sounds rather resilient than highly-availible, since it focuses on MITR but not explicitly on up-time."
      },
      {
        "date": "2023-11-11T03:59:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-09-28T03:35:00.000Z",
        "voteCount": 1,
        "content": "A, D, F"
      },
      {
        "date": "2023-06-26T16:14:00.000Z",
        "voteCount": 1,
        "content": "it's of course ADF"
      },
      {
        "date": "2023-06-24T08:40:00.000Z",
        "voteCount": 1,
        "content": "For high availability, need to spin up instances in another zone with auto scaling and multi AZ options"
      },
      {
        "date": "2023-05-23T06:38:00.000Z",
        "voteCount": 1,
        "content": "ADF will meet the described provisions"
      },
      {
        "date": "2023-05-12T11:19:00.000Z",
        "voteCount": 1,
        "content": "Fit the best the question"
      },
      {
        "date": "2023-03-28T14:08:00.000Z",
        "voteCount": 1,
        "content": "ADF the correct answers \u2705"
      },
      {
        "date": "2023-03-27T22:47:00.000Z",
        "voteCount": 1,
        "content": "ADF is the best fit."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/90945-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.<br>After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.<br>While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.<br>Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T00:30:00.000Z",
        "voteCount": 33,
        "content": "A &amp; E\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html#custom-error-pages-procedure"
      },
      {
        "date": "2024-09-22T21:22:00.000Z",
        "voteCount": 8,
        "content": "Option A - This option helps: Allows exposing custom error pages from a highly-available location\n\nOption B - This option requires a lot of set up\n\nOption C - This option might not work because modifying DNS will redirect all traffic publicly accessible webpage\n\nOption D - This option requires a lot of set up\n\nOption E - This option helps: Shows a custom error page when the error occurs"
      },
      {
        "date": "2024-10-08T01:03:00.000Z",
        "voteCount": 1,
        "content": "AC, beacuse for E to work, another s3 origin needs to be created, ( OAI when not static website ) , and a behaviour for the path routing to the error html.. Also unclear what the \"DNS record motification\" is made for."
      },
      {
        "date": "2024-09-22T21:22:00.000Z",
        "voteCount": 2,
        "content": "Custom error pages need to setup in different location then source (where web pages is hosted), configure CloudFront to use those custom error pages"
      },
      {
        "date": "2024-09-22T21:22:00.000Z",
        "voteCount": 5,
        "content": "We need a combination, so A provides the error page; should we go with DNS health-check (C+A) or CloudFront (E+A)? In my case, I try to stick to a single service to do failover, and DNS is a great option, but it looks like, in this question, CloudFront is already present with the least-operational overhead."
      },
      {
        "date": "2024-09-22T21:22:00.000Z",
        "voteCount": 1,
        "content": "A&amp;E are the correct answers imo"
      },
      {
        "date": "2024-08-31T01:50:00.000Z",
        "voteCount": 1,
        "content": "A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.\n E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page."
      },
      {
        "date": "2024-07-03T01:44:00.000Z",
        "voteCount": 1,
        "content": "Option A - Allow us to expose a error page with low effort.\nOption B - Requires a lot of set up\nOption C - Allow us to redirect all the traffic to our error page exposed by S3 in case of errors.\nOption D - requires a lot of set up\nOption E - Custom Error Pages in CloudFront refers to the same Origin (in our case the Load Balancer) so it does not work with all the other answers.\n\nSo correct answer are A and C"
      },
      {
        "date": "2024-06-13T13:10:00.000Z",
        "voteCount": 1,
        "content": "A &amp; E \nhttps://aws.amazon.com/blogs/aws/custom-error-pages-and-responses-for-amazon-cloudfront/"
      },
      {
        "date": "2024-06-04T17:38:00.000Z",
        "voteCount": 3,
        "content": "I think it is wordplay. Option A says to upload \"error pages\", which will be an overhead for creating a page for each error and unnecessary. that's where C &amp; E are correct"
      },
      {
        "date": "2024-05-26T01:24:00.000Z",
        "voteCount": 1,
        "content": "A and E according to AWS documentation: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html#custom-error-pages-procedur"
      },
      {
        "date": "2024-03-15T05:45:00.000Z",
        "voteCount": 6,
        "content": "The only problem with E is that it say \"Modify DNS records to point to a publicly accessible web page\" at the end. It doesn't make sense to begin with. And configuring custom error responses in CF has nothing to do with DNS anyway."
      },
      {
        "date": "2024-03-12T06:46:00.000Z",
        "voteCount": 1,
        "content": "I think why is not A is because of this sentence - \"The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.\" - so let's not think it as requiring a maintenance page"
      },
      {
        "date": "2023-11-26T10:37:00.000Z",
        "voteCount": 2,
        "content": "should be AE"
      },
      {
        "date": "2023-11-11T04:21:00.000Z",
        "voteCount": 2,
        "content": "I haven't found out why should we use C."
      },
      {
        "date": "2023-08-29T20:30:00.000Z",
        "voteCount": 3,
        "content": "Agree with Raj40"
      },
      {
        "date": "2023-08-27T05:05:00.000Z",
        "voteCount": 2,
        "content": "C &amp; E.\nB &amp; D are incorrect. Managing lambda is overhead.\nA is incorrect. Static page from S3 need to retrieve with custom code."
      },
      {
        "date": "2023-11-23T02:29:00.000Z",
        "voteCount": 1,
        "content": "Do you have any further reference to your explanation of custom code requirement to fetch the error page from S3?"
      },
      {
        "date": "2024-02-05T21:32:00.000Z",
        "voteCount": 1,
        "content": "not really , you just need to static url provided by aws when you use the bucket for static webpage and embed it anywhere to reach to the static website"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/91006-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.<br>The company\u2019s infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.<br>Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway in the infrastructure account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable resource sharing from the AWS Organizations management account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 62,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T09:03:00.000Z",
        "voteCount": 28,
        "content": "I go with BD"
      },
      {
        "date": "2023-01-13T10:09:00.000Z",
        "voteCount": 12,
        "content": "Step B is needed because it enables the organization to share resources across accounts.\nStep D is needed because it allows the infrastructure account to share specific subnets with the other accounts in the organization, so that the other accounts can create resources within those subnets without having to manage their own networks."
      },
      {
        "date": "2024-07-29T07:05:00.000Z",
        "voteCount": 2,
        "content": "Note that B says it enables sharing from the management account, but the infrastructure team must use the infrastructure account to manage the network\", so there is nothing to share form the management account. Also, options D and E also enable resource sharing (you don't need to enable it from the management account, other accounts can enable resource sharing too).\n\nVPCs can't talk to each other by default. You need to do something to 'glue' them together in a larger network."
      },
      {
        "date": "2022-12-21T10:40:00.000Z",
        "voteCount": 10,
        "content": "A - Doesn't seem correct as the question didnt state multiple VPs, so transit gateway is not relevant. \nI will go with B &amp; D"
      },
      {
        "date": "2024-07-29T07:06:00.000Z",
        "voteCount": 1,
        "content": "There are multiple VPCs because each account must have at least one."
      },
      {
        "date": "2024-10-16T04:24:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A and D.\nB is a wrong option. \n\nWhile AWS Organizations is required to manage multiple accounts, enabling resource sharing through AWS RAM is done in the infrastructure account (where the VPC resides), not in the AWS Organizations management account. Resource sharing is configured via RAM in the account that owns the resources, not through Organizations directly."
      },
      {
        "date": "2024-09-22T21:25:00.000Z",
        "voteCount": 1,
        "content": "The correct answers are D and B.\n\nD will allow the infrastructure team to create a resource share in AWS Resource Access Manager in the infrastructure account. This will allow them to share the VPC with the other accounts in the organization.\n\nB will enable resource sharing from the AWS Organizations management account. This is required to allow the resource share to be created.\n\nC is not necessary, as the resource share will allow the other accounts to create resources in the shared VPC.\n\nA is not necessary, as the resource share will allow the other accounts to connect to the shared VPC through the transit gateway.\n\nE is not necessary, as the resource share will allow the other accounts to create resources in the shared VPC without the need for prefix lists."
      },
      {
        "date": "2024-09-22T21:24:00.000Z",
        "voteCount": 4,
        "content": "Option B allows the infrastructure team to manage the network in the infrastructure account. It also allows individual accounts to create AWS resources within subnets. This is done by creating a resource share in AWS Resource Access Manager (RAM) in the infrastructure account. The resource share is then associated with the specific AWS Organizations OU that will use the shared network. The subnets are then associated with the resource share.\n\nOption D is also necessary because it allows the infrastructure team to control who has access to the shared network. This is done by assigning permissions to the resource share.\n\nHere are the steps involved in implementing this solution:\n\nCreate a resource share in RAM in the infrastructure account.\nSelect the specific AWS Organizations OU that will use the shared network.\nSelect each subnet to associate with the resource share.\nAssign permissions to the resource share."
      },
      {
        "date": "2024-09-22T21:24:00.000Z",
        "voteCount": 1,
        "content": "I would go BD \nWhen you share a subnet using AWS Resource Access Manager (RAM) with another AWS account, the resources within that shared subnet can communicate with each other and with the resources in the account that owns the subnet. However, for outbound network connectivity to other VPCs, on-premises networks, or the internet, you need to set up additional networking components."
      },
      {
        "date": "2024-09-22T21:23:00.000Z",
        "voteCount": 1,
        "content": "2.\tInter-VPC Communication:\no\tIf the resources in the shared subnet need to communicate with resources in another VPC (either within the same AWS account or in a different AWS account), you can use VPC Peering or a Transit Gateway.\no\tVPC Peering: Establish a peering connection between the VPCs and update the route tables accordingly.\no\tTransit Gateway: Create a Transit Gateway, attach both VPCs to the Transit Gateway, and configure the necessary route tables and Transit Gateway route tables."
      },
      {
        "date": "2024-06-18T02:38:00.000Z",
        "voteCount": 1,
        "content": "Here's a breakdown of different scenarios and the required setup:\n1.\tInternet Access:\no\tIf you need resources in the shared subnet to access the internet, ensure that the subnet is a public subnet with an associated Internet Gateway (IGW) and appropriate route table entries.\no\tThe account that owns the VPC will typically manage the IGW and the route tables."
      },
      {
        "date": "2024-06-18T02:39:00.000Z",
        "voteCount": 1,
        "content": "3.\tOn-Premises Connectivity:\no\tIf the resources in the shared subnet need to communicate with an on-premises network, you can use AWS Direct Connect or a Site-to-Site VPN.\no\tThese connections can be routed through a Transit Gateway for more scalable and manageable network architecture."
      },
      {
        "date": "2024-09-22T21:23:00.000Z",
        "voteCount": 3,
        "content": "Answer - B &amp; D.\nA is wrong. No TGW needed as customer has just 1 VPC. \nE is wrong - can't share resources via RAM using prefix lists. \nC is wrong - talks about creating VPCs with same CIDR ranges and VPC peering (not possible with overlapping CIDRs and not needed for this solution as there is just 1 VPC)."
      },
      {
        "date": "2024-10-16T04:25:00.000Z",
        "voteCount": 1,
        "content": "How do you think the Accounts got subnets without VPCs?"
      },
      {
        "date": "2024-09-22T21:23:00.000Z",
        "voteCount": 2,
        "content": "I don't see the way you can share a prefix list."
      },
      {
        "date": "2024-10-12T21:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/vpc/latest/userguide/sharing-managed-prefix-lists.html"
      },
      {
        "date": "2024-07-29T07:13:00.000Z",
        "voteCount": 2,
        "content": "You don't share a prefix list, you associate it with the shared resource (which here is a TGW). The way you do it is you add the prefixes to the route tables inside the account's VPCs. The prefixes will point towards the TGW. This makes the network traffic destined to other account go through the TGW into these accounts based on the TGW routing table. The TGW routing table can only be controlled from the infrastructure account."
      },
      {
        "date": "2024-09-22T21:23:00.000Z",
        "voteCount": 2,
        "content": "B&amp;D is the only correct answer"
      },
      {
        "date": "2024-09-22T21:23:00.000Z",
        "voteCount": 4,
        "content": "Option A - Does not assist with allowing OUs to create resources in the subnets\n\nOption B - Allows sharing resources across the entire organization\n\nOption C - This option does not work as a way to share subnets because it creates multiple VPCs and subnets in the accounts rather than allowing managing resources in shared subnets\n\nOption D - Directly shares the subnets\n\nOption E - Does not assist because it only shares pre-built CIDR blocks rather than subnets"
      },
      {
        "date": "2024-07-29T07:08:00.000Z",
        "voteCount": 1,
        "content": "Subnets cannot be shared"
      },
      {
        "date": "2024-09-22T21:23:00.000Z",
        "voteCount": 1,
        "content": "Voting A &amp; E"
      },
      {
        "date": "2024-08-31T01:53:00.000Z",
        "voteCount": 1,
        "content": "B. Enable resource sharing from the AWS Organizations management account.\n D. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share."
      },
      {
        "date": "2024-07-29T06:54:00.000Z",
        "voteCount": 1,
        "content": "It's AD. To form a network between multiple accounts, each with their own VPCs, you can use VPC peering or Transit Gateway. But VPC peering is only suitable for a few acounts, and we have many, so we need to create a TGW (A). Then we need to associate it with the VPCs across all acounts, we do this through RAM, and we need to configure the route tables in all accounts to use the TGW, which is done through prefixes (D).\n\nSee https://docs.aws.amazon.com/prescriptive-guidance/latest/integrate-third-party-services/architecture-3-1.html\n\nThe question is a bit weird because the answer could allow accounts to manage the network inside their own VPCs, so probably some SCP policies are needed to prevent this. But the accounts cannot edit the TGW routing, so probably that's what they were trying to suggest."
      },
      {
        "date": "2024-07-29T06:57:00.000Z",
        "voteCount": 1,
        "content": "I meant to say AE, but I can't edit the post now."
      },
      {
        "date": "2024-03-29T19:27:00.000Z",
        "voteCount": 1,
        "content": "B E correta"
      },
      {
        "date": "2024-03-29T19:26:00.000Z",
        "voteCount": 1,
        "content": "BE com certeza"
      },
      {
        "date": "2024-03-16T09:39:00.000Z",
        "voteCount": 1,
        "content": "BD, as mentioned in other comments"
      },
      {
        "date": "2023-12-03T12:58:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/91007-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. The third-party SaaS application also runs on AWS inside a VPC.<br>The company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the company\u2019s VPC. All permissions must conform to the principles of least privilege.<br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T01:49:00.000Z",
        "voteCount": 19,
        "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html"
      },
      {
        "date": "2022-12-12T09:07:00.000Z",
        "voteCount": 8,
        "content": "I go with A"
      },
      {
        "date": "2023-01-13T10:11:00.000Z",
        "voteCount": 11,
        "content": "A. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.\nThis solution uses AWS PrivateLink, which creates a secure and private connection between the company's VPC and the third-party SaaS application VPC, without the traffic traversing the internet. The use of a security group and limiting access to the endpoint service conforms to the principle of least privilege."
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 1,
        "content": "Access Saas products throgh AWS Private Link is the answer."
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 1,
        "content": "Create an AWS PrivateLink interface VPC endpoint."
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 1,
        "content": "it s a"
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 1,
        "content": "It's A because in this scenario we are consuming a service , not providing one, so that eliminates E ."
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 1,
        "content": "Answer - A. \nVPC Interface end point to access any service privately without traversing the internet. \nAWS Private Link VPC endpoint to access the SaaS application."
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 2,
        "content": "Option A - The interface VPC Endpoint will provide local access to the SaaS service from within the company's VPC. Moreover, traffic to and access from the SaaS VPC will traverse the AWS network rather than the internet. This is considered private traffic.\n\nOption B - This option might not work: Nothing is said about whether the CIDR blocks in each VPC overlap. Moreover, nothing is said about whether bandwidth limitations on Site-Site VPN could be an issue. \n\nOption C - This option might not work: Nothing is said about whether the CIDR blocks in each VPC overlap.\n\nOption D - This option will not work: A PrivateLink Endpoint service is used for facilitating access to AWS services."
      },
      {
        "date": "2024-09-22T21:26:00.000Z",
        "voteCount": 1,
        "content": "A, the service provider creates an endpoint service and grants their customers access to the endpoint service. As the service consumer, you create an interface VPC endpoint, which establishes connections between one or more subnets in your VPC and the endpoint service."
      },
      {
        "date": "2024-08-31T01:53:00.000Z",
        "voteCount": 1,
        "content": "A. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint."
      },
      {
        "date": "2023-11-11T21:24:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-11-01T21:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer : A"
      },
      {
        "date": "2023-09-12T22:39:00.000Z",
        "voteCount": 4,
        "content": "A VS D\nA. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides.\nD. Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service\nD is right SaaS provider has create interface VPC endpoint for this endpoint service"
      },
      {
        "date": "2024-02-05T21:42:00.000Z",
        "voteCount": 1,
        "content": "exactly , we need to access the resource from SAAS Provider and not vice versa , Hence in this case the VPC Gateway endpoint should be provided from SAAS Provider for the privatelink endpoint we provide it to them - we use this for Snowflake Saas :)"
      },
      {
        "date": "2023-08-19T06:20:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html\nhttps://aws.amazon.com/blogs/apn/enabling-new-saas-strategies-with-aws-privatelink/"
      },
      {
        "date": "2023-03-27T22:53:00.000Z",
        "voteCount": 1,
        "content": "Create an AWS PrivateLink interface VPC endpoint."
      },
      {
        "date": "2023-03-07T03:12:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html"
      },
      {
        "date": "2022-12-22T23:30:00.000Z",
        "voteCount": 4,
        "content": "It's A .clearly"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/91096-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. Management requires a single report showing the patch status of all the servers and instances.<br>Which set of actions should a solutions architect take to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T09:13:00.000Z",
        "voteCount": 14,
        "content": "A is good\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patch-management-hybrid-cloud/design-on-premises.html"
      },
      {
        "date": "2023-01-13T10:14:00.000Z",
        "voteCount": 13,
        "content": "A is correct. AWS Systems Manager can manage patches on both on-premises servers and EC2 instances and can generate patch compliance reports. AWS OpsWorks and Amazon Inspector are not specifically designed for patch management and therefore would not be the best choice for this use case. Using Amazon EventBridge rule and AWS X-Ray to generate patch compliance reports is not a practical solution as they are not designed for patch management reporting."
      },
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "Option A - Systems Manager patches and generates patch compliance reports. \n\nOption B - This option does not apply because Chef or Puppet are not mentioned in the question. Moreover, either one does not directly perform patch management.\n\nOption C - Inspector would generate a report for on-premise resources\n\nOption D - This option does not apply because Chef or Puppet are not mentioned in the question. Moreover, X-Ray does apply."
      },
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "AWS OpsWorks is a configuration management service that provides a way to automate the deployment, configuration, and management of applications on EC2 instances. It is designed to help you manage the entire lifecycle of your applications."
      },
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-08-31T01:54:00.000Z",
        "voteCount": 1,
        "content": "A. Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports."
      },
      {
        "date": "2024-03-16T09:47:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-07-28T21:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct:\nhttps://www.amazonaws.cn/en/systems-manager/"
      },
      {
        "date": "2023-07-23T18:01:00.000Z",
        "voteCount": 1,
        "content": "Other options are distractors. Opswork would be right only if customer wanted to make use of existing script or know-how in chef or puppet."
      },
      {
        "date": "2023-06-27T19:04:00.000Z",
        "voteCount": 1,
        "content": "yep - A"
      },
      {
        "date": "2023-05-25T03:39:00.000Z",
        "voteCount": 1,
        "content": "A is the best but Systems Manager cannot generate the patch compliance reports.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patch-management-hybrid-cloud/design-on-premises.html\n- A resource data sync in Systems Manager Inventory gathers the patching details and publishes them to an S3 bucket.\n\n- Patch compliance reporting and dashboards are built in Amazon QuickSight from the S3 bucket information."
      },
      {
        "date": "2023-05-08T05:14:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer for this question as per information shared by them"
      },
      {
        "date": "2023-04-18T12:21:00.000Z",
        "voteCount": 1,
        "content": "Easy question :)\nA is the answer."
      },
      {
        "date": "2023-03-27T22:54:00.000Z",
        "voteCount": 1,
        "content": "Use AWS Systems Manager to manage patches"
      },
      {
        "date": "2023-03-07T03:15:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patch-management-hybrid-cloud/design-on-premises.html"
      },
      {
        "date": "2023-03-02T00:06:00.000Z",
        "voteCount": 2,
        "content": "AWS System Manager support On-premise and EC2 instance patching"
      },
      {
        "date": "2023-02-23T17:29:00.000Z",
        "voteCount": 1,
        "content": "A is correct ofc..  easy one )"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/91139-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.<br>Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 53,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 12,
        "content": "B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance. This approach will use the Auto Scaling lifecycle hook to execute the script that copies log files to S3, before the instance is terminated, ensuring that all log files are copied from the terminated instances."
      },
      {
        "date": "2022-12-13T11:24:00.000Z",
        "voteCount": 7,
        "content": "https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      },
      {
        "date": "2024-09-22T21:28:00.000Z",
        "voteCount": 5,
        "content": "Option A - This option might not work: Preventing ASG termination could create further trouble and there is no guarantee the script will run if the instance happens to be unhealthy\nOption B - This option could work: Running the script from the SSM API guarantees the script will run, using EventBridge to capture the ASG termination event provides a perfect place to hook in the call to SSM which will also pause the termination until the script runs. Then CONTINUE allows the ASG termination to continue.\nOption C - This option does not work because it does not solve the problem: Terminating instances within the 15 minute window causes log files to be lost. \nOption D - This option might not work: It does not rely on EventBridge to detect the ASG termination event. It also could create further trouble because no other actions will be performed due to sending ABANDON though nothing is said about other actions in the question"
      },
      {
        "date": "2024-09-22T21:28:00.000Z",
        "voteCount": 3,
        "content": "A- Wrong because prevent termination is not needed. \nC- Wrong because 5-minute frequency creates an overhead or delay . Using user data for the script adds complexity\nD- Wrong because SNS"
      },
      {
        "date": "2024-09-22T21:28:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer due to  Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send"
      },
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "I think this is B. It could be A as well, but B is better solution because the document with SM can be re-utilized with other instances. Also A would require using a custom image with the script or user data to create the script, so more points of failure."
      },
      {
        "date": "2024-09-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance."
      },
      {
        "date": "2024-08-31T01:55:00.000Z",
        "voteCount": 1,
        "content": "B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance."
      },
      {
        "date": "2024-03-16T09:52:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-11-11T21:50:00.000Z",
        "voteCount": 2,
        "content": "both abandon and continue will lead to instance termination, the difference is abandon will prevent from running other lifycycle hooks"
      },
      {
        "date": "2023-08-29T04:19:00.000Z",
        "voteCount": 1,
        "content": "I think this is B. It could be A as well, but B is better solution because the document with SM can be re-utilized with other instances. Also A would require using a custom image with the script or user data to create the script, so more points of failure."
      },
      {
        "date": "2023-08-18T00:37:00.000Z",
        "voteCount": 2,
        "content": "d is wrong, shouldn't be \"ABANDON\""
      },
      {
        "date": "2023-06-27T19:07:00.000Z",
        "voteCount": 1,
        "content": "it's a B"
      },
      {
        "date": "2023-04-18T12:27:00.000Z",
        "voteCount": 3,
        "content": "B.\nSmart solution :)"
      },
      {
        "date": "2023-03-27T22:56:00.000Z",
        "voteCount": 4,
        "content": "Systems manager + eventbridge"
      },
      {
        "date": "2023-03-07T03:28:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      },
      {
        "date": "2022-12-27T21:39:00.000Z",
        "voteCount": 4,
        "content": "B\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/91008-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company\u2019s applications and databases are running in Account B.<br>A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.<br>During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.<br>Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance\u2019s private IP in the private hosted zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 75,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T10:21:00.000Z",
        "voteCount": 33,
        "content": "C and E are correct.\n\nC. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\nThis step is necessary because the VPC in Account B needs to be associated with the private hosted zone in Account A to be able to resolve the DNS records.\n\nE. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.\nThis step is necessary because the association authorization needs to be removed in Account A after the association is done in Account B."
      },
      {
        "date": "2023-03-07T03:33:00.000Z",
        "voteCount": 9,
        "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html"
      },
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 4,
        "content": "With comments and links the answer is C and E. (Ty robertohy\u00e8ne and Josu\u00e9Xu)\n\nC = 6. Run the following command to create the association between Account A's private hosted zone and Account B's VPC. Use the hosted zone's ID from step 3. B account.\nE = 7.  It is recommended to remove the association permission after the association is created. This will prevent you from recreating the same association later.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/route53-private-hosted-zone/"
      },
      {
        "date": "2022-12-12T10:26:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/36113-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 4,
        "content": "C and E.\nIn order to resolve the issue, the solutions architect should create an authorization to associate the private hosted zone in Account A with the new VPC in Account B (Option C). This will allow the new VPC in Account B to access the DNS records stored in the private hosted zone in Account A.\n\nIn addition, the solutions architect should associate the new VPC in Account B with the hosted zone in Account A (Option E) and delete the association authorization in Account A. This will ensure that the new VPC in Account B is properly configured to use the private hosted zone in Account A and resolve the db.example.com CNAME record set correctly."
      },
      {
        "date": "2024-09-22T21:28:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/route53-private-hosted-zone\n\nCreate an authorization to associate the private hosted zone and as a best practice , it is recommended to delete the association authorization in account A-This step prevents you from recreating the same association later. To delete the authorization, reconnect to the EC2 instance in Account A"
      },
      {
        "date": "2024-09-16T20:43:00.000Z",
        "voteCount": 1,
        "content": "A account's DNS Zone authorization is associated with B's VPC, and after B's VPC is associated with A's Priviate Zone, A's authorization permission is deleted for security reasons."
      },
      {
        "date": "2024-08-31T01:56:00.000Z",
        "voteCount": 1,
        "content": "C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\nE. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A."
      },
      {
        "date": "2024-04-11T19:15:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/route53-private-hosted-zone"
      },
      {
        "date": "2024-02-07T18:57:00.000Z",
        "voteCount": 1,
        "content": "Correct answers"
      },
      {
        "date": "2024-09-22T21:28:00.000Z",
        "voteCount": 4,
        "content": "Explanation:\n    *    Option C is correct because, in a multi-account AWS setup, to use a Route 53 private hosted zone from one account (Account A) in another account\u2019s VPC (Account B), you first need to create an authorization. This authorization is necessary for allowing the private hosted zone in one account to be associated with a VPC in another account. This step enables the resolution of DNS records stored in the private hosted zone across accounts.\n    *    Option E is correct as it follows up on the authorization created in Option C. Once the authorization is in place, you can then associate the new VPC in Account B with the private hosted zone in Account A. This association is what actually allows the EC2 instances within the VPC in Account B to resolve DNS queries using the private hosted zone in Account A, ensuring that db.example.com can be resolved as intended."
      },
      {
        "date": "2024-02-07T18:59:00.000Z",
        "voteCount": 3,
        "content": "Why the others are incorrect:\n\n    *    Option A is not a direct solution to the problem of DNS resolution across AWS accounts. Deploying the database on an EC2 instance does not address the issue of DNS resolution for the RDS endpoint across accounts.\n    *    Option B is not a scalable or AWS-recommended solution. Manually adding RDS endpoint IP addresses to the /etc/resolv.conf file on an EC2 instance is not practical for environments that require automation and could lead to issues if the RDS endpoint changes.\n    *    Option D involves creating a separate private hosted zone in Account B and configuring Route 53 replication between AWS accounts. This option is unnecessary and more complex than required. The direct association of VPCs across accounts to a single hosted zone is a simpler and more effective solution.\nTherefore, Options C and E are the steps that directly address the issue with the least complexity and enable the intended DNS resolution across AWS accounts."
      },
      {
        "date": "2023-12-19T00:49:00.000Z",
        "voteCount": 1,
        "content": "Option A - This option does not work - It does not provide for solving address name resolution in the new VPC \n\nOption B - This option works but it breaks the company\u2019s architecture where all DNS names are stored in the private zone in Account A\n\nOption C - This option contributes to the solution.\n\nOption D - Breaks the company\u2019s architecture\n\nOption E - This option contributes to the solution"
      },
      {
        "date": "2023-11-11T21:55:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-10-16T22:10:00.000Z",
        "voteCount": 1,
        "content": "C and E are correct.\nB is not a best solution. It's a manual setup and it may lose the configuration if we are using ASG and launching new instance."
      },
      {
        "date": "2023-10-08T00:25:00.000Z",
        "voteCount": 2,
        "content": "Why is B marked as correct?"
      },
      {
        "date": "2023-10-16T22:11:00.000Z",
        "voteCount": 2,
        "content": "B is not a best solution. It's a manual setup and it may lose the configuration if we are using ASG and launching new instance."
      },
      {
        "date": "2023-06-27T19:12:00.000Z",
        "voteCount": 1,
        "content": "it's CE"
      },
      {
        "date": "2023-06-21T02:20:00.000Z",
        "voteCount": 1,
        "content": "ccccccccccccceeeeeeeeeeeeee"
      },
      {
        "date": "2023-06-17T20:32:00.000Z",
        "voteCount": 1,
        "content": "C &amp; E as Issue is associated with authorization"
      },
      {
        "date": "2023-06-10T12:52:00.000Z",
        "voteCount": 1,
        "content": "C &amp; E as Issue is associated with authorization"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/91237-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.<br>The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.<br>Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReconfigure Amazon EFS to enable maximum I/O.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 25,
        "content": "C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.\n\nAmazon CloudFront is a content delivery network (CDN) that can be used to deliver content to users with low latency and high data transfer speeds. By configuring a CloudFront distribution for the blog site and pointing it at an S3 bucket, the videos can be cached at edge locations closer to users, reducing buffering and timeout issues. Additionally, S3 is designed for scalable storage and can handle high levels of user traffic. Migrating the videos from EFS to S3, would also improve the performance and scalability of the website."
      },
      {
        "date": "2022-12-22T01:49:00.000Z",
        "voteCount": 9,
        "content": "No brainer"
      },
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 1,
        "content": "Not A as Max I/O increase IOPS but negatively impact latency, ultimately you will have little to no performance improvement. Also you cannot enable Max IO on an existing filesystem.\nNot B as this is not a cheap option (instance store generally cost more than EBS backed), also without a CDN there will be little performance improvement\nNot D as this provides performance improvements, but this provide comparable performance to option C at higher costs as in D videos are stored on EFS that cost more than S3 and all traffic goes trough CDN rather than only videos that actually needs eddge caching\n\nThus C provide performance improvements (thanks for CloudFront) with cost-effective approach (S3 is cheap)"
      },
      {
        "date": "2023-12-14T05:59:00.000Z",
        "voteCount": 1,
        "content": "Also this follows AWS best practices to separate static content from dynamic content allowing for better scalability"
      },
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 1,
        "content": "Option A - This option might not work and is not cheap: It will increase costs and has limited scalability. EFS is an expensive storage solution for videos\n\nOption B - This option might not work: Nothing is mentioned about whether the application is stateful or stateless and whether the ALB has client stickiness so using instance store could provide an inconsistent user experience. S3 is a cheap storage option\n\nOption C - This option will work and is cheap: A CloudFront distribution and S3 will provide the most scalability and availability possible from AWS; and both are very cheap options for distribution and storage of content\n\nOption D - This option might work but is not cheap: Moving all content to CloudFront ensures it will be served from the edge cache for the duration of the cache mitigating issues during high usage. However, nothing is said in the question about usage patterns, i.e performance issue will happen again for older content. Moreover, EFS is an expensive storage solution for video files compared to S3."
      },
      {
        "date": "2024-08-31T01:58:00.000Z",
        "voteCount": 1,
        "content": "C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3."
      },
      {
        "date": "2024-06-15T23:36:00.000Z",
        "voteCount": 2,
        "content": "The most cost-efficient and scalable deployment that will resolve the issues for users, given the requirements and the described scenario, is:\n\nD. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB."
      },
      {
        "date": "2024-02-12T01:57:00.000Z",
        "voteCount": 2,
        "content": "Option C - Does not support new content added later by users, does not accelerate site content\nOption D - Accelerate site and videos, allow content added"
      },
      {
        "date": "2024-03-06T11:48:00.000Z",
        "voteCount": 2,
        "content": "Cloudfront caches data to serve more rapidly at the edge and not have to serve content from the backend, that is acceleration. Also you can now write to S3 for new data. Sorry your choice is not correct."
      },
      {
        "date": "2023-12-01T11:37:00.000Z",
        "voteCount": 1,
        "content": "C is good"
      },
      {
        "date": "2023-11-26T10:44:00.000Z",
        "voteCount": 1,
        "content": "C is good"
      },
      {
        "date": "2023-11-11T21:57:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-08-31T06:27:00.000Z",
        "voteCount": 1,
        "content": "No doubt it's C. To me the keyword there is scalable. S3 will be able to handle any amount of content users can generate. EFS is not the right solution for object storage, s3 is. EFS is a solution for a sharable network filesystem, that can be mounted and used by many operation systems."
      },
      {
        "date": "2023-07-08T15:36:00.000Z",
        "voteCount": 2,
        "content": "C and D are both viable. But D would be less overhead as you would most likely need to reconfigure the web application more to get it working with S3. Option D with Elastic Beanstalk provides a higher level of abstraction and automates many aspects of the application management, which can reduce operational overhead and simplify the re-architecting process"
      },
      {
        "date": "2023-07-22T04:59:00.000Z",
        "voteCount": 1,
        "content": "D is not cost effective, which was the demand for the question.\nIf it was about less changes, I would go with it.\nHere, right answer is C."
      },
      {
        "date": "2023-06-27T19:14:00.000Z",
        "voteCount": 1,
        "content": "C more cost efficient"
      },
      {
        "date": "2023-06-19T15:37:00.000Z",
        "voteCount": 1,
        "content": "C without a doubt"
      },
      {
        "date": "2023-05-08T05:29:00.000Z",
        "voteCount": 1,
        "content": "C is only option which meet their requirement"
      },
      {
        "date": "2023-03-27T23:01:00.000Z",
        "voteCount": 2,
        "content": "Configure an Amazon CloudFront distribution."
      },
      {
        "date": "2023-03-07T03:35:00.000Z",
        "voteCount": 2,
        "content": "y configuring a CloudFront distribution for the blog site and pointing it at an S3 bucket, the videos can be cached at edge locations closer to users, reducing buffering and timeout issues."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/91239-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company with global offices has a single 1&nbsp;Gbps AWS Direct Connect connection to a single AWS Region. The company\u2019s on-premises network uses the connection to communicate with the company\u2019s resources in the AWS Cloud. The connection has a single private virtual interface that connects to a single VPC.<br>A solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must provide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.<br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 52,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 23,
        "content": "A. Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC.\n\nThis solution provides a redundant Direct Connect connection in the same Region by creating a new private virtual interface on each connection, and connecting both private virtual interfaces to a Direct Connect gateway. The Direct Connect gateway is then connected to the single VPC. This solution also allows the company to expand into other Regions while providing connectivity through the same pair of Direct Connect connections.\nThe Direct Connect Gateway allows you to connect multiple VPCs and on-premises networks in different accounts and different regions to a single Direct Connect connection.\nIt also provides automatic failover and routing capabilities."
      },
      {
        "date": "2023-01-13T10:31:00.000Z",
        "voteCount": 14,
        "content": "Option D is not the best solution because it uses a Transit Gateway, which is used to connect multiple VPCs and on-premises networks in different accounts and different regions, but it is not necessary in this scenario. The company only wants to add a redundant Direct Connect connection in the same Region and connect it to the same VPC. Additionally, using a Transit Gateway in this scenario would add more complexity and might not be necessary.\nAlso, Transit Gateway does not provide automatic failover and routing capabilities, which is required in this scenario.\nThe Direct Connect Gateway is a better choice in this scenario as it provides the necessary functionality of automatic failover and routing capabilities, and it is more suitable for connecting multiple Direct Connect connections to a single VPC."
      },
      {
        "date": "2023-02-21T09:13:00.000Z",
        "voteCount": 2,
        "content": "All options here are problematic. The DX-GW is a control plane-only device; in other words, no actual traffic goes over it; it is just a Route-Reflector it only carries the routing table. TGW is not a region construct, so by itself, it cannot provide regional redundancy. In any case, all things considered, maybe A is the closest but it should mention VGW."
      },
      {
        "date": "2023-02-21T09:16:00.000Z",
        "voteCount": 1,
        "content": "I meant to say, \"TGW is a region construct\"."
      },
      {
        "date": "2023-02-19T01:51:00.000Z",
        "voteCount": 8,
        "content": "Option D is not possible at all. You connect to TGW using transit VIF, not private VIF"
      },
      {
        "date": "2023-09-15T04:37:00.000Z",
        "voteCount": 1,
        "content": "Transit GW - connects both over Private VIF and Transit VIF"
      },
      {
        "date": "2024-03-15T15:40:00.000Z",
        "voteCount": 5,
        "content": "What I don't understand is why do you need to delete the existing private VIF? Can't that be reassigned?"
      },
      {
        "date": "2024-09-22T21:30:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option might work however it is missing a step: Connecting the Direct Connect Gateway to a Virtual Private Gateway in the single VPC (and any VPC in a new region)\n\nOption B - This option will not work: It does not allow to grow into new regions and it does not create a redundant link\n\nOption C - This option will not work: Using a Public Virtual interface does not connect VPC resources to on-premise\n\nOption D - This option might work however it missing multiple steps: Each VPC will require its own Transit Gateway. Each Transit Gateway will connect through an association with Direct Connect gateway. Each Direct Connect connection will connect to the Direct Connect Gateway using a Transit VIF"
      },
      {
        "date": "2024-09-22T21:29:00.000Z",
        "voteCount": 3,
        "content": "I have to admit that initially I picked a wrong answer, here is my findings after some docs browsing:\nNot B as this will provide Direct Connect (DX) redundancy but does not provide connectivity to other Regions\nNot C as this will not even provide DX redundancy for the VPC because the public VIF on the new connection does not provide access to the VPC\nNot D as Transit Gateway (TGW) is a regional resources and does not allows to provide connectivity to other Regions (you can peer with a TGW in another Region). Also you need to have a Transit virtual interface to connect a DX to a TGW or you need to have  DXGW to connect a VIF to a TGW.\n\nA is correct as a DXGW is a global resources that allows cross-region attachments"
      },
      {
        "date": "2024-08-31T01:59:00.000Z",
        "voteCount": 1,
        "content": "A. Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC."
      },
      {
        "date": "2024-03-13T07:05:00.000Z",
        "voteCount": 1,
        "content": "Private Virtual Interface is a logical connection between your Direct Connect connection and a Direct Connect gateway. It is a virtual representation of the physical connection and allows you to establish connectivity to the VPCs associated with the Direct Connect gateway."
      },
      {
        "date": "2023-12-28T22:41:00.000Z",
        "voteCount": 1,
        "content": "A\nBecause \u201cTransit GW can also communicate from on-premises to AWS, but this one uses Site to Site VPN (IPSec VPN).\u201c"
      },
      {
        "date": "2023-12-03T21:16:00.000Z",
        "voteCount": 3,
        "content": "Answer A. DCGW is the only option here as it supports both DC connections plus allows expansion into other regions. TGW does not span regions."
      },
      {
        "date": "2023-11-11T22:04:00.000Z",
        "voteCount": 1,
        "content": "multiple regions - dx gateway"
      },
      {
        "date": "2023-09-15T04:50:00.000Z",
        "voteCount": 1,
        "content": "None of the options seem to satisfy the condition \"Solution must provide connectivity to other  regions through same pair of Direct Connect Connections.\n\nIn both option A and D, we don't talk of associating second region VPC to the Transit GW or Direct Connect GW."
      },
      {
        "date": "2023-08-22T18:13:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/"
      },
      {
        "date": "2023-06-27T19:20:00.000Z",
        "voteCount": 1,
        "content": "It's A.\nD is not suported"
      },
      {
        "date": "2023-06-17T20:37:00.000Z",
        "voteCount": 1,
        "content": "A\nkeyword  === Direct Connect gateway"
      },
      {
        "date": "2023-05-08T05:32:00.000Z",
        "voteCount": 1,
        "content": "A. Is the Correct Option as Direct Connect Gateway with Private Virtual Interface will meet the requirement"
      },
      {
        "date": "2023-03-27T23:03:00.000Z",
        "voteCount": 2,
        "content": "Provision a Direct Connect gateway."
      },
      {
        "date": "2023-02-21T22:08:00.000Z",
        "voteCount": 3,
        "content": "Logical answer : B and C are good for existing architecture in question. But with redundant DX connection requirement, only solution is Gateway. that resolves to A(Direct connect gateway) or D(Transit gateway), but D as transit gateway is wrong because it mentions private interfaces connecting with transit gateway which is weird [usually VPC attachments are made connecting transit gateway]. So answer is A - Direct Connect Gateway. (Infact, this is future proof when we want different VPCs in different regions later with this architecture)"
      },
      {
        "date": "2023-01-30T05:18:00.000Z",
        "voteCount": 5,
        "content": "A is the correct solution and the best"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/91203-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.<br>The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.<br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 78,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T21:30:00.000Z",
        "voteCount": 30,
        "content": "This solution meets the requirements by using multiple managed services offered by AWS which can reduce the operational overhead. Hosting the web application in Amazon S3 would make it highly available, scalable and can handle variable traffic. The uploaded videos can be stored in S3 and processed using S3 event notifications that trigger a Lambda function, which calls the Amazon Rekognition API to categorize the videos. SQS can be used to process the event notifications and also it is a managed service.\nThis solution eliminates the need to manage EC2 instances, EBS volumes and the custom software. Additionally, using Lambda function in this case, eliminates the need for managing additional servers to process the SQS queue which will reduce operational overhead.\n\nBy using this solution, the company can benefit from the scalability, reliability, and cost-effectiveness that these services offer, which can help to reduce operational overhead and improve the overall performance and security of the application."
      },
      {
        "date": "2023-09-25T21:05:00.000Z",
        "voteCount": 1,
        "content": "Any explanation on option A ?"
      },
      {
        "date": "2024-09-09T12:00:00.000Z",
        "voteCount": 1,
        "content": "ECS is managed to an extent, but the question fails to elaborate, no mention of fargate etc. There's unnecessary mentions of spot instances to confuse you. The web application has static content which can be hosted in S3 instead of ECS"
      },
      {
        "date": "2023-04-06T09:12:00.000Z",
        "voteCount": 12,
        "content": "D. Because, you cannot host web application in S3, only static web assets. ElasticBeanStalk provides an easy way to onboard autoscaling web apps with minimal operational overheads."
      },
      {
        "date": "2024-04-11T19:43:00.000Z",
        "voteCount": 1,
        "content": "Rekognition no consulta directamente EBS, pero puedes cargar datos en un recurso de almacenamiento compatible con Rekognition, como S3, para que Rekognition realice an\u00e1lisis sobre esos datos."
      },
      {
        "date": "2024-04-11T19:44:00.000Z",
        "voteCount": 1,
        "content": "Rekognition does not query EBS directly, but you can upload data to a Rekognition-compatible storage resource, such as S3, for Rekognition to perform analysis on that data."
      },
      {
        "date": "2024-03-16T10:19:00.000Z",
        "voteCount": 2,
        "content": "\"The company wants to re-architect the application \"..."
      },
      {
        "date": "2023-08-31T00:55:00.000Z",
        "voteCount": 3,
        "content": "But it is specifically specified that the web app is just static content..."
      },
      {
        "date": "2023-09-06T07:12:00.000Z",
        "voteCount": 1,
        "content": "\"The website contains static content\"\n\nContains do not means that all the website is just static"
      },
      {
        "date": "2023-09-09T05:49:00.000Z",
        "voteCount": 10,
        "content": "They also do not mention the website has any dynamic content so there's that"
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 4,
        "content": "The answer is C.\n\nThis solution eliminates the need for managing and scaling EC2 instances for the web application and the worker environment for processing the SQS queue. Instead, Amazon S3 can host the web application, and store the uploaded videos, which can trigger S3 event notifications to send messages to the SQS queue. Then, an AWS Lambda function can process the messages in the SQS queue and use Amazon Rekognition API to categorize the videos. This approach also takes advantage of AWS-managed services, such as S3, SQS, and Lambda, which reduces operational overhead and dependency on third-party software."
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 1,
        "content": "C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.\n\nExplanation:\nHosting the Web Application in Amazon S3:\n\nCost-effective and Scalable: Amazon S3 is a cost-effective and scalable solution for hosting static web content. It can handle variable traffic efficiently without the need to manage servers.\nStatic Content Hosting: Ideal for serving static content like HTML, CSS, JavaScript, and media files."
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 1,
        "content": "While I vote for C, I do think however that whether we can go with C really depends on the application codebase.\nThe use case mentions that the application enables file uploads. We know that handling files require a backend, if your application is written in something like Java. If that's the case, you won't be able to host your application in S3. The phrase \"website contains static content\" is really vague, as it does not reveal anything about the backend of the application.\nNow, the fact that the application has EBS to store Video files give up a hint, that suggests that the application has some BE code. \nI am taking a hint from \"re-architect\" I assume involves some revamping of the applications codebase. So, here's how I'd go about \"re-architecting\"\n1. Move storage of files to S3.\n2. Eliminate the BE codebase, revamp the FE codebase to rely entirely on AWS JS SDK and handle file uploads with that. Now you don't need to manage any compute resources at all.\n3. Go about the rest of the solution."
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 2,
        "content": "The mention of static content really throws this question off and clearly the community thinks this as well. The argument of static website vs static content being the key to selecting D isn't really a strong argument but that doesn't exclude D from being a viable solution. Operational overhead is minimized with Elastic Beanstalk and removes dependencies on third party tools/software."
      },
      {
        "date": "2024-03-13T17:55:00.000Z",
        "voteCount": 1,
        "content": "thanks, this is the best explain"
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 1,
        "content": "If it were D - how would Rekognition access the videos to classify? Rekognition would need to ssh into the EBS volume of various beanstalk instances running under an ASG (impossible as far as I know).  I agree though - I think the wording is terrible for 'contains static content'; as how on earth would this type of app practically run on s3 alone for login/ user auth etc.. would need to be coupled with other serverless products such as lambda/cognito etc."
      },
      {
        "date": "2024-01-09T15:08:00.000Z",
        "voteCount": 1,
        "content": "per my previous comment; s3 is the only viable data source for rekognition https://aws.amazon.com/rekognition/faqs/#:~:text=Amazon%20Rekognition%20Video%20operations%20can,are%20MPEG%2D4%20and%20MOV. \n\nfrom my experience this is the same too with similar services like elastic transcoder"
      },
      {
        "date": "2024-08-31T02:01:00.000Z",
        "voteCount": 1,
        "content": "C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos."
      },
      {
        "date": "2024-08-06T13:54:00.000Z",
        "voteCount": 1,
        "content": "I saw this question in other question bank (owner of the questions) and it is A, reason is assuming is moving files back and forth cannot be static page, so it is A."
      },
      {
        "date": "2024-06-16T11:55:00.000Z",
        "voteCount": 1,
        "content": "Only Answer C  is the solution that covers all the requirements, where the videos are stored, how SQS messages are produced and consumed, how web app is hosted."
      },
      {
        "date": "2024-03-16T10:19:00.000Z",
        "voteCount": 1,
        "content": "C, this is a typical scenario"
      },
      {
        "date": "2024-03-13T07:17:00.000Z",
        "voteCount": 1,
        "content": "re-architect the application to reduce operational overhead"
      },
      {
        "date": "2023-12-05T02:57:00.000Z",
        "voteCount": 1,
        "content": "Elastic bean stack is not required , it is a static content only, better can go with S3. So Answer is C"
      },
      {
        "date": "2023-11-26T10:47:00.000Z",
        "voteCount": 1,
        "content": "C videos in Amazon S3"
      },
      {
        "date": "2023-11-25T00:03:00.000Z",
        "voteCount": 2,
        "content": "Web application is never hosted in S3, that is storage normally."
      },
      {
        "date": "2023-11-11T22:13:00.000Z",
        "voteCount": 1,
        "content": "C is a well-explained and detailed solution. For D it isn't like that, for instance, there is no solution provided for storing images."
      },
      {
        "date": "2023-10-03T03:46:00.000Z",
        "voteCount": 2,
        "content": "It's A, I had the same question in Jon Bonzo's tests and the right answer is A."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/91242-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.<br>How can this be accomplished?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 49,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T10:39:00.000Z",
        "voteCount": 29,
        "content": "AWS Serverless Application Model (SAM) is a framework that helps you build, test and deploy your serverless applications. It uses CloudFormation under the hood, so it is a way to simplify the process of creating, updating, and deploying CloudFormation templates. CodeDeploy is a service that automates code deployments to any instance, including on-premises instances and Lambda functions.\nWith AWS SAM you can use the built-in CodeDeploy to deploy new versions of the Lambda function, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code.\nYou can also define CloudWatch Alarms to trigger a rollback in case of any issues.\nThis allows for a faster and more efficient deployment process, as well as a more reliable rollback process when errors are identified. This way you can increase the speed of deployment and reduce the time to detect and revert when errors are identified."
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\n\nAWS Serverless Application Model (AWS SAM) comes built-in with CodeDeploy to provide gradual AWS Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you:\n\nDeploys new versions of your Lambda function, and automatically creates aliases that point to the new version.\n\nGradually shifts customer traffic to the new version until you're satisfied that it's working as expected. If an update doesn't work correctly, you can roll back the changes.\n\nDefines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and that your application operates as expected.\n\nAutomatically rolls back the deployment if CloudWatch alarms are triggered."
      },
      {
        "date": "2024-09-22T21:31:00.000Z",
        "voteCount": 2,
        "content": "Option A - This work will allow reverting to previous versions of the Lambda functions but reverting means all functions will be reverted. This does not minimize the the time needed to detect and revert errors.\n\nOption B - This option minimizes the time needed to deploy functions and detect and revert errors: As each function is deployed it can be tested and reverted individually. Moreover, the option provides a straightforward mechanism to detect and revert errors: Detect errors in CloudWatch, fix the functions' code in SAM, redeploy with AWS CodeDeploy.\n\nOption C - This option does not minimize the time needed to detect and revert errors. It only automates the current process.\n\nOption D - This option does not minimize the time needed to detect and revert errors: It takes time for CloudFormation to switch origins and nothing has been done to about the current process for deploying and testing functions."
      },
      {
        "date": "2024-09-09T12:10:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\n\nPretty much what the question wants"
      },
      {
        "date": "2024-08-31T02:06:00.000Z",
        "voteCount": 1,
        "content": "B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered."
      },
      {
        "date": "2024-06-15T21:48:00.000Z",
        "voteCount": 2,
        "content": "why not a\uff1f"
      },
      {
        "date": "2024-03-16T10:27:00.000Z",
        "voteCount": 1,
        "content": "B, use SAM to deploy serverless applications on aws"
      },
      {
        "date": "2023-12-04T06:14:00.000Z",
        "voteCount": 1,
        "content": "Answer B. Use SAM and Codedeploy. Revert if any errors to the previous version."
      },
      {
        "date": "2023-11-11T22:15:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-08-22T18:58:00.000Z",
        "voteCount": 1,
        "content": "requirmeents : \ndecrease the time to deploy new versions of the application logic provided by the Lambda functions,\nrevert when erros identified"
      },
      {
        "date": "2023-06-27T19:23:00.000Z",
        "voteCount": 1,
        "content": "B no do0ubt"
      },
      {
        "date": "2023-06-26T02:55:00.000Z",
        "voteCount": 1,
        "content": "100% B"
      },
      {
        "date": "2023-05-08T05:51:00.000Z",
        "voteCount": 1,
        "content": "B solve the problem which is causing in the current scenario"
      },
      {
        "date": "2023-04-19T07:21:00.000Z",
        "voteCount": 1,
        "content": "Definitile B\nhttps://docs.aws.amazon.com/es_es/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
      },
      {
        "date": "2023-03-27T23:06:00.000Z",
        "voteCount": 1,
        "content": "Use AWS SAM and built-in AWS CodeDeploy"
      },
      {
        "date": "2023-03-07T03:46:00.000Z",
        "voteCount": 1,
        "content": "AWS Serverless Application Model (SAM)"
      },
      {
        "date": "2022-12-22T08:03:00.000Z",
        "voteCount": 3,
        "content": "sam typical use case"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/91211-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.<br>The documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.<br>Which solution will meet these requirements at the LOWEST cost?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 76,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 37,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T10:46:00.000Z",
        "voteCount": 38,
        "content": "A - Glacier Deep Archive can't be used for web hosting, regardless if the company says retrieval time is no concern."
      },
      {
        "date": "2022-12-20T10:52:00.000Z",
        "voteCount": 17,
        "content": "Nevermind, I go for D.\nIt should be technically possible - and mostly dependent on the intranet web application logic - It could present users with the ability to start file retrieval, for then to later access the data."
      },
      {
        "date": "2022-12-12T06:48:00.000Z",
        "voteCount": 21,
        "content": "A is correct. HA is not required here. \nD use Glacier deep archive that need hours to access that will cause time out for web"
      },
      {
        "date": "2024-09-22T21:33:00.000Z",
        "voteCount": 4,
        "content": "The requirements are to store a large number of archived documents that are not publicly accessible, and make them available to employees through a corporate intranet. As the number of requests is low and speed of retrieval is not a concern, we can use the low-cost S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class. We can configure the S3 bucket for website hosting and create an S3 interface endpoint to allow access to the documents only through the corporate intranet. This solution is the lowest cost as it eliminates the need to launch and manage EC2 instances.\n\nOption B and C involve launching an EC2 instance which increases the operational overhead and is more expensive than using S3. Also, EFS One Zone-IA storage class is not recommended for storing large files.\n\nOption D involves using the S3 Glacier Deep Archive storage class which is intended for long-term archival storage of data and not suitable for retrieving data frequently."
      },
      {
        "date": "2023-07-30T22:04:00.000Z",
        "voteCount": 1,
        "content": "S3 interface endpoint doesn't support web hosting.\nThe question does not say large files, but large number of archived documents, which could be small-sized. Hence EFS OZ-IA (being cheaper than SC1) could be the right answer."
      },
      {
        "date": "2024-09-22T21:33:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\nStore large number of archived docs, and available through corp intranet. \nCopies of data held on physical media elsewhere (could be re-created). \nRequests low (but it doesn't say RARE so think monthly/quarterly).\n\"AVAILABILITY\" and speed of retrieval are not concerns.\n\nIt is A, yes Glacier is \"cheaper\", but I have to leave the archives for at least 180 days, would be available on corp intranet and it is more cost-effective if I want to migrate the data to Glacier if I monitor use and see it is \"rarely\" touched and know I have to hold it due to regulatory for at minimal 180 days."
      },
      {
        "date": "2024-09-22T21:33:00.000Z",
        "voteCount": 4,
        "content": "While D is most probably the cheapest solution.\nWhen you try to download an object in deep archive you get a warning that it is not possible. You need to retrieve it: got to actions and restore which will need at least 12 hours for *deep* archive.\nOnly after that you can access the document. \nThe answer D says enable webhosting: thats afaik not going to work but users will end up in above mentioned warning.\nTherefore we need to go for  A which is not as cheap but users can access the documents."
      },
      {
        "date": "2024-09-22T21:33:00.000Z",
        "voteCount": 3,
        "content": "Given the requirements and the need for the lowest cost solution, the best option would be:\n\nA. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\nOptions B and C involve launching EC2 instances which would add unnecessary complexity and cost since the company's priority is to minimize costs. Additionally, option D involves using the S3 Glacier Deep Archive storage class which is intended for long-term archival data and has longer retrieval times, making it less suitable for the given requirements."
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 2,
        "content": "Answer A. \nB and C are not relevant.\nD is close to create confusion but can't be used as an option for 2 reasons:\n1. You can't create a S3 bucket with Glacier deep archive as a default storage class. Need lifecycle transition from any other S3 classes. \n2. S3 Glacier deep archive can't be used for website hosting."
      },
      {
        "date": "2024-02-06T18:53:00.000Z",
        "voteCount": 2,
        "content": "1 You can create - read here https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\n2&gt; Yes you are correct on this - So will go with answer A too.."
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option will work and S3 One Zone is a cheap storage solution for a large number of documents\n\nOption B - This option might not work: Nothing is said in the question about whether the Client VPN is connecting to a private subnet. Moreover, EFS might not be a cheap storage solution for a large number of documents\n\nOption C - This option might not work: Nothing is said in the question about whether the Client VPN is connecting to a private subnet. Moreover, EBS Cold HDD might not be a cheap storage solution for a large number of documents\n\nOption D - This option will not work: S3 Deep Glacier Vaults cannot be configured for static hosting. You would need to write an application for accessing the archives."
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 7,
        "content": "pay attention to \"copies of data that is held on physical media elsewhere\", this is hint for one zone.  Using Glacier is possible in theory, but won't work out of box.  Need to develop a whole new application to submit unarchive request when user request a file, wait for up to 48 hour,  create the s3 link, notify the user and ask user to come back to view the file.  This is ANOTHER application"
      },
      {
        "date": "2024-03-13T18:01:00.000Z",
        "voteCount": 1,
        "content": "I agree, \"copies of data that is held on physical media elsewhere\", this is hint for one zone, \n\nHowever, it could be multiple zone as well.\n\nAvailability and speed of retrieval are not concerns of the company.\n\nSo I go with D"
      },
      {
        "date": "2024-03-15T17:59:00.000Z",
        "voteCount": 1,
        "content": "This! It's also worth mentioning that, the application we have to develop for option D, will be very difficult, if not impossible to be hosted in S3, because it will be a stateful application."
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 1,
        "content": "For me is A.\nI'm not sure that S3 Glacier Deep Archive can be used as website. Also more than 12 hours to retrieve is so much for documental systems (also if is not a concern the speed up).\nGoing with A"
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 2,
        "content": "Objects in Glacier Deep Archive needs to be 'restored'. A click on simple static website will not make AWS API call to restore the object and make it available."
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 1,
        "content": "Option D:  2 major points for the company. 1. Availability and speed of retrieval are NOT concerns of the company. 2. Meets these requirements at the LOWEST cost.  Only S3 Glacier Deep Archive gives the company those requirement. The questions doesn't state how fast the employees need to access the files but the company does, see point 1.  S3 Glacier Deep Archive is the lowest-cost storage option in AWS. Standard-IA and S3 One Zone-IA objects are available for millisecond access\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
      },
      {
        "date": "2024-09-22T21:32:00.000Z",
        "voteCount": 1,
        "content": "The answer is A.\nD is wrong because the data stored on Glacier Deep Archive can't be accessed directly without initiating a retrieval request to restore the data to either S3 Standard or S3 Standard-IA first. Needless to say, use it as static web site."
      },
      {
        "date": "2024-09-02T00:54:00.000Z",
        "voteCount": 1,
        "content": "A - Even Deep Glacier is lowest cost and company says retrieval time is no concern, Deep Glacier cannot be used for web hosting."
      },
      {
        "date": "2024-08-31T02:08:00.000Z",
        "voteCount": 1,
        "content": "A. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint."
      },
      {
        "date": "2024-08-10T11:06:00.000Z",
        "voteCount": 1,
        "content": "A because : No web static hosting on Glacier deep archive + Glacier is the cheapest but can be more expensive than One zone-IA if the employees retrieve the document (retrieval costs are high) + timeout on the front-end because it will take hours to retrieve the file."
      },
      {
        "date": "2024-06-19T15:13:00.000Z",
        "voteCount": 2,
        "content": "I went with D so did ChatGPT yet the majority of folks have chosen A ...How do we know what is the exact answer...I see why One Zone IA should be used but I am not confident, please help"
      },
      {
        "date": "2024-07-29T10:01:00.000Z",
        "voteCount": 4,
        "content": "You cannot have a website on Glacier, so D is clearly wrong. To retrieve documents from Glacier you need to first call 'restore' on it. The object becomes available after considerable amount of time in standard storage class a for a limited duration. This wouldn't work on a static website.\n\nI suppose technically you could build a non-static application to manage restoring files for users, but it's awkward, and the solution would likely cost more due to development costs. Glacier's purpose is to store data that you never want to see again, but there is a 0.001% chance you might actually need it at some point. It is comparable to tape storage.\n\nChatGPT cannot answer these questions accurately because it is unable to reason."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/91245-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the company\u2019s AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all the company\u2019s AWS accounts.<br>The company\u2019s security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn one of the company\u2019s AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn one of the company\u2019s AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 68,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T10:52:00.000Z",
        "voteCount": 28,
        "content": "https://www.examtopics.com/discussions/amazon/view/74174-exam-aws-certified-solutions-architect-professional-topic-1/\n\nBoth option C and option A are valid solutions that meet the requirements for the scenario.\n\nABAC, or attribute-based access control, is a method of granting access to resources based on the attributes of the user, the resource, and the action. This allows for fine-grained access control, which can be useful for implementing a security policy that requires conditional access to the accounts based on user groups and roles.\n\nAWS IAM Identity Center (AWS SSO) allows you to connect to your on-premises Active Directory service using SAML 2.0. With this, you can enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol, which allows for the management of user identities in a single location."
      },
      {
        "date": "2023-01-13T10:52:00.000Z",
        "voteCount": 2,
        "content": "In option C, the company will use IAM to use a SAML 2.0 identity provider, and it will use the appropriate groups in Active Directory to grant access to the required AWS accounts by using cross-account IAM users. In this way, it can implement its security policy of conditional access to the accounts based on user groups and roles.\n\nIn summary, both option A and C are valid solutions, both of them allow you to use your on-premises Active Directory service for user authentication, and both of them allow you to manage user identities in a single location and grant access to the AWS accounts based on user groups and roles."
      },
      {
        "date": "2023-01-18T09:12:00.000Z",
        "voteCount": 14,
        "content": "A is has options for SAML and SCIM configuration with AD\nC is all about users and no roles are mentioned. AD User attributes cannot be mapped to IAM users direct\nD is openID based, MS AD would not support this\n\nso I go with A"
      },
      {
        "date": "2023-11-12T10:29:00.000Z",
        "voteCount": 3,
        "content": "native AD  doesn't support SAML 2.0 without an ADFS server. SCIM is also not supported at all. SCIM provisioning is supported by other IDPs like Azure AD"
      },
      {
        "date": "2024-04-07T12:00:00.000Z",
        "voteCount": 1,
        "content": "Si, si son compatibles.  https://aws.amazon.com/es/directoryservice/faqs/"
      },
      {
        "date": "2023-11-12T10:31:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/supported-idps.html"
      },
      {
        "date": "2024-08-15T03:11:00.000Z",
        "voteCount": 1,
        "content": "AWS IAM Identity Center + SAML"
      },
      {
        "date": "2024-01-26T12:06:00.000Z",
        "voteCount": 2,
        "content": "A is correct\n\nReasons -\n\nOption A mentions about Active Directory as identity Source configuration which solves the purpose of establishing trust and sync from on prem AD using Directory Service. Solves the purpose of using on-prem AD as Single Sign On asked in the question. \n\nIt is also mentioned that AWS org is in place, which works well with AWS Identity Centre. Gives another validation. It gives us hint of efficiently managing AWS Org accounts / OUs with Identity Centre (Permission Set behind the scene ) to manage RBAC within accounts.\n\nFinally this line - \"The company's security policy requires conditional access to the accounts based on user groups and roles.\" is talking about conditional access which can only be solved by ABAC(Attribute Based Access Control). For example user with green attribute should only get access to resources with green attribute. This can be solved by Tag functionality within AWS Identity Centre."
      },
      {
        "date": "2023-12-19T18:32:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option works however it moves authentication and managing user identities from Active Directory to Identity Center but the question states the company wants to use the same authentication service to sign into AWS in reference to Active Directory\n\nOption B - This option works but it moves user identity management and authentication tie Identity Center which is not what the question states the company wants to do\n\nOption C - This option does not work because in AWS you provision cross-account IAM roles rather than users.\n\nOption D - This option might work but it is missing AD FS, a component that enables OIDC flows in AD. Otherwise it maintains user identity management in one place and allows the company to keep using Active Directory for authentication as the question states"
      },
      {
        "date": "2023-12-16T09:08:00.000Z",
        "voteCount": 2,
        "content": "Didn't spent time checking if C and D works, because when you have an AWS Organitazion and need to use AD to sign-in to the company\u2019s AWS accounts AWS IdC is the way to go. \n\nNow, with AWS IdC we need ADFS and while ADFS does not support SCIM, it is possible to still have your users and groups automatically synchronize with the IAM IDC by using the SCIM API and PowerShell as per https://aws.amazon.com/blogs/modernizing-with-aws/synchronize-active-directory-users-to-aws-iam-identity-center-using-scim-and-powershell/#:~:text=While%20ADFS%20does%20not%20support,the%20SCIM%20API%20and%20PowerShell.\n\nFinally, ABAC is an authorization strategy and it is not alternative to IdC Permission Sets. Also the scenario requires conditional access to the accounts based on user groups and roles, this point me to RBAC strategy. I would pick ABAC if the request mentioned user attributes like Department, Cost Center or Project thus."
      },
      {
        "date": "2024-01-30T02:01:00.000Z",
        "voteCount": 1,
        "content": "After reviewing it, the correct answer is A. \"User identities must be managed in a single location\" -&gt; \"Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0\" while B states \"Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source\". Using AWs IdC as identity source will not meet requirement to manage all users in a single place"
      },
      {
        "date": "2023-12-12T17:54:00.000Z",
        "voteCount": 3,
        "content": "Answer A for AWS SSO would the right answer at first glance since IAM roles can be mapped to AD groups but it would require additional AD functions like ADFS for SCIM so the next best option is D."
      },
      {
        "date": "2023-12-05T03:20:00.000Z",
        "voteCount": 1,
        "content": "A is a correct one, because need to use the SAML for single sign on from the on-premise directory and also C is not correct because the federated should not come in to the picture federated is for only facebook, twitter, gmail account sign on - but we should use the companies active directory, so A is a correct one."
      },
      {
        "date": "2023-11-27T15:29:00.000Z",
        "voteCount": 1,
        "content": "AD and SCIM don't go together so forget A and B. I've never seen a document talking about integrating OpenID with AWS account login so D is also out. C is doable so I go with C."
      },
      {
        "date": "2024-04-07T12:02:00.000Z",
        "voteCount": 1,
        "content": "P: \u00bfPuedo usar la autenticaci\u00f3n basada en lenguaje de marcado de aserci\u00f3n de seguridad (SAML) 2.0 con aplicaciones de la nube que usen AWS Managed Microsoft AD?\n\nS\u00ed. Puede usar los servicios federados de Microsoft Active Directory (AD FS) para Windows 2016 con su dominio administrado de AWS Managed Microsoft AD para autenticar usuarios en aplicaciones en la nube compatibles con SAML.     https://aws.amazon.com/es/directoryservice/faqs/"
      },
      {
        "date": "2023-11-27T13:45:00.000Z",
        "voteCount": 1,
        "content": "I am with B on this one. A is incorrect because you can only use ABAC (Attribute-Based Access Control) with IAM Identity Center Identity Store NOT with Active Directory"
      },
      {
        "date": "2023-12-16T08:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with you on B, but:\n\n- You can use IAM Identity Center to manage access to your AWS resources across multiple AWS accounts using user attributes that come from any IAM Identity Center identity source - https://docs.aws.amazon.com/singlesignon/latest/userguide/abac.html\n\n- ABAC is an authorization strategy that defines permissions based on attributes and it is implemented using IdC Permission Sets."
      },
      {
        "date": "2023-11-25T08:59:00.000Z",
        "voteCount": 1,
        "content": "As mentioned, SAML 2.0 doesn't directly integrate with AD and requires ADFS proxy as a go between, so the lack of ADFS being mentioned in A or B is throwing people off.  However, AD on-premise with direct/VPN connectivity...IAM identify center is the way to go for SSO.  I believe ADFS is implied when the question casually mentions \"IAM Identify Center connect to AD using SAML 2.0\"."
      },
      {
        "date": "2023-11-11T22:54:00.000Z",
        "voteCount": 1,
        "content": "federated IdP is required and access to multiple accounts"
      },
      {
        "date": "2023-11-09T07:17:00.000Z",
        "voteCount": 3,
        "content": "Answer A and B are wrong!!! \nActive Directory doesn't support SAML without the use of Active Directory Federation Server!! SCIM is also not supported. The articles that all are pasting here mention the need of an AD connect or the  trust between the local AD and an AWS managed Microsoft AD which is not the case here.\nC is also wrong. Cross account IAM users option doesn't exist.\nThe correct is D!! You can use an OpenID Connect (OIDC) identity provider (e.g OKTA or Azure AD) and sync AD  groups in it. You can then use cross account roles to grant access to the federated users\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\nhttps://help.okta.com/en-us/content/topics/directory/ad-agent-manage-users-groups.htm\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html"
      },
      {
        "date": "2023-10-03T04:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/onelogin-idp.html#onelogin-passing-abac"
      },
      {
        "date": "2023-09-28T21:40:00.000Z",
        "voteCount": 3,
        "content": "A: combination SSO + SAML2.0 + AD sounds correct. Automatic provisioning with SCIM means creating users and groups that synced with AD. ABAC seems not too fit for this as the requirements is \"requires conditional access to the accounts based on user groups and roles\" but that already satisfied with SCIM. \n\nB: \"use Identity Center as an identity source\" -&gt; not using on premise AD -&gt; wrong\n\nD: use OIDC -&gt; wrong as on premise AD does not support OIDC. Cannot find an exact source for this but ChatGpt says so..\n\nC: creating users mapped to federated users sounds red flags. Could have been correct if it was \"creating roles\", the same way with the classic \"creating roles for EC2 to access S3 instead of user...\"\n\nConclusion: A"
      },
      {
        "date": "2023-08-31T10:50:00.000Z",
        "voteCount": 2,
        "content": "More compreshensive approach \nhow to map users, grant access based on groups, and utilize cross-account IAM users."
      },
      {
        "date": "2023-08-31T10:48:00.000Z",
        "voteCount": 1,
        "content": "C provides more comprehensve approach"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/91247-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an Amazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a small number of clients that are authenticated with specific API keys.<br>A solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API\u2019s reputation.<br>What should the solutions architect recommend to improve the customer experience?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 65,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T12:18:00.000Z",
        "voteCount": 40,
        "content": "API throttling is a technique that can be used to control the rate of requests to an API. This can be useful in situations where a small number of clients are making a large number of requests, which is causing errors. By implementing API throttling through a usage plan at the API Gateway level, the solutions architect can limit the number of requests that a client can make, which will help to reduce the number of errors.\n\nIt's important that the client application handles the code 429 replies without error, this will help to improve the customer experience by reducing the number of errors that are displayed to customers. Additionally, it will prevent the API's reputation from being damaged by the errors."
      },
      {
        "date": "2023-01-13T12:18:00.000Z",
        "voteCount": 15,
        "content": "It is important to note that other solutions such as retry logic with exponential backoff and irregular variation in the client application or turn on API caching to enhance responsiveness for the production stage may help to improve the customer experience and reduce errors, but they do not address the root cause of the problem which is a large number of requests coming from a small number of clients.\n\nImplementing reserved concurrency at the Lambda function level can provide resources that are needed during sudden increases in traffic, but it does not address the issue of a client making a large number of requests and causing errors."
      },
      {
        "date": "2022-12-28T10:41:00.000Z",
        "voteCount": 8,
        "content": "B is correct. API gateway throttling is applied to single account - https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html. Retry will make it even worse."
      },
      {
        "date": "2024-08-31T02:22:00.000Z",
        "voteCount": 1,
        "content": "B. Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error."
      },
      {
        "date": "2024-08-15T03:14:00.000Z",
        "voteCount": 1,
        "content": "API gateway throttling"
      },
      {
        "date": "2024-08-03T05:07:00.000Z",
        "voteCount": 2,
        "content": "Key word: a large number of the PUT requests, one client\n\nSeeing this will ring a bell on throttling on API Gateway. But normally you also need to make sure when the client side see \"429 too many attempts\", the app can capture that error code and show some reasonable error message(e.g. You have sent too many requests .Please try again later)"
      },
      {
        "date": "2024-03-16T10:48:00.000Z",
        "voteCount": 1,
        "content": "B. C only will help with GET requests, and A and D don't prevent it"
      },
      {
        "date": "2024-03-05T14:28:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html"
      },
      {
        "date": "2024-02-16T22:15:00.000Z",
        "voteCount": 1,
        "content": "B ans : https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html"
      },
      {
        "date": "2024-02-03T03:52:00.000Z",
        "voteCount": 1,
        "content": "This question missing MASSIVE information.. none of the answers can fulfil the requirements.."
      },
      {
        "date": "2024-01-15T13:05:00.000Z",
        "voteCount": 1,
        "content": "There is no evidence indicating the problem is with the throughput. If it is throughput, other clients will have similar problem.\nAnd \u201cthe errors are displayed to customers and are causing damage to the API\u2019s reputation.\u201d, this means the solution should be able to reduce the error message showed on the client side, while, throttling the client will actually close the service for this particular client, which is against the \u201cclients can tolerate retries of unsuccessful calls\u201d.\nI vote A for this question."
      },
      {
        "date": "2023-12-19T22:55:00.000Z",
        "voteCount": 1,
        "content": "The solutions architect should recommend option B: Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.\n\nOption B is the most directly related recommendation to improving the customer experience, as it addresses the issue of API rate limiting and ensures a more predictable and controlled experience for users."
      },
      {
        "date": "2023-12-19T21:49:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option will make retries take longer on each retry for all clients rather than for the few causing issues in the application\n\nOption B - This option will work: An usage plan will allow throttling requests from specific clients identified by their API Key and ensuring client applications can handle throttling errors provides a consistent experience\n\nOption C - This option has no relation with the problem at hand\n\nOption D - This option assumes there is a capacity issue managing the increase in volumes but given that errors occur due to a small number of clients then reserved concurrency will not address the cause of the issue"
      },
      {
        "date": "2023-12-19T18:49:00.000Z",
        "voteCount": 1,
        "content": "Option A - This option will make retries take longer on each retry for all clients rather than for the few causing issues in the application\n\nOption B - This option will work: An usage plan will allow throttling requests from specific clients identified by their API Key and ensuring client applications can handle throttling errors provides a consistent experience\n\nOption C - This option has no relation with the problem at hand\n\nOption D - This option assumes there is a capacity issue managing the increase in volumes but given that errors occur due to a small number of clients then reserved concurrency will not address the cause of the issue"
      },
      {
        "date": "2023-12-17T01:06:00.000Z",
        "voteCount": 1,
        "content": "Usage Plan throttling prevents a group of users or a single user to saturate the API concurrency capacity. Thus B. Also A and D can help in this scenario, but they will have less benefit with respect to B. While C does not help in this scenario as I do not see how API Gateway caching can help PUT requests"
      },
      {
        "date": "2023-11-11T22:56:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-08-31T11:12:00.000Z",
        "voteCount": 1,
        "content": "Implementing API throttling through a usage plan at the API Gateway level would directly address the issue of too many requests from a single client causing errors. Properly handling status code 429 can help clients understand the situation, and throttling ensures fair usage and prevents overload, ultimately improving the customer experience."
      },
      {
        "date": "2023-08-30T02:19:00.000Z",
        "voteCount": 2,
        "content": "B. Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.\n\nOptions A and D might help with general improvements in resilience and resource allocation, but they do not specifically address the issue of a single client causing a large number of errors.\n\nOption C, involving API caching, is not the most appropriate solution in this scenario, as caching might not directly address the issue of the client generating a high volume of errors. It might improve responsiveness for frequently accessed data, but it doesn't directly address the issue of client errors."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/91447-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200&nbsp;TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72&nbsp;hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region.<br>A solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.<br>Which solution will provide the LARGEST overall cost reduction while meeting these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-27T02:17:00.000Z",
        "voteCount": 19,
        "content": "A: Lazy loading is cost-effective because only a subset of data is used at every job\nB: There are hundreds of EC2 instances using the volume which is not possible (one EBS volume is limited to 16 nitro instances attached)\nC: Batching would load too much data\nD: storage gateway is used for on premises data access, I don't know is you can install a gateway in AWS, but Amazon would never advise this"
      },
      {
        "date": "2023-08-08T02:03:00.000Z",
        "voteCount": 1,
        "content": "file storage gateway can be installed on EC2 and it is exactly used for accessing S3 from EC2 as a file system"
      },
      {
        "date": "2023-10-08T03:31:00.000Z",
        "voteCount": 1,
        "content": "It's used a lot, I've used it for customers to access and analyze data imported via Snowball from Windows machines."
      },
      {
        "date": "2023-08-17T18:35:00.000Z",
        "voteCount": 1,
        "content": "There is one S3 file gateway\n\nhttps://aws.amazon.com/storagegateway/file/s3/"
      },
      {
        "date": "2023-09-24T21:54:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/"
      },
      {
        "date": "2023-08-04T08:15:00.000Z",
        "voteCount": 13,
        "content": "Answer: D\n\nI think the main point here is to understand what they mean by \"The file system must provide high performance access to the needed data\" while \"provide the LARGEST overall cost reduction\"?\n\nFor answer A, we have to remember that lazy load is SLOW for the first time you try to access the file (as it is being fetched from S3), BUT, as we are talking about hundreds of instances, then it might be OK. S3 Intelligent-Tiering, although doesn't seem to fit much, the part that says \"The job runs once monthly, reads a subset of the files from the shared file system\", indicates that at least part of the 200TB of data won't be accessed, which helps not going for answer C, for example.\n\nMy only issue with answer D is that Storage Gateway can be slower than FSx for Lustre, HOWEVER, what is the cost X performance ratio they are seeking here? We can guess that costs trumps maximum performance here: \"Which solution will provide the LARGEST overall cost reduction\" and, as Storage Gateway is way cheaper than FSx for Lustre per TB, it's safe to say that D is the most correct answer."
      },
      {
        "date": "2024-08-31T02:23:00.000Z",
        "voteCount": 1,
        "content": "A. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete."
      },
      {
        "date": "2024-08-11T01:56:00.000Z",
        "voteCount": 1,
        "content": "A or D : confusion. I wish they can provide explanation about their answers when it is not the most voted one"
      },
      {
        "date": "2024-06-16T13:52:00.000Z",
        "voteCount": 2,
        "content": "I vote D instead A because the requirement in the question is \"modifies the data on the shared file system\" Fsx imported data from s3 and lost the relationship to s3 after import is done  Without explicitly copy back to s3, the change stays on shared file system only. Answer A solution doesn't provide a step to copy the modification back to s3. \nStorage gateway presents s3 storage to the  OS as shared file system. Any modification on the shared file system will be automatically saved on s3."
      },
      {
        "date": "2024-03-16T10:53:00.000Z",
        "voteCount": 1,
        "content": "A: Lazy loading is cost-effective because only a subset of data is used at every job"
      },
      {
        "date": "2024-03-15T20:56:00.000Z",
        "voteCount": 2,
        "content": "Problem with D is that, AWS Storage GW and File GW are solutions for integrating on-premise storage with AWS storage solutions, particularly (but not limited to) S3.\n\nhttps://aws.amazon.com/storagegateway/\nhttps://aws.amazon.com/storagegateway/file\n\nCompute resources are residing in AWS, so having Storage GW and File GW won't solve a thing.\nAs far as option B is concerned, it comes down to the limitations of EBS (such as the max block size, and max number of instance that can be attached etc). Also, attaching and detaching of the EBS volumes seems a bit complicated too. On top of that, EBS does not offer the cost optimizations offered by S3 Intelligent Tiering. The question clearly mentions that only a subset of the data will be used. Intelligent tiering ensures a substantial cost optimization over time.\nHence, the answer should be A."
      },
      {
        "date": "2024-03-12T23:35:00.000Z",
        "voteCount": 1,
        "content": "Option D, migrating the data to an Amazon S3 bucket and using AWS Storage Gateway, seems to provide the largest overall cost reduction while meeting the requirements of high-performance access during the job run and minimizing costs when the storage is not actively being used. Therefore, Option D is the most suitable choice."
      },
      {
        "date": "2024-03-05T15:44:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/"
      },
      {
        "date": "2023-12-19T22:37:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option might work. However, AWS FSx for Lustre does not have a feature called \"lazy loading\" - its default behavior is to load a file from S3 when it is first accessed (restore). It can provide high-performance as needed though nothing is said in the question about whether a slow initial load time due to restore operations could be an issue. S3 Intelligent-Tiering minimizes storage costs.\n\nOption B - This option will provide a high-performance storage option. However, storage in EBS is expensive compared to other AWS storage services\n\nOption C - This option might work. However, AWS FSx for Luster does not have a feature called \"batch loading\". Files can be pre-loaded issuing a hsm-restore command. S3 Standard is a cheap storage option yet not the cheapest option in S3\n\nOption D - This option does not work as described in the option"
      },
      {
        "date": "2024-01-29T06:53:00.000Z",
        "voteCount": 1,
        "content": "Actually AWS FSx for Lustre does not have a direct  feature 'Lazy loading' but the question is the support of that when  Amazon FSx will import the objects in our S3 bucket as files, and \u201clazy-load\u201d the file contents from S3 when first access the files.. Any data processing job on Lustre with S3 as an input data source can be started without Lustre doing a full download of the dataset\nfirst -  Data is lazy loaded: only the data that is\nactually processed is loaded, meaning you can\ndecrease your costs and latency"
      },
      {
        "date": "2023-12-18T01:47:00.000Z",
        "voteCount": 1,
        "content": "Not B because using EBS still involves EC2 instances that are expensive (the instances that host the shared file system run continuously). Also, multi-attach is supported only for io1/oi2 EBS disk types that are expensive;\nNot C as batch loading does not exists in the doc/console, I think they might refer to the option to pre-populate the data using lfs hsm_restore command as mentioned here https://docs.aws.amazon.com/fsx/latest/LustreGuide/preload-file-contents-hsm-dra.html. This would be a more expensive option\nNot D as Storage Gateway provide less performance than FSx for Lustre and it requires at least an EC2 instance and this will introduce additional cost\n\nAA is a viable option as S3 is cheaper storage, FSx for Lustre provides performance. Lazy loading allows to actually move in the filesystem data that is actually used and intelligent tiering make sure those files that are not used are moved to less expensive S3 storage tiers."
      },
      {
        "date": "2023-12-05T03:28:00.000Z",
        "voteCount": 1,
        "content": "Intelligent tiering is not required, because the job would be running for every month, so there is no purpose for intelligent tiering, The question is having cost impact also one of the option. So go with option D."
      },
      {
        "date": "2024-03-06T12:34:00.000Z",
        "voteCount": 1,
        "content": "\"Only a subset of data is accessed each run\" So that means after 30 days data can tier down so yes there is cost savings in using INT"
      },
      {
        "date": "2023-11-03T00:38:00.000Z",
        "voteCount": 5,
        "content": "Functional requirements should be met before non-functional requirements.\nIn the first place, only option D allows the application to change the data in the shared file during the monthly job execution. With options A and C, data changes made during the job are discarded after the job runs.\nOn top of that, although D is inferior to A in performance, it meets the requirements because it is the cheapest."
      },
      {
        "date": "2024-03-14T05:41:00.000Z",
        "voteCount": 1,
        "content": "You can configure a DRA for automatic import only, for automatic export only, or for both. \"A data repository association configured with both automatic import and automatic export propagates data in both directions between the file system and the linked S3 bucket. As you make changes to data in your S3 data repository, FSx for Lustre detects the changes and then automatically imports the changes to your file system. As you create, modify, or delete files, FSx for Lustre automatically exports the changes to Amazon S3 asynchronously once your application finishes modifying the file.\"\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"
      },
      {
        "date": "2024-01-09T17:41:00.000Z",
        "voteCount": 1,
        "content": "Oh yeh - of course; if you delete the FSx volume; the changes are lost."
      },
      {
        "date": "2023-08-30T02:31:00.000Z",
        "voteCount": 1,
        "content": "A. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.\n\nOption B (using Amazon EBS) would result in higher costs due to the continuous usage of large EBS volumes. Similarly, option C involves creating a new FSx for Lustre file system with batch loading, which may not be as cost-effective as lazy loading.\n\nOption D (using AWS Storage Gateway) would involve additional complexity and potentially higher costs compared to directly utilizing S3 and FSx for Lustre."
      },
      {
        "date": "2023-08-17T19:16:00.000Z",
        "voteCount": 1,
        "content": "@chico already explain the logic behind, @sambb chose A because S3 file gateway wasn't clear to him"
      },
      {
        "date": "2023-07-29T21:51:00.000Z",
        "voteCount": 1,
        "content": "Question mentioned \"The file system must provide high performance access to the needed data for the duration of the 72-hour run.\" Assuming S3 Intelligent-Tiering don't move data into Archive Access tiers(which are optional) [Ref: docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html] . Thus, need to ensure data is always in storage tiers that provide \"low latency and high throughput performance.\". As S3 Intelligent-Tiering delivers automatic storage cost savings, Answer A can be the potential answer."
      },
      {
        "date": "2023-07-24T08:22:00.000Z",
        "voteCount": 4,
        "content": "A\u4e00\u5b9a\u662f\u9519\u7684\uff0c\u56e0\u4e3a\u6570\u636e\u90fd\u662f\u4e0d\u5e38\u8bbf\u95ee\u7684\uff0c\u5982\u679c\u653e\u5230s3\u7684\u667a\u80fd\u5b58\u50a8\u4e2d\uff0c\u4f1a\u9ed8\u8ba4\u53d8\u6210\u51b7\u6570\u636e\uff0c\u518d\u88ab\u8bbf\u95ee\u65f6\uff0c\u9700\u8981\u91cd\u65b0\u6fc0\u6d3b\uff0c\u9700\u8981\u4ed8\u51fa\u5f88\u9ad8\u7684\u6210\u672c"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/91449-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.<br>Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-22T17:56:00.000Z",
        "voteCount": 12,
        "content": "Logical answer : Non http port like TCP should hint to NLB immediately.(ALB does not fit here) Sharing IP address of EC2 is not apt\nwhether it is from individual EC2 instances or those from ECS cluster.this eliminates A,B.D, infact the NLB's address which stays in front of / associates to ec2 instances need to be shared. So, only solution is C"
      },
      {
        "date": "2023-01-13T12:29:00.000Z",
        "voteCount": 6,
        "content": "A more appropriate solution would be option C. Create an Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set. As it uses the NLB as the resource in the A-record, traffic will be routed through the NLB, and it will automatically route the traffic to the healthy instances based on the health checks and also it provides the fixed address assignments as the other companies can add the NLB's Elastic IP addresses to their allow lists."
      },
      {
        "date": "2024-08-31T02:24:00.000Z",
        "voteCount": 1,
        "content": "C. Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set."
      },
      {
        "date": "2024-08-15T03:23:00.000Z",
        "voteCount": 1,
        "content": "Ec2+NLB"
      },
      {
        "date": "2024-04-22T22:20:00.000Z",
        "voteCount": 1,
        "content": "A looks ok\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
      },
      {
        "date": "2024-03-16T10:57:00.000Z",
        "voteCount": 1,
        "content": "C: NLB with elastic IPs"
      },
      {
        "date": "2024-01-28T09:04:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer - Few key points -  TCP static Port (go with NLB), IP Whitelisting required which can only be done with NLB. ALB doesn't support static IPs. And sharing Static (Elastic) IPs of instances of no use when using NLB. We need to share NLB Elsatic IPs from Multi AZs and create DNS record for NLB Domain Name to Domain entry."
      },
      {
        "date": "2024-01-04T08:10:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/elb-attach-elastic-ip-to-public-nlb"
      },
      {
        "date": "2023-09-04T00:01:00.000Z",
        "voteCount": 1,
        "content": "Other option haven't mention multi AZ"
      },
      {
        "date": "2023-07-06T14:20:00.000Z",
        "voteCount": 1,
        "content": "Static IP-&gt; NLB"
      },
      {
        "date": "2023-06-28T14:18:00.000Z",
        "voteCount": 1,
        "content": "I suppose C, although you can;'t do this with A record, only alias"
      },
      {
        "date": "2023-06-17T20:56:00.000Z",
        "voteCount": 2,
        "content": "Create one Elastic IP address for each Availability Zone."
      },
      {
        "date": "2023-05-11T20:06:00.000Z",
        "voteCount": 1,
        "content": "C is the only option that talks about more than one AZ."
      },
      {
        "date": "2023-03-27T23:19:00.000Z",
        "voteCount": 2,
        "content": "Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone."
      },
      {
        "date": "2023-03-07T04:17:00.000Z",
        "voteCount": 1,
        "content": "IP address using NLB"
      },
      {
        "date": "2023-02-21T14:36:00.000Z",
        "voteCount": 2,
        "content": "C looks correct."
      },
      {
        "date": "2023-01-30T06:27:00.000Z",
        "voteCount": 1,
        "content": "C. NLB with one Elastic IP per AZ to handle TCP traffic. Alias record set named my.service.com.\nhttps://www.examtopics.com/discussions/amazon/view/28045-exam-aws-certified-solutions-architect-professional-topic-1/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/91214-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company\u2019s data center.<br>The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.<br>A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments. The solution must maintain high availability and must not affect the SLAs.<br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 62,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-27T07:09:00.000Z",
        "voteCount": 25,
        "content": "Voted D because of the 65% / 35% proportion. C seems to be good but with only 50% instances available we break the SLA"
      },
      {
        "date": "2023-08-27T16:31:00.000Z",
        "voteCount": 7,
        "content": "Can not be C because Savings Plans requirement long term commitment."
      },
      {
        "date": "2024-08-31T02:25:00.000Z",
        "voteCount": 1,
        "content": "D. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance."
      },
      {
        "date": "2024-06-16T14:38:00.000Z",
        "voteCount": 1,
        "content": "I vote C.\nThe 65% of scheduled jobs is the portion of the total work load. I don't believe it's SLA since SLA will be 99.99% or more. The jobs is hourly from 0.3 to 2 hours. There are 12 servers on prem. If the number of jobs per server can handle is N. Then to cover the worst situation that all the jobs run 2 hours, by given 12 servers and tight SLA, the number of hourly jobs is 12 / 2 = 6N. Answer C has 6 servers and since the number of job per server is N then 6 server can handle 6N jobs match the hourly job number 6N. \n2 ec2 with saving plan + 2 spot instances is more cost effective than 3 ec2 with capacity plan(not saving a penny by capacity reservation plan) + 1 spot instance."
      },
      {
        "date": "2024-03-16T11:02:00.000Z",
        "voteCount": 2,
        "content": "D is more cost-effective than C"
      },
      {
        "date": "2023-12-20T03:04:00.000Z",
        "voteCount": 2,
        "content": "Option A - This option might not work: it might not provide sufficient processing capacity for the batch jobs to meet the SLAs during outages. Moreover, 4 servers will not provide sufficient capacity to meet the SLAs of batch jobs\n\nOption B - This option might not work: In case of an outage affecting the On-Demand instances there might not be enough processing capacity to meet batch job SLAs\nOption C - This option will not meet the requirement not to make any long-term commitments\n\nOption D - This option will work: There is enough sufficient processing capacity to meet the SLAs of batch jobs and keep processing One-off jobs"
      },
      {
        "date": "2023-12-05T03:56:00.000Z",
        "voteCount": 1,
        "content": "D would be perfect, because it requires more cpu usage, we should have more capacity CPU ."
      },
      {
        "date": "2023-11-24T18:30:00.000Z",
        "voteCount": 1,
        "content": "The answer is D.\nSince it originally had a completely redundant configuration, it is thought that scheduled tasks are executed on 4 machines and user tasks are executed on 2 machines.\nA,B: Requirements cannot be met when a specific region falls.\nC: No Savings Plan required.\nD: Even if a specific region goes down, 6 machines will be maintained, so service can be maintained."
      },
      {
        "date": "2023-08-06T09:51:00.000Z",
        "voteCount": 3,
        "content": "About 65% or about 8 instances have to run at the same time to meet the SLA."
      },
      {
        "date": "2023-08-03T06:07:00.000Z",
        "voteCount": 4,
        "content": "Correct C.\nOption D is incorrect because running three instances in each Availability Zone as On-Demand Instances with Capacity Reservations will increase the cost of the solution without providing any additional benefit. Capacity Reservations are not necessary when using a Savings Plan, as they both offer guaranteed capacity at a discounted pricehttps://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/amazon-ec2.html. Also, running only one instance in each Availability Zone as a Spot Instance will not provide enough capacity for the user jobs that account for 35% of system usage."
      },
      {
        "date": "2023-08-27T16:32:00.000Z",
        "voteCount": 4,
        "content": "Can\u2019t be C it says it can\u2019t require long term commitment. Savings plans like reserved instance require long term commitments with a contract."
      },
      {
        "date": "2023-07-05T07:44:00.000Z",
        "voteCount": 3,
        "content": "D. 3 AZ (Redundancy), 3 EC2 in each AZ as on demand and 1 spot (addresses SLA in 65/35 ratio)\nRuling out Factors:\nA. Only 2 AZ (low redundancy), all EC2 in capacity reservation (Not Cost effective as SLA requirement is in 65/35 ratio).\nB. All 4 on-demand in 1 AZ (low redundancy), rest spot (Will efect tight SLA - is actually 35/65 instead of 65/35).\nC. Savings Plan (Against no long term commitments requirement)."
      },
      {
        "date": "2023-06-28T14:24:00.000Z",
        "voteCount": 1,
        "content": "D\n1 - need capacity reservation\n2 - need to cover 65% with HA"
      },
      {
        "date": "2023-05-23T11:37:00.000Z",
        "voteCount": 3,
        "content": "Just D is the right one. We need to garantee 65% (about 8 instances of 12) of capacity for the SLA, so 9 can do it and then let the others as spot. \nAnother point Saving Plans need commitment \"Savings Plans are a flexible pricing model that offer low prices on Amazon EC2, AWS Lambda, and AWS Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term\" - https://aws.amazon.com/savingsplans/compute-pricing/"
      },
      {
        "date": "2023-05-08T23:47:00.000Z",
        "voteCount": 1,
        "content": "Voted C, the reason for this option is Spot Instance which is truely cost saving when we are performing Batch jobs and if you plan the cost properly this is best solution"
      },
      {
        "date": "2023-04-20T12:30:00.000Z",
        "voteCount": 1,
        "content": "65% SLA can be reached only on answer D. Yeah - 9 instances are a bit too much but that's the only answer that meets the SLA"
      },
      {
        "date": "2023-04-18T09:47:00.000Z",
        "voteCount": 4,
        "content": "Option D splits the 12 instances across three AZs and runs three instances in each AZ as On-Demand Instances with Capacity Reservations, and one instance in each AZ as a Spot Instance. This option can provide better redundancy and capacity for scheduled jobs while still providing some cost savings through Spot Instances. Additionally, the user jobs can be easily absorbed by the available Spot Instances during On-Demand Instance failures."
      },
      {
        "date": "2023-04-09T14:27:00.000Z",
        "voteCount": 2,
        "content": "Option C as per ChatGPT"
      },
      {
        "date": "2023-04-18T09:47:00.000Z",
        "voteCount": 3,
        "content": "ChatGPT gave me option D"
      },
      {
        "date": "2023-11-01T01:30:00.000Z",
        "voteCount": 2,
        "content": "This is proof that ChatGPT does make mistakes! Savings plans are 1 year or 3 year commitments. So C is incorrect."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/91216-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:<br>The database must use strong, randomly generated passwords stored in a secure AWS managed service.<br>The application resources must be deployed through AWS CloudFormation.<br>The application must rotate credentials for the database every 90&nbsp;days.<br>A solutions architect will generate a CloudFormation template to deploy the application.<br>Which resources specified in the CloudFormation template will meet the security engineer\u2019s requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90&nbsp;days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90&nbsp;days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90&nbsp;days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90&nbsp;days."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 48,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T01:03:00.000Z",
        "voteCount": 17,
        "content": "A\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/cloudformation.html\nOption B is wrong. The ParameterStore::RotationSchedule resource does not exist in CloudFormation.\nOption C is wrong. It does not meet the requirement because it does not use CloudFormation.\nOption D is wrong. The AWS::AppSync::DataSource resource is what to create data sources for resolvers in AWS AppSync to connect to."
      },
      {
        "date": "2023-04-05T09:01:00.000Z",
        "voteCount": 9,
        "content": "Agree with A but I want to nitpick on this reply \"The ParameterStore::RotationSchedule resource does not exist in CloudFormation\". It is technically more correct to say ParameterStore does not support automated rotation of secrets instead of saying ParameterStore::RotationSchedule is not supported by CF."
      },
      {
        "date": "2023-05-12T13:19:00.000Z",
        "voteCount": 12,
        "content": "Ans A but answer is badly phrased. Why is the Lambda needed ?\nRefer docs: Some services offer managed rotation, where the service configures and manages rotation for you. With managed rotation, you don't use an AWS Lambda function to update the secret and the credentials in the database. The following services offer managed rotation:\n\nAmazon RDS offers managed rotation for master user credentials. For more information, see Password management with Amazon RDS and AWS Secrets Manager in the Amazon RDS User Guide."
      },
      {
        "date": "2024-01-15T06:37:00.000Z",
        "voteCount": 3,
        "content": "I agree with you. Secret Manager support to rotate credentials."
      },
      {
        "date": "2024-08-31T02:26:00.000Z",
        "voteCount": 1,
        "content": "A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days."
      },
      {
        "date": "2024-08-11T03:18:00.000Z",
        "voteCount": 1,
        "content": "Secrets Manager ($$$): Automatic rotation of secrets with AWS Lambda // SSM Parameter Store ($): No secret rotation (can enable rotation using Lambda triggered by EventBridge) --&gt; more overhead even if it is cheaper ==&gt; Answer A"
      },
      {
        "date": "2024-05-06T07:18:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A\nCloudformation template::systems manager has no resource called: RotationSchedule. where as Cloudformation template::secrets manager Indeed has a resource called: RotationSchedule.\nTherefore the correct answer is A only."
      },
      {
        "date": "2024-03-16T14:45:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-02-07T19:03:00.000Z",
        "voteCount": 3,
        "content": "Option A is the most straightforward and provides the least amount of operational overhead because it leverages AWS Secrets Manager\u2019s native capabilities for secret rotation. This eliminates the need for custom rotation logic or external triggers for rotation, unlike the other options that either rely on AWS Systems Manager Parameter Store (which does not have built-in secret rotation capabilities like Secrets Manager) or require additional resources such as Amazon EventBridge or AWS AppSync for triggering rotations, which complicates the architecture and increases operational overhead.\nTherefore, Option A is the correct choice as it directly addresses all the specified requirements using the intended features of AWS services, ensuring security and efficiency with minimal operational complexity."
      },
      {
        "date": "2024-02-03T04:09:00.000Z",
        "voteCount": 3,
        "content": "OK.. A ..but.. lambda to rotate for Secret Managers ? it does rotation natively ! why is that"
      },
      {
        "date": "2023-12-20T11:24:00.000Z",
        "voteCount": 3,
        "content": "Option A - This option will work: This option takes advantage of the Automatic Rotation feature in Secrets Manager which reduces operational overhead during secret rotation, i.e. CloudTrail will show a secret was rotated\n\nOption B - This option will not work: Parameter Store does not have a feature called RotationSchedule\n\nOption C - This option might work but increases overhead: Rotation will be triggered on the 90 day schedule but more work will be necessary to validate the secret was rotated and tested, i.e. CloudTrail logs will only show a lambda function was triggered\n\nOption D -  This option will not work: Parameter Store does not have a feature called RotationSchedule"
      },
      {
        "date": "2023-12-04T07:37:00.000Z",
        "voteCount": 1,
        "content": "Answer A. Password rotation -&gt; Secrets Manager"
      },
      {
        "date": "2023-09-01T06:48:00.000Z",
        "voteCount": 1,
        "content": "Which resources specified in the CloudFormation template will meet the security engineer\u2019s requirements with the LEAST amount of operational overhead?\n\nuse https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html"
      },
      {
        "date": "2023-08-17T14:06:00.000Z",
        "voteCount": 2,
        "content": "All - I feel the answer is A but why does it says Correct Answer \"B\" - What is the rationale behind B, can anyone explain. I am so confused??"
      },
      {
        "date": "2023-10-30T20:25:00.000Z",
        "voteCount": 4,
        "content": "The answers shown as correct are almost never the right ones on these test dumps, just pay attention to what was most voted and the discussions in the comments"
      },
      {
        "date": "2023-08-04T10:08:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2023-06-28T14:27:00.000Z",
        "voteCount": 1,
        "content": "it's n A"
      },
      {
        "date": "2023-05-24T09:07:00.000Z",
        "voteCount": 1,
        "content": "A poorly phrased but seems to be the best option in this scenario"
      },
      {
        "date": "2023-05-08T23:56:00.000Z",
        "voteCount": 1,
        "content": "AWS Secret Manager is the best option for Password safety and option fulfill all the requirement"
      },
      {
        "date": "2023-04-27T05:54:00.000Z",
        "voteCount": 1,
        "content": "A correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/91452-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.<br>Which solutions meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway\u2019s AWS integration type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway\u2019s AWS integration type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 66,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T05:23:00.000Z",
        "voteCount": 28,
        "content": "A and C.\nAPI Gateway REST API can invoke DynamoDB directly.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html"
      },
      {
        "date": "2023-12-12T09:34:00.000Z",
        "voteCount": 2,
        "content": "CD is right. \nWhile this option A works for private access, it does not support public access as DynamoDB tables are not publicly accessible by default."
      },
      {
        "date": "2023-12-19T04:31:00.000Z",
        "voteCount": 2,
        "content": "Option A has the ability to specify an execution role. This IAM role should have the GetItem/PutItem permissions for the given DynamoDB table. That way you can have access to your private table via the DynamoDB API while your API Gateway is publicly available.\nSo I agree with A and C"
      },
      {
        "date": "2023-12-29T09:22:00.000Z",
        "voteCount": 4,
        "content": "You cannot choose A and C, you choose A OR C, one excludes the other. When a question says to choose two answers, one shall complement the other. \nI agree that the API can integrate directly with DynamoDB, but if I have to choose two answers that complement each other, the A option cannot go with any of the others.\nSaying that, the only possible choices should be C and D, you create the Lambda functions to integrate with Dynamodb and then deploy them at Edge, as extra to improve performance and latency you use Global Accelerator. Yes, it is true that this is not a requirement, but it is good to have."
      },
      {
        "date": "2023-12-20T15:43:00.000Z",
        "voteCount": 12,
        "content": "Option A - This option might work: REST APIs can run over HTTPS and the integration type DynamoDB is possible\n\nOption B - This option will not work: HTTP APIs do not support integration types for DynamoDB\n\nOption C - This option will work: HTTP APIs support integration with Lambda functions\n\nOption D - This option will not work: Lambda@Edge is a function of CloudFront\n\nOption E - This option will not work: NLB Target groups can target Lambda functions however NLBs are not a Serverless solution (They are deployed on VPCs)."
      },
      {
        "date": "2024-10-16T05:17:00.000Z",
        "voteCount": 1,
        "content": "A. Amazon API Gateway can be configured as a REST API with direct integration to DynamoDB. This is done using API Gateway\u2019s AWS integration type, allowing direct interaction with DynamoDB without needing a Lambda function in between.\nC. API Gateway HTTP API can be used to route requests to AWS Lambda functions. The Lambda functions can then interact with DynamoDB to retrieve or modify data and return it to the client through the API."
      },
      {
        "date": "2024-08-31T02:28:00.000Z",
        "voteCount": 1,
        "content": "A. Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway\u2019s AWS integration type.\nC. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables."
      },
      {
        "date": "2024-08-22T05:22:00.000Z",
        "voteCount": 1,
        "content": "On the fact of simplicity it looks like BC, but with C there is an issue of Lambda fetching data, question does not indicate fetching, only put. So it looks like AB"
      },
      {
        "date": "2024-03-16T14:47:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2024-03-16T09:15:00.000Z",
        "voteCount": 1,
        "content": "The solutions that meet the requirements of using a serverless architecture to make the data accessible publicly through a simple API over HTTPS and scaling automatically in response to demand are: C AND D"
      },
      {
        "date": "2024-03-16T09:32:00.000Z",
        "voteCount": 1,
        "content": "Actually, Option D is out, reason: you cannot use AWS Lambda@Edge with Global accelerator"
      },
      {
        "date": "2024-03-06T13:58:00.000Z",
        "voteCount": 1,
        "content": "a, c\nhttps://medium.com/brlink/rest-api-just-with-apigateway-and-dynamodb-8a9b0cd76b7a"
      },
      {
        "date": "2024-03-05T16:23:00.000Z",
        "voteCount": 1,
        "content": "API Gateway REST API can invoke DynamoDB directly."
      },
      {
        "date": "2024-02-11T01:05:00.000Z",
        "voteCount": 1,
        "content": "Sometimes when multiple answers are required, they're supposed to complement each other, but sometimes these have to be just 2 valid but independent solutions... Well API GW with Rest endpoint is a valid solution, since it's had DynamoDB proxy integration lately. We use it in production, and it's a good fit, if you want to have a lot of control and features in your API GW and no lambda functions in between, reason being VTL supports a big set of mutations which is enough to us. \nOn the flip side, since we're forced to use a combination, then CD is  the right answer.\nIn terms of simplicity, it is the question, what you consider simple. API GW REST endpoint is considered simple, because it provides caching, api keys, usage plans, rate limiting, authorization, deployment stages etc. out of the box. So the plethora of out-of-the-box features is rather simple than implementing them oneself."
      },
      {
        "date": "2023-12-18T03:54:00.000Z",
        "voteCount": 1,
        "content": "Not E as I think NLB listener rules don't provide the required capability to to forward requests to the appropriate Lambda (you need to have and ALB)\nNot D as Lambda@Edge is a CloudFront feature\n\nA, B and C they all works here however the question requires \"a simple API over HTTPS\". Both REST APIs and HTTP APIs are RESTful API products. REST APIs support more features than HTTP APIs, while HTTP APIs are designed with minimal features so that they can be offered at a lower price. Thus I would go for B and C"
      },
      {
        "date": "2023-12-18T04:06:00.000Z",
        "voteCount": 2,
        "content": "My answer is wrong, double check that DynamoDB is not supported as first-class integration with API Gateway as per doc https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop-integrations-aws-services-reference.html\n\nThus the correct answer is A and C"
      },
      {
        "date": "2023-12-05T04:48:00.000Z",
        "voteCount": 2,
        "content": "C and D is the correct option \n1) C- Need server less architecture so need to use lamda function instead of REST API\n2) D - Global accelerator work with lamda edge would be best the option compare to NLB for auto scale up and down. It has static address and fixed entry point if we deply multiple region."
      },
      {
        "date": "2023-11-30T21:16:00.000Z",
        "voteCount": 1,
        "content": "REST API - is not simple and limitaiton around scability. NLB with listener rules can be used to forward request based on specified conditions to appropriate AWS lambda function"
      },
      {
        "date": "2023-11-11T23:53:00.000Z",
        "voteCount": 1,
        "content": "lambda can have https endpoints available"
      },
      {
        "date": "2023-09-14T22:58:00.000Z",
        "voteCount": 3,
        "content": "I've read similar questions previously, keyword is \"simple API\".\nREST API adds more features than HTTP API and is consider \"more\" complex.\nSo it has to be HTTP just for that reason.\n\nYou can use API Gateway (HTTP)-&gt;dynamodb:\nhttps://aws.amazon.com/fr/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/\n\nso B and C"
      },
      {
        "date": "2023-11-24T21:07:00.000Z",
        "voteCount": 2,
        "content": "BC\nHTTP API support AWS Integrations + Simple\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html"
      },
      {
        "date": "2023-08-30T22:29:00.000Z",
        "voteCount": 3,
        "content": "B. Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to DynamoDB by using API Gateway\u2019s AWS integration type.\n\nC. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.\n\nOptions A, D, and E do not align with the requirements as well:\n\nA. Amazon API Gateway REST API with Direct DynamoDB Integration: While REST APIs could work, HTTP APIs are generally more lightweight and cost-effective. Also, direct integration with DynamoDB using REST APIs could be more complex to set up compared to HTTP APIs."
      },
      {
        "date": "2023-08-06T11:15:00.000Z",
        "voteCount": 1,
        "content": "Option C suggests configuring an Amazon API Gateway HTTP API with integrations to AWS Lambda functions that return data from the DynamoDB tables. However, this approach would introduce unnecessary complexity and additional steps since it involves using AWS Lambda as a middle layer to fetch data from DynamoDB"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/91453-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.<br>A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.<br>Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer that includes HTTP and HTTPS listeners.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution. Deploy a Lambda@Edge function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CEF",
        "count": 57,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 46,
        "isMostVoted": false
      },
      {
        "answer": "BEF",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CDF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T12:53:00.000Z",
        "voteCount": 35,
        "content": "C: By creating an AWS Lambda function, the solution architect can use the JSON document to look up the target URLs for each domain and respond with the appropriate redirect URL. This way, the solution does not need to rely on a web server to handle the redirects, which reduces operational effort.\n\nE: By creating an Amazon CloudFront distribution, the solution architect can deploy a Lambda@Edge function that can look up the target URLs for each domain and respond with the appropriate redirect URL. This way, CloudFront can handle the redirection, which reduces operational effort.\n\nF: By creating an SSL certificate with ACM and including the domains as Subject Alternative Names, the solution architect can ensure that the redirect service can handle both HTTP and HTTPS requests, which is required by the company."
      },
      {
        "date": "2023-02-02T11:47:00.000Z",
        "voteCount": 1,
        "content": "SAN cannot handle redirects.  We need to do http - https"
      },
      {
        "date": "2023-01-13T12:54:00.000Z",
        "voteCount": 6,
        "content": "A and B are not the right answer because they would require configuring and maintaining a web server to handle the redirects, which would increase operational effort.\nD is not the right answer because it would require creating an API Gateway API, which increases operational effort."
      },
      {
        "date": "2023-03-22T12:04:00.000Z",
        "voteCount": 9,
        "content": "Wrong for B, Lambda can be an ALB target, you do not need web server"
      },
      {
        "date": "2023-05-29T21:23:00.000Z",
        "voteCount": 29,
        "content": "If you go with a Cloudfront what is the origin? Lambda@edge is not origin. The function mentioned in C is Lambda and in E it says about Lambda@edge, which are two things. If you handle redirect from the Lambda@edge in CF there is no need of the Lambda you wrote in Answer C. \n\nMY Answer: \nCreate an ALB with HTTP and HTTPS listeners (B), Use the TLS cert created in F for the HTTPS listener. As the backend for the ALB write a Lambda with endpoint mapping JSON (C)\n\nIs this full serverless? No, but you do not have to worry about scaling or operational overhead, AWS Handles everything for us."
      },
      {
        "date": "2023-12-23T08:17:00.000Z",
        "voteCount": 6,
        "content": "This is the only answer that is completed by using all three options selected BCF.  F is mandatory to resolve the marketing domains URLs that are HTTPS.  So B and C then work together to redirect to those URLs  as a full solution like https://aws.amazon.com/ko/blogs/networking-and-content-delivery/automating-http-s-redirects-and-certificate-management-at-scale/\n\nE may have partial potential to do something, but you have no origin with it - and what would the origin be?  \nWith BCF you hit the ALB get a redirect as a result of the Marketing URL and your done-- its a complete redirect solution which is what the whole requirement is."
      },
      {
        "date": "2024-10-02T14:03:00.000Z",
        "voteCount": 1,
        "content": "Setting lambda as an ALB target seems less effort than configuring CFN origins."
      },
      {
        "date": "2024-08-31T02:30:00.000Z",
        "voteCount": 1,
        "content": "C. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.\nE. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.\nF. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names."
      },
      {
        "date": "2024-08-19T20:34:00.000Z",
        "voteCount": 1,
        "content": "Initially, i thought E however, i realized that Lambda@Edge needs an origin to send traffic to. The purpose of Lambda on edge is to make adjustments to the request/response not to reroute like what we're trying to achieve.\nD is out because Amazon API Gateway does not support unencrypted (HTTP) endpoints.\nA is too much overhead\nBCF."
      },
      {
        "date": "2024-08-04T07:34:00.000Z",
        "voteCount": 1,
        "content": "BCF (Application Load Balancer, AWS Lambda, ACM) is preferred for its simplicity, ease of setup, and cost predictability. It handles both HTTP and HTTPS traffic effectively with less operational complexity compared to the CloudFront and Lambda@Edge setup.\n\nCEF (AWS Lambda, CloudFront, ACM) is a powerful solution for global low-latency requirements but may introduce unnecessary complexity and higher costs for a simple redirection service."
      },
      {
        "date": "2024-06-26T00:57:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is BEF\nMain discussion here is \nLambda@Edge can redirect viewers request to based on information in the request based on domain and path"
      },
      {
        "date": "2024-07-27T14:21:00.000Z",
        "voteCount": 1,
        "content": "B is not correct. While an ALB could handle HTTP/HTTPS requests, it still requires managing target groups and does not directly integrate with a simple Lambda function for routing."
      },
      {
        "date": "2024-06-19T06:00:00.000Z",
        "voteCount": 1,
        "content": "CDF is answer. Check it again"
      },
      {
        "date": "2024-06-16T15:07:00.000Z",
        "voteCount": 1,
        "content": "Vote BCF"
      },
      {
        "date": "2024-04-22T09:13:00.000Z",
        "voteCount": 1,
        "content": "Guys the Answer has to be B,C, and E. period."
      },
      {
        "date": "2024-03-21T09:56:00.000Z",
        "voteCount": 1,
        "content": "CEF is the correct combination."
      },
      {
        "date": "2024-03-13T18:49:00.000Z",
        "voteCount": 1,
        "content": "E is a bit blur, it seems like an unifished sentence to me"
      },
      {
        "date": "2023-12-21T02:06:00.000Z",
        "voteCount": 4,
        "content": "Option A - This option could work but it increases operational overhead: Deploying an EC2 instance requires building a VPC with one public subnet. Moreover, the architect will need to write an application to process the event. \nOption B - This option could work and it reduces operational overhead: An ALB helps expose the solution and respond to HTTP/S requests; a VPC will needed. It can target EC2 instances and Lambda functions\nOption C - This option could work and minimizes operational overhead: The architect can focus on writing the code to process the event. A VPC is not necessarily needed to deploy a Lambda function\nOption D - This option might not work: AWS Gateway APIs only respond to HTTPS\nOption E - This option might not work: It can respond to HTTP/S requests and send events to API Gateway as an origin. However, this would remove the need to deploy a Lambda@Edge function\nOption F - This option will contribute to the solution: It enables HTTPS for the 10 domains"
      },
      {
        "date": "2023-11-23T21:48:00.000Z",
        "voteCount": 5,
        "content": "Lambda@Edge allows you to execute custom business logic closer to the viewer. This capability enables intelligent/programmable processing of HTTP requests at locations that are closer (for the purpose of latency) to your viewer. In this case the Lambda@Edge function can be written so that it redirects viewers based on information in the request based on domain and path.\n\nTo accept multiple custom domains on the CloudFront distribution a certificate can be created in ACM that includes multiple subject alternative names. These names can then be used in Route 53 records pointing to the distribution.\n\nThe ALB may will need to be configured with both an HTTP and an HTTPS listener. The HTTPS listener will also require a certificate, and this could use the same certificate used in the CloudFront distribution or it could be a separate certificate."
      },
      {
        "date": "2023-11-23T21:49:00.000Z",
        "voteCount": 3,
        "content": "INCORRECT: \"Create a dynamic webpage and host it on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL\" is incorrect. While designing such a solution, serverless should be utilized and hence EC2 isn\u2019t an appropriate use case for this scenario.\n\nINCORRECT: \"Create an AWS Lambda function that uses the event message and specified JSON document to look up and respond with a redirect URL\" is incorrect. With this solution, Lambda would need a change every time config file changes and would increase effort, hence this is not an efficient option.\n\nINCORRECT: \"Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function\" is incorrect. With this option as well, for each domain addition or change, API gateway stages would have to be re deployed hence this is again an ineffective choice."
      },
      {
        "date": "2023-11-23T21:50:00.000Z",
        "voteCount": 3,
        "content": "References:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-to-another-domain-with-alb/\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works-tutorial.html\n\nSave time with our AWS cheat sheets:\n\nhttps://digitalcloud.training/amazon-cloudfront/"
      },
      {
        "date": "2023-11-12T00:12:00.000Z",
        "voteCount": 1,
        "content": "as they fit each other"
      },
      {
        "date": "2023-11-09T03:48:00.000Z",
        "voteCount": 2,
        "content": "B, E, F - Lambda@Edge will allow for processing before directing to ALB"
      },
      {
        "date": "2023-10-07T23:15:00.000Z",
        "voteCount": 3,
        "content": "BCF. We need to choose the solution with three steps and here is the blog with same situation with old possible scenario with limited scale ( S3, cloudfront without lambda@edge ) \nhttps://aws.amazon.com/ko/blogs/networking-and-content-delivery/automating-http-s-redirects-and-certificate-management-at-scale/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/91457-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that has multiple AWS accounts is using AWS Organizations. The company\u2019s AWS accounts host VPCs, Amazon EC2 instances, and containers.<br>The company\u2019s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of \u201ccostCenter\u201d and a value or \u201ccompliance\u201d.<br>The company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team\u2019s AWS account. The cost calculation must be as accurate as possible.<br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team\u2019s AWS account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T13:09:00.000Z",
        "voteCount": 18,
        "content": "Answer A : because we do not depend on the users, I prefer management account\n\nOption C or A would be the correct answer. In option C, the solution architect would activate the costCenter user-defined tag in the member accounts of the organization, and then schedule a monthly AWS Cost and Usage Report from the management account to retrieve the reports and calculate the total cost for the costCenter tagged resources. In option A, the management account of the organization would activate the costCenter user-defined tag and configure monthly AWS Cost and Usage Reports to be saved to an Amazon S3 bucket in the management account. Then, use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources. Both options would allow the company to accurately identify the cost of the security tools running on the EC2 instances and charge the compliance team\u2019s AWS account."
      },
      {
        "date": "2023-06-26T04:39:00.000Z",
        "voteCount": 13,
        "content": "Only a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console.\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html"
      },
      {
        "date": "2023-05-29T21:28:00.000Z",
        "voteCount": 2,
        "content": "User-defined tags can not be allowed from management accounts in AWS Organization. It must done from the management Account."
      },
      {
        "date": "2024-07-02T02:08:00.000Z",
        "voteCount": 1,
        "content": "Did you mean from member account? in this sentence \"User-defined tags can not be allowed from management accounts in AWS Organization.\""
      },
      {
        "date": "2022-12-28T06:13:00.000Z",
        "voteCount": 6,
        "content": "I vote A.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/configurecostallocreport.html"
      },
      {
        "date": "2024-08-31T02:31:00.000Z",
        "voteCount": 1,
        "content": "A. In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources."
      },
      {
        "date": "2024-08-03T05:52:00.000Z",
        "voteCount": 1,
        "content": "The most ideal way to get this job done is to use: AWS Cost Explorer\n\n\nBut among all the given options, we should go with option A, as the user defined tag can only be managed in management account"
      },
      {
        "date": "2024-03-16T15:06:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-05T05:04:00.000Z",
        "voteCount": 1,
        "content": "A is ccorect, we need to login to management account to create"
      },
      {
        "date": "2023-11-12T00:51:00.000Z",
        "voteCount": 2,
        "content": "yes, you need to activate cost allocation tags before using, you can do this the same place where you would like to see your reports - management account"
      },
      {
        "date": "2023-10-17T19:36:00.000Z",
        "voteCount": 1,
        "content": "lines up correctly \nactivate tag in member accounts and generating AWS CUR from management account ( has ability to see costs across all member accounts) and Tag breakfdown in report"
      },
      {
        "date": "2023-10-01T00:41:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html\n\"For tags to appear on your billing reports, you must activate them.\"\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html\n\"Only a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console.\"\n-&gt; eliminate B,C. D is not relevant"
      },
      {
        "date": "2023-09-04T19:27:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/building-a-cost-allocation-strategy.html"
      },
      {
        "date": "2023-08-30T22:43:00.000Z",
        "voteCount": 3,
        "content": "Only a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console."
      },
      {
        "date": "2023-06-28T15:18:00.000Z",
        "voteCount": 1,
        "content": "it's an A"
      },
      {
        "date": "2023-05-24T09:53:00.000Z",
        "voteCount": 1,
        "content": "I go with D"
      },
      {
        "date": "2023-03-27T23:29:00.000Z",
        "voteCount": 1,
        "content": "Cost center tag int he management account."
      },
      {
        "date": "2023-03-07T13:32:00.000Z",
        "voteCount": 1,
        "content": "Management account for reports"
      },
      {
        "date": "2023-01-30T07:53:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      },
      {
        "date": "2022-12-27T22:04:00.000Z",
        "voteCount": 1,
        "content": "Should be a C"
      },
      {
        "date": "2022-12-27T22:07:00.000Z",
        "voteCount": 5,
        "content": "Change to A, the activation of user tag for billing can only be done by management account"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/91458-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company wants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is created, the company wants to automate the process of creating a new VPC and a transit gateway attachment.<br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the management account, share the transit gateway with member accounts by using AWS Service Catalog."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T13:17:00.000Z",
        "voteCount": 21,
        "content": "Option A is sharing the transit gateway with member accounts by using AWS Resource Access Manager, which allows the management account to share resources with member accounts. Option C is launching an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account, and associates the attachment with the transit gateway in the management account by using the transit gateway ID. This automation of creating a new VPC and transit gateway attachment in new member accounts can help to streamline the process and reduce operational effort."
      },
      {
        "date": "2023-11-23T21:55:00.000Z",
        "voteCount": 1,
        "content": "Precisely!"
      },
      {
        "date": "2024-08-31T02:32:00.000Z",
        "voteCount": 1,
        "content": "A. From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.\nC. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID."
      },
      {
        "date": "2024-03-16T15:09:00.000Z",
        "voteCount": 1,
        "content": "AC are correct"
      },
      {
        "date": "2023-12-06T09:53:00.000Z",
        "voteCount": 2,
        "content": "I am working on a project doing the exact same thing :D"
      },
      {
        "date": "2023-10-07T23:58:00.000Z",
        "voteCount": 1,
        "content": "AC. \nhttps://aws.amazon.com/ko/blogs/networking-and-content-delivery/automating-aws-transit-gateway-attachments-to-a-transit-gateway-in-a-central-account/\nhttps://cloudjourney.medium.com/aws-ram-and-transit-gateway-8ac230f298e8"
      },
      {
        "date": "2023-09-04T01:17:00.000Z",
        "voteCount": 1,
        "content": "You can use AWS Resource Access Manager (RAM) to share a transit gateway for VPC attachments across accounts or across your organization in AWS Organizations."
      },
      {
        "date": "2023-06-28T15:20:00.000Z",
        "voteCount": 1,
        "content": "AC of course"
      },
      {
        "date": "2023-03-27T23:32:00.000Z",
        "voteCount": 2,
        "content": "AC are my choice."
      },
      {
        "date": "2023-01-30T07:54:00.000Z",
        "voteCount": 2,
        "content": "A and C are the answer for me"
      },
      {
        "date": "2022-12-29T03:51:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C\nhttps://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-transitgatewayattachment.html"
      },
      {
        "date": "2022-12-13T08:43:00.000Z",
        "voteCount": 3,
        "content": "https://www.examtopics.com/discussions/amazon/view/60090-exam-aws-certified-solutions-architect-professional-topic-1/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/91461-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The procurement team\u2019s policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private Marketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.<br>What is the MOST efficient way to design an architecture to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T13:21:00.000Z",
        "voteCount": 16,
        "content": "The most efficient way to design an architecture to meet these requirements is option C. By creating an IAM role named procurement-manager-role in all the shared services accounts in the organization and adding the AWSPrivateMarketplaceAdminFullAccess managed policy to the role, the procurement managers will have the necessary permissions to administer Private Marketplace. Then, by creating an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role and another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization, the company can restrict access to Private Marketplace administrative access to only the procurement managers."
      },
      {
        "date": "2023-08-17T14:39:00.000Z",
        "voteCount": 3,
        "content": "The catch is the \"Create an organization root-level SCP to deny permissions\". I'd refrain from creating a root-level SCP"
      },
      {
        "date": "2024-08-31T02:34:00.000Z",
        "voteCount": 1,
        "content": "C. Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization."
      },
      {
        "date": "2024-08-11T05:01:00.000Z",
        "voteCount": 1,
        "content": "Not D, why ? : D. Placing the procurement-manager-role in developer accounts with full Private Marketplace admin access increases the risk of mismanagement. Additionally, applying an SCP only to shared services accounts does not adequately restrict access across the entire organization."
      },
      {
        "date": "2024-06-19T04:47:00.000Z",
        "voteCount": 1,
        "content": "Why C is right and D is wrong....\nFocus on the end of the question :\nOther IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.\nWhat is the MOST efficient way to design an architecture to meet these requirements? \n\nWho should be excluded? Other IAM users, groups, roles, and account administrators in the company\nWhat is the MOST efficient way? Apply SCP at the root level\nD is more work than C, this is a good reason to choose C over D"
      },
      {
        "date": "2024-05-03T19:46:00.000Z",
        "voteCount": 1,
        "content": "C. Most efficient and secure:\n\nCreating the procurement-manager-role in shared services accounts limits its scope to specific OUs, aligning with the organizational structure.\nGranting AWSPrivateMarketplaceAdminFullAccess to this role provides the necessary permissions for managing Private Marketplace within the OU.\nAn organization root-level SCP denying Private Marketplace administration to everyone except the procurement-manager-role ensures centralized control and restricts unauthorized access.\nAnother SCP preventing the creation of the procurement-manager-role outside of shared services accounts adds an extra layer of security."
      },
      {
        "date": "2024-03-05T16:35:00.000Z",
        "voteCount": 1,
        "content": "C, D doesn't make sense."
      },
      {
        "date": "2023-12-19T09:51:00.000Z",
        "voteCount": 2,
        "content": "Not A as it does not implement the requirement to enforce procurement managers to use the shared services account in each organizational unit\nNot B as this would allow developers to administer private market place\nnot D as this would allow developers to administer private market place\n\nC is correct as it configure the required role (with required permission) only in the shared service account, uses an SCP to deny private market place management to to everyone except the role named procurement-manager-role and another SCP to prevent creating a role nmaed procurement-manager-role"
      },
      {
        "date": "2023-12-19T09:54:00.000Z",
        "voteCount": 1,
        "content": "Actually D would to the job, but creating a role in every account is nt strictly necessary and would cause more work"
      },
      {
        "date": "2023-12-05T05:16:00.000Z",
        "voteCount": 2,
        "content": "C is the better one than D . because we need to apply scp to the root level with deny policy is the best practices. create the role and apply to each account is not a correct way and it is overhead to the adminstrator."
      },
      {
        "date": "2023-11-12T01:02:00.000Z",
        "voteCount": 1,
        "content": "look on whenthan's answer"
      },
      {
        "date": "2023-10-18T06:36:00.000Z",
        "voteCount": 3,
        "content": "creation of role in all shared services\nadding required policy to the role\ncreation of org root-level to guardrail who can have those privileges\ncreation of SCP to close out workaround of creation of another role with same access"
      },
      {
        "date": "2023-09-28T14:01:00.000Z",
        "voteCount": 2,
        "content": "C and D options are most relevant. Once you create a role, you cannot create another role with same name. So option C doesn't make sense. So my answer Option D"
      },
      {
        "date": "2024-02-07T18:22:00.000Z",
        "voteCount": 1,
        "content": "i am on same page"
      },
      {
        "date": "2024-02-07T18:23:00.000Z",
        "voteCount": 1,
        "content": "its C - the role should be in shared service accounts and not all accounts"
      },
      {
        "date": "2023-09-04T20:56:00.000Z",
        "voteCount": 1,
        "content": "Clearly, it's C."
      },
      {
        "date": "2023-08-15T05:37:00.000Z",
        "voteCount": 2,
        "content": "Selected answer: C\n\noption D:  \"Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers\", the procurement-manager-role  is used by manager not used by developers"
      },
      {
        "date": "2023-10-18T16:31:00.000Z",
        "voteCount": 1,
        "content": "the first sentense \"An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace.\""
      },
      {
        "date": "2023-11-23T22:00:00.000Z",
        "voteCount": 2,
        "content": "Developers has to ask procurement manager and not purchase by themselves."
      },
      {
        "date": "2023-08-10T11:11:00.000Z",
        "voteCount": 2,
        "content": "Its D - According to this : https://aws.amazon.com/blogs/awsmarketplace/controlling-access-to-a-well-architected-private-marketplace-using-iam-and-aws-organizations/"
      },
      {
        "date": "2023-08-10T11:35:00.000Z",
        "voteCount": 2,
        "content": "Its C. D is wrong - missed : \"procurement-manager-role in all AWS accounts that will be used by DEVELOPERS\""
      },
      {
        "date": "2023-07-08T12:49:00.000Z",
        "voteCount": 1,
        "content": "Its a C"
      },
      {
        "date": "2023-06-24T17:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct-"
      },
      {
        "date": "2023-06-19T12:33:00.000Z",
        "voteCount": 4,
        "content": "D is a distractor since the developers do not need to administer the private marketplace. Plus that the procurement team acts only in the shared accounts. That leaves C as the only option"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/91103-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image5.png\"><br>When this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.<br>What should the solutions architect do to eliminate the developers\u2019 ability to use services outside the scope of this policy?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an explicit deny statement for each AWS service that should be constrained.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the FullAWSAccess SCP from the developers account\u2019s OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the FullAWSAccess SCP to explicitly deny all services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an explicit deny statement using a wildcard to the end of the SCP."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 57,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T18:51:00.000Z",
        "voteCount": 17,
        "content": "B is correct because default FullAWSAccess SCP is applied"
      },
      {
        "date": "2023-09-15T14:39:00.000Z",
        "voteCount": 9,
        "content": "If you go to AWS management console and look up how SCP works, you will find that by default FullAWSAccess policy is attached to all OUs by default if you have SCP enabled."
      },
      {
        "date": "2023-11-23T22:20:00.000Z",
        "voteCount": 2,
        "content": "That's correct. You can disable AWSFullAccess SCP from member accounts as long as you are replacing it with another policy with specific permissions required."
      },
      {
        "date": "2024-08-31T02:35:00.000Z",
        "voteCount": 1,
        "content": "B. Remove the FullAWSAccess SCP from the developers account\u2019s OU."
      },
      {
        "date": "2024-08-11T05:16:00.000Z",
        "voteCount": 1,
        "content": "B. Remove the FullAWSAccess SCP from the developers account\u2019s OU.\n\nExplanation:\nFullAWSAccess SCP: By default, AWS Organizations attaches a FullAWSAccess SCP to all OUs and accounts, allowing access to all AWS services unless restricted by another SCP. If this SCP is still attached to the developers' OU, it will allow access to all services, regardless of the more restrictive SCP you have applied.\n\nSCP Behavior: SCPs are evaluated in an \"implicit deny\" model. If an action is not explicitly allowed by the SCPs, it is implicitly denied. However, if multiple SCPs are attached and one allows an action (like FullAWSAccess), that action is permitted unless explicitly denied in another SCP."
      },
      {
        "date": "2024-08-07T03:38:00.000Z",
        "voteCount": 1,
        "content": "AWS Organizations attaches an AWS managed SCP named FullAWSAccess to every root, OU and account when it's created. This policy allows all services and actions. You can replace FullAWSAccess with a policy allowing only a set of services so that new AWS services are not allowed unless they are explicitly allowed by updating SCPs.\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html"
      },
      {
        "date": "2024-07-29T14:44:00.000Z",
        "voteCount": 1,
        "content": "Best practice would be to create an explicit deny statement. The reason is that other SCPs could be in effect, aside from AWSFullAccess, that could grant access to other services. If the goal is to deny access to any other service, then this must be made explicit."
      },
      {
        "date": "2024-07-29T10:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct\nRemove from develop account OU --&gt; implicitly deny all service --&gt;add explicity 'allow' to restirct only allow related services in SCP."
      },
      {
        "date": "2024-07-23T12:50:00.000Z",
        "voteCount": 2,
        "content": "{\n      \"Sid\": \"ExplicitDeny\",\n      \"Effect\": \"Deny\",\n      \"NotAction\": [\n        \"ec2:*\",\n        \"dynamodb:*\",\n        \"s3:*\"\n      ],\n      \"Resource\": \"*\"\n    }"
      },
      {
        "date": "2024-06-16T18:29:00.000Z",
        "voteCount": 2,
        "content": "FullAWSAccess SCP is inherited from root. Can't be removed from OU.\nD is correct answer."
      },
      {
        "date": "2024-08-12T07:02:00.000Z",
        "voteCount": 1,
        "content": "It can be, read \"How SCPs work with Allow\" in here it shows example:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html"
      },
      {
        "date": "2024-04-15T03:41:00.000Z",
        "voteCount": 4,
        "content": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowEC2\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"ec2:*\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"AllowDynamoDB\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"dynamodb:*\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"AllowS3\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ExplicitDeny\",\n      \"Effect\": \"Deny\",\n      \"NotAction\": [\n        \"ec2:*\",\n        \"dynamodb:*\",\n        \"s3:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}"
      },
      {
        "date": "2024-03-11T08:32:00.000Z",
        "voteCount": 2,
        "content": "D - the alternative doesn't mention an ASG which must be taken as implied.\nThe other solutions are simply absurd:\nA: The operational overhead is ENORMOUS. To those who think that \"operational overhead\" is only day-to-day maintenance: it is not. It encompasses ALL CHANGES to the infrastructure.\nB: Kubernetes is the very definition of operational overhead. Always avoid unless there is an absolutely compelling reason to use it.\nC: And what do you people think the function of the Lambda is? None. \nD: This works and is the most straightforward as soon as you realise that the ASG is implied.\n\nIn the final analysis, this is another example of how AWS exam questions leave out information in order to trip you up."
      },
      {
        "date": "2024-02-16T18:24:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\n\nFullAWSAccess NOT inherited. It must be set at every OU layer.\nB is the most inadvisable choice because target account will get a explicitly DENY for all AWS services including EC2 etc if delete FullAWSAccess at it OU."
      },
      {
        "date": "2024-02-08T08:28:00.000Z",
        "voteCount": 4,
        "content": "To eliminate the developers\u2019 ability to use AWS services outside the scope of Amazon EC2, Amazon S3, and Amazon DynamoDB, the solutions architect should:\n    *    D. Add an explicit deny statement using a wildcard to the end of the SCP.\nThis action effectively restricts access to only the specified services by explicitly denying access to all other AWS services. The corrected Service Control Policy (SCP) would look something like this:\n{\n      \"Sid\": \"ExplicitDenyAllOtherServices\",\n      \"Effect\": \"Deny\",\n      \"NotAction\": [\n        \"ec2:\",\n        \"dynamodb:\",\n        \"s3:\"\n      ],\n      \"Resource\": \"*\"\n    }"
      },
      {
        "date": "2024-02-08T08:29:00.000Z",
        "voteCount": 2,
        "content": "Full SCP:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowEC2\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"ec2:\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"AllowDynamoDB\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"dynamodb:\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"AllowS3\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ExplicitDenyAllOtherServices\",\n      \"Effect\": \"Deny\",\n      \"NotAction\": [\n        \"ec2:\",\n        \"dynamodb:\",\n        \"s3:\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}"
      },
      {
        "date": "2024-02-08T08:31:00.000Z",
        "voteCount": 3,
        "content": "Explanation:\n    *    Option A is less efficient because creating an explicit deny statement for each AWS service except EC2, S3, and DynamoDB would be impractical given the large number of services AWS offers.\n    *    Option B suggests removing the FullAWSAccess SCP from the developers account\u2019s OU. While removing FullAWSAccess could potentially restrict access, it\u2019s not as direct or effective as implementing an explicit deny. The FullAWSAccess SCP allows all actions on all resources within the account or OU it\u2019s applied to, and simply removing it doesn\u2019t automatically restrict access to only the specified services.\n    *    Option C suggests modifying the FullAWSAccess SCP to explicitly deny all services. However, the FullAWSAccess SCP is a default SCP applied by AWS Organizations and should generally be left as is. Custom SCPs should be created to enforce specific policies.\n    *    Option D is the most direct and effective approach."
      },
      {
        "date": "2024-02-01T03:04:00.000Z",
        "voteCount": 2,
        "content": "ignore my previous comment"
      },
      {
        "date": "2024-02-01T02:48:00.000Z",
        "voteCount": 1,
        "content": "By default, FullAWSAccess is applied at the root, so all member accounts in all OUs will inherit this policy. Removing FullAWSAccess SCP from a specific OU isn't enough. Answer is A."
      },
      {
        "date": "2024-02-01T02:53:00.000Z",
        "voteCount": 1,
        "content": "Ahh, thanks to @gustori99 for pointing out my incorrect understanding. SCPs are not inherited. See https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html"
      },
      {
        "date": "2024-02-01T03:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is B."
      },
      {
        "date": "2024-01-29T11:37:00.000Z",
        "voteCount": 1,
        "content": "A is correct - Removing FullAWSAccess SCP from the developer account only is not going to help. As FullAWSAccess allowing all is also being inherited from the root and Parent OUs. When SCP is enable  FullAWSAccess is enabled by default. \n\nOne option is replacing FullAWSAccess on root and all Parent OUs and developer account to the SCP mentioned in question allowing only three service. \n\nIf we are only removing FullAWSAccess SCP from developer's account then we will have to explicitly deny all other services not required."
      },
      {
        "date": "2024-01-29T08:39:00.000Z",
        "voteCount": 1,
        "content": "It seams that almost no one understands how SCPs are evaluated:\n\nFrom the documentation: For a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself).\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\n\nSo FullAWSAccess at the root level is NOT inherited.  It must be present at ALL levels. \n\nB is wrong because when you remove FullAWSAccess at the OU level and do not replace it with an allow list of the permitted services, ALL services will be denied even if you have an allow list on account level.\n\nC and D does't make sense."
      },
      {
        "date": "2024-02-01T03:03:00.000Z",
        "voteCount": 1,
        "content": "The question states clearly that 3 services are permitted by the new SCP attached to the developer OU. The answer is B."
      },
      {
        "date": "2024-02-23T01:28:00.000Z",
        "voteCount": 1,
        "content": "The question states \"the solutions architect has implemented the following SCP on the developers account\". In my understanding the SCP is attached on the developers account not on the OU level. \n\nIf SCP is attached on OU level then B is correct. If it is attached on the account B cannot be correct."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/91462-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients connect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic. The app has not been able to keep up with the traffic.<br>A solutions architect needs to implement a solution so that the app can handle the new and varying load.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 87,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 76,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 51,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T04:02:00.000Z",
        "voteCount": 33,
        "content": "Serverless requires least operational effort."
      },
      {
        "date": "2023-02-26T11:00:00.000Z",
        "voteCount": 18,
        "content": "How can this be the answer ?? It says: Separate the API into individual AWS Lambda functions. Can you calculate the operational overhead to do that?"
      },
      {
        "date": "2023-04-14T17:58:00.000Z",
        "voteCount": 13,
        "content": "Separating would be development overhead, but once done, the operational overheard (operational = ongoing day-to-day) will be the least."
      },
      {
        "date": "2024-03-14T17:29:00.000Z",
        "voteCount": 1,
        "content": "disagree, ASG in Option D, after set up, operational is not overheat as well"
      },
      {
        "date": "2024-03-14T17:30:00.000Z",
        "voteCount": 1,
        "content": "i mean Option C not D"
      },
      {
        "date": "2024-03-14T17:31:00.000Z",
        "voteCount": 1,
        "content": "never mind, A is simpler than C"
      },
      {
        "date": "2023-05-19T02:50:00.000Z",
        "voteCount": 1,
        "content": "From any type of real-world perspective, this just can't be the answer IMHO. Surely AWS takes \"real world\" into account."
      },
      {
        "date": "2023-08-18T18:11:00.000Z",
        "voteCount": 3,
        "content": "I guess multivalue answer routing in Route53 is not proper load balancing so replacing multivalue answer routing with ALB would proper balance the load (with minimal effort)"
      },
      {
        "date": "2023-01-30T19:10:00.000Z",
        "voteCount": 27,
        "content": "Suppose there are a 100 REST APIs (Since this application is monolithic, it's quite common).\nAre you still going to copy and paste all those API codes into lambda?\nWhat if business logic changes?\nThis is not MINIMAL. I would go with C."
      },
      {
        "date": "2023-05-29T23:13:00.000Z",
        "voteCount": 1,
        "content": "\"Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record. \"\nThis does not make any sense, why do you need to change R53 records using a Lambda?"
      },
      {
        "date": "2023-08-10T06:41:00.000Z",
        "voteCount": 4,
        "content": "Because if you have 4 ec2 in your ASG you need to have 4 records in domain name if ASG scale up to 6 for example you need 2 add 2 records more in domain name"
      },
      {
        "date": "2024-03-03T03:02:00.000Z",
        "voteCount": 3,
        "content": "Too contrived in my opinion, and what about DNS caches in the clients?. You coul get stuck for a while with the previous list of  servers. I think it's has to be A (but it would involve a considerable development effort) or D which is extremely easy to implement but and the same time it sounds a little bit fishy because they don't mention anything about ASG or scaling\n\nI hate this kind of questions and I don't understand what kind of useful insight they provide unless they want us to become masters of the art of dealing with ambiguity"
      },
      {
        "date": "2024-06-19T21:05:00.000Z",
        "voteCount": 1,
        "content": "Agree that D does not scale to meet demand, it's just a better way to load balance which was being done at R53 before so the scaling issue has not been resolved.\nAlso agree A requires more dev effort and less ops effort, so I would have to lean to A...\nAnswer selection is poor IMO"
      },
      {
        "date": "2023-02-23T15:14:00.000Z",
        "voteCount": 8,
        "content": "It says \"a monolithic REST-based API \" - hence only 1 API. Initially I thought C, but I'll go with A as it says least operation overhead (not least implementation effort). Lambda has virtually no operation overhead compared to EC2."
      },
      {
        "date": "2023-07-06T07:08:00.000Z",
        "voteCount": 1,
        "content": "Answer A says \"Separate the API into individual AWS Lambda functions.\" Makes me think there may be many APIs.\n\nHowever, we are looking to minimize operational effort, not development effort..."
      },
      {
        "date": "2023-05-01T02:32:00.000Z",
        "voteCount": 5,
        "content": "A monolithic REST api likely has a gazillion individual APIs. This refactor would not be a small one."
      },
      {
        "date": "2023-11-23T22:29:00.000Z",
        "voteCount": 1,
        "content": "Dealing with business logic change is applicable to existing solution or any solution based on the complexity. Rather it's easier to deal when these are microservices. You shouldn't hesitate to refactor your application by putting one time effort (dev overhead) to save significant operational overhead on daily basis. AWS is pushing for serverless only for this."
      },
      {
        "date": "2024-10-08T19:14:00.000Z",
        "voteCount": 1,
        "content": "Option C is correct.\nOption A: You have to migrate all the business logic to lambda functions. I don't know how the option A is marked as correct when the question says with the Least Operation Overhead.\nOption D is good because we have an ALB in front of the instances and we don't have to handle the load balancing in Route 53, however this option doesn't scale, so if we got a traffic increase the application will fail.\nOption C: we have an ASG, so our application instances scales horizontally on demand, the downside is that we have to keep managing the load balancing at Route 53."
      },
      {
        "date": "2024-10-01T13:07:00.000Z",
        "voteCount": 1,
        "content": "Operational overhead is the cost of the day-to-day operation of the service in question. The questions usually differentiated themselves by having answers that demonstrated knowing a service could potentially be expensive and you might be able to minimize the cost using an a different AWS service or sometimes not an AWS service at all. Whenever you see these in a question think \"serverless\" and \"easiest to implement and operate day 2\" it basically eliminates any answers that deploy infra (EC2 instances) that you will have to patch and manage."
      },
      {
        "date": "2024-09-26T10:25:00.000Z",
        "voteCount": 1,
        "content": "After reading this discussions I am also with A..  because D has no AutoScaling"
      },
      {
        "date": "2024-09-19T05:16:00.000Z",
        "voteCount": 1,
        "content": "In my opinion the key is this part of the question \"LEAST operational overhead\". Serverless is the best fit here."
      },
      {
        "date": "2024-09-17T05:06:00.000Z",
        "voteCount": 1,
        "content": "A: It requires huge code restructuring normally re-writting the code would be the last option of any architectural changes.\nB: Kubernetes kind of increases the operational overhead in terms of knowledge to handle them and complexity in configurations\nC: Supports scalling and meets all the requirements with minimal effort.\nD. ALB load balances effectively, would not exactly be able to \u201chandle the new and varying load\u201d I guess. \n\nA &amp; C: Kind of satisfy the requirement and has the Least Overhead. But considering the other factors Personally I would go with option C"
      },
      {
        "date": "2024-09-10T12:03:00.000Z",
        "voteCount": 1,
        "content": "Option A will have the LEAST operational effort."
      },
      {
        "date": "2024-08-31T02:36:00.000Z",
        "voteCount": 1,
        "content": "D. Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB."
      },
      {
        "date": "2024-08-03T21:11:00.000Z",
        "voteCount": 2,
        "content": "It has to be A, period.\n\nProblem with C: muti-value has an upper limit: 8. Route 53 responds to DNS queries with up to eight healthy records and gives different answers to different DNS resolvers. Also you need to manage the elastic IP's attachment everytime when new instances scale up for route53 multi-value routing\n\nProblem with D: multi-value cannot work with load balancers. please check doc here: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html"
      },
      {
        "date": "2024-08-03T21:12:00.000Z",
        "voteCount": 1,
        "content": "So for option C, if you upscale to 8 instances and the API still get overwhelmed, then there's nothing more you can do about it"
      },
      {
        "date": "2024-07-29T09:51:00.000Z",
        "voteCount": 1,
        "content": "A requires more work and it's not practical\nD cannot be the answer, why are we even moving instances to the private subnet in the first place? No security issues or other issues mentioned here."
      },
      {
        "date": "2024-07-30T02:48:00.000Z",
        "voteCount": 1,
        "content": "Creating an Auto Scaling group and managing updates to Route 53 records via a Lambda function involves more complexity and management. The use of an ALB (as in Option D) is more efficient, as it inherently provides load balancing and scaling features without the need to update DNS records constantly."
      },
      {
        "date": "2024-07-16T06:44:00.000Z",
        "voteCount": 3,
        "content": "Keep it simple, we can't assume if the API is a large/small application, the idea is make the least operational overhead and that is only add a ALB, We don't know the effort to move the application to a lambda, Answer is D"
      },
      {
        "date": "2024-07-29T14:50:00.000Z",
        "voteCount": 1,
        "content": "True, some apps won't work well on Lambda. On the other hand option D is missing auto-scaling, which means it won't cope with increasing traffic. Assuming the app can be ported to Lambda, A satisfies all requirements: scalability and very low operational effort."
      },
      {
        "date": "2024-07-12T11:53:00.000Z",
        "voteCount": 4,
        "content": "Response is D\nA- Requires significant refactoring of the application\nB- solution complex and requires containerizing application.\nC- The multi-value answer routings less flexible compared to using an ALB for load balancing."
      },
      {
        "date": "2024-07-29T14:53:00.000Z",
        "voteCount": 1,
        "content": "Refactoring is not operational effort. Operational effort is the routine work done once the application is in production (patching OS, monitoring logs, restarting servers, increasing capacity, etc). Serverless always has the lowest operational effort for the customer because AWS do it behind the scenes."
      },
      {
        "date": "2024-07-27T08:21:00.000Z",
        "voteCount": 1,
        "content": "ALB won't help you with scaling. Obviously clear case for C"
      },
      {
        "date": "2024-06-09T23:23:00.000Z",
        "voteCount": 1,
        "content": "D is a perfect, least operation effort. C needs to write lamda func which is over head."
      },
      {
        "date": "2024-06-04T23:39:00.000Z",
        "voteCount": 17,
        "content": "The least operational overhead solution is:\n\nD. Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets. Add the instances as targets for the ALB. Update the Route 53 record to point to the ALB."
      },
      {
        "date": "2024-06-19T21:09:00.000Z",
        "voteCount": 2,
        "content": "D does not scale to meet demand, it's just a better way to load balance which was being done at R53 before so the scaling issue has not been resolved.\nA requires more dev effort (not a consideration in the question) and less ops effort, so I would have to lean to A...\nAnswer selection is poor IMO for this question .."
      },
      {
        "date": "2024-06-04T14:22:00.000Z",
        "voteCount": 1,
        "content": "The questions requires least operational effort... Nothing mentions the dev work to refactor!"
      },
      {
        "date": "2024-06-04T14:25:00.000Z",
        "voteCount": 1,
        "content": "In addition, a monolithic REST-API does not necessarily require huge work to work effectively on lambda.... It depends on how it is written, might be very easy or very complicated!"
      },
      {
        "date": "2024-05-22T15:21:00.000Z",
        "voteCount": 2,
        "content": "C should be the answer\nA - IMO, it's not feasible given the entire application is a monolithic, so we can't just refactor to separate into Lambda functions.\nD - Since there is no mention of ASG, this is ruled out. This does nothing to address the high volume requests."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/91463-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization has hundreds of AWS accounts.<br>A solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.<br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems Manager OpsCenter dashboards."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T13:33:00.000Z",
        "voteCount": 19,
        "content": "B is the correct answer. The solution would be to create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. This would allow the management account to view the usage costs across all the member accounts, and the teams can visualize the CUR through an Amazon QuickSight dashboard. This allows the organization to have a centralized place to view the cost breakdown and the teams to access the cost breakdown in an easy way."
      },
      {
        "date": "2024-09-10T12:05:00.000Z",
        "voteCount": 1,
        "content": "Option B: it must be done from the management accoint"
      },
      {
        "date": "2024-08-31T02:37:00.000Z",
        "voteCount": 1,
        "content": "B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard."
      },
      {
        "date": "2024-05-01T08:39:00.000Z",
        "voteCount": 2,
        "content": "Option C: I hate this questions because you have 2 correct answers however only ONE real correct answer.  I have to read the question like 20x until I understood it, the questions is asking for \" solution so the EACH OU can VIEW a breakdown of usage across ITS account\".   \nIts only asking for each OU breakdown for its members can see the usage cost and NOT the organization.  Prior to Dec 2020 Option B would be correct however after its Option C: \n\nRead the following AWS Update - https://aws.amazon.com/about-aws/whats-new/2020/12/cost-and-usage-report-now-available-to-member-linked-accounts/?pg=ln&amp;sec=uc"
      },
      {
        "date": "2024-03-16T15:36:00.000Z",
        "voteCount": 1,
        "content": "B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard."
      },
      {
        "date": "2024-02-24T08:29:00.000Z",
        "voteCount": 1,
        "content": "C\nAs target is to design a solution so that each OU can view a breakdown of usage costs across its AWS accounts"
      },
      {
        "date": "2024-02-09T14:52:00.000Z",
        "voteCount": 1,
        "content": "The question specifies that each OU should only view their own AWS accounts, not all accounts in the organization. While creating the solution in the management account might offer a centralized approach, it violates this crucial requirement."
      },
      {
        "date": "2024-02-09T14:56:00.000Z",
        "voteCount": 3,
        "content": "Sorry, I'm wrong, RAM can't create a Cost Report."
      },
      {
        "date": "2023-11-26T11:00:00.000Z",
        "voteCount": 1,
        "content": "B From management account of each account"
      },
      {
        "date": "2023-10-01T18:05:00.000Z",
        "voteCount": 1,
        "content": "AWS Resource Access Manager has nothing to do with creating CUR.\nAnswer B is correct. Use AWS Organization management account"
      },
      {
        "date": "2023-08-14T08:42:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/mt/visualize-and-gain-insights-into-your-aws-cost-and-usage-with-cloud-intelligence-dashboards-using-amazon-quicksight/"
      },
      {
        "date": "2023-06-28T19:57:00.000Z",
        "voteCount": 1,
        "content": "B by elimination"
      },
      {
        "date": "2023-05-09T03:46:00.000Z",
        "voteCount": 1,
        "content": "B As AWS Organizations Management account is only correct option"
      },
      {
        "date": "2023-04-17T20:32:00.000Z",
        "voteCount": 1,
        "content": "Can anyone explain why A is wrong? Thank you."
      },
      {
        "date": "2023-04-29T21:35:00.000Z",
        "voteCount": 4,
        "content": "AWS Resource Access Manager has nothing to do with creating CURs. It's for sharing resources with other accounts."
      },
      {
        "date": "2023-03-28T01:26:00.000Z",
        "voteCount": 2,
        "content": "B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account."
      },
      {
        "date": "2022-12-13T09:00:00.000Z",
        "voteCount": 3,
        "content": "https://www.examtopics.com/discussions/amazon/view/71951-exam-aws-certified-solutions-architect-professional-topic-1/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/91464-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is storing data on premises on a Windows file server. The company produces 5&nbsp;GB of new data daily.<br>The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS.<br>Which data migration strategy should the company use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 35,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T13:36:00.000Z",
        "voteCount": 14,
        "content": "B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.\nD. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS) are also valid options. They both use DataSync to schedule a daily task to replicate the data between on-premises and cloud, the main difference is the type of file system in the cloud, Amazon FSx or Amazon Elastic File System (Amazon EFS)."
      },
      {
        "date": "2023-05-02T14:02:00.000Z",
        "voteCount": 22,
        "content": "EFS only support Linux FS. this is why we need to go for FSx . option B"
      },
      {
        "date": "2023-08-15T06:04:00.000Z",
        "voteCount": 1,
        "content": "thanks for this explaination.\n&gt; EFS only support Linux FS. this is why we need to go for FSx . option B"
      },
      {
        "date": "2023-08-31T12:30:00.000Z",
        "voteCount": 12,
        "content": "For an and b we need FSx. Data Sync is useful for a batch and is able to process large data volumes. in (a) the data is also accessible from on prem. The data volume is quite small (5 GB) per day therefore (a) is feasible. In my opinion, the key requirement is \"data to be available on a file system in the cloud\" and \",, migrating workloads\" and I think this includes that it can be accessed from servers on prem. In addition (a) replaces only a Windows File server and not the overall windows landscape in AWS. There I vote for (a), AWS Data Sync.\n\nSee https://tutorialsdojo.com/aws-datasync-vs-storage-gateway/ for a comparison"
      },
      {
        "date": "2023-11-27T11:37:00.000Z",
        "voteCount": 3,
        "content": "Correct point here is migration not daily sync and replication."
      },
      {
        "date": "2023-09-04T02:00:00.000Z",
        "voteCount": 3,
        "content": "needs the data to be available on a file system in the cloud"
      },
      {
        "date": "2024-10-09T21:21:00.000Z",
        "voteCount": 1,
        "content": "Using Amazon FSx File Gateway, you can access data with low latency from on-premise and also in-cloud always. Why do you neet to batch datasync as like B? "
      },
      {
        "date": "2024-08-31T02:39:00.000Z",
        "voteCount": 1,
        "content": "B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx."
      },
      {
        "date": "2024-07-29T15:21:00.000Z",
        "voteCount": 2,
        "content": "Because part of the workloads have already been migrated we need a solution that keeps the data consistent between on prem and the cloud. With DataSync files stored by systems on-prem would be visible in the cloud only the following day. This could cause data inconsistencies and business disruption. The best solution is to use a file gateway to maintain files synchronised at all times. 5GBs/day is easily transferable over DX"
      },
      {
        "date": "2024-07-11T10:49:00.000Z",
        "voteCount": 1,
        "content": "B, for sure.\nNeeds the data to be available on a file system in the cloud."
      },
      {
        "date": "2024-04-21T05:47:00.000Z",
        "voteCount": 1,
        "content": "Windows file server -&gt; FSx (cristal clear)"
      },
      {
        "date": "2024-04-01T16:44:00.000Z",
        "voteCount": 2,
        "content": "A is not the data migration"
      },
      {
        "date": "2024-03-22T13:59:00.000Z",
        "voteCount": 1,
        "content": "option B is the most suitable data migration strategy for the company. It leverages AWS DataSync to automate the replication of daily data increments from the on-premises Windows file server to Amazon FSx for Windows File Server. This approach provides a seamless integration for Windows-based workloads with minimal disruption and supports the company's needs for a cloud-native file system that is fully managed and integrates well with AWS services."
      },
      {
        "date": "2024-07-29T15:13:00.000Z",
        "voteCount": 1,
        "content": "Batch sync is not seamles and might not be with minimal disruption depending on how it's used. Is this generated with ChatGPT?"
      },
      {
        "date": "2024-03-22T13:50:00.000Z",
        "voteCount": 2,
        "content": "This option is particularly suitable for the company's requirements because it allows for scheduled daily tasks to efficiently replicate the 5 GB of new data to Amazon FSx, providing a cloud-native file system that integrates well with Windows-based workloads."
      },
      {
        "date": "2024-03-16T15:41:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2024-03-03T06:42:00.000Z",
        "voteCount": 1,
        "content": "B is right, but, I wish they change \"FSx\" to \"FSx for windows file server\""
      },
      {
        "date": "2024-03-03T02:26:00.000Z",
        "voteCount": 1,
        "content": "The key here is the word \"migration\". This suggests DataSync. If the objective was to set up a permanent hybrid solution, then AWS Storage Gateway would be the solution. Again an example where the entire question hinges on one single word."
      },
      {
        "date": "2024-07-29T15:14:00.000Z",
        "voteCount": 1,
        "content": "My thinking is that a file gateway would be better in a migration where applications are moved in batches because it provides a drop-in replacement. Batch sync could break some of the apps that have not yet migrated."
      },
      {
        "date": "2024-02-23T11:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html"
      },
      {
        "date": "2024-02-08T18:00:00.000Z",
        "voteCount": 1,
        "content": "The most appropriate data migration strategy for the company, considering the need for the data to be available on a file system in the cloud and the existing AWS Direct Connect connection, is:\n\n    *    B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.\nOption B is the best choice because AWS DataSync is a data transfer service designed to make it easy to move large amounts of data online between on-premises storage systems and AWS storage services. Amazon FSx provides fully managed Windows file servers in the cloud, offering native Windows file system capabilities, making it an ideal target for Windows-based workloads that the company has migrated to AWS. Using DataSync to automate the daily replication of data ensures the new data produced is consistently available in the cloud with minimal manual effort."
      },
      {
        "date": "2024-01-29T19:38:00.000Z",
        "voteCount": 1,
        "content": "company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. -&gt; This line is key here, they have already moved part of Windows workload and require data to be available on another file system in the cloud. This is possible by Data Sync migrating data to FSx for Windows Server (SMB supporting file server). \n\nFile Gateway - would for connection to S3 and hardware compliance locally will give illusion of File Server. This is good for DR, Backup and migration to cheaper storage. But doesn't solve the purpose of moving data and creating another file share in Cloud."
      },
      {
        "date": "2024-01-11T03:50:00.000Z",
        "voteCount": 1,
        "content": "B incorrect. Since you created a DX between AWS and on-premises, you can mount FSx in your local server directly. It doesn\u2019t make sense to schedule a daily task."
      },
      {
        "date": "2024-03-06T13:38:00.000Z",
        "voteCount": 1,
        "content": "This is wrong look at what the question actually asks, it says what is a proper MIGRATION strategy, File Gateway only lets you access data but does not migrate data from on premises."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/91102-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company\u2019s solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T18:45:00.000Z",
        "voteCount": 17,
        "content": "C is correct.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      },
      {
        "date": "2024-08-31T02:39:00.000Z",
        "voteCount": 1,
        "content": "C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins."
      },
      {
        "date": "2024-08-11T05:55:00.000Z",
        "voteCount": 1,
        "content": "Why not D ? D. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region:\n\nManual Failover: This option involves manual updates to the application code in the event of a failover, which adds operational overhead and complexity. CloudFront provides automatic failover and load balancing, making it a more streamlined solution."
      },
      {
        "date": "2024-04-20T18:49:00.000Z",
        "voteCount": 1,
        "content": "C IS THE answer"
      },
      {
        "date": "2024-03-16T15:43:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-02-23T03:53:00.000Z",
        "voteCount": 1,
        "content": "Straightforward"
      },
      {
        "date": "2024-02-08T18:07:00.000Z",
        "voteCount": 1,
        "content": "Option C is the most efficient solution because it leverages S3\u2019s built-in replication feature to automatically replicate objects to a second bucket in another Region, ensuring that the data is resiliently stored across multiple Regions. By using Amazon CloudFront with an origin group containing both S3 buckets, the application benefits from CloudFront\u2019s global content delivery network, which improves load times and provides a built-in failover mechanism. This setup minimizes operational overhead while achieving the desired resiliency and performance improvements.\nOption C provides a seamless, automated solution for achieving resiliency across multiple AWS Regions with minimal operational effort, leveraging AWS services designed for replication, content delivery, and failover."
      },
      {
        "date": "2024-01-29T20:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct because,\n\nYou can server Dynamic Websites with Static Content with CDN by having origins for both and  in your webserver app refer to DNS for s3 origin from CF to deliver static content. For webserver on EC2 (Custom Origins can be used). \nSo in above scenario, if you would like to have resiliency. Add another S3 Origin with bucket in different region. Create Origin Group with both S3 Origins. Set priority on Origins and select 4XX and 5XX error codes for failover. You can use DNS returned for Origin Group from Cloud front in your web app and that would do automatic failover with least overheads. \n\nD also solves the purpose, but you will need to build failover mechanism in your app. However, with above Cloudfront Origin group is taking care of that for you."
      },
      {
        "date": "2023-12-20T03:35:00.000Z",
        "voteCount": 2,
        "content": "All options does the job, but:\nA would require code maintenance and managing public hosted zone -&gt; No\nB would require Lambda and CloudFront operations -&gt; No\nC would require only CloudFront operations -&gt; Yes\nD requires a lot of work for failover that appears to be manual -&gt; No"
      },
      {
        "date": "2023-12-05T05:47:00.000Z",
        "voteCount": 1,
        "content": "C is mostly correct, A is not correct - B and D required the code changes. C will take care of the cloud front orgin failover."
      },
      {
        "date": "2023-11-26T11:01:00.000Z",
        "voteCount": 1,
        "content": "C is good"
      },
      {
        "date": "2023-11-12T20:57:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2023-09-30T06:08:00.000Z",
        "voteCount": 4,
        "content": "Here's why Option C is the most suitable choice:\n\nReplication: Amazon S3 Cross-Region replication is designed to replicate objects from one S3 bucket to another in a different Region. This ensures data resiliency across Regions with minimal operational overhead. Once configured, replication happens automatically.\n\nCloudFront: Setting up an Amazon CloudFront distribution with an origin group containing the two S3 buckets allows you to use a single CloudFront distribution to serve content from both Regions. CloudFront provides low-latency access to your content, and using an origin group allows for failover if one of the S3 buckets becomes unavailable."
      },
      {
        "date": "2023-09-30T06:08:00.000Z",
        "voteCount": 1,
        "content": "Option A suggests configuring the application to write each object to both S3 buckets, which can result in higher operational overhead and may not provide immediate failover capabilities.\n\nOption B involves creating a Lambda function to copy objects, which adds complexity and requires code maintenance for each object written to the S3 bucket in us-east-1.\n\nOption D relies on manual updates to the application code for failover, which is less automated and could result in higher operational overhead.\n\nTherefore, Option C is the most efficient and operationally streamlined solution to achieve data resiliency and availability across multiple AWS Regions."
      },
      {
        "date": "2023-09-09T21:48:00.000Z",
        "voteCount": 1,
        "content": "C, LEAST operational overhead"
      },
      {
        "date": "2023-08-29T14:07:00.000Z",
        "voteCount": 1,
        "content": "C should incur the least operational cost while D still requires the cx to update the code in whatever way they deem as appropriate"
      },
      {
        "date": "2023-08-15T06:06:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      },
      {
        "date": "2023-08-09T13:11:00.000Z",
        "voteCount": 1,
        "content": "Its completely asking CRR Right one is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/91101-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.<br>Which steps should the solutions architect take to design an appropriate solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company\u2019s domain to the NLB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company\u2019s domain to the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company\u2019s domain to the ALB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 60,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-14T00:44:00.000Z",
        "voteCount": 27,
        "content": "Agree with B.\nNot A: we will not use NLB for web app\nNot C: Beanstalk is region service. It CANNOT \"automatically scaling web server environment that spans two separate Regions\"\nNot D:  spot instances cant meet 'highly available'"
      },
      {
        "date": "2024-03-16T16:19:00.000Z",
        "voteCount": 1,
        "content": "I don't think ASGs are cross-region either. This answer in SO gives a serious perspective on this regard. https://stackoverflow.com/a/12907101/3126973"
      },
      {
        "date": "2023-01-13T13:44:00.000Z",
        "voteCount": 6,
        "content": "That's correct, option C is not a valid solution because AWS Elastic Beanstalk is a region-specific service, it cannot span multiple regions. Option B is a valid solution that uses CloudFormation to launch a stack with an Application Load Balancer in front of an Auto Scaling group, a Multi-AZ Aurora MySQL cluster and Route 53 to route traffic to the load balancer, it meets the requirements of scalability and high availability with a good performance and with less operational overhead."
      },
      {
        "date": "2023-06-14T11:55:00.000Z",
        "voteCount": 5,
        "content": "if I am not mistaken you can deploy the same EB to a different region. why does that eliminate C? it further increases your availability with geolocation weighted routing, as well as you having DR which even further increases availability along with low RPO and RTO"
      },
      {
        "date": "2023-12-30T08:14:00.000Z",
        "voteCount": 1,
        "content": "I agree with you, that's the best option, two EBs, one in each region to deploy, manage and monitor all the environment."
      },
      {
        "date": "2023-01-13T13:42:00.000Z",
        "voteCount": 8,
        "content": "B is correct. The solution architect should use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB.\n\nThis solution provides scalability and high availability for the web application by using an Application Load Balancer and an Auto Scaling group in multiple availability zones, which can automatically scale in and out based on traffic demand. The use of a Multi-AZ Amazon Aurora MySQL DB cluster provides high availability for the database layer and the Retain deletion policy ensures the data is retained even if the DB instance is deleted. Additionally, the use of Route 53 with an alias record ensures traffic is routed to the correct location."
      },
      {
        "date": "2024-08-31T02:41:00.000Z",
        "voteCount": 1,
        "content": "B. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company\u2019s domain to the ALB."
      },
      {
        "date": "2024-07-14T00:38:00.000Z",
        "voteCount": 2,
        "content": "B, for sure.\nElastic Beanstalk is region specific.\nThe \"Retain\" deletion policy in AWS Aurora ensures that when you delete a database cluster, the automated backups and snapshots of the cluster are retained. This means that even though the database cluster itself is deleted, the backups and snapshots remain, allowing you to restore the cluster from those backups at a later time."
      },
      {
        "date": "2024-07-11T10:52:00.000Z",
        "voteCount": 1,
        "content": "B, for sure.\nElastic Beanstalk environments are typically created within a single AWS region."
      },
      {
        "date": "2024-04-02T06:35:00.000Z",
        "voteCount": 3,
        "content": "Option C:  The only AWS documentation I found that support .NET application migration is for Elastic Beanstalk, it said \" EB is the fastest and simplest way to deploy .NET applications on AWS\"  Many suggestion is selection option \"B\", the question is not asking about cost or least operational overhead, just scalable and highly available for the migration for a .NET application.  Also, I can see why so many people are selecting option \"B\".\n\nhttps://docs.aws.amazon.com/whitepapers/latest/develop-deploy-dotnet-apps-on-aws/aws-elastic-beanstalk.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.concepts.design.html"
      },
      {
        "date": "2024-03-16T17:15:00.000Z",
        "voteCount": 2,
        "content": "B however is not a highly available solution IMO because it is restricted to a region. By any chance if the region goes down, the webapp goes down as well.\nA is out of the picture because it involves an NLB.\nD is out of the picture because it involves spot instances which is not the choice for HA requirements.\nC, everything is good except the mention of \"Elastic Beanstalk environment that spans across regions\". This is wrong. EB environments are a region construct. You can's have them spanning cross region. You can however have EB in multiple regions."
      },
      {
        "date": "2024-01-16T15:06:00.000Z",
        "voteCount": 1,
        "content": "Guessing the question designer prefers B. But it is wrong. When talking about R53 Alias record, it is wrong. Cause Alias record points to IP address while ALB endpoint is not an IP address.\nA has flaw. The question says 3-tier web application. AWS question designers often mess up the definition of 3-tier application, which means there isn\u2019t a very clear definition of 3 tier: browser/application server/database is one definition, another one is WebServer/Application Server/database. Looks like A means the latter. Then, if the Elastic Beanstalk is hosting a web server, what are the ASG hosting? And why the R53 is pointing to the NLB which is pointing to the ASG?\nC is wrong, cause Elastic Beanstalk cannot span regions.\nD is wrong because spot instance is not HA.\nWeighting the flaws of different answers, B has the least flaw."
      },
      {
        "date": "2023-12-20T03:58:00.000Z",
        "voteCount": 1,
        "content": "Not C as we do not need to span multiple Region (DR, global reach, ...), also cross-Region read replica does not fail-over automatically (you need to promote it to primary). Finally from the wording it seems that this imply having a single environment that spans two separate Regions which is not supported (you need two separate environments) \nNot D as we have a single RDS DB instance, no HA\n\nBoth A and B does the job, but B provides better scalability as it make use of Aurora Multi-AZ that allows secondary (reader) instance(s) to be accessed for reads, while RDS Multi-AZ instance does not allow standby instance endpoint to be accessed. This could be circumvented by using Multi-AZ DB cluster deployment that provides 2 readable standby instance"
      },
      {
        "date": "2023-12-11T08:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nThe best way to migrate a .NET application to AWS is via Beanstalk (see: https://docs.aws.amazon.com/whitepapers/latest/develop-deploy-dotnet-apps-on-aws/aws-elastic-beanstalk.html) \n\nI think that the question regarding spanning a deployment across two regions has triggered some to reject based on the multi-region but if you continue you will notice the separate regional deployments based on two ALBs etc.  Just my two pennie :)"
      },
      {
        "date": "2023-12-05T05:52:00.000Z",
        "voteCount": 1,
        "content": "B is the correct,"
      },
      {
        "date": "2023-12-04T09:53:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-11-26T11:01:00.000Z",
        "voteCount": 1,
        "content": "B is good"
      },
      {
        "date": "2023-09-30T06:12:00.000Z",
        "voteCount": 1,
        "content": "Here's why Option B is the best choice:\n\nHigh Availability: The use of an Application Load Balancer (ALB) and Amazon Aurora Multi-AZ deployment ensures high availability and fault tolerance for the web application and the MySQL database. The Multi-AZ setup for Aurora provides automatic failover.\n\nScalability: Using an EC2 Auto Scaling group across multiple Availability Zones allows the application to automatically scale to meet traffic demands. This is crucial for handling the surge in traffic from 200,000 daily users.\n\nDeletion Policy: The Retain deletion policy for the Aurora MySQL DB cluster ensures that even if the CloudFormation stack is deleted, the database is retained, which is important for data preservation and recovery.\n\nRoute 53 Routing: Route 53 with an alias record provides efficient DNS routing, directing traffic to the ALB, which then distributes it to the EC2 instances. This ensures that users can access the application reliably."
      },
      {
        "date": "2023-09-30T06:12:00.000Z",
        "voteCount": 1,
        "content": "Option C introduces unnecessary complexity by spanning two separate Regions and using geoproximity routing. This is typically used for disaster recovery and global deployments, which may not be necessary here."
      },
      {
        "date": "2023-09-09T22:14:00.000Z",
        "voteCount": 1,
        "content": "The question required to \u201cdesign a scalable and highly available solution\u201d. Cause the different between Beanstalk and CloudFormation is, Beanstalk is PaaS (platform as a service) while CloudFormation is IaC (infrastructure as code). So I go for Answer B, as it is related to infrastructure."
      },
      {
        "date": "2023-08-31T12:58:00.000Z",
        "voteCount": 1,
        "content": "\"web server environment\" doesn't require a single instance to spawns multiple regions, multiple AWS Beanstalks for each region are also feasible.  With geoproximity routing it is guaranteed the requests are routed to the same region. In addition the requirement is \"highly available\", which can be achieve with a multi region architecture"
      },
      {
        "date": "2023-08-30T07:38:00.000Z",
        "voteCount": 1,
        "content": "A. I do not quite understand the choice of NLB for this, but Multi-AZ DB instance, EC2 auto-scaling in multiple AZ sure sounds good.\n\nC. Elastic Beanstalk does not \"span multiple regions\". Geoproximity routing does not sound right for a disaster recovery scenario.\n\nB. I like CloudFormation, and I like the Retain deletion policy. In order to switch to the other region, one will need to update the Route 53 alias...\n\nD. I do not like the Snapshot deletion policy... The DB is not Multi-AZ, nor has a read-replica in the fail-over region. Spot instance is not great for HA."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/91486-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.<br>A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.<br>What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T13:47:00.000Z",
        "voteCount": 19,
        "content": "The best solution is C, because it involves creating the stack set in the management account of the organization, which is the central point of control for all the member accounts. This allows the solutions architect to manage the deployment of the stack set across all member accounts from a single location. Service-managed permissions are used, which allows the CloudFormation service to deploy the stack set to all member accounts. The deployment options are set to deploy to the organization and automatic deployment is enabled, which ensures that the stack set is automatically deployed to all member accounts as soon as it is created in the management account."
      },
      {
        "date": "2022-12-13T11:26:00.000Z",
        "voteCount": 5,
        "content": "https://www.examtopics.com/discussions/amazon/view/47723-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2024-08-31T02:41:00.000Z",
        "voteCount": 1,
        "content": "C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment."
      },
      {
        "date": "2024-01-30T17:04:00.000Z",
        "voteCount": 1,
        "content": "C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.\n\nC is more suitable as Enable CloudFormation StackSets automatic deployment will take care of any new account in the Org. Set deployment options to deploy to the organization helps deploying Stack Instances to targeted account in Org. Use service-managed permissions is hassle free as it takes care or roles for you.\n\nD. Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.\n\nD is good option too as StackSets drift detection is a good option to have but not a requirement. It only saves from future troubleshooting of drift scenarios."
      },
      {
        "date": "2024-01-22T16:54:00.000Z",
        "voteCount": 2,
        "content": "D is wrong - Drift Detection identifies unmanaged changes (Outside CloudFormation)"
      },
      {
        "date": "2023-11-23T22:43:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C since it satisfies all the requirements with minimum operational overhead. But wondering if \"Stack Sets drift detection\" is just a distractor here. Can someone throw some light on this?"
      },
      {
        "date": "2023-12-20T06:03:00.000Z",
        "voteCount": 1,
        "content": "I am not an expert, just sharing my thoughts: \n\"Stack Sets drift detection\" is a feature of stack set, however this is not needed according to the scenario.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-drift.html. \n\nD is a no-go for me because it deploys in each managed account without making use of stack sets, so you cannot then use stack sets drift detection."
      },
      {
        "date": "2023-10-01T18:24:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2023-06-28T20:09:00.000Z",
        "voteCount": 1,
        "content": "C no brainer"
      },
      {
        "date": "2023-03-28T01:47:00.000Z",
        "voteCount": 2,
        "content": "Create a stack set in the Organizations management account."
      },
      {
        "date": "2023-02-17T14:02:00.000Z",
        "voteCount": 2,
        "content": "Stack Set in Mgmt account"
      },
      {
        "date": "2022-12-13T13:56:00.000Z",
        "voteCount": 1,
        "content": "I THINK I SHOULD BE A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/95087-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-premises infrastructure that consists of physical machines and VMs that host numerous applications.<br><br>The company must capture details about the system configuration, system performance, running processes, and network connections of its on-premises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs recommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup servers into applications for migration by using AWS Systems Manager Application Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup servers into applications for migration by using AWS Migration Hub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate recommended instance types and associated costs by using AWS Migration Hub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "ADF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-18T16:55:00.000Z",
        "voteCount": 24,
        "content": "trusted advisor doesn't have option to upload data, so option F is irrelavent"
      },
      {
        "date": "2023-12-20T06:12:00.000Z",
        "voteCount": 7,
        "content": "A vs B -&gt; A because we need to use AWS Application Discovery and it provides its own agent\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-agent.html\nC vs D -&gt; D because AWS Application Discovery is integrated with AWS Migration Hub and it can be used to group servers into applications\nhttps://aws.amazon.com/migration-hub/faqs/#:~:text=How%20do%20I%20group%20servers%20into%20an%20application%3F\nE vs. F -&gt; E as AWS Migration Hub allows to generate recommendation for instance types\nhttps://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html"
      },
      {
        "date": "2024-08-31T02:42:00.000Z",
        "voteCount": 1,
        "content": "A. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.\nD. Group servers into applications for migration by using AWS Migration Hub.\nE. Generate recommended instance types and associated costs by using AWS Migration Hub."
      },
      {
        "date": "2024-08-11T06:17:00.000Z",
        "voteCount": 1,
        "content": "Why not B ? B. Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs:\n\nExplanation: AWS Systems Manager Agent is used for managing and automating tasks on EC2 instances, not for capturing detailed application and performance data during an assessment phase. AWS Application Discovery Agent is more appropriate for this purpose."
      },
      {
        "date": "2024-03-16T15:58:00.000Z",
        "voteCount": 1,
        "content": "ADE is correct"
      },
      {
        "date": "2024-02-08T18:33:00.000Z",
        "voteCount": 3,
        "content": "The correct answers are:\n    *    A. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs. The AWS Application Discovery Service helps gather detailed information about on-premises data centers, including servers, network dependencies, and performance metrics. \n    *    D. Group servers into applications for migration by using AWS Migration Hub. AWS Migration Hub provides a centralized location to track the progress of application migrations across multiple AWS and partner solutions. It allows grouping discovered servers into applications, which simplifies the organization of migration tasks.\n    *    E. Generate recommended instance types and associated costs by using AWS Migration Hub. After servers are discovered and grouped into applications, AWS Migration Hub can analyze the collected data to recommend suitable Amazon EC2 instance types. This ensures that the migrated applications are hosted on the most cost-effective resources."
      },
      {
        "date": "2023-09-09T22:31:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/tw/blogs/mt/using-aws-migration-hub-network-visualization-to-overcome-application-and-server-dependency-challenges/"
      },
      {
        "date": "2023-06-28T20:11:00.000Z",
        "voteCount": 1,
        "content": "ADE no brainer"
      },
      {
        "date": "2023-06-03T03:16:00.000Z",
        "voteCount": 3,
        "content": "B in incorrect as System Manager doesn't do discovery however, SSM Agent makes it possible for Systems Manager to update, manage, and configure the resources in AWS as well as on-premises. ADE"
      },
      {
        "date": "2023-04-10T11:38:00.000Z",
        "voteCount": 1,
        "content": "ADE is correct answer."
      },
      {
        "date": "2023-04-09T23:51:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-agent.html\nhttps://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html"
      },
      {
        "date": "2023-03-29T20:56:00.000Z",
        "voteCount": 1,
        "content": "B is incorrect because the servers are on prem."
      },
      {
        "date": "2023-12-20T06:14:00.000Z",
        "voteCount": 1,
        "content": "SSM can be installed on on-premise server. This is not the point for not picking B"
      },
      {
        "date": "2023-03-15T15:33:00.000Z",
        "voteCount": 1,
        "content": "ADE no doubts \u2705"
      },
      {
        "date": "2023-02-24T16:13:00.000Z",
        "voteCount": 2,
        "content": "Logical answer : Falls under the domain \"Accelerate Workload Migration and Modernization\"\npromoting MigrationHub\nStep 1 - Identify the apps\nStep 2 - Group them\nStep 3 - Before hand, find out what instance types would need to be in when \nactual migration happens\nhttps://d1.awsstatic.com/Product-Page-Diagram_AWS-Migration-Hub-Orchestrator%402x.0c34c9483d13ebd26cf9072193384a58531624f3.png\nFor OnPremises migrations, first phase is Discovery which can be done with\nDiscovery agent , A\nhttps://d1.awsstatic.com/products/application-discovery-service/Product-Page-Diagram_AWS-Application-Discovery-Service%201.9d81c27f3de50349a9406b8def61b8eb914e2930.png\n\nI wont go with Trusted Advisor although it advises how cost can be advised because-\nThis applies for already aws available environment. Here, about to get migrated into\nAWS and Architects need to discover lot of info before hand to plan alot. So I choose E between E and F. My answer - A,D,E"
      },
      {
        "date": "2023-02-14T10:10:00.000Z",
        "voteCount": 1,
        "content": "Why  Option C Group servers into applications for migration by using AWS Systems Manager Application Manager is incorrect?"
      },
      {
        "date": "2023-02-28T05:23:00.000Z",
        "voteCount": 1,
        "content": "AWS SSM Application Manager is used for existing resources deployed to AWS"
      },
      {
        "date": "2023-02-10T13:22:00.000Z",
        "voteCount": 1,
        "content": "A is better than B.\n\n&gt; Agent-based discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers. The agent installer is available for Windows and Linux operating systems. It collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running.\n\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html"
      },
      {
        "date": "2023-01-20T07:46:00.000Z",
        "voteCount": 1,
        "content": "BDE. Trusted Advisor is not for onprem assessments. Migration hub does EC2 ones"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/95088-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone contains one public subnet and one private subnet.<br><br>The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service. The service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve approximately 1 \u0422\u0412 of data from an S3 bucket each day.<br><br>The company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the service\u2019s security posture or increasing the time spent on ongoing operations.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the EC2 instances to the public subnets. Remove the NAT gateways.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:06:00.000Z",
        "voteCount": 15,
        "content": "C. Setting up an S3 gateway VPC endpoint in the VPC and attaching an endpoint policy to the endpoint will allow the EC2 instances to securely access the S3 bucket for image storage without the need for NAT gateways, reducing costs without compromising security or increasing ongoing operations. This option reduces the costs associated with the NAT gateways and allows for faster data retrieval from the S3 bucket as traffic does not have to go through the internet gateway."
      },
      {
        "date": "2023-02-24T16:54:00.000Z",
        "voteCount": 7,
        "content": "The only reason for C is - Gateway endpoints are not Billed and so cost effective (https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3) If the question changes from single region to across region, the answer would be B (overhead of NAT gateways and traversing TBs of data across NAT is expensive) because gateway endpoints are region specific"
      },
      {
        "date": "2023-02-27T15:03:00.000Z",
        "voteCount": 1,
        "content": "B wouldn\u2019t be highly secure and data transfer would also be slower"
      },
      {
        "date": "2024-02-08T18:43:00.000Z",
        "voteCount": 1,
        "content": "Option C is the most cost-effective solution that maintains the service\u2019s security posture. An S3 gateway VPC endpoint allows private connections between the VPC and S3 without requiring traffic to go through the internet or NAT gateways. This eliminates the need for NAT gateways when accessing S3, which can significantly reduce costs, especially considering the 1 TB of data retrieved daily from S3. Endpoint policies ensure that the security posture is not compromised by allowing only the required actions on the specific S3 bucket."
      },
      {
        "date": "2024-01-11T13:22:00.000Z",
        "voteCount": 1,
        "content": "Any chance someone could fix the typo in the correct answer; \"VPC. Attach...\" instead of VPAttach; terribly misleading."
      },
      {
        "date": "2023-10-01T21:56:00.000Z",
        "voteCount": 2,
        "content": "C for using an endpoint."
      },
      {
        "date": "2023-06-28T20:13:00.000Z",
        "voteCount": 1,
        "content": "C of course"
      },
      {
        "date": "2023-05-09T04:12:00.000Z",
        "voteCount": 2,
        "content": "C is the Correct option as S3 Gateway will reduce the cost for NAT gateway"
      },
      {
        "date": "2023-03-28T02:06:00.000Z",
        "voteCount": 3,
        "content": "Set up an S3 gateway VPC endpoint"
      },
      {
        "date": "2023-03-15T15:36:00.000Z",
        "voteCount": 3,
        "content": "C - easy one \u2705"
      },
      {
        "date": "2023-01-30T09:02:00.000Z",
        "voteCount": 4,
        "content": "C for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/95089-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and configured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more writes to the table than reads of the table.<br><br>A solutions architect needs to implement a solution to minimize the cost of the table.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure on-demand capacity mode for the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T06:51:00.000Z",
        "voteCount": 25,
        "content": "A is correct. On demand mode is for unknown load pattern, auto scaling is for know burst pattern"
      },
      {
        "date": "2024-02-03T05:46:00.000Z",
        "voteCount": 1,
        "content": "But the pattern here is known.. 4 hours peak time etc.. not sure if that would be the write answer"
      },
      {
        "date": "2023-08-22T17:05:00.000Z",
        "voteCount": 1,
        "content": "How AWS Application Auto Scaling scale the read/write performance of DynamoDB?"
      },
      {
        "date": "2023-09-08T06:38:00.000Z",
        "voteCount": 1,
        "content": "You can scale DynamoDB tables and global secondary indexes using target tracking scaling policies and scheduled scaling.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-dynamodb.html"
      },
      {
        "date": "2023-01-23T02:11:00.000Z",
        "voteCount": 16,
        "content": "A\non-demand prices can be 7 times higher, given the options it is better to have reserved WCU and RCU and auto scale in the given schedule"
      },
      {
        "date": "2024-08-31T02:44:00.000Z",
        "voteCount": 1,
        "content": "A. Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load."
      },
      {
        "date": "2024-08-23T21:56:00.000Z",
        "voteCount": 1,
        "content": "I think B is correct. because reserved is not required, ondemand would be better because it requireds only 4 hours per week. so B would be better. Autoscaling of the application can not impact dynamo db tables."
      },
      {
        "date": "2024-07-20T17:53:00.000Z",
        "voteCount": 1,
        "content": "T\u00f4i ch\u1ecdn A"
      },
      {
        "date": "2024-07-17T07:00:00.000Z",
        "voteCount": 2,
        "content": "Auto-scaling is for known traffic pattern, On-demand is for unknown traffic patter and also could be more expensive"
      },
      {
        "date": "2024-05-21T06:46:00.000Z",
        "voteCount": 1,
        "content": "AWS documentation suggests A is correct:\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html"
      },
      {
        "date": "2024-04-28T05:19:00.000Z",
        "voteCount": 1,
        "content": "A is correct. The focus is minimizing the cost of tables."
      },
      {
        "date": "2024-03-22T15:30:00.000Z",
        "voteCount": 2,
        "content": "Considering the application's need to handle a peak load that is double the average and the fact that the workload is write-heavy, option B (Configure on-demand capacity mode for the table) is the most suitable solution. It directly addresses the variability in workload without requiring upfront capacity planning or additional management overhead, thus likely providing the best cost optimization for this scenario. On-demand capacity mode eliminates the need to scale resources manually or through Auto Scaling and ensures that you only pay for the write and read throughput you consume."
      },
      {
        "date": "2024-03-22T15:30:00.000Z",
        "voteCount": 1,
        "content": "A. AWS Application Auto Scaling with Reserved Capacity\nPros: Auto Scaling allows you to automatically adjust the provisioned throughput to meet demand, and purchasing reserved RCUs and WCUs can reduce costs for the capacity you know you'll consistently use.\nCons: This option might not be as cost-effective for workloads with significant variability and a high write-to-read ratio, especially if the peak load is much higher than the average load. Reserved capacity benefits consistent usage patterns, but the peak load being double the average may not be fully optimized here."
      },
      {
        "date": "2024-03-22T15:31:00.000Z",
        "voteCount": 1,
        "content": "B. On-demand Capacity Mode\nPros: On-demand capacity mode is ideal for unpredictable workloads because it automatically scales to accommodate the load without provisioning. You pay for what you use without managing capacity planning. This mode is particularly suitable for the described scenario where the load spikes significantly and unpredictably.\nCons: While potentially more expensive per unit than provisioned capacity with auto-scaling, it eliminates the risk of over-provisioning or under-provisioning."
      },
      {
        "date": "2024-03-16T18:49:00.000Z",
        "voteCount": 2,
        "content": "A is badly worded however, because it says \"application\" autoscaling. We are not talking about that here. Either it should be reworded as \"DynamoDB autoscaling\" for the answer to be correct.\nOn-demand capacity mode is for unknown read/write patterns. Since the load change patterns are known, anything that involves on-demand capacity modes can be eliminated (hence not B).\nDAX is a caching service deployed in front of DynamoDB. It is geared towards \"performance at scale\". Problem in the use case, is to optimize table costs. Using DAX will incur additional costs. Hence anything that involves DAX (C and D) can also be eliminated."
      },
      {
        "date": "2024-05-21T06:44:00.000Z",
        "voteCount": 1,
        "content": "I initially thought the same but the AWS definition of Application autoscaling listed here includes DynamoDB: https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html"
      },
      {
        "date": "2024-03-06T16:09:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/#:~:text=You%20can%20approximate%20a%20blend,save%20money%20as%20reserved%20capacity"
      },
      {
        "date": "2024-02-08T18:57:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most cost-effective solution for workloads with significant fluctuations and unpredictable access patterns. The on-demand capacity mode automatically adjusts the table\u2019s throughput capacity as needed in response to actual traffic, eliminating the need to manually configure or manage capacity. This mode is ideal for applications with irregular traffic patterns, such as a significant peak once a week, because you only pay for the read and write requests your application performs, without having to provision throughput in advance. Option B directly addresses the requirement to minimize costs associated with fluctuating loads, especially when the load significantly exceeds the average only during a brief period, by leveraging DynamoDB\u2019s on-demand capacity mode to automatically scale and pay only for what is used."
      },
      {
        "date": "2024-02-02T03:22:00.000Z",
        "voteCount": 1,
        "content": "I think there is mistake in answer A, and it should be DynamoDb auto scaling instead of application autos calling. Or application and dynamoDB auto scaling."
      },
      {
        "date": "2024-02-02T03:57:00.000Z",
        "voteCount": 2,
        "content": "Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity."
      },
      {
        "date": "2023-12-30T10:37:00.000Z",
        "voteCount": 1,
        "content": "I choose option D, because DAX is not only an accelerator for the Reads, it also cache releasing a lot of load from the DB."
      },
      {
        "date": "2023-12-20T06:36:00.000Z",
        "voteCount": 2,
        "content": "A -&gt; You can scale DynamoDB tables and global secondary indexes using target tracking scaling policies and scheduled scaling. In this I would go for scheduled scaling.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-dynamodb.html\nB -&gt; on-demand capacity mode is for unknown workload, this is not the case\nC -&gt; DAX come with costs and it helps with reads, while here we have a more write-bound workload\nD -&gt; See B and C comments"
      },
      {
        "date": "2023-11-12T23:02:00.000Z",
        "voteCount": 1,
        "content": "we use scheduled scaling here"
      },
      {
        "date": "2023-10-18T11:18:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/#:~:text=You%20can%20approximate%20a%20blend,save%20money%20as%20reserved%20capacity."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/95090-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours.<br><br>What is the MOST cost-effective migration recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 52,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T11:14:00.000Z",
        "voteCount": 23,
        "content": "The correct answer would be option D.\n\nThis option suggests creating a queue using Amazon SQS, configuring the existing web server to publish to the new queue, and using EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. The EC2 instances can be scaled based on the SQS queue length, which ensures that the resources are available during peak usage times and reduces costs during non-peak times.\n\nOption A is not correct because it suggests using AWS Lambda which has a maximum execution time of 15 minutes.\nOption B is not correct because it suggests creating a new EC2 instance for each message in the queue, which is not cost-effective.\nOption C is not correct because it suggests using Amazon EFS, which is not a suitable option for long-term storage of large files."
      },
      {
        "date": "2023-12-24T08:06:00.000Z",
        "voteCount": 7,
        "content": "Not A - Lambda max execution time is 15 minutes, image processing can take up to 1 hour\nNot B - Amazon MQ is not needed (more expensive then SQS) and EFS is more expensive then S3\nNot C - Amazon MQ is not needed (more expensive then SQS) and Lambda max execution time is 15 minutes, image processing can take up to 1 hour\n\nD does the job with the lower cost thanks to SQS, S3 and EC2 Auto Scaling Group"
      },
      {
        "date": "2024-05-21T06:50:00.000Z",
        "voteCount": 1,
        "content": "Lambda will not work, so A is not possible.\nD is going to be the most cost-effective as the resources will scale based on queue length."
      },
      {
        "date": "2024-03-22T15:34:00.000Z",
        "voteCount": 1,
        "content": "Given the need to process files that can take up to 1 hour each and the variability in workload, option D (Amazon SQS, EC2 Auto Scaling, and S3) appears to be the most cost-effective and practical solution. It leverages SQS for queue management, enabling efficient handling of the processing queue's variability. EC2 Auto Scaling allows for flexible and cost-effective scaling of processing capacity, ramping up during high-demand periods and scaling down when demand wanes, thus optimizing costs. Finally, Amazon S3 offers a highly durable and cost-effective solution for storing the processed media files. This option provides the necessary flexibility for long processing tasks while efficiently managing the variable demand and optimizing storage costs."
      },
      {
        "date": "2023-09-10T19:12:00.000Z",
        "voteCount": 1,
        "content": "Simple Queuing Service\nSQS is based on pull model. Here are some of the important features:\n\nReliable, scalable, fully-managed message queuing service\nHigh availability\nUnlimited scaling\nAuto scale to process billions of messages per day\nLow cost (Pay for use)"
      },
      {
        "date": "2023-08-31T01:06:00.000Z",
        "voteCount": 2,
        "content": "This is quite simple. Any answer (A and C) consisting of using Lambda for processing the files is out because of the 15 minutes limit on Lambda processes.\n\nB is out because using EFS is expensive and it does not specify how to launch and terminate the EC2 instances. Amazon MQ is not required either.\n\nThis leaves D which uses SQS, Auto Scaling Groups and publishes the resulting files to S3."
      },
      {
        "date": "2023-08-05T01:55:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\n\nYou can eliminate A and C right in the beginning: Lambda functions can run up to 15 minutes.\nB won't help much as you need to create new EC2 instances (manually, apparently) and EFS is more expensive than S3."
      },
      {
        "date": "2023-06-30T10:58:00.000Z",
        "voteCount": 1,
        "content": "d for sure"
      },
      {
        "date": "2023-06-16T08:31:00.000Z",
        "voteCount": 1,
        "content": "Because of \"Each media file can take up to 1 hour to process\" and we know Lambda has a limit in 15 minutes, The correct answer is D"
      },
      {
        "date": "2023-05-29T03:23:00.000Z",
        "voteCount": 1,
        "content": "D - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html"
      },
      {
        "date": "2023-04-12T07:38:00.000Z",
        "voteCount": 1,
        "content": "I sure is B , becauce\n1. SQS , SNS are \" cloud - native \" services : proprietary protocols from AWS \n2. Traditional applications running from on - premises may use open protocols such as : MQTT , AMQP ,.., so When migrating to the cloud , instead of re-engineering the application to use SQS and SNS will very expensive, we can use Amazon MQ.\n3.  Amazon MQ doesn't \" scale \" as much as SQS / SNS Amazon MQ runs on servers but Amazon MQ has both queue feature ( ~ SQS ) and topic features ( ~ SNS )\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-difference-from-amazon-mq-sns.html"
      },
      {
        "date": "2023-07-06T08:05:00.000Z",
        "voteCount": 1,
        "content": "In terms of cost (which is a point on the question), Amazon SQS is generally more cost-effective compared to Amazon MQ for this specific use case. SQS pricing is based on the number of requests and message data transfer, whereas Amazon MQ pricing includes additional costs associated with broker instances and data transfer."
      },
      {
        "date": "2023-04-01T18:36:00.000Z",
        "voteCount": 2,
        "content": "SQS and autoscaling no doubt answer is D"
      },
      {
        "date": "2023-03-28T02:12:00.000Z",
        "voteCount": 2,
        "content": "SQS and Auto Scaling"
      },
      {
        "date": "2023-03-15T15:57:00.000Z",
        "voteCount": 4,
        "content": "D - makes sense.. Lambda can\u2019t run more than 15m.\nAnd Amazon MQ is only recommended when migrating existing message brokers that rely on compatibility with APIs such as JMS or protocols such as AMQP, MQTT, OpenWire, and STOMP.. in the question there is no mention for these services .."
      },
      {
        "date": "2023-02-24T21:11:00.000Z",
        "voteCount": 2,
        "content": "A and C are out because lambda does not support more than 15 min. B says, to create an EC2 for each new message which is certainly not cost effective and bad design as well. So answer is D"
      },
      {
        "date": "2023-02-15T20:52:00.000Z",
        "voteCount": 2,
        "content": "The most cost-effective migration recommendation to handle peak loads during business hours is to use Amazon SQS to create a queue, configure the existing web server to publish to the new queue, and use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. The EC2 instances should be scaled based on the SQS queue length. Storing the processed files in an Amazon S3 bucket will help in reducing the storage cost. This approach is scalable and can handle peak loads during business hours, while still being cost-effective during non-business hours. Option A is also a possible solution, but using EC2 instances in an EC2 Auto Scaling group is a more scalable and cost-effective solution. Options B and C involve using Amazon EFS, which can be more expensive than Amazon S3."
      },
      {
        "date": "2023-01-30T09:12:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/95091-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.<br><br>The company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:17:00.000Z",
        "voteCount": 20,
        "content": "B is the most cost-effective solution as it reduces the number of data nodes in the cluster to 2 and adds UltraWarm nodes to handle the expected capacity. By configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data, the company can take advantage of the lower storage costs of UltraWarm. Additionally, by transitioning the input data to S3 Glacier Deep Archive after 1 month using an S3 Lifecycle policy, the company can further reduce costs by using the lower storage costs of S3 Glacier Deep Archive for long-term data retention."
      },
      {
        "date": "2023-01-13T14:17:00.000Z",
        "voteCount": 3,
        "content": "Option C can meet the requirements of reducing the number of data nodes in the cluster and using UltraWarm and cold storage nodes to handle the expected capacity and moving the data to lower cost storage after 1 month. However, it may not be the most cost-effective solution as it involves additional complexity in configuring the indexes to transition between different storage tiers, and may also require additional management and maintenance of the cold storage nodes. Option B, where the data is transitioned from S3 Standard to S3 Glacier Deep Archive using an S3 Lifecycle policy is simpler and more cost-effective as it eliminates the need for additional storage tiers and management."
      },
      {
        "date": "2023-02-24T22:10:00.000Z",
        "voteCount": 5,
        "content": "B says to delete but question asks for saving on compliance purposes."
      },
      {
        "date": "2023-02-24T22:10:00.000Z",
        "voteCount": 5,
        "content": "* I meant C says.."
      },
      {
        "date": "2024-08-31T02:46:00.000Z",
        "voteCount": 1,
        "content": "B. Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy."
      },
      {
        "date": "2024-05-21T07:08:00.000Z",
        "voteCount": 1,
        "content": "Why can't I switch all nodes to ultrawarm. I can't find it anywhere in the documentation and it's not listed in the pre-requisites. \n\nAlso why can the number of nodes be reduced from 10 to 2? is that because Ultrawarm use S3?"
      },
      {
        "date": "2024-04-22T19:53:00.000Z",
        "voteCount": 1,
        "content": "why not D?"
      },
      {
        "date": "2023-12-24T09:18:00.000Z",
        "voteCount": 1,
        "content": "I need help here:\n\nTo use UltraWarm storage, domains must have dedicated master nodes as per doc https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ultrawarm.html\n\nThe scenario mentions \"an OpenSearch Service cluster with 10 data nodes\". Assuming you only have these nodes in the cluster, in all answers you need to add dedicated master node(s). Assuming we also have dedicated master node why not replacing all data nodes with UltraWarm nodes?"
      },
      {
        "date": "2023-12-24T09:22:00.000Z",
        "voteCount": 1,
        "content": "I think I got it, UltraWarm is for read-only data. Thus you still need to have at least a data node"
      },
      {
        "date": "2023-08-20T06:55:00.000Z",
        "voteCount": 3,
        "content": "Option A says to replace all Data Nodes with ultra warm nodes. But this is NOT possible. There has to be atleast one data node"
      },
      {
        "date": "2023-06-30T11:08:00.000Z",
        "voteCount": 2,
        "content": "B I think :/"
      },
      {
        "date": "2023-03-20T15:38:00.000Z",
        "voteCount": 2,
        "content": "If you look at the IAM documentation here, you can see that the ec2:AuthorizeSecurityGroupIngress action doesn't have any conditions that would allow you to specify the ip addresses in the inbound/outbound rules.https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html"
      },
      {
        "date": "2023-06-14T10:11:00.000Z",
        "voteCount": 1,
        "content": "I think you are referring All AWS Certified Solutions Architect - Professional SAP-C02 Questions, question 44. yes, I changed from D to A after reading this link."
      },
      {
        "date": "2023-07-04T21:57:00.000Z",
        "voteCount": 1,
        "content": "You can specify the IP address with the CIDR parameter\n\nhttps://ec2.amazonaws.com/?Action=AuthorizeSecurityGroupIngress\n&amp;GroupId=sg-112233\n&amp;IpPermissions.1.IpProtocol=tcp\n&amp;IpPermissions.1.FromPort=3389\n&amp;IpPermissions.1.ToPort=3389\n&amp;IpPermissions.1.IpRanges.1.CidrIp=192.0.2.0/24\n&amp;IpPermissions.1.IpRanges.1.Description=Access from New York office\n\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AuthorizeSecurityGroupIngress.html"
      },
      {
        "date": "2023-03-15T16:14:00.000Z",
        "voteCount": 4,
        "content": "B - makes more sense"
      },
      {
        "date": "2023-03-03T22:21:00.000Z",
        "voteCount": 3,
        "content": "UltraWarm provides a cost-effective way to store large amounts of read-only data on Amazon OpenSearch Service. Standard data nodes use \"hot\" storage, which takes the form of instance stores or Amazon EBS volumes attached to each node. Hot storage provides the fastest possible performance for indexing and searching new data."
      },
      {
        "date": "2023-02-10T15:09:00.000Z",
        "voteCount": 2,
        "content": "I asked ChatGPT. Can I use all UltraWarm nodes in AWS OpenSearch instead of data nodes? :)\n\nNo, UltraWarm nodes in AWS OpenSearch are designed for storage and retrieval of infrequently accessed data, while data nodes are optimized for faster indexing and searching of data. While UltraWarm nodes can be used as a complement to data nodes, they are not a replacement for them."
      },
      {
        "date": "2023-02-25T17:40:00.000Z",
        "voteCount": 2,
        "content": "This eliminates option A"
      },
      {
        "date": "2023-01-29T08:12:00.000Z",
        "voteCount": 4,
        "content": "Option B is the most cost-effective solution that meets the requirements. Reducing the number of data nodes in the cluster and adding UltraWarm nodes will help to reduce the ongoing costs of running the OpenSearch Service cluster. Configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data will further reduce costs. Additionally, transitioning the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy will lower the storage costs of retaining the input data for compliance purposes."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/95092-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong to either the Prod OU or the NonProd OU.<br><br>The company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company\u2019s security team is subscribed to the SNS topic.<br><br>For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.<br><br>Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 140,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 91,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:20:00.000Z",
        "voteCount": 51,
        "content": "The solution that meets this requirement with the LEAST operational overhead is D. Configuring an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0, and applying the SCP to the NonProd OU. This solution would prevent the security group inbound rule from being created in the first place and will not require any additional steps or actions to be taken in order to remove the rule. This is less operationally intensive than modifying the EventBridge rule to invoke an AWS Lambda function, adding a Config rule or allowing the ec2:AuthorizeSecurityGroupIngress action with a specific IP."
      },
      {
        "date": "2023-01-13T14:21:00.000Z",
        "voteCount": 6,
        "content": "Option C does not meet the requirement that the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source. It only allows the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. It does not prevent the creation of a security group inbound rule that includes 0.0.0.0/0 as the source, it only allows for the ingress action on non-0.0.0.0/0 IPs.\nOption D is the best solution as it denies the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. This will prevent the creation of any security group inbound rule that includes 0.0.0.0/0 as the source."
      },
      {
        "date": "2023-05-21T05:41:00.000Z",
        "voteCount": 3,
        "content": "the answer can't be C or D because aws:SourceIp condition key don't exist with SCP.\nSo answer is A"
      },
      {
        "date": "2024-04-22T09:24:00.000Z",
        "voteCount": 2,
        "content": "You mean something like this? It's from the AWS portal...\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": {\n        \"Effect\": \"Deny\",\n        \"Action\": \"*\",\n        \"Resource\": \"*\",\n        \"Condition\": {\n            \"NotIpAddress\": {\n                \"aws:SourceIp\": [\n                    \"192.0.2.0/24\",\n                    \"203.0.113.0/24\"\n                ]\n            }\n        }\n    }\n}"
      },
      {
        "date": "2023-08-08T17:37:00.000Z",
        "voteCount": 3,
        "content": "have you actually tested it? if you haven't, please do it and then comment."
      },
      {
        "date": "2023-11-19T01:11:00.000Z",
        "voteCount": 1,
        "content": "I think the reason why C is wrong is not because C does not meet the requirement but simply because it is too strong: All users can do is to set ingress rule in SG and all other actions are all blocked. Both C and D results the same which users can no longer able to open port to 0.0.0.0/0, but D is more precise without blocking other actions."
      },
      {
        "date": "2023-06-20T05:35:00.000Z",
        "voteCount": 33,
        "content": "I literally just created the SCP and it works. I saw some comments that \"ec2:AuthorizeSecurityGroupIngress action doesn't have any conditions\" - that is not correct. This is my scp : \n    {\n      \"Sid\": \"Statement1\",\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"ec2:AuthorizeSecurityGroupIngress\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ],\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": [\n            \"0.0.0.0/0\"\n          ]\n        }\n      }\n    }"
      },
      {
        "date": "2023-08-08T17:36:00.000Z",
        "voteCount": 5,
        "content": "Tested and confirmed!"
      },
      {
        "date": "2023-08-22T18:31:00.000Z",
        "voteCount": 1,
        "content": "I guess proving D works doesn't show C is incorrect. I feel that both C and D could be correct because as CuteRunRun mentioned, the SCP deny is default.\n\nJust have one more question, what is the ec2:AuthorizeSecurityGroupIngress if the SourceIp is not 0.0.0.0/0?"
      },
      {
        "date": "2023-08-31T17:49:00.000Z",
        "voteCount": 1,
        "content": "For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.\nyou think C can \"remove the ability to create\" carry ? SCP allow all by default?"
      },
      {
        "date": "2023-08-31T17:52:00.000Z",
        "voteCount": 1,
        "content": "Sorry typo.\nyou think C can \"remove the ability to create\" crazy ? SCP allow all by default"
      },
      {
        "date": "2023-09-30T19:39:00.000Z",
        "voteCount": 3,
        "content": "This will deny all action create a inbound rule not only Inbound rule which have source ip \"0.0.0.0/0\""
      },
      {
        "date": "2024-05-21T07:20:00.000Z",
        "voteCount": 1,
        "content": "I think that is incorrect. the SCP action is ec2:AuthorizeSecurityGroupIngress and specifically applies to ingress"
      },
      {
        "date": "2024-10-01T16:08:00.000Z",
        "voteCount": 2,
        "content": "Given that the aws:SourceIp condition key refers to the IP address of the principal making the request, and not the IP address specified in the security group rule, D is not appropriate for this scenario."
      },
      {
        "date": "2024-08-31T02:47:00.000Z",
        "voteCount": 1,
        "content": "D. Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU."
      },
      {
        "date": "2024-08-30T20:27:00.000Z",
        "voteCount": 1,
        "content": "Service Control Policy (SCP):\n\nRestrictive Policy Enforcement: An SCP (Service Control Policy) is used in AWS Organizations to enforce account-level restrictions across accounts that belong to a particular Organizational Unit (OU). By setting an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the aws:SourceIp condition is 0.0.0.0/0, you effectively prevent all users within the NonProd OU from creating any security group rule that opens inbound traffic to the entire internet.\nLeast Operational Overhead: SCPs are centrally managed and enforced automatically, requiring no further intervention once applied. This reduces the operational overhead to nearly zero, as it does not require ongoing monitoring, function deployments, or manual rule updates."
      },
      {
        "date": "2024-08-11T10:44:00.000Z",
        "voteCount": 1,
        "content": "Why Option D is Better than Option C:\nExplicit Deny vs. Implicit Allow:\n\nOption C allows the action unless the aws:SourceIp is 0.0.0.0/0. This creates an implicit allow policy, which means that if any condition is not met, the action is allowed.\nOption D uses an explicit deny, which is more secure and straightforward. An explicit deny ensures that if the condition is met (aws:SourceIp is 0.0.0.0/0), the action is blocked regardless of other permissions."
      },
      {
        "date": "2024-07-08T07:54:00.000Z",
        "voteCount": 1,
        "content": "It's A. Definitely A. Don't get confused."
      },
      {
        "date": "2024-06-28T03:24:00.000Z",
        "voteCount": 1,
        "content": "Voting for A"
      },
      {
        "date": "2024-05-30T00:27:00.000Z",
        "voteCount": 1,
        "content": "It's A, D is incorrect as it shouldn\u00b4t be source IP but destination address"
      },
      {
        "date": "2024-05-21T07:27:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-05-10T23:33:00.000Z",
        "voteCount": 3,
        "content": "SourceIP is for requester IP address, not the CIDR referenced in the SG rule."
      },
      {
        "date": "2024-04-26T17:22:00.000Z",
        "voteCount": 2,
        "content": "A (Incorrect): SG is created for a briefly. This goes against the question requirement of \"remove the ability to create a security group inbound rule...\"\nB (Incorrect): Regardless of rule, SGs can be created and remain non-complaint.\nC (Incorrect): See D \nD (Incorrect): SourceIP condition key of IAM policy is the requestor's IP address. This has nothing to do with SG's inbound rule's sourceIP. This won't allow creating any SG inbound rules when the requestor is making AWS API calls from anywhere (0.0.0.0/0). \n\nJust a crap question and choices."
      },
      {
        "date": "2024-03-22T15:42:00.000Z",
        "voteCount": 1,
        "content": "The goal is to prevent the creation of Amazon EC2 security group inbound rules that include 0.0.0.0/0 as the source for all accounts in the NonProd Organizational Unit (OU) with the least operational overhead. \nOption D is the most straightforward and effective solution to meet the requirement with the least operational overhead. By configuring a Service Control Policy (SCP) to deny the ec2:AuthorizeSecurityGroupIngress action when the aws:SourceIp condition key is 0.0.0.0/0 and applying this policy to the NonProd OU, the company can ensure that no account within this OU can create security group inbound rules that expose resources to the entire internet. This approach leverages AWS Organizations' capability to apply governance and compliance policies at scale, thereby reducing the need for individual resource monitoring or post-creation remediation."
      },
      {
        "date": "2024-03-16T16:20:00.000Z",
        "voteCount": 1,
        "content": "D is going to avoid to create the rule. A is not going to prevent, is going to remediate it..."
      },
      {
        "date": "2024-03-03T03:36:00.000Z",
        "voteCount": 2,
        "content": "A is out because creation of the SG is allowed albeit briefly before being updated\nB is noise\nC is out because SCPs don't allow\nD is the correct answer"
      },
      {
        "date": "2024-02-16T23:11:00.000Z",
        "voteCount": 3,
        "content": "To everyone who claimed tested D, \nplz try create inbound rules other than 0.0.0.0/0.\nD will deny all AuthorizeSecurityGroupIngress operation from your IP. that's why D is \"worked\""
      },
      {
        "date": "2024-02-09T12:20:00.000Z",
        "voteCount": 2,
        "content": "Option D is the most direct and efficient solution. By creating an SCP that explicitly denies the ec2:AuthorizeSecurityGroupIngress action when the source IP is 0.0.0.0/0, it prevents users in all accounts under the NonProd OU from creating such open security group rules. This enforcement happens at the API level, blocking the action before the rule is created, which aligns with the goal of reducing operational overhead and proactively enforcing security best practices.\nIt is not option C because, Option C mentions configuring a Service Control Policy (SCP) to allow the ec2:AuthorizeSecurityGroupIngress action except when the source IP is 0.0.0.0/0. While the intention is correct, SCPs do not support allow-listing in this manner; they are designed to explicitly allow or deny actions across accounts in an AWS Organization."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/95093-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 49,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:24:00.000Z",
        "voteCount": 24,
        "content": "B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint. This solution will provide low operational overhead as it utilizes the serverless capabilities of AWS Lambda and API Gateway, which automatically scales and manages the underlying infrastructure and resources. It also allows for the webhook logic to be easily managed and updated through the API Gateway interface.\n\nThe answer should be B because it is the best solution in terms of operational overhead."
      },
      {
        "date": "2023-01-13T14:24:00.000Z",
        "voteCount": 8,
        "content": "Option A would require updating the Git servers to call individual Lambda function URLs for each webhook, which would be more complex and time-consuming than calling a single API Gateway endpoint. \n\nOption C would require deploying the webhook logic to AWS App Runner, which would also be more complex and time-consuming than using an API Gateway. \n\nOption D would also require containerizing the webhook logic and creating an ECS cluster and Fargate, which would also add complexity and operational overhead compared to using an API Gateway."
      },
      {
        "date": "2023-02-25T18:43:00.000Z",
        "voteCount": 2,
        "content": "I do agree with B. \n\nHowever on Git server side it does make no difference if one calls aws or do a rest call via gateway. \nEg. if you use Python it makes no difference if you use boto(call Lambda) or request(rest api) module.\nIf one implemets via shell it makes no difference if one uses aws-cli(invoke Lambda directly) or curl(do a rest call).\nSimilar for other implementations."
      },
      {
        "date": "2023-02-25T18:50:00.000Z",
        "voteCount": 2,
        "content": "As addition why B is still better: it hides the implementation details and decouples by introducing a interface.\nWith that a team for Aws may change what ever it needs to change to implement the interface. On the other hand on git side can use whatever deems necessary without caring about implementation details."
      },
      {
        "date": "2023-12-26T01:32:00.000Z",
        "voteCount": 6,
        "content": "I need help here: what's wrong with Lambda Function URL?\n\nWith A I just need to handle my Lambda functions, updates go trough updating my aliases pointing to a new version. Here I am just missing all the capabilities provided by API Gateway that seems not to be requested (transformations, throttling, quotas, cache, api keys, auth, OpenAPI, ...). With B I still need to implement each webhook logic in a separate AWS Lambda function and update git server + I need to operates API Gateway.\n\nAny other option requires 2 or more services thus generating more operations, also:\nNot C as app runner is not a target for ALB (private IP, ECS, EC2 instance, Lambda)\nNot D as you cannot set Fargate as API Gateway target (while you can use ECS as target)\n\nCan you help me understand why B requires less operations overhead?"
      },
      {
        "date": "2024-05-21T07:53:00.000Z",
        "voteCount": 2,
        "content": "Option A requires that you update the webhooks for each lambda function. This will create a considerable operational overhead not just for the initial change but going forward as well.\n\nAPI Gateway (B) decouple the functions from the Webhooks."
      },
      {
        "date": "2024-09-30T02:21:00.000Z",
        "voteCount": 1,
        "content": "Deploy the web hook logic to the Apprunner which takes a minor effort to build and deploy the container image automatically without underlying infrastructure management."
      },
      {
        "date": "2024-08-31T02:53:00.000Z",
        "voteCount": 1,
        "content": "B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint."
      },
      {
        "date": "2024-08-23T22:28:00.000Z",
        "voteCount": 1,
        "content": "C is the best one . Operational over head - exisiting web logic needs to be change into the lambda. But in C - just we can use the same logic just deployment activities. Please go with C"
      },
      {
        "date": "2024-05-21T07:56:00.000Z",
        "voteCount": 2,
        "content": "A: large operational overhead\nB: Choice\nC: App runner doesn't use ALB\nD: Unnecessary complexity with containers"
      },
      {
        "date": "2024-05-09T23:13:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/ko/solutions/implementations/git-to-s3-using-webhooks/"
      },
      {
        "date": "2024-05-09T23:14:00.000Z",
        "voteCount": 1,
        "content": "choose B"
      },
      {
        "date": "2024-03-16T22:34:00.000Z",
        "voteCount": 1,
        "content": "Given the current answers, I think B is the only possible option with least overhead.\n\nC would have been a better candidate over B, if it mentioned to include the App Runner in a Target Group TG and assign TG as the target for the API Gateway. As it stands now, C is not correct because App Runner app can't be directly assigned as a target for API Gateway."
      },
      {
        "date": "2024-03-16T16:30:00.000Z",
        "voteCount": 4,
        "content": "A, because is the solution with less operational overhead. Also option B also will create new lambda functions per webhook, and you have to define the specific path in the apigateway and integrate it with your specific lambda..."
      },
      {
        "date": "2024-03-06T15:43:00.000Z",
        "voteCount": 1,
        "content": "Lambda function is the easiest way to implement the webhook logic. App Runner and ECS all requires more ops overhead. \nSo the answer is between A and B. Someone argue that using A introduces ops overhead of mapping every Lambda function to the webhooks, but actually with B, users don\u2019t need to map Lambda function in git webhooks, but move the Lambda function mapping ops to API gateway. The mapping need to be done, that\u2019s an ops overhead that cannot be ignored. \nI\u2019m guessing the question designer prefers to use API GW, because the description \u201cUpdate the Git servers to call the individual Lambda function URLs.\u201d doesn\u2019t look good. While, in reality, the repo developers create the Lambda function, and they know the URL, it\u2019s very easy to launch the Lambda function from the web hook. No additional API GW is required."
      },
      {
        "date": "2024-01-22T18:58:00.000Z",
        "voteCount": 1,
        "content": "You can set App Runner as a target for ALB.\n\nAWS App Runner can use your code. You can use AWS App Runner to create and manage services based on two fundamentally different service sources: source code and source image. App Runner starts, runs, scales, and balances your service regardless of the source type. You can use the CI/CD capability of App Runner to track changes to your source image or code. When App Runner discovers a change, it automatically builds (for source code) and deploys the new version to your App Runner service"
      },
      {
        "date": "2024-02-26T06:19:00.000Z",
        "voteCount": 1,
        "content": "Looks like App Runner is built more for deploying web applications rather than hosting webhook logics."
      },
      {
        "date": "2023-12-28T08:54:00.000Z",
        "voteCount": 2,
        "content": "A. is the right answer as no need to introduce gateway here"
      },
      {
        "date": "2023-12-05T09:34:00.000Z",
        "voteCount": 1,
        "content": "Least operations is the key. App runner is a aws managed one and can deploy it easily,  A and B we need to create lamda for each web hook it is very complex . So C would be correct"
      },
      {
        "date": "2023-12-31T04:47:00.000Z",
        "voteCount": 2,
        "content": "ninomfr64 says that App runner cannot be a target for ALB, so that's the reason you cannot select C."
      },
      {
        "date": "2023-11-12T23:40:00.000Z",
        "voteCount": 1,
        "content": "Don't see the exact reasons to not choose A for now, but B will work for sure."
      },
      {
        "date": "2023-11-12T23:41:00.000Z",
        "voteCount": 1,
        "content": "UPD: Don't see the exact reasons why A won't work for now, but B will work for sure."
      },
      {
        "date": "2023-10-18T11:44:00.000Z",
        "voteCount": 1,
        "content": "reducing operational overhead!"
      },
      {
        "date": "2023-10-13T03:49:00.000Z",
        "voteCount": 1,
        "content": "B vs C. Looking at App Runner C makes more sense."
      },
      {
        "date": "2023-09-24T00:03:00.000Z",
        "voteCount": 1,
        "content": "The comments below supported Option B are only focusing on how Lambda + API Gateway can help reduce operational overhead. Thinking of the scenario in the question that we have already had the source code, wouldn't it be easier if we only specify the code repo on App Runner and let it process and finish the task? Implement all logic again would consume a lot more time."
      },
      {
        "date": "2023-11-05T15:02:00.000Z",
        "voteCount": 3,
        "content": "Watch this video from AWS. At 4:05 he says Apprunner is serverless, and also there are no load balancers. Since the answer mentions load balancers it is incorrect.\n\nhttps://www.youtube.com/watch?v=HJsULvSJWes\n\nI found the video from this AWS post\n\nhttps://aws.amazon.com/blogs/containers/introducing-aws-app-runner/"
      },
      {
        "date": "2024-08-21T05:55:00.000Z",
        "voteCount": 1,
        "content": "App Runner tem autoscaling \u00e9 s\u00f3 configurar no ELB"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/95095-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company\u2019s data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 77,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T12:12:00.000Z",
        "voteCount": 45,
        "content": "The correct answer is D: Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.\n\nHere is why the other choices are not correct:\n\nA. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select. - AWS Agentless Discovery Connector will help in discovering and inventory servers but it does not provide the same level of detailed metrics as the AWS Application Discovery Agent, it also does not cover process information."
      },
      {
        "date": "2023-01-17T12:13:00.000Z",
        "voteCount": 6,
        "content": "B. Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight. - It does not cover process information and it's not the best way to collect the required data, it's not efficient and it might miss some important information.\n\nC. Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console. - this solution might not be very reliable and it does not cover process information, also it does not provide a way to query and analyze the data."
      },
      {
        "date": "2023-01-17T12:13:00.000Z",
        "voteCount": 5,
        "content": "D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3. - This is the correct answer as it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics, including process information and it provides a way to query and analyze the data using Amazon Athena."
      },
      {
        "date": "2023-01-18T05:44:00.000Z",
        "voteCount": 6,
        "content": "Choosing between A and D. For A, how can S3 select query?"
      },
      {
        "date": "2023-02-04T18:24:00.000Z",
        "voteCount": 3,
        "content": "I think A is a better solution because the Agentless discovery connector is custom-made for the VMware environment. It will save us time and collect all the necessary data we need. Installing a Discovery agent in every server would be very time-consuming. S3 select allows simple select operations against your raw data. I don't think we need athena for"
      },
      {
        "date": "2024-02-26T07:47:00.000Z",
        "voteCount": 1,
        "content": "As written by jainparag1, S3 Select is definitely the wrong solution here. As you said, it only allows for very simple select operations. Athena is a better way to go once you have configured the Migration hub settings correctly."
      },
      {
        "date": "2023-11-24T01:46:00.000Z",
        "voteCount": 3,
        "content": "A is horrible. You can write only simple SQLs using S3 select. But here you need a sophisticated solution to query these special metrics. D is satisfying all the requirements."
      },
      {
        "date": "2024-09-17T02:29:00.000Z",
        "voteCount": 2,
        "content": "If precise information about each running Process is required, it is necessary to consider using Agent-based Discovery."
      },
      {
        "date": "2024-08-31T02:54:00.000Z",
        "voteCount": 1,
        "content": "D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3."
      },
      {
        "date": "2024-08-04T03:58:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2024-05-15T00:31:00.000Z",
        "voteCount": 1,
        "content": "see https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html\n\nfor VMs hosted on VMware, you can use both the Agentless Collector and Discovery Agent to perform discovery simultaneously. \n\nAgentless Collector captures system performance information and resource utilization for each VM running in the vCenter, regardless of what operating system is in use. However, it cannot \u201clook inside\u201d each of the VMs, and as such, cannot figure out what processes are running on each VM nor what network connections exist. Therefore, if you need this level of detail and want to take a closer look at some of your existing VMs in order to assist in planning your migration, you can install the Discovery Agent on an as-needed basis."
      },
      {
        "date": "2024-03-16T16:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-02-27T21:29:00.000Z",
        "voteCount": 1,
        "content": "Definetely A\n\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/agentless-collector-data-collected-vmware.html\n\nVmware supports agentless connector with AWS, and data can be imported ove Migration Hub"
      },
      {
        "date": "2024-02-09T12:32:00.000Z",
        "voteCount": 1,
        "content": "Option D is the most efficient and streamlined solution for the requirements. Deploying the AWS Application Discovery Agent on each on-premises server allows for detailed collection of server metrics, including CPU usage, RAM usage, operating system details, and running processes. By configuring Data Exploration in AWS Migration Hub, the collected data can be analyzed and queried effectively. Using Amazon Athena for querying enables powerful SQL-based exploration of the data stored in Amazon S3, offering a flexible and scalable way to analyze the migration readiness and planning data.\n\nIt is not option C because,  Option C involves creating a custom script to gather server information and using the AWS CLI to store data in AWS Migration Hub. While this approach could potentially work, it requires significant manual effort to develop, deploy, and maintain the scripts across 1,000 servers, which is not ideal for minimizing operational overhead."
      },
      {
        "date": "2023-12-26T01:58:00.000Z",
        "voteCount": 1,
        "content": "Not A - as AWS Agentless Discovery Connector does not provide processes visibility\nNot B - as Migration Hub Import functionality does not support process datahttps://docs.aws.amazon.com/cli/latest/reference/mgh/put-resource-attributes.html, also I do not see how to query with QuickSight as there is not direct integration with Migration Hub to my knowledge\nNot C - as it seems that put-resource-attributes command does not support process data https://docs.aws.amazon.com/cli/latest/reference/mgh/put-resource-attributes.html \n\nD is correct as Discovery Agent collects the required data including processes, Data Exploration in Migration Hub allows to use Amazon Athena and comes with pre-defined queries as well. https://docs.aws.amazon.com/application-discovery/latest/userguide/explore-data.html"
      },
      {
        "date": "2023-11-25T05:13:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/application-discovery/latest/userguide/explore-data.html"
      },
      {
        "date": "2023-08-13T17:31:00.000Z",
        "voteCount": 1,
        "content": "The agent-based collector can collect data related to running processes which is not available to the Agentless Collector. \n\nCheck out for yourself in the FAQs:\nhttps://aws.amazon.com/application-discovery/faqs/"
      },
      {
        "date": "2023-08-09T13:49:00.000Z",
        "voteCount": 2,
        "content": "As far as i learned for VM based envs we can go with agentless. And we can use a OVA image via collect the metrics and so on. im going with A . https://docs.aws.amazon.com/application-discovery/latest/userguide/agentless-data-collected.html"
      },
      {
        "date": "2023-08-05T06:43:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\n\nThe requirement: \"the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes.\"\n\nFrom https://aws.amazon.com/application-discovery/faqs/:\n\n=== AWS Application Discovery Service Discovery Agent \nQ: What data does the AWS Application Discovery Service Discovery Agent capture?\nThe Discovery Agent captures system configuration, system performance, running processes, and details of the network connections between systems."
      },
      {
        "date": "2023-08-05T06:43:00.000Z",
        "voteCount": 1,
        "content": "=== Agentless Collector\nQ: What data does the Agentless Collector capture?\nThe Agentless Collector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. The type of data collected will depend on the capabilities that you configure. If the credentials are provided to connect to vCenter, the Agentless Collector will collect VM inventory, configuration, and performance history data such as CPU, memory, and disk usage. If credentials are provided to connect to databases such as Oracle, SQL Server, MySQL, or PostgreSQL, the Agentless Collector will collect version, edition, and schema data. Server and database information is uploaded to the Application Discovery Service data store. Database information can be sent to AWS DMS Fleet Advisor for analysis."
      },
      {
        "date": "2023-08-03T18:53:00.000Z",
        "voteCount": 1,
        "content": "I prefer D"
      },
      {
        "date": "2023-08-02T09:08:00.000Z",
        "voteCount": 1,
        "content": "Correct A.\n\n\nD uses agent-based discovery, which requires installing an agent on each on-premises server. This can be cumbersome and intrusive for a large number of servers. It also does not explain how to use AWS Glue to perform an ETL job against the data."
      },
      {
        "date": "2023-07-02T08:01:00.000Z",
        "voteCount": 1,
        "content": "it's a D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/95096-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate the application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses that are in an allow list.<br><br>The company must provide a single public IP address to the external provider before the application can start using the new service.<br><br>Which solution will give the application the ability to access the new service?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 71,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:26:00.000Z",
        "voteCount": 30,
        "content": "A. Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.\n\nThis solution will give the Lambda function access to the internet by routing its outbound traffic through the NAT gateway, which has a public Elastic IP address. This will allow the external provider to whitelist the single public IP address associated with the NAT gateway, and enable the application to access the new service."
      },
      {
        "date": "2023-04-06T23:30:00.000Z",
        "voteCount": 5,
        "content": "Options A are not appropriate solutions because they involve deploying a NAT gateway or an egress-only internet gateway, which are used for different purposes, such as allowing resources in a private subnet to access the internet while using a static public IP address. These options will not provide the Lambda function with a single public IP address to be used for external requests."
      },
      {
        "date": "2023-12-30T06:49:00.000Z",
        "voteCount": 1,
        "content": "The question includes \"The external provider supports only requests that come from public IPv4 addresses that are in an allow list\" this imply the Lambda needs to call the external provider"
      },
      {
        "date": "2023-12-26T22:38:00.000Z",
        "voteCount": 2,
        "content": "Big Thank to you. masetromain."
      },
      {
        "date": "2023-03-22T08:25:00.000Z",
        "voteCount": 8,
        "content": "A\n\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/networking-vpc.html\n\n\"By default, Lambda functions have access to the public internet. This is not the case after they have been configured with access to one of your VPCs. If you continue to need access to resources on the internet, set up a NAT instance or Amazon NAT Gateway. Alternatively, you can also use VPC endpoints to enable private communications between your VPC and supported AWS services.\""
      },
      {
        "date": "2024-10-10T22:32:00.000Z",
        "voteCount": 1,
        "content": "There are many misleading explanations here.\nYou cannot attath ElasticIP to Internet Gateway which use instance public ip for NAT. - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-igw-internet-access.html#ip-addresses-and-nat \nBut NAT can be used with Elastic IP for fixed outbound ip. That's difference. - https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/nat-gateway-scenarios.html#private-nat-allowed-range"
      },
      {
        "date": "2024-08-31T02:54:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway."
      },
      {
        "date": "2024-08-23T23:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct, NAT not only provides the internet outbound , but also provides single public IP address, So Selected Answer: A"
      },
      {
        "date": "2024-08-04T03:55:00.000Z",
        "voteCount": 2,
        "content": "THE ANSWER HAS TO BE A!!!!\n\nFor B:\nWrong. Egress only internet gateway is for IPV6, not for IPV4\nFor C&amp;D:\nInternet gateway is for both inbound and outbount traffic. In our case we only need outbound traffic, so it has to be NAT Gateway."
      },
      {
        "date": "2024-06-16T20:44:00.000Z",
        "voteCount": 2,
        "content": "NAT gateway doesn't allow inbound traffic flow into service behind NAT gateway. ALB or internet gateway can. However internet gateway can't be attached to lambda service directly. I vote D as correct answer."
      },
      {
        "date": "2024-03-17T02:38:00.000Z",
        "voteCount": 2,
        "content": "Option A will be the only solution that matches the given requirements.\n\nThe problem with any solution that involves IGw is that IGw DOES NOT perform NAT. In fact, it does not alter the source IP field at all, meaning that we don't really have a mechanism of having a static public IP address set to the outbound traffic, while ensuring security. So, the only practical solution is to go with the NAT option."
      },
      {
        "date": "2024-03-16T16:36:00.000Z",
        "voteCount": 1,
        "content": "A, deploy nat gateway and associate an elastic ip"
      },
      {
        "date": "2024-03-03T04:03:00.000Z",
        "voteCount": 3,
        "content": "Can an admin please take a look at _all_ the \"correct answers\" in this exam? They really cannot be trusted and reduce the usefulness of ExamTopics altogether. As things are, you should always just disregard the correct answer as it so often is insane.\n\nThe correct answer is of course A."
      },
      {
        "date": "2024-02-22T20:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct option, Other approach to enable internet access\nhttps://www.linkedin.com/pulse/aws-lambda-accessing-private-vpc-resources-internet-without-vokhmin-pyxbe/"
      },
      {
        "date": "2024-02-09T12:41:00.000Z",
        "voteCount": 2,
        "content": "The solution that enables the Lambda function in a VPC to access an external service that requires requests to come from a specific public IPv4 address, and to provide a single public IP address for allow listing, is:\n    *    Option A is correct because a NAT (Network Address Translation) gateway allows instances or AWS Lambda functions in a private subnet of a VPC to initiate outbound traffic to the internet (or external services) while preventing unsolicited inbound traffic from the internet. By associating an Elastic IP address with the NAT gateway, all outbound traffic from the Lambda function routed through the NAT gateway will appear to come from this single public IP address, which can be provided to the external provider for allow listing."
      },
      {
        "date": "2024-02-09T12:41:00.000Z",
        "voteCount": 3,
        "content": "It is not option C because, Option C describes deploying an internet gateway and associating an Elastic IP address with it. However, Lambda functions cannot be directly associated with Elastic IP addresses, and internet gateways are used to route traffic between a VPC and the internet, not to provide a static public IP address for outbound traffic."
      },
      {
        "date": "2023-12-30T06:55:00.000Z",
        "voteCount": 1,
        "content": "Not B. egress-only internet gateway is IPv6 only, the question is about IPv6\nNot C. you cannot associated Elastic IP to IGW also Lambda deployed in VPC cannot egress to internet via IGW, you need a NAT Gateway / NAT Instance\nNot D. same as C.\n\nA is the right solution (even if it is not well explained in my opinion)"
      },
      {
        "date": "2023-12-13T10:56:00.000Z",
        "voteCount": 1,
        "content": "As per https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html, \"To access private resources, connect your function to private subnets. If your function needs internet access, use network address translation (NAT). Connecting a function to a public subnet doesn't give it internet access or a public IP address.\""
      },
      {
        "date": "2023-11-25T04:05:00.000Z",
        "voteCount": 2,
        "content": "Just to clarify...If the Lambda function is already attached to a VPC, it's implied that it's in a private subnet since Lambda functions can't be directly placed in public subnets.  So C and D are out."
      },
      {
        "date": "2023-11-04T20:32:00.000Z",
        "voteCount": 2,
        "content": "Option B is definitely out as egress-only internet gateway is applicable solely for IPv6 traffic."
      },
      {
        "date": "2023-10-18T12:06:00.000Z",
        "voteCount": 1,
        "content": "internet gateway - cant assign elastic IP to internet gateway"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/95100-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.<br><br>During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application\u2019s performance.<br><br>Which actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the cluster endpoint of the Aurora database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Lambda Provisioned Concurrency feature.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the code for opening the database connection in the Lambda function outside of the event handler.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the API Gateway endpoint to an edge-optimized endpoint."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 62,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:32:00.000Z",
        "voteCount": 45,
        "content": "The correct answer is B and D.\n\nB. Using RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database can help improve the performance of the application by reducing the number of connections opened to the database. RDS Proxy manages the connection pool and routes incoming connections to the available read replicas, which can help with connection management and reduce the number of connections that need to be opened and closed.\n\nD. Moving the code for opening the database connection in the Lambda function outside of the event handler can help to improve the performance of the application by allowing the database connection to be reused across multiple requests. This avoids the need to open and close a new connection for each request, which can be time-consuming and resource-intensive."
      },
      {
        "date": "2023-01-13T14:32:00.000Z",
        "voteCount": 11,
        "content": "A. Using the cluster endpoint of the Aurora database instead of the reader endpoint would not help improve performance in this case, because the solution architect is already using read replicas to offload read traffic from the primary instance.\n\nC. Using the Lambda Provisioned Concurrency feature would not help improve performance in this case, as the problem is related to the number of connections to the database, not the number of instances running the Lambda function.\n\nE. Changing the API Gateway endpoint to an edge-optimized endpoint would not help improve performance in this case, as the problem is related to the number of connections to the database, not the location of the API Gateway endpoint."
      },
      {
        "date": "2024-08-31T02:55:00.000Z",
        "voteCount": 1,
        "content": "B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.\nD. Move the code for opening the database connection in the Lambda function outside of the event handler."
      },
      {
        "date": "2024-05-21T08:12:00.000Z",
        "voteCount": 1,
        "content": "The issue is with the number of database connections, thee are the only two changes that would impact the number of concurrent DB connections."
      },
      {
        "date": "2024-03-16T16:39:00.000Z",
        "voteCount": 1,
        "content": "B and D"
      },
      {
        "date": "2023-10-01T09:02:00.000Z",
        "voteCount": 3,
        "content": "B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.\n\nRDS Proxy helps manage and efficiently pool database connections, reducing the number of database connections required by the application. It helps improve performance and reduces the load on the database.\nD. Move the code for opening the database connection in the Lambda function outside of the event handler.\n\nBy reusing database connections, you can reduce the overhead of opening and closing connections for each Lambda invocation. You can use the Lambda execution context to keep the database connection open and reuse it across multiple requests within the same execution context."
      },
      {
        "date": "2023-07-02T08:06:00.000Z",
        "voteCount": 1,
        "content": "BD for sure"
      },
      {
        "date": "2023-03-28T05:02:00.000Z",
        "voteCount": 4,
        "content": "RDS proxy + Lambda function"
      },
      {
        "date": "2023-03-15T17:51:00.000Z",
        "voteCount": 3,
        "content": "RDX proxy &amp; connecting outside the handler method is up to 5 times faster than connecting inside."
      },
      {
        "date": "2023-03-08T03:43:00.000Z",
        "voteCount": 2,
        "content": "he Lambda function only queries an Amazon Aurora MySQL database- so i would reject option C"
      },
      {
        "date": "2023-02-25T09:35:00.000Z",
        "voteCount": 3,
        "content": "This may be too logical answer :-) - Setting up RDS proxy will help connection pooling, So B is one answer. Now C vs D\nThis question focuses on serverless solutions and best practices of lambda. and question hints that lambda only contains simple code.so lambda concurrency improvements may not be be the cause for performance issues detected while testing, and guess what - app is still in testing phase. so code might have a flaw can be reviewed and changed as per lambda best practices - https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html. I choose B and D"
      },
      {
        "date": "2023-02-10T17:10:00.000Z",
        "voteCount": 2,
        "content": "According to ChatGPT,\nBy reusing the same database connection across multiple invocations of the function, you can reduce the number of database connections that are opened and closed, which can help conserve resources and reduce the risk of running into database connection limits."
      },
      {
        "date": "2023-02-04T19:29:00.000Z",
        "voteCount": 4,
        "content": "BD\nhttps://awstut.com/en/2022/04/30/connect-to-rds-outside-of-lambda-handler-method-to-improve-performance-en/"
      },
      {
        "date": "2023-01-20T09:57:00.000Z",
        "voteCount": 1,
        "content": "B/C\nlambda provisioned concurrency and RDS proxy are mentioned in same page.\nhttps://quintagroup.com/blog/aws-lambda-provisioned-concurrency"
      },
      {
        "date": "2023-01-19T23:34:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
      },
      {
        "date": "2023-01-17T17:39:00.000Z",
        "voteCount": 1,
        "content": "B/C\nProvisioned Concurrency needed: https://www.reddit.com/r/aws/comments/gcwtqt/lambda_provisioned_concurrency_with_aurora/\nWith connection Pool, no to worry D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/95169-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to host a web application on AWS and wants to load balance the traffic across a group of Amazon EC2 instances. One of the security requirements is to enable end-to-end encryption in transit between the client and the web server.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Export the SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure it to use the SSL certificate. Set CloudFront to use the target group as the origin server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 87,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 67,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-23T07:43:00.000Z",
        "voteCount": 46,
        "content": "Amazon-issued public certificates can\u2019t be installed on an EC2 instance. To enable end-to-end encryption, you must use a third-party SSL certificate. https://aws.amazon.com/premiumsupport/knowledge-center/acm-ssl-certificate-ec2-elb/ so it's C or D. I choose C as it's ALB"
      },
      {
        "date": "2024-02-12T17:09:00.000Z",
        "voteCount": 3,
        "content": "in C , the encryption will terminate at ALB so its not an end-2-end encryption , for e2e end encryption need NLB"
      },
      {
        "date": "2023-02-25T20:05:00.000Z",
        "voteCount": 2,
        "content": "correct, but then you would use that ordered certificate for the alb as well. The other reason to order certificates is because some clients cannot verify ACM certificates which is not acceptable for a productive public service.\n\nBetween ALB and EC2 a self signed certificate is sufficient as alb does no verification of the EC2's certificate at all."
      },
      {
        "date": "2024-04-03T13:10:00.000Z",
        "voteCount": 4,
        "content": "that means you are decrypting the data on ALB and encrypt it again to send it to EC2. Does that sound E2E?"
      },
      {
        "date": "2023-01-19T23:32:00.000Z",
        "voteCount": 38,
        "content": "Vote D.\nIf you need to pass encrypted traffic to targets without the load balancer decrypting it, you can create a Network Load Balancer or Classic Load Balancer with a TCP listener on port 443.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html"
      },
      {
        "date": "2023-02-25T20:07:00.000Z",
        "voteCount": 6,
        "content": "coorect. but they want to upload the the certificate to the NLB for unknown reasons."
      },
      {
        "date": "2023-03-22T22:32:00.000Z",
        "voteCount": 2,
        "content": "You can use NLB with ACM cert on it. NLB can do TLS termination (https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/) and re-encrypt to target"
      },
      {
        "date": "2023-02-27T03:20:00.000Z",
        "voteCount": 11,
        "content": "how can this be true? Option D says to install on NLB.\nYou say bypass the NLB. If you bypass the NLB why are you installing the cert?"
      },
      {
        "date": "2024-09-11T05:45:00.000Z",
        "voteCount": 2,
        "content": "I'm leaning closer to D because, NLB supports e2e. I feel that if the question asked about offloading then the ALB options may have been better. But here it's asking for e2e and can only be done with an NLB"
      },
      {
        "date": "2024-08-31T02:56:00.000Z",
        "voteCount": 1,
        "content": "D. Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances."
      },
      {
        "date": "2024-07-23T15:43:00.000Z",
        "voteCount": 1,
        "content": "it is D, C is more complex."
      },
      {
        "date": "2024-05-23T15:13:00.000Z",
        "voteCount": 1,
        "content": "To achieve end-to-end encryption for a web application using AWS, place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM) and associate it with the ALB to handle HTTPS traffic from clients to the ALB. Additionally, install a third-party SSL certificate on each EC2 instance to ensure that traffic between the ALB and the instances is also encrypted. Configure the ALB to listen on port 443 and forward traffic to port 443 on the instances. This setup ensures that all data in transit is encrypted from the client through the ALB to the backend EC2 instances, meeting security requirements for end-to-end encryption while leveraging ACM for simplified certificate management \ufffc \ufffc \ufffc."
      },
      {
        "date": "2024-05-21T08:23:00.000Z",
        "voteCount": 1,
        "content": "The key here is end-to-end, so that rules out ALB. Instead Use NLB with TLS termination which will pass the traffic on encrypted.\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html#:~:text=The%20load%20balancer%20passes%20the,combination%20of%20protocols%20and%20ciphers."
      },
      {
        "date": "2024-05-20T12:28:00.000Z",
        "voteCount": 2,
        "content": "\u201cTo enable END-TO-END encryption, you must procure an SSL certificate from a third-party vendor.\nYou can then install the certificate on the EC2 instance and also associate the SAME certificate with the (network) Load Balancer by importing it into Amazon Certificate Manager.\u201d\nhttps://www.youtube.com/watch?v=6Nz0RFfBqVE&amp;t=44s \n\nTLS listeners for your Network Load Balancer\n\"\u2026 if you need to pass encrypted traffic to the targets without the (network) load balancer decrypting it, create a TCP listener on port 443 instead of creating a TLS listener.\"\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html\n\nP.S. The answer is misleading because it says to install the certificate on the NLB; read it as \u201cimport it to ACM and associate it with the NLB."
      },
      {
        "date": "2024-05-15T00:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct because\nALB+Self-signed Certification\nNLB+Public Certification"
      },
      {
        "date": "2024-03-19T16:48:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: A. Public Certificates: You can request Amazon-issued public certificates from ACM. ACM manages the renewal and deployment of public certificates that are used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. https://aws.amazon.com/es/certificate-manager/faqs/"
      },
      {
        "date": "2024-03-16T16:45:00.000Z",
        "voteCount": 2,
        "content": "C: use ACM in the ALB and third-party SSL certificate in the EC2 instances"
      },
      {
        "date": "2024-03-11T11:43:00.000Z",
        "voteCount": 1,
        "content": "The only solution that encrypts all the way is D."
      },
      {
        "date": "2024-03-06T16:41:00.000Z",
        "voteCount": 1,
        "content": "The different opinions are mainly on C or D. Both C and D are good for end to end encryption \u201cin transit\u201d. But actually the data is unencrypted on the ALB, and then encrypted again. Technically speaking, the ALB should be considered as part of the \u201ctransit\u201d. This is a flaw of C. And it is complicated to introduce another certificate.  \nThe flaws of answer D are: \n- mentioning installing SSL certificate to the NLB, which is not necessary.\n- It doesn\u2019t mention which listener is used. TLS listener does SSL termination while TCP listener does not."
      },
      {
        "date": "2024-02-26T13:32:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/aws/mutual-authentication-for-application-load-balancer-to-reliably-verify-certificate-based-client-identities/"
      },
      {
        "date": "2023-12-30T08:07:00.000Z",
        "voteCount": 2,
        "content": "Not A. You cannot export ACM certificate https://repost.aws/knowledge-center/configure-acm-certificates-ec2\nNot B. You cannot set CloudFront to use the target group as the origin server, you need to set the ELB the target group is assigned\nNot C. This terminates SSL in the load balancer and then re-encrypt, while the question asks for end-to-end encryption in transit between the client and the web server. https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\n\nNLB configured with TCP listener on port 443 is the right option. This answer is misleading as it mention to install the SSL certificate on the NLB, this is not needed if you do not use a TLS listener."
      },
      {
        "date": "2023-12-05T10:08:00.000Z",
        "voteCount": 1,
        "content": "D would be fine transport level security. No need any encrypt and decrypt."
      },
      {
        "date": "2023-11-25T07:03:00.000Z",
        "voteCount": 1,
        "content": "Application Load Balancers do not support mutual TLS authentication (mTLS). For mTLS support, create a TCP listener using a Network Load Balancer or a Classic Load Balancer and implement mTLS on the target.\nRef: 4th paragraph of https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/95170-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.<br><br>The company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company\u2019s customers.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-16T00:39:00.000Z",
        "voteCount": 16,
        "content": "Option A, B and D have some similarities with Option C but also have some key differences:\n\nOption A uses a Network Load Balancer (NLB) instead of an Application Load Balancer (ALB) and does not use AWS Database Migration Service (AWS DMS) for continuous data replication. Instead, it sets up the Aurora MySQL database as a replication target for the on-premises database.\nOption B does use AWS DMS for continuous data replication and sets up collection endpoints behind an ALB as Amazon EC2 instances in an Auto Scaling group. However, it does not create an Aurora Replica for the Aurora MySQL database or use Amazon RDS Proxy to write to the Aurora MySQL database.\nOption D does not use AWS DMS for continuous data replication or set up collection endpoints behind an ALB. Instead, it sets up collection endpoints as an Amazon Kinesis data stream and uses Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database."
      },
      {
        "date": "2024-08-31T02:57:00.000Z",
        "voteCount": 1,
        "content": "C. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS."
      },
      {
        "date": "2023-12-30T08:26:00.000Z",
        "voteCount": 2,
        "content": "Not A. not clear how the on-premises database is replicated on the Aurora MySQL, also you cannot place Lambda behind NLB as BLB only supports private IPs, instances and ALB https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html\nNot B. this will keep executing the aggregation job and the load on the same database instance and this will not resolve loading issues\nNot D. using Kinesis Data Firehose to replicate the database is not recommended, the solution should involve DMS. also moving to Kinesis Data Stream for data load requires some changes on the customer side which is not part of the request.\n\nC is the right solution: use DMS to migrate on-premise database, move the aggregation job to the read replica, using Lambda (that supports node.js) behind ALB will not impact client side"
      },
      {
        "date": "2023-12-04T11:18:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-07-02T08:42:00.000Z",
        "voteCount": 1,
        "content": "It's a c"
      },
      {
        "date": "2023-06-18T08:26:00.000Z",
        "voteCount": 2,
        "content": "Keyworks = DMS &amp; RDS Proxy\nThen C"
      },
      {
        "date": "2023-05-04T00:36:00.000Z",
        "voteCount": 3,
        "content": "AD: restart = interruption?\nB: ASG...Why?"
      },
      {
        "date": "2023-05-31T06:39:00.000Z",
        "voteCount": 1,
        "content": "why ...oh...why?"
      },
      {
        "date": "2023-03-28T05:12:00.000Z",
        "voteCount": 1,
        "content": "ill go with C"
      },
      {
        "date": "2023-03-16T13:47:00.000Z",
        "voteCount": 3,
        "content": "C.. even though question didn\u2019t mention the total time of each job. If the job takes more than 15m then Lambda can\u2019t be used. Probably the solution with ASG and EC2 is better .. not sure!"
      },
      {
        "date": "2023-03-13T21:02:00.000Z",
        "voteCount": 1,
        "content": "ALB because you are pointing to to Lambda function, not a network address\n\nLook at AWS DMS feature https://aws.amazon.com/dms/features/ \n\nMain requirement - needs the migration to occur w/out interruptions or changes to the company's customers.\n\nC keeps it stupid simple w/ no service interruption"
      },
      {
        "date": "2023-03-08T23:57:00.000Z",
        "voteCount": 1,
        "content": "Could anybody explain why ALB? I'd go with API Gateway"
      },
      {
        "date": "2023-03-13T21:04:00.000Z",
        "voteCount": 1,
        "content": "Application - you are using Lambda functions that will be sending api commands, you would use network when it is just about routing"
      },
      {
        "date": "2023-02-27T14:11:00.000Z",
        "voteCount": 1,
        "content": "I would say C."
      },
      {
        "date": "2023-02-26T06:30:00.000Z",
        "voteCount": 1,
        "content": "I have a feeling that none of the approaches will work.\na) We have two sources that change the database: migration and new data coming in. In a relational database this results in inconsistent data. Constraints will not be fulfilled. \nb) until the database is fully synced the second database has inconsistent data. Some parts of relations and parts of entities are still missing. Constraints will not be fulfilled. \nNone if the approaches addresses that aggregation tasks fail because of inconsistency of the data base."
      },
      {
        "date": "2023-02-26T06:50:00.000Z",
        "voteCount": 1,
        "content": "ACID principle:  atomicity, consistency, isolation and durability.  All solutions violate this basic principle of relational databases.\nhttps://en.wikipedia.org/wiki/ACID"
      },
      {
        "date": "2023-02-25T23:24:00.000Z",
        "voteCount": 2,
        "content": "Issue could be because of same db used for writing and reading heavily. solution to separate this into\nread replica only for reading. DMS for data migration to aws from onpremises.Writing app to DB and Reading app from DB for reports. Writing app needs RDSProxy and saves data.Reading app reads from replica.\nB is wrong because, Reading job (aggregation) needs to use replica  which is mentioned in C. C is correct."
      },
      {
        "date": "2023-02-17T20:00:00.000Z",
        "voteCount": 1,
        "content": "is it C or B?\nSame person answers two times two different answers"
      },
      {
        "date": "2023-01-30T11:41:00.000Z",
        "voteCount": 3,
        "content": "C is corect"
      },
      {
        "date": "2023-01-17T12:22:00.000Z",
        "voteCount": 4,
        "content": "C. \nThis option would meet the requirements of resolving the data loading issue and migrating without interruption or changes for the company's customers. By using AWS DMS for continuous data replication, the company can ensure that the data being migrated is up to date. By setting up an Aurora Replica and moving the aggregation jobs to run against it, the company can offload some of the read workload from the primary database and reduce the risk of issues with the load jobs. By using AWS Lambda functions behind an ALB and Amazon RDS Proxy to write to the Aurora MySQL database, the company can add an extra layer of security and scalability to the data collection process. Finally, by pointing the collector DNS record to the ALB after the databases are synced and disabling the AWS DMS sync task, the company can ensure a smooth cutover to the new environment."
      },
      {
        "date": "2023-01-17T12:22:00.000Z",
        "voteCount": 2,
        "content": "A. \nThis option would not work as it would require to change the primary database and also it may cause interruption for the company's customers during the cutover process.\n\nB.\nThis option would not work as it would not include Aurora Replica to offload the read workload, this would result in aggregation jobs running on the primary database which can cause the load jobs to fail during heavy loads.\n\nD.\nThis option would not work as it would require to use kinesis data stream which may cause performance issues and also it may not be the best fit for this use case. Additionally, using Kinesis Data Firehose would add complexity to the data replication process, and may result in increased latency or data loss."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/95171-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company\u2019s security team manages. The S3 bucket does not have versioning enabled.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 99,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 58,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T03:23:00.000Z",
        "voteCount": 42,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\n\nSo the correct answer is B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket."
      },
      {
        "date": "2023-01-14T03:23:00.000Z",
        "voteCount": 8,
        "content": "Option A is not correct because it uses SSE-S3 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it only denies unencrypted PutObject requests but does not specify how the objects will be encrypted.\n\nOption C is not correct because it does not specify how the security team will manage the encryption keys and it does not specify how the objects will be encrypted.\n\nOption D is not correct because it uses AES-256 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it simply denies unencrypted PutObject requests, but it doesn't specify how the objects will be encrypted."
      },
      {
        "date": "2024-01-02T02:58:00.000Z",
        "voteCount": 1,
        "content": "And adding to this in option D they specify uses default AES-256, but KMS also uses the same, so this option just don't make sense."
      },
      {
        "date": "2023-01-29T09:58:00.000Z",
        "voteCount": 9,
        "content": "What about the requirement of customer managed keys?"
      },
      {
        "date": "2023-09-03T19:49:00.000Z",
        "voteCount": 3,
        "content": "Not B. \"must be encrypted by keys that the company\u2019s security team manages\".  This implies the company does not wanna use AWS KMS."
      },
      {
        "date": "2024-02-14T08:37:00.000Z",
        "voteCount": 2,
        "content": "This is why they would use Customer-managed keys in AWS KMS. It is absolutely B"
      },
      {
        "date": "2024-01-02T02:57:00.000Z",
        "voteCount": 2,
        "content": "Hamimmelon, the Company's security Team can manage the AWS KMS service, so B is the right answer. All the others are not valid."
      },
      {
        "date": "2023-02-26T07:22:00.000Z",
        "voteCount": 4,
        "content": "Completely ignores the task to solve: \"all current and future objects in the S3 bucket must be encrypted by keys that the company\u2019s security team manages. \""
      },
      {
        "date": "2023-03-11T02:22:00.000Z",
        "voteCount": 1,
        "content": "Use the AWS CLI to re-upload all objects in the S3 bucket. - \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html\nChanges to note before enabling default encryption\nAfter you enable default encryption for a bucket, the following encryption behavior applies:\n\nThere is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.\n\nWhen you upload objects after enabling default encryption:\n\nIf your PUT request headers don't include encryption information, Amazon S3 uses the bucket\u2019s default encryption settings to encrypt the objects."
      },
      {
        "date": "2023-04-02T05:17:00.000Z",
        "voteCount": 4,
        "content": "Task  is to replace any AWS Managed keys to ones \"that the company\u2019s security team manages\" \nSo they tell us to find a solution that does not use  AWS Managed Keys."
      },
      {
        "date": "2024-02-14T08:41:00.000Z",
        "voteCount": 1,
        "content": "No, the task was to replace SSE-SE keys which have no relation to AWS KMS. \n\n\"Amazon S3 automatically enables server-side encryption with Amazon S3 managed keys (SSE-S3) for new object uploads.\n\nUnless you specify otherwise, buckets use SSE-S3 by default to encrypt objects. However, you can choose to configure buckets to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) instead. \"\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
      },
      {
        "date": "2023-01-20T04:58:00.000Z",
        "voteCount": 19,
        "content": "I think D is correct.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html"
      },
      {
        "date": "2024-02-27T10:00:00.000Z",
        "voteCount": 1,
        "content": "The issue with D is that it doesn't make it clear where the encryption is happening like all the other options do. Is it server-side (we assume that it is, but it is not what is written)? Or is it client-side?"
      },
      {
        "date": "2024-08-31T02:58:00.000Z",
        "voteCount": 1,
        "content": "B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket."
      },
      {
        "date": "2024-08-04T02:29:00.000Z",
        "voteCount": 1,
        "content": "AWS KMS (Key Management Service) allows for customer-managed keys (CMKs), which can indeed be considered as \"keys that the company\u2019s security team manages\""
      },
      {
        "date": "2024-06-17T05:52:00.000Z",
        "voteCount": 1,
        "content": "In s3 option there is no option to select AES256 custom key."
      },
      {
        "date": "2024-05-23T15:17:00.000Z",
        "voteCount": 1,
        "content": "To meet the requirement for encrypting all current and future objects in an Amazon S3 bucket with keys managed by the company\u2019s security team, change the S3 bucket\u2019s default encryption to server-side encryption with AWS KMS managed keys (SSE-KMS). Implement an S3 bucket policy to deny unencrypted PutObject requests, ensuring all new uploads are encrypted with the specified KMS key. Then, use the AWS CLI to re-upload all existing objects to the S3 bucket, enforcing the new encryption policy on current data. This approach ensures compliance by applying KMS encryption to both new and existing objects without causing disruptions \ufffc \ufffc \ufffc."
      },
      {
        "date": "2024-05-21T08:39:00.000Z",
        "voteCount": 1,
        "content": "The solutions need to use SSE-KMS so that the security team can manage the keys, but they also need to ensure that current and future objects are encrypted using customer-managed keys."
      },
      {
        "date": "2024-05-02T06:40:00.000Z",
        "voteCount": 1,
        "content": "Not Option D:  \" Amazon S3 server-side encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode (AES-GCM) to encrypt all uploaded objects.\"   AES- 256 is already the default, so you can't change it. \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html"
      },
      {
        "date": "2024-03-22T16:29:00.000Z",
        "voteCount": 2,
        "content": "Correct Approach: This option is accurate and meets all the specified requirements. By changing the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS), the company can use customer managed keys (CMKs) for encryption. This allows the security team to manage the keys, addressing the core requirement.\nSetting an S3 bucket policy to deny unencrypted PutObject requests ensures future compliance with the encryption policy.\nRe-uploading all objects using the AWS CLI ensures that existing objects are encrypted under the new policy, making sure that both current and future objects are encrypted with the keys managed by the company's security team."
      },
      {
        "date": "2024-03-16T23:28:00.000Z",
        "voteCount": 1,
        "content": "B, option D confuses encryption options. AES-256 is part of the SSE-S3 encryption method and doesn't directly involve customer-managed keys"
      },
      {
        "date": "2024-02-09T12:58:00.000Z",
        "voteCount": 1,
        "content": "The solution that meets the requirements for encrypting all current and future objects in the Amazon S3 bucket with keys that the company\u2019s security team manages, while ensuring server-side encryption, is:\nOption B is correct because it directly addresses the new requirement by changing the default encryption method to SSE-KMS, which allows the use of AWS Key Management Service (KMS) keys managed by the company\u2019s security team. This option ensures that all future uploads are encrypted with the specified KMS key. It also includes re-uploading existing objects to ensure they are encrypted under the new scheme. Setting an S3 bucket policy to deny unencrypted PutObject requests enforces the encryption requirement for all new uploads."
      },
      {
        "date": "2024-02-09T12:58:00.000Z",
        "voteCount": 1,
        "content": "Option D is incorrect because it refers to \u201cAES-256 with a customer managed key\u201d in a way that mixes concepts. AES-256 is the encryption standard used by SSE-S3 and does not directly apply to the use of customer managed keys. For managing keys, the correct approach is through SSE-KMS, which allows specifying a customer managed AWS KMS key."
      },
      {
        "date": "2024-01-03T00:51:00.000Z",
        "voteCount": 2,
        "content": "Not A. SSE-S3 with a customer managed key is not an actual option as SSE-S3 uses S3 managed keys\nNot C. S3 bucket policy cannot automatically encrypt objects on GetObject and PutObject requests. With policies you can only allow/deny actions from specific principals\nNot D. AES-256 with a customer managed key is not an actual option as AES-256 is used as value for the header x-amz-server-side-encryption to set SSE-S3 on putObject and SSE-S3 uses S3 managed keys\n\nB is correct as server-side encryption with AWS KMS managed encryption keys (SSE-KMS) is an actual default encryption settings for S3 bucket and you can use S3 bucket policy to deny unencrypted PutObject. These ensure all new objects will be encrypted with customer managed keys. Then using aws cli to re-upload all object will overwrite existing objects (versioning is not enabled)"
      },
      {
        "date": "2023-12-19T08:33:00.000Z",
        "voteCount": 1,
        "content": "i think D is correct as B is mentioned KMS managed key.."
      },
      {
        "date": "2023-12-19T07:20:00.000Z",
        "voteCount": 1,
        "content": "A - You cannot define your own key\nB - Correct. Using SSE-KMS and your own KMS customer managed key, you adhere to the requirements\nC - Does not encrypt existing objects, and you cannot \"change\" the request to \"automatically\" encrypt\nD - You can only choose between SSE-S3 and SSE-KMS (or now DSSE-KMS as well) for default encryption. Underlying the SSE-S3 refers to AES-256 (cfr. \"s3:x-amz-server-side-encryption\": \"AES256\") but you cannot specify your customer managed key in that case."
      },
      {
        "date": "2023-12-11T18:14:00.000Z",
        "voteCount": 1,
        "content": "If use KMS-CMK , wouldn't it be possible to manage keys directly while using KMS? Does anyone have an opinion on this?"
      },
      {
        "date": "2023-11-29T08:07:00.000Z",
        "voteCount": 2,
        "content": "B is correct\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html#aws-managed-customer-managed-keys\n\nWhen you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed key, or you can specify a customer managed key that you have already created."
      },
      {
        "date": "2023-11-24T06:21:00.000Z",
        "voteCount": 1,
        "content": "Between B and D, it's D which is correct because of cust managed clause."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/95198-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).<br><br>The company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.<br><br>A solutions architect must configure the application so that itis highly available and fault tolerant.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T04:42:00.000Z",
        "voteCount": 24,
        "content": "The correct answer is B. Provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS region provides redundancy and failover capability for the application. By creating a second origin for the new ALB in the second region, the CloudFront distribution can automatically route traffic to the healthy origin in case of an issue with the primary origin. This ensures that the application remains highly available and fault-tolerant.\n\nOption A is not correct because it uses Route 53 failover records, which can result in increased latency and DNS resolution time for clients. Option C is not correct because it doesn't provide redundancy for the load balancer, which is a critical component of the application. Option D is not correct because it does not provide redundancy for the application in case of an issue with the primary origin in the first region."
      },
      {
        "date": "2023-02-26T00:44:00.000Z",
        "voteCount": 10,
        "content": "For HA, always user second region but its there in all options. Here Cloudfront distribution multiple origin groups is the key point Solution Architects should know of. Configuring 2nd origin as ALB --&gt; EC2 instances target group in another regions setup makes highly available. If Cloudfront detects that response is Http error (fault) code like 4XX,5XX etc, it will failover to secondary origin (ALB of another region) which makes this fault tolerant. Answer is B.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      },
      {
        "date": "2024-08-31T02:59:00.000Z",
        "voteCount": 1,
        "content": "B. Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary."
      },
      {
        "date": "2024-07-30T07:13:00.000Z",
        "voteCount": 1,
        "content": "This architecture is an active-active DR strategy. You would do it with R53 failover because R53 has healthchecks, and once the primary is down all requests go to the failover. With CloudFront failover, all requests would continue to hit the failed primary before being routed to the failover distribution, which increases latency and possibly compounds problems in the failed stack. Interestigly, the best solution would actually be a combination between A and B, as this blog post shows:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/improve-web-application-availability-with-cloudfront-and-route53-hybrid-origin-failover/"
      },
      {
        "date": "2024-03-21T10:07:00.000Z",
        "voteCount": 2,
        "content": "A is wrong because CloudFront distros can't be added to Route 53.\nB is correct\nC is wrong because ALBs are single region and don't do failover.\nD would work, but is overengineered in this context."
      },
      {
        "date": "2024-07-30T07:16:00.000Z",
        "voteCount": 1,
        "content": "You can add CloudFront distros to R53 using alias records: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html"
      },
      {
        "date": "2024-02-09T13:07:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct because it involves creating a redundant setup in another AWS Region with its own ALB, Auto Scaling group, and EC2 instances. By updating the CloudFront distribution to include a second origin for the new ALB and creating an origin group with primary and secondary origins, CloudFront can automatically route traffic to the secondary origin if the primary is unhealthy. This setup leverages CloudFront\u2019s global reach to improve availability and fault tolerance without the need for DNS-level changes.\nOption A is not correct because it suggests creating a secondary deployment and updating the Route 53 A record to be a failover record with both CloudFront distributions as values. While Route 53 health checks and failover records can improve availability, CloudFront distributions themselves cannot be directly specified as values in A records for failover purposes. This option might lead to confusion in its implementation details."
      },
      {
        "date": "2024-01-18T22:43:00.000Z",
        "voteCount": 2,
        "content": "Who the hell cooked this terrible question design.\nUsually, HA means single region, DR means cross region. The question is asking HA while all the answer are using cross region solutions.\nWhen Dynamic content is involved, the dynamic content has to be store in a persistent storage, while question says the dynamic content is store on the EC2 instances in an ASG, which means the EC2 instances are ephemeral. \nAnd when Dynamic content is involved, no matter HA or DR, a replication component must be built so that the Dynamic content will be replicated to the other side so that it can be available when the event happens. While, none of the answers mentions replication at all."
      },
      {
        "date": "2024-01-03T01:09:00.000Z",
        "voteCount": 3,
        "content": "Not A. CloudFront is a global service, having two distributions will not increase fault-tolerance\nNot C. Single ALB is a single-point-of-failure and also you cannot have  Target Group in a different region\nNot D. CloudFront is a global service, having two distributions will not increase fault-tolerance and combining CloudFront with AWS Global Accelerator makes no sense\n\nB is correct as provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS region provides redundancy and failover capability for the application. The origin group is the right way to enable failover for CloudFront distributions origin"
      },
      {
        "date": "2023-11-15T06:03:00.000Z",
        "voteCount": 1,
        "content": "Not Create a second CloudFront Distribution, it's update the distribution with multi origins.\n\nRef:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html#concept_origin_groups.creating\n\"Make sure the distribution has more than one origin. If it doesn\u2019t, add a second origin.\""
      },
      {
        "date": "2023-07-02T08:49:00.000Z",
        "voteCount": 1,
        "content": "it's a B"
      },
      {
        "date": "2023-06-26T06:22:00.000Z",
        "voteCount": 2,
        "content": "Both A and B would work, but A is tangibly worse in terms of performing fail-over (because it relies on DNS) and gains you little, since CloudFront is highly available by its nature, making a second CF distribution doesn't improve your application's robustness."
      },
      {
        "date": "2023-03-28T05:48:00.000Z",
        "voteCount": 1,
        "content": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region."
      },
      {
        "date": "2023-03-16T14:24:00.000Z",
        "voteCount": 1,
        "content": "B is the best solution with very high availability (compared to the R53 failover solution)"
      },
      {
        "date": "2023-03-04T08:44:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      },
      {
        "date": "2023-02-27T14:36:00.000Z",
        "voteCount": 1,
        "content": "B looks good."
      },
      {
        "date": "2023-01-20T23:54:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nC is not correct, because ALB is regional service, so ALB have to be added too."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/95209-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company\u2019s global offices and the transit account. The company has AWS Config enabled on all of its accounts.<br><br>The company\u2019s networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account\u2019s security group by using a nested security group reference of \u201c<transit-account-id>/sg-1a2b3c4d\u201d.\n\t\t\t\t\t\t\t\t\t\t</transit-account-id>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 37,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T04:53:00.000Z",
        "voteCount": 23,
        "content": "The correct answer is option C. In this solution, a VPC prefix list is created in the transit account with all of the internal IP address ranges, and then shared to all of the other accounts using AWS Resource Access Manager. This allows for central management of the IP address ranges, and eliminates the need for manual updates to security group rules in each account. This solution also allows for compliance checks to be run using AWS Config and for any non-compliant security groups to be automatically remediated.\n\nOption A is not correct because it would require manual updates to the JSON file and would also require developers to manually update their security group rules, which would lead to operational overhead.\n\nOption B is not correct because it would require the creation of a new AWS Config managed rule and it would also require manual updates to the security group rules in each account.\n\nOption D is not correct because it would require manual updates to the security group in the transit account and it would also lead to operational overhead."
      },
      {
        "date": "2024-01-02T03:25:00.000Z",
        "voteCount": 1,
        "content": "I agree that option C is probable the best one, but B is also correct, there is no manual updates to the SG, the remediation is automated in ASW  Config. In option C you also need to manual update the prefix list, no? Imagine a new CIDR appears in the offices."
      },
      {
        "date": "2024-01-08T19:32:00.000Z",
        "voteCount": 2,
        "content": "I doubt all the security groups in the accounts will use the same CIDR ranges. They just need a way to centrally manage the CIDR prefixes. The question did not say that everyone has to comply and any non-compliant resources needs to be remdiated."
      },
      {
        "date": "2024-08-31T03:00:00.000Z",
        "voteCount": 1,
        "content": "C. In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts."
      },
      {
        "date": "2024-01-05T06:00:00.000Z",
        "voteCount": 2,
        "content": "Not A. This requires to maintain the JSON file, SNS topic in each account, Lambda to update SG. This is a lot of work, also not clear what accounts holds the S3 with the JSON\nNot B. I was not able to spot a managed AWS Config rule that could help in this case https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html (but I do not recall managed rule by hart and this doesn't sound like a remote use case, so in the exam this could trick me)"
      },
      {
        "date": "2024-01-05T06:00:00.000Z",
        "voteCount": 2,
        "content": "Not D. You can reference a VPC SG in other account VPCs when you have VPC peering in place, this is not mentioned in the scenario https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html. Since there is a Transit Gateway involved it is unlikely to have VPC peering and the resources in a VPC attached to a transit gateway cannot access the security groups of a different VPC that is also attached to the same transit gateway https://docs.aws.amazon.com/vpc/latest/tgw/tgw-vpc-attachments.html (this option initially was not bad for me)\n\nC works well as prefix lists are created exactly for this purpose https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html"
      },
      {
        "date": "2023-07-02T08:52:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-12T11:56:00.000Z",
        "voteCount": 1,
        "content": "Definitely prefix"
      },
      {
        "date": "2023-03-28T05:50:00.000Z",
        "voteCount": 2,
        "content": "prefix list and RAM"
      },
      {
        "date": "2023-03-16T14:48:00.000Z",
        "voteCount": 2,
        "content": "C makes sense \u2705"
      },
      {
        "date": "2023-01-30T11:57:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/82131-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-01-19T03:30:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/simplify-network-routing-and-security-administration-with-vpc-prefix-lists/#:~:text=A%20Prefix%20List%20is%20a,Resource%20Access%20Manager%20(RAM)."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/95211-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.<br><br>The company wants to create a CSV report every 2 weeks to show each API Lambda function\u2019s recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.<br><br>Which solution will meet these requirements with the LEAST development time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T04:56:00.000Z",
        "voteCount": 22,
        "content": "The correct answer is B. Opting in to AWS Compute Optimizer and creating a Lambda function that calls the ExportLambdaFunctionRecommendations operation is the least development time solution. This option allows you to use the built-in AWS Compute Optimizer service to extract metrics data and export it as a CSV file, which can then be stored in an S3 bucket.\n\nOption A is not correct because it requires the development of a Lambda function that extracts metrics data and collates it into tabular format, which adds development time. Option C is not correct because it requires the setup of enhanced infrastructure metrics, which adds development time. Option D is not correct because it requires purchasing the AWS Business Support plan and using the Trusted Advisor console, which adds development time."
      },
      {
        "date": "2023-01-30T12:03:00.000Z",
        "voteCount": 9,
        "content": "AWS compute optimizer+ lambda"
      },
      {
        "date": "2024-08-31T03:02:00.000Z",
        "voteCount": 1,
        "content": "B. Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks."
      },
      {
        "date": "2024-07-30T08:25:00.000Z",
        "voteCount": 1,
        "content": "Why would anyone need to memorize whether Compute Optimizer reports can be scheduled from the UI or must be done through API calls? This is so unnecessary *rolls eyes"
      },
      {
        "date": "2024-03-09T23:49:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is C.\nOption A involves creating a custom Lambda function to extract metrics data from CloudWatch Logs and generate the CSV report, which would require more development time compared to using the Compute Optimizer service.\n\nOption B is partially correct, as it involves using Compute Optimizer and a Lambda function, but it misses the ability to schedule recurring exports directly within the Compute Optimizer console.\n\nOption D suggests using AWS Trusted Advisor, which is a service for monitoring best practices and resources, but it does not provide the specific Lambda function memory and cost recommendations required in this scenario."
      },
      {
        "date": "2024-09-02T21:50:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-02-09T14:06:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most efficient and straightforward solution. By opting into AWS Compute Optimizer, the company can leverage AWS\u2019s service for recommendations on optimal AWS resource configurations based on utilization metrics. Using the ExportLambdaFunctionRecommendations operation allows for automating the retrieval of the desired optimization data with minimal code. Scheduling this operation with an Amazon EventBridge rule to run every 2 weeks and exporting the results directly to a CSV file in an S3 bucket meets all the stated requirements with minimal development effort."
      },
      {
        "date": "2024-01-04T02:16:00.000Z",
        "voteCount": 2,
        "content": "Not A. This requires some serious development, also not 100% sure CW Logs alone provides all the required info.\nNot B. This requires some coding to to call the ExportLambdaFunctionRecommendations API\nNot D. To create CSV reports (organizational view reports) in Trusted Advisor you need to enable Trusted Advisor in you organization, and AWS Organization is not mentioned in the scenario https://docs.aws.amazon.com/awssupport/latest/user/organizational-view.html \n\nC is the right solution as it allows to schedule report with the required info with no development https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html. This is was misleading for me as it mentions to set up enhanced infrastructure metrics that is only available for EC2, but you can do it without development (you can do it from console), this add cost but the ask focus on development effort."
      },
      {
        "date": "2024-02-09T14:06:00.000Z",
        "voteCount": 5,
        "content": "It is not C. Option C describes using AWS Compute Optimizer and setting up a job within the Compute Optimizer console. However, as of the last update, Compute Optimizer does not provide a direct scheduling feature within the console for exporting recommendations to a CSV file. This option suggests functionality that is not directly available in Compute Optimizer."
      },
      {
        "date": "2024-01-02T10:12:00.000Z",
        "voteCount": 3,
        "content": "B is correct\nNot C because Enhanced infrastructure metrics is a paid feature of Compute Optimizer that applies to Amazon EC2 instances and instances that are part of Auto Scaling groups."
      },
      {
        "date": "2023-11-24T13:33:00.000Z",
        "voteCount": 2,
        "content": "Lambda = development.  Option D has no development.  If you are not familiar with dev'ing - publishing a simple Lambda function can require you to wrap all the Node.js or Python or whatever programming language libraries with it in order to execute correctly within AWS Lambda.  Configuring Trusted Advisor (GUI) or scheduling a job is NOT considered Development."
      },
      {
        "date": "2023-10-21T15:56:00.000Z",
        "voteCount": 4,
        "content": "Basic plan of Trusted Advisor only has 7 core checks.  Business plan has all these, so with LEAST development, it must be business plan.\nCheck categories\n**Cost optimization**\nPerformance\nSecurity\nFault tolerance\nService limits"
      },
      {
        "date": "2023-10-08T16:57:00.000Z",
        "voteCount": 1,
        "content": "B. \nOption C is not correct because \"Enhanced infrastructure metrics is a paid feature of Compute Optimizer that applies to Amazon EC2 instances.\" \nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/enhanced-infrastructure-metrics.html"
      },
      {
        "date": "2023-09-11T18:10:00.000Z",
        "voteCount": 4,
        "content": "Computer Optimizer could generate Export for Lambda Functions one-time. In order to schedule every 2 weeks, EventBridge Scheduler/Schedule Rule should be used."
      },
      {
        "date": "2023-09-11T17:54:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\nhttps://aws.amazon.com/blogs/compute/optimizing-aws-lambda-cost-and-performance-using-aws-compute-optimizer/"
      },
      {
        "date": "2023-09-11T00:55:00.000Z",
        "voteCount": 4,
        "content": "AWS Compute Optimizer helps avoid overprovisioning and underprovisioning four types of AWS resources\u2014Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volumes, Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda functions\u2014based on your utilization data."
      },
      {
        "date": "2023-07-02T09:03:00.000Z",
        "voteCount": 1,
        "content": "its a  B"
      },
      {
        "date": "2023-05-31T02:18:00.000Z",
        "voteCount": 3,
        "content": "B - https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html"
      },
      {
        "date": "2023-05-13T07:37:00.000Z",
        "voteCount": 3,
        "content": "Option D i would say as purchasing business support and truster advisor is money but not development time."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/95217-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company\u2019s factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon RDS.<br><br>The company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for the cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM access for daily activities.<br><br>The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and Cost Management solution that provides these cost reports.<br><br>Which combination of actions will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate the user-define cost allocation tags that represent the application and the team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate the AWS generated cost allocation tags that represent the application and the team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cost category for each application in Billing and Cost Management.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate IAM access to Billing and Cost Management.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cost budget.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Cost Explorer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 88,
        "isMostVoted": true
      },
      {
        "answer": "ADF",
        "count": 63,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T05:04:00.000Z",
        "voteCount": 41,
        "content": "A, C and F are the correct answers because they provide the required cost reports and analysis for the company's applications and teams.\n\nA. Activating user-defined cost allocation tags that represent the application and the team allows the company to assign costs to specific applications and teams. This allows the company to see how much each application and team is costing them, which is important for cost forecasting and budgeting.\n\nC. Creating a cost category for each application in Billing and Cost Management allows the company to group costs by application. This makes it easier to understand the costs associated with each application and to compare the costs of different applications over time.\n\nF. Enabling Cost Explorer allows the company to analyze costs and usage over time, and to create custom reports and forecasts. This is important for understanding the costs associated with each application and team, and for forecasting future costs."
      },
      {
        "date": "2023-01-14T05:04:00.000Z",
        "voteCount": 7,
        "content": "B is not correct because AWS generated cost allocation tags are automatically created for some AWS resources, but it does not provide the required cost reports and analysis for the company's applications and teams.\n\nOption D is not correct because IAM access controls are used to limit access to the billing and cost management features, but it is not necessary to configure it to meet the requirements.\n\nE is not correct because Creating a cost budget allows the company to set a budget for their costs and to receive alerts when costs exceed the budget, but it does not provide the required cost reports and analysis for the company's applications and teams."
      },
      {
        "date": "2023-05-06T13:42:00.000Z",
        "voteCount": 10,
        "content": "With out granting IAm Access, IAM users cannot access Billing console, so s cannot see the Cost explorer \nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html.\nQuestion says teams are responsible for cost\nI"
      },
      {
        "date": "2024-02-27T12:51:00.000Z",
        "voteCount": 2,
        "content": "In addition to the IAM access problem answer ACF will face, the problem statement already presents us with the information that resources are already tagged by team/application. Creating cost category seems redundant and even if you did create this redundancy, you are faced with the IAM access problem.\n\nIf each team is responsible for the cost and the performance, they would need access to the billing console for their team."
      },
      {
        "date": "2024-04-06T07:50:00.000Z",
        "voteCount": 3,
        "content": "So you are wrong, tags can be applied to applications so you can easily find them but unless they are actually activated as user defined billing tags then you will not be able to use those tags in cost analysis. Also you have to enable cost explorer it is not enabled by default and cost explorer lets you see the previous 12 months and creates projections for the next 12, so without that option you will not meet the objective."
      },
      {
        "date": "2023-02-16T15:07:00.000Z",
        "voteCount": 18,
        "content": "Correct ADF - SInce resources are tagged, C may not require ?"
      },
      {
        "date": "2024-08-31T03:03:00.000Z",
        "voteCount": 2,
        "content": "A. Activate the user-define cost allocation tags that represent the application and the team.\nC. Create a cost category for each application in Billing and Cost Management.\nE. Create a cost budget"
      },
      {
        "date": "2024-07-18T03:23:00.000Z",
        "voteCount": 3,
        "content": "The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. - Tagging and Cost Categories\n\n The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. -Cost Explorer"
      },
      {
        "date": "2024-07-02T22:11:00.000Z",
        "voteCount": 1,
        "content": "A. User defined cost allocation tags: application, team \nD. Activate IAM access to Billing and Cost Management: \n\"The teams use IAM access for daily activities.\"\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/control-access-billing.html\nF. Enable Cost Explorer: https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html\n\n\nC is NOT needed, because A already will give a usage view by \"tags that represent their application and team\" \"The company needs to determine which costs on the monthly AWS bill are attributable to each application or team\""
      },
      {
        "date": "2024-04-02T08:36:00.000Z",
        "voteCount": 2,
        "content": "Option ACF and NOT ADF -  Cost allocation helps you identify who is spending what, within your organization. Cost categories is a cost allocation service to help you map your AWS costs, to your unique internal business structures.\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html"
      },
      {
        "date": "2024-03-23T06:33:00.000Z",
        "voteCount": 2,
        "content": "Focusing on enabling the company to attribute AWS costs to each application or team, create cost comparison reports for the last 12 months, and forecast costs for the next 12 months,..Answer: A, C, F."
      },
      {
        "date": "2024-03-23T06:36:00.000Z",
        "voteCount": 1,
        "content": "Explanation of Exclusions: B, D, F"
      },
      {
        "date": "2024-03-23T06:37:00.000Z",
        "voteCount": 2,
        "content": "E. Create a cost budget: Creating a cost budget is valuable for managing expenses and avoiding overspending, but it does not directly facilitate the attribution of costs to applications or teams, nor does it aid in the creation of historical comparison reports or forecasts in the manner required by the company."
      },
      {
        "date": "2024-03-23T06:37:00.000Z",
        "voteCount": 1,
        "content": "D. Activate IAM access to Billing and Cost Management: While important for ensuring that team members can access billing information, this action itself doesn't contribute directly to organizing or reporting on costs by application or team, nor does it facilitate forecasting."
      },
      {
        "date": "2024-03-23T06:38:00.000Z",
        "voteCount": 1,
        "content": "[correction for typo error above] Explanation of Exclusions: B, D, E"
      },
      {
        "date": "2024-03-23T06:34:00.000Z",
        "voteCount": 1,
        "content": "here's the detailed recommendation:"
      },
      {
        "date": "2024-03-23T06:35:00.000Z",
        "voteCount": 1,
        "content": "A. Activate user-defined cost allocation tags: User-defined tags need to be activated for cost allocation purposes. These tags, representing applications and teams, are crucial for attributing costs accurately to the responsible entities within the company. Once activated, these tags will appear in the AWS Billing and Cost Management dashboard, enabling detailed tracking and reporting based on the specified tags."
      },
      {
        "date": "2024-03-23T06:34:00.000Z",
        "voteCount": 1,
        "content": "F. Enable Cost Explorer: Cost Explorer is essential for analyzing past spending and forecasting future costs. It allows for detailed reports that can compare costs from the last 12 months and helps in forecasting for the next 12 months. With the data segmented by user-defined cost allocation tags, Cost Explorer can provide the insights needed to meet the company's reporting and forecasting requirements.\nC. Create a cost category for each application in Billing and Cost Management: Cost categories allow for the organization of cost and usage data into logical groups that reflect the company's internal structure, such as by application or team. By leveraging the user-defined tags activated in step A, cost categories can automate the process of cost attribution to these entities, simplifying the creation of targeted reports and forecasts."
      },
      {
        "date": "2024-03-17T00:00:00.000Z",
        "voteCount": 3,
        "content": "Agree with ACF"
      },
      {
        "date": "2024-03-11T11:59:00.000Z",
        "voteCount": 3,
        "content": "For the full granularity, C is needed rather than D."
      },
      {
        "date": "2024-03-03T07:50:00.000Z",
        "voteCount": 2,
        "content": "C is not needed.  Option A activated the tag, so we could use tags to generate reports. There is no need to create cost category for individual applications, which could be a huge effort and not practical,  what if you have hundreds of applications..."
      },
      {
        "date": "2024-02-27T14:59:00.000Z",
        "voteCount": 1,
        "content": "Correct ADF - SInce resources are tagged"
      },
      {
        "date": "2024-02-09T14:16:00.000Z",
        "voteCount": 2,
        "content": "Correct answers are:\nA. Activate the user-defined cost allocation tags that represent the application and the team. User-defined cost allocation tags allow you to organize your AWS bill by categorizing costs according to your business\u2019s organizational structures (e.g., by application or team). \nC. Create a cost category for each application in Billing and Cost Management. Cost categories enable you to create custom groupings of your AWS costs. By creating a cost category for each application, you can group costs more granularly, which is helpful for detailed reporting and cost attribution to specific teams or applications.\nF. Enable Cost Explorer. Cost Explorer is a tool that allows you to visualize, understand, and manage your AWS costs and usage over time. By enabling Cost Explorer, you can create detailed reports to compare costs from the last 12 months and forecast costs for the next 12 months, meeting the company\u2019s requirements for cost management and planning."
      },
      {
        "date": "2024-02-09T14:18:00.000Z",
        "voteCount": 1,
        "content": "Option B is not correct. It refers to activating AWS generated cost allocation tags. While AWS-generated tags can provide useful information, they do not typically represent specific applications or teams unless those entities are directly associated with AWS-defined resources or actions. For custom application and team tracking, user-defined tags (Option A) are more appropriate."
      },
      {
        "date": "2024-01-05T05:59:00.000Z",
        "voteCount": 4,
        "content": "Not B. AWS generated tags do not allow you to identify app. You need user-defined tags for this\nNot C. Cost Categories allows to define rule to group costs into categories using different dimensions such as: account, tag, service, charge type, and other cost categories. In this scenario User-defined tags are enough to identify applications and teams.\nNot E. Budget doesn't help you in creating reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. Use Cost Explorer instead-"
      },
      {
        "date": "2024-01-02T08:46:00.000Z",
        "voteCount": 3,
        "content": "See below severlight explanation. I agree with it."
      },
      {
        "date": "2023-12-17T10:46:00.000Z",
        "voteCount": 1,
        "content": "can someone help me with this solutions, as I am confused between  ACF and ADF"
      },
      {
        "date": "2023-12-12T19:44:00.000Z",
        "voteCount": 3,
        "content": "ACF is right."
      },
      {
        "date": "2023-11-13T01:48:00.000Z",
        "voteCount": 6,
        "content": "without IAM access activated only the root account has access to billing info, you need to enable Cost Explorer by simply opening the Cost Explorer console the first time."
      },
      {
        "date": "2023-11-13T01:59:00.000Z",
        "voteCount": 5,
        "content": "and you don't need a cost category for each application, as a category is needed for aggregation and you already have all the tags you need to aggregate on the application level."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/95219-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client\u2019s allow list.<br><br>The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets.<br><br>How should a solutions architect ensure that the web application can continue to call the third-party API after the migration?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T05:07:00.000Z",
        "voteCount": 19,
        "content": "The correct solution is B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC. This will ensure that the web application can continue to call the third-party API after the migration by using the customer-owned public IP addresses that were assigned to the NAT gateways. This ensures that the third-party API will only see traffic coming from the customer-owned IP addresses that are on the allow list. Option A,C and D doesn't make sense in this context."
      },
      {
        "date": "2024-08-31T03:04:00.000Z",
        "voteCount": 1,
        "content": "B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC."
      },
      {
        "date": "2024-01-05T06:39:00.000Z",
        "voteCount": 2,
        "content": "In this scenario EC2 instances access the 3P APIs via NAT Gateway. 3P API FW see IP of the NAT Gateway. You can assign Elastic IP to NAT Gateway and you can allocate an IP address from a pool that you have brought to your AWS account to the Elastic IP. Thus B is correct. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"
      },
      {
        "date": "2023-07-02T09:15:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-06-18T08:38:00.000Z",
        "voteCount": 2,
        "content": "KEYWORD = NAT gateways in the VPC"
      },
      {
        "date": "2023-05-21T19:26:00.000Z",
        "voteCount": 1,
        "content": "B is the only option that makes sense."
      },
      {
        "date": "2023-05-20T10:41:00.000Z",
        "voteCount": 1,
        "content": "B make sense"
      },
      {
        "date": "2023-03-28T09:27:00.000Z",
        "voteCount": 2,
        "content": "Register a block of customer owned public IP's"
      },
      {
        "date": "2023-03-16T15:15:00.000Z",
        "voteCount": 2,
        "content": "B is the only solution"
      },
      {
        "date": "2023-01-30T12:17:00.000Z",
        "voteCount": 4,
        "content": "The correct solution is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/95227-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:<br><br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image6.png\"><br><br>Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd s3:CreateBucket with \u201cAllow\u201d effect to the SCP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the account from the OU, and attach the SCP directly to account 1111-1111-1111.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the developers to add Amazon S3 permissions to their IAM entities.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the SCP from account 1111-1111-1111."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T20:18:00.000Z",
        "voteCount": 23,
        "content": "SCP doesn\u2019t grant permission"
      },
      {
        "date": "2023-02-22T14:14:00.000Z",
        "voteCount": 7,
        "content": "Per the DOCS:\nService control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization\u2019s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. For instructions on enabling SCPs, see Enabling and disabling policy types."
      },
      {
        "date": "2023-02-22T14:15:00.000Z",
        "voteCount": 11,
        "content": "SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies."
      },
      {
        "date": "2023-01-15T08:12:00.000Z",
        "voteCount": 13,
        "content": "C is correct\nSCP policy allow everything except cloudtrail. SCP is boundary but it does not give allow to IAM users. You have to configure allow for every IAM"
      },
      {
        "date": "2024-08-31T03:05:00.000Z",
        "voteCount": 1,
        "content": "B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC."
      },
      {
        "date": "2024-09-02T21:55:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-03-17T00:10:00.000Z",
        "voteCount": 3,
        "content": "C, SCP is just a distractor, the users need direct permissions"
      },
      {
        "date": "2024-02-09T14:35:00.000Z",
        "voteCount": 1,
        "content": "The problem described does not originate from the Service Control Policy (SCP) itself based on the SCP content provided. The SCP allows all actions (\"Action\": \"\") except for actions related to AWS CloudTrail (\"Action\": \"CloudTrail:\"), which are explicitly denied. Therefore, the inability for developers to create Amazon S3 buckets is not due to this SCP, as the SCP does not restrict S3 actions.\nGiven the situation, the correct way to address the developers\u2019 inability to create Amazon S3 buckets would be:\n    *    C. Instruct the developers to add Amazon S3 permissions to their IAM entities.\n\nOption C is the correct action because the issue likely stems from the IAM permissions (or lack thereof) assigned to the developers\u2019 IAM entities (users, groups, or roles). IAM permissions are required to perform actions within AWS accounts, such as creating S3 buckets. If developers lack the necessary IAM permissions, they would not be able to create S3 buckets regardless of the SCP settings."
      },
      {
        "date": "2024-01-05T06:48:00.000Z",
        "voteCount": 3,
        "content": "The SCP in the scenario is allowing any actions with the exception of cloudtrail. Thus, the SCP is not preventing user to create S3 bucket. If the user cannot create a bucket, then the user IAM user/role is missing permissions to create S3 bucket."
      },
      {
        "date": "2023-12-04T12:12:00.000Z",
        "voteCount": 1,
        "content": "Answer C."
      },
      {
        "date": "2023-07-02T09:34:00.000Z",
        "voteCount": 1,
        "content": "it's a C"
      },
      {
        "date": "2023-06-25T08:23:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-18T08:41:00.000Z",
        "voteCount": 2,
        "content": "I just wanted to add my vote to the mix to hopefully drown out the wrong votes.\nIts definitely C. SCP is only a guardrail, it doesn't actually grant access. So the users would need to be given s3 access separately.\nAnd to address the wrong answer, A isn't correct because creating an s3 bucket is not a cloudtrail action. Being denied cloudtrail wouldn't deny s3 actions."
      },
      {
        "date": "2023-06-13T14:06:00.000Z",
        "voteCount": 2,
        "content": "C is the answer. SCP DONT grant permissions. They just set boundaries on what account is capable of giving access to all users. For example, we applied a SCP on an OU that has account A. This SCP has S3fullAWSaccess. This does NOT mean that any IAM user can perform any S3 action. You still need to explicitly define IAM permissions for user to perform action on S3. This is called whitelisting. \nAnother example, You wrote an SCP that DENIES S3 access and applied it to an OU that has account B. Now Lets say ROOT user of Account B (who got admin previleges) tries to create S3 bucket, they get DENIED error as SCP has already set a bounday saying NOONE in this OU can access S3"
      },
      {
        "date": "2023-06-12T12:17:00.000Z",
        "voteCount": 1,
        "content": "Need to deal with iam policy auth now"
      },
      {
        "date": "2023-06-12T12:18:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2023-05-11T23:46:00.000Z",
        "voteCount": 2,
        "content": "I am not sure the given situation is possible.\nWhen I tested, member (1111-1111-1111) could create bucket without any policy which can be attached or detached by the oneself."
      },
      {
        "date": "2023-05-09T02:25:00.000Z",
        "voteCount": 2,
        "content": "Are developers allowed to modify their IAM entities in the situation of option C? If so, I am not sure this is the best practice."
      },
      {
        "date": "2023-03-28T09:28:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-03-16T15:19:00.000Z",
        "voteCount": 2,
        "content": "SCP is not enough. IAM permission is needed"
      },
      {
        "date": "2023-03-16T07:18:00.000Z",
        "voteCount": 5,
        "content": "C - Users and roles must still be granted permissions with appropriate IAM permission policies. A user without any IAM permission policies has no access at all, even if the applicable SCPs allow all services and all actions."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/95233-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a monolithic application that is critical to the company\u2019s business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company\u2019s application team receives a directive from the legal department to back up the data from the instance\u2019s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 86,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 84,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T05:23:00.000Z",
        "voteCount": 36,
        "content": "The correct answer is C. Taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (DLM) will meet the requirements because it allows you to create a backup of the volume without the need to access the instance or its SSH key pair. Additionally, DLM allows you to schedule the backups to occur at specific intervals and also enables you to copy the snapshots to an S3 bucket. This approach will not impact the running application as the backup is performed on the EBS volume level.\n\nOption A is not correct because the instance would need an IAM role with permission to write to S3 and access to the instance via Systems Manager Session Manager.\n\nOption B is not correct because it would require stopping the instance, which would impact the running application.\n\nOption D is not correct because it would require stopping the instance and creating a new EC2 instance, which would impact the running application."
      },
      {
        "date": "2024-03-23T15:20:00.000Z",
        "voteCount": 5,
        "content": "Not true! Feel free to challenge me if you think I am wrong.\nTaking a snapshot of the EBS volume using Amazon DLM is a straightforward approach to ensure data durability and availability. However, this option does not directly address the requirement to move data to an S3 bucket. While EBS snapshots are stored on S3, they are not accessible as regular S3 objects for direct file manipulation or viewing, meaning additional steps would be required to access and use the data in the format specified by the requirement.\n\nVerdict: Does Not Fully Meet Requirements. DLM manages snapshots for EBS volumes but doesn't facilitate direct, accessible backups to S3 as described."
      },
      {
        "date": "2024-10-05T18:01:00.000Z",
        "voteCount": 1,
        "content": "I agree with this A. In addition, the application team has no SSH key access, you can not think that the team has the DLM permission as well. Infrastructure teams generally take this type of role."
      },
      {
        "date": "2024-08-27T19:45:00.000Z",
        "voteCount": 2,
        "content": "This is valid, A is the correct answer.\n\nC is wrong because \nExplanation: This option indirectly involves copying data to S3. The primary action is taking a snapshot of the EBS volume, which can be managed by DLM. However, moving the data from a snapshot directly to S3 isn't straightforward. Snapshots are stored in S3 by AWS internally, but this storage is opaque to users and can't be accessed directly as regular S3 objects."
      },
      {
        "date": "2024-04-04T03:23:00.000Z",
        "voteCount": 3,
        "content": "I'll try to challange you :-)\nYou can use EBS direct APIs to access data from an EBS snapshot. This is how you can  read the data from the snapshot and copy it to S3.\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-accessing-snapshot.html"
      },
      {
        "date": "2023-11-06T07:41:00.000Z",
        "voteCount": 2,
        "content": "Your reasoning is wrong . Option A has mentioned that instance profile role is attached to EC2 instance."
      },
      {
        "date": "2023-01-14T15:32:00.000Z",
        "voteCount": 3,
        "content": "thank you for correcting some of these answers and for the explanations to them"
      },
      {
        "date": "2023-01-24T09:45:00.000Z",
        "voteCount": 9,
        "content": "Assuming that EBS is encrypted, I think that is much easier to run the copy command from AW system manager"
      },
      {
        "date": "2023-01-23T11:16:00.000Z",
        "voteCount": 10,
        "content": "taking a backup of the data to s3. aws doesn't allow up to view snapshots in s3"
      },
      {
        "date": "2024-01-11T22:35:00.000Z",
        "voteCount": 1,
        "content": "The requirement is only 'back up'"
      },
      {
        "date": "2024-08-31T03:05:00.000Z",
        "voteCount": 1,
        "content": "C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3."
      },
      {
        "date": "2024-08-04T01:50:00.000Z",
        "voteCount": 2,
        "content": "Key point: The application must continue to serve the users.\nIf we choose A, then it may impact the application.\nC wouldn't have that problem"
      },
      {
        "date": "2024-07-24T01:28:00.000Z",
        "voteCount": 3,
        "content": "c - Amazon Data Lifecycle Manager allows creation of EBS snapshots"
      },
      {
        "date": "2024-07-07T02:09:00.000Z",
        "voteCount": 2,
        "content": "C looks more better than A according to keep application running all time"
      },
      {
        "date": "2024-06-26T08:05:00.000Z",
        "voteCount": 2,
        "content": "currect answer is C\n\nData Lifecycle Manager (DLM)  direct APIs can be used to read the data from the snapshot and copy the data to Amazon S3."
      },
      {
        "date": "2024-06-21T19:41:00.000Z",
        "voteCount": 1,
        "content": "C\nReason\nYou can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs. When you automate snapshot and AMI management, it helps you to:\n\n    Protect valuable data by enforcing a regular backup schedule.\n\n    Create standardized AMIs that can be refreshed at regular intervals.\n\n    Retain backups as required by auditors or internal compliance.\n\n    Reduce storage costs by deleting outdated backups.\n\n    Create disaster recovery backup policies that back up data to isolated Regions or accounts."
      },
      {
        "date": "2024-06-17T07:07:00.000Z",
        "voteCount": 1,
        "content": "Vote D because other than it doesn't mention choose no reboot when creating image, the rest steps cover all the necessities to backup data on ebs to s3. But consider B explicitly mention with reboot option while D not reason to assume D will use no reboot option.\nAnswer C and A have too much assumption that not state in the question and answer. \nA: not sure ssm agent is installed and configure to work with system manager.\nC: missing steps to mount volume on new create ec2 with s3 instance profile attached."
      },
      {
        "date": "2024-06-12T10:39:00.000Z",
        "voteCount": 3,
        "content": "Option A is a manual process where you have to connect via SSM Session manager - too tedious and requires huge manual effort to maintain backups\n\nSo going with C, as you can't manage the snapshot in S3 but you can restore it if anything goes wrong"
      },
      {
        "date": "2024-06-07T11:31:00.000Z",
        "voteCount": 3,
        "content": "One issue with option A is that an ec2 instance with a role granting access to only S3, wouldn\u2019t be registered with the session manager and it won\u2019t be possible to create a session."
      },
      {
        "date": "2024-05-26T08:36:00.000Z",
        "voteCount": 1,
        "content": "I don't see an easy way to copy files to an S3 bucket other the answer A. C copying block data to a bucket is also posible but it's binary data, so not in a easy usable format."
      },
      {
        "date": "2024-05-10T23:30:00.000Z",
        "voteCount": 2,
        "content": "The answer is C.\nOption C: Amazon Data Lifecycle Manager provides an automated, policy-based lifecycle management solution for Amazon Elastic Block Store (EBS) Snapshots and EBS-backed Amazon Machine Images (AMIs). Automate the creation of point-in-time copy of your block storage data with user-defined policies that you can customise based on data protection needs. Amazon Data Lifecycle Manager requires no scripting or special training.\n\nYou can use the Amazon Elastic Block Store (Amazon EBS) direct APIs to create EBS snapshots, write data directly to your snapshots, read data on your snapshots, and identify the differences or changes between two snapshots. These APIs can be used to read the data from the snapshot and copy the data to Amazon S3.\n\nOption A is not correct: Running manual commands on a business-critical instance isn't recommended and DLM can safely take the snapshot without needing to log in to the instance in any way."
      },
      {
        "date": "2024-04-02T08:55:00.000Z",
        "voteCount": 4,
        "content": "Option C:  You can back up the data on your Amazon EBS volumes by making point-in-time copies, known as Amazon EBS snapshot. EBS snapshots are stored in Amazon S3\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-snapshots.html\nAWS DLM - https://docs.aws.amazon.com/ebs/latest/userguide/ebs-creating-snapshot.html"
      },
      {
        "date": "2024-03-23T15:15:00.000Z",
        "voteCount": 5,
        "content": "Answer.. A!\nThis option stands out because it allows secure, keyless access to the EC2 instance without requiring the administrative SSH key pair. By attaching an IAM role with S3 write permissions to the instance, you can use Session Manager to execute data copy commands directly to S3. This method does not disrupt the running application, meeting the requirement for continuous operation."
      },
      {
        "date": "2024-03-17T00:34:00.000Z",
        "voteCount": 3,
        "content": "A meets the requirements by allowing the application team to back up data without interrupting the service and without needing the SSH key pair."
      },
      {
        "date": "2024-03-09T17:30:00.000Z",
        "voteCount": 2,
        "content": "\"A\" seems ok as an option.\n\n\"C\" is wrong because the question asks you to copy the DATA=FILES to S3. You cannot copy the files from a snapshot made by an encrypted volume to S3 bucket."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/95265-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.<br><br>Which combination of steps will successfully copy the data? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bucket policy to allow a user in the destination account to list the source bucket\u2019s contents and read the source bucket\u2019s objects. Attach the bucket policy to the source bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDF",
        "count": 60,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-18T04:09:00.000Z",
        "voteCount": 25,
        "content": "\"The above command should be executed with destination AWS IAM user account credentials only otherwise the copied objects in destination S3 bucket will still have the source account permissions and won\u2019t be accessible by destination account users.\" According to https://medium.com/tensult/copy-s3-bucket-objects-across-aws-accounts-e46c15c4b9e1."
      },
      {
        "date": "2023-01-18T08:45:00.000Z",
        "voteCount": 5,
        "content": "You are correct, step E should be executed using the IAM user credentials from the destination account. This is because when objects are copied from one bucket to another, the object's permissions (ACLs) are also copied. Therefore, if the objects are copied using the IAM user credentials from the source account, the objects will have the same permissions as they did in the source bucket, which may not include permissions for the user in the destination account. By using the IAM user credentials from the destination account, the objects will have the appropriate permissions for the user in the destination account once they are copied."
      },
      {
        "date": "2023-01-18T08:47:00.000Z",
        "voteCount": 16,
        "content": "I switch to BDF;\nStep B is necessary so that the user in the destination account has the necessary permissions to access the source bucket and list its contents, read its objects. \n\nStep D is needed so that the user in the destination account has the necessary permissions to access the destination bucket and list contents, put objects, and set object ACLs\n\nStep F is necessary because the aws s3 sync command needs to be run using the IAM user credentials from the destination account, so that the objects will have the appropriate permissions for the user in the destination account once they are copied.\n\nThe other choices are not correct because :\nA. and C. are about creating policies in the source account but the user who wants to access the data is in the destination account\nE. is about running the command with the source account, which is not suitable because it will lead to copied objects in destination S3 bucket still have the source account permissions and won\u2019t be accessible by destination account users."
      },
      {
        "date": "2024-08-31T03:06:00.000Z",
        "voteCount": 1,
        "content": "B. Create a bucket policy to allow a user in the destination account to list the source bucket\u2019s contents and read the source bucket\u2019s objects. Attach the bucket policy to the source bucket.\nD. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.\nF. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data."
      },
      {
        "date": "2024-02-09T14:58:00.000Z",
        "voteCount": 1,
        "content": "B. Create a bucket policy to allow a user in the destination account to list the source bucket\u2019s contents and read the source bucket\u2019s objects. Attach the bucket policy to the source bucket. This step ensures that the destination account has the necessary permissions to access the data in the source bucket.\nD. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user. This step provides the necessary permissions for a user in the destination account to both access the source bucket\u2019s contents and write to the destination bucket."
      },
      {
        "date": "2024-02-09T14:58:00.000Z",
        "voteCount": 1,
        "content": "F. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data. Performing the sync operation as a user in the destination account, who has been granted the appropriate permissions, ensures that the data can be copied from the source bucket to the destination bucket successfully."
      },
      {
        "date": "2024-01-05T07:37:00.000Z",
        "voteCount": 1,
        "content": "Not A. A bucket policy attached to destination bucket cannot allow the source bucket to execute actions\nNot C. Because we are picking option B which relies on a policy allowing a user in the destination account.\nNot E. Because we are picking options B and D which rely on a user in the destination account"
      },
      {
        "date": "2024-01-02T09:12:00.000Z",
        "voteCount": 1,
        "content": "No need for more explanations, the ones below are enough."
      },
      {
        "date": "2023-11-25T23:45:00.000Z",
        "voteCount": 1,
        "content": "BD:\nhttps://repost.aws/knowledge-center/cross-account-access-s3\nF:\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html"
      },
      {
        "date": "2023-08-31T07:15:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect since a bucket policy cannot allow another bucket to do anything. B. Is however an option since you can indeed create a bucket policy to allow a user in another account to perform operations on the bucket.\n\nOnce you have chosen B, then D and F are the only possible choices."
      },
      {
        "date": "2023-08-17T02:05:00.000Z",
        "voteCount": 1,
        "content": "BCE should also work \nCreate bucket policy at destination bucket to allow permission on source aws user\nCreate IAM policy for source aws user to list/get/put on both buckets\nRun s3 sync command from source bucket to destination bucket"
      },
      {
        "date": "2023-08-04T22:10:00.000Z",
        "voteCount": 1,
        "content": "I prefer BDF, I do not know why the correct answer is ADF"
      },
      {
        "date": "2023-07-06T15:54:00.000Z",
        "voteCount": 2,
        "content": "source bucket: allow destination user + list &amp; get contents permission\ndestination bucket: allow IAM user to get source bucket contents + destination bucket get/list/put objects + aws sync command"
      },
      {
        "date": "2023-07-02T09:53:00.000Z",
        "voteCount": 1,
        "content": "it's BDF for sure"
      },
      {
        "date": "2023-06-20T09:25:00.000Z",
        "voteCount": 2,
        "content": "The entire idea of A is wrong (you achieve nothing by giving rights from one bucket to another) so we start from B and the rest are a common sense"
      },
      {
        "date": "2023-04-14T01:24:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html"
      },
      {
        "date": "2023-02-28T11:05:00.000Z",
        "voteCount": 6,
        "content": "Logical answer : Who ever uploads to a bucket becomes its owner. So A should ring a flaw in it. Similar issue in C. So straight away, A, C are wrong. that points to B,D to be correct. Refer https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-in-one-account-and-region-to-another-account-and-region.html\n\nNow E or F ? the hint is in D. Destination account user has the necessary privileges to get/put objects permission. So choose destination account or run sync/copy commands. So the answer should be B, D , F"
      },
      {
        "date": "2023-02-27T09:01:00.000Z",
        "voteCount": 1,
        "content": "The parts BDF fit together in a way that works. \n\nI  think choosing this direction (pulling from the destination account) is slightly more secure than then  the other other way round(pushing from source to destination) as only read access is granted to the foreign account but no write access - especially regarding human error: one cannot accidentally tamper with the source, so the worst thing that could happen is that one needs to sync again. The other options don't fit together with other parts."
      },
      {
        "date": "2023-01-30T12:42:00.000Z",
        "voteCount": 4,
        "content": "BDF are the answers"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/95268-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-18T08:52:00.000Z",
        "voteCount": 20,
        "content": "A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load is the correct answer as it meets the requirement of supporting a canary release.\n\nOption B is not correct because while it would allow for a canary release, it would involve deploying the new version of the application into a separate CloudFormation stack, which would be a more complex and time-consuming process compared to creating an alias for a new version of the Lambda function.\n\nOption C is not correct because while it would allow for a canary release, it would involve creating a version for every new deployed Lambda function, which would be more complex and time-consuming process compared to creating an alias for a new version of the Lambda function."
      },
      {
        "date": "2023-01-18T08:53:00.000Z",
        "voteCount": 6,
        "content": "Option D is not correct because AWS CodeDeploy is a deployment service that allows you to automate code deployments to a variety of compute services like EC2 and on-premises servers, but it does not support routing configuration for a canary release on AWS Lambda."
      },
      {
        "date": "2023-05-13T08:09:00.000Z",
        "voteCount": 1,
        "content": "Thank you masetromain, you have been really helpful for taking the time and providing explanation."
      },
      {
        "date": "2023-05-15T15:09:00.000Z",
        "voteCount": 8,
        "content": "He copied from chatgpt, you didn't find it ?"
      },
      {
        "date": "2024-01-05T08:16:00.000Z",
        "voteCount": 2,
        "content": "This is not 100% correct. Actually CodeDeploy support deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. You can shift traffic using a canary, linear, or all-at-once deployment configuration. The following lists the predefined configurations available for AWS Lambda canary deployments:\n- CodeDeployDefault.LambdaCanary10Percent5Minutes \n- CodeDeployDefault.LambdaCanary10Percent10Minutes \n- CodeDeployDefault.LambdaCanary10Percent15Minutes\n- CodeDeployDefault.LambdaCanary10Percent30Minutes"
      },
      {
        "date": "2024-08-04T01:25:00.000Z",
        "voteCount": 1,
        "content": "Yeah the reason D is wrong is not because CodeDeploy doesn't support lambda canary deployment, it's because `OneAtATime` deployment strategy is only for EC2 instances but not for lambdas"
      },
      {
        "date": "2023-01-15T20:02:00.000Z",
        "voteCount": 10,
        "content": "https://www.examtopics.com/discussions/amazon/view/28312-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2024-08-31T03:07:00.000Z",
        "voteCount": 1,
        "content": "A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load."
      },
      {
        "date": "2024-07-27T03:27:00.000Z",
        "voteCount": 1,
        "content": "A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load. &nbsp; \n\nExplanation:\nThis option provides a granular level of control for canary deployments:\n\nVersioning: Creating a new version for each deployment ensures that you have a clear record of changes.\nAlias: An alias acts as a stable endpoint, allowing you to gradually shift traffic to the new version.\nRouting configuration: The routing-config parameter provides fine-grained control over traffic distribution between versions.\nBy using this approach, you can gradually increase the percentage of traffic to the new version, monitor its performance, and roll back if necessary, minimizing the impact of potential issues."
      },
      {
        "date": "2024-07-27T03:27:00.000Z",
        "voteCount": 1,
        "content": "Breakdown of other options:\nB: While Route 53 weighted routing can distribute traffic, it's less granular than using Lambda aliases and doesn't provide the same level of control.\nC: Using the update-function-configuration command doesn't provide the flexibility to gradually shift traffic.\nD: CodeDeploy is primarily for deploying code to EC2 instances, not for managing Lambda function traffic.\nBy using Lambda aliases and the routing-config parameter, you can effectively implement a canary release strategy for your Lambda functions."
      },
      {
        "date": "2024-01-05T08:12:00.000Z",
        "voteCount": 1,
        "content": "Not B. This introduces R53 in the scenario, but we are not sure if R53 fits in the scenario. To combine R53 and Lambda we should use function URL that is not mentioned and we don't know if the app is public. A lot of uncertainty here\nNot C. routing-config is an Alias specific configuration aka Weighted Alias and it is not available for the update-function-configuration command https://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-configuration.html\nNot D. CodeDeployDefault.OneAtATime is a CodeDeploy option for EC2/on-premise, while in this scenario we need a canary option for Lambda such as CodeDeployDefault.LambdaCanary10Percent5Minutes\n\nA does the job https://docs.aws.amazon.com/cli/latest/reference/lambda/update-alias.html and https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html#configuring-alias-routing"
      },
      {
        "date": "2023-12-27T21:01:00.000Z",
        "voteCount": 2,
        "content": "100% A is correct. :) I was confused between D and A. But, this url says Codedeployee.AllatOnce deploy option is not for 'canary release'.\nhttps://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/deployment-configurations.html"
      },
      {
        "date": "2023-10-01T12:32:00.000Z",
        "voteCount": 3,
        "content": "Here's why Option A is suitable:\n\nCreate an alias: For every new version of your Lambda function, create an alias. Aliases allow you to associate a user-friendly name with a specific version of the function.\n\nRouting configuration: AWS Lambda supports routing configurations that allow you to gradually shift traffic from one alias to another. Using the \"routing-config\" parameter with the AWS CLI \"update-alias\" command, you can specify how much traffic each alias should receive.\n\nGradual release: By configuring the routing, you can control the percentage of traffic directed to the new version (canary). You can gradually increase the traffic percentage as you gain confidence in the new release. If issues arise, you can quickly roll back by adjusting the routing configuration."
      },
      {
        "date": "2023-07-06T15:56:00.000Z",
        "voteCount": 2,
        "content": "new release-&gt; lambda alias-&gt; update-alias: aws lambda update-alias --function-name my-function --name alias-name --function-version version-number"
      },
      {
        "date": "2023-07-02T10:00:00.000Z",
        "voteCount": 2,
        "content": "D would be an optionn if used Lambda-specific config"
      },
      {
        "date": "2023-06-18T08:52:00.000Z",
        "voteCount": 2,
        "content": "keyword = alias for every new deployed version \nis a classic usage for deployment canary for lambdas other option usually is codeDeploy but in this options AllAtOnce \nthen A"
      },
      {
        "date": "2023-05-05T01:34:00.000Z",
        "voteCount": 1,
        "content": "Sorry OneAtTime"
      },
      {
        "date": "2023-05-05T01:34:00.000Z",
        "voteCount": 1,
        "content": "CodeDeploy: Although CodeDeploy can help but AllAtOnce is not used for canary traffic shifting."
      },
      {
        "date": "2023-02-28T11:45:00.000Z",
        "voteCount": 4,
        "content": "aws update-alias command has routing-config option to route the weighted % traffic\nAs is correct\nhttps://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/\n# Point alias to new version, weighted at 5% (original version at 95% of traffic)\naws lambda update-alias --function-name myfunction --name myalias --routing-config '{\"AdditionalVersionWeights\" : {\"2\" : 0.05} }'"
      },
      {
        "date": "2023-02-10T19:16:00.000Z",
        "voteCount": 4,
        "content": "According to ChatGPT, The \"update-alias\" command is a feature of AWS Lambda service. It is used to update the configuration of a Lambda alias, including the routing configuration which can be used for canary releases, blue/green deployments, and other deployment strategies."
      },
      {
        "date": "2023-06-15T03:37:00.000Z",
        "voteCount": 1,
        "content": "or you know, you could start thinking yourself rather than use glorified rubbish google"
      },
      {
        "date": "2023-06-19T18:11:00.000Z",
        "voteCount": 1,
        "content": "Dont get mad, get Glad."
      },
      {
        "date": "2023-01-15T09:44:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nD does not have routing to distribute load"
      },
      {
        "date": "2023-01-14T06:54:00.000Z",
        "voteCount": 1,
        "content": "AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and on-premises instances. CodeDeploy allows to perform a canary release, which is a technique that releases new versions of software to a small subset of users or systems before releasing it to the entire infrastructure. This makes it possible to test the new version of the software before releasing it to the entire population.\n\nOption A creates an alias for every new deployed version of the Lambda function, but it doesn't include the ability to perform a canary release.\nOption B Deploy the application into a new CloudFormation stack, and use an Amazon Route 53 weighted routing policy to distribute the load, this option can be used for canary release, but it is not the best solution for it.\nOption C creates a version for every new deployed Lambda function, but it does not include the ability to perform a canary release."
      },
      {
        "date": "2023-02-20T10:18:00.000Z",
        "voteCount": 5,
        "content": "You have 2 different answers.....I think it is better you delete this."
      },
      {
        "date": "2023-06-01T00:31:00.000Z",
        "voteCount": 2,
        "content": "he can't.....nobody can delete once posted"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/95272-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.<br><br>What should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 46,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-08T16:32:00.000Z",
        "voteCount": 28,
        "content": "A=ALB cannot be used with SFTP\nB = Correct\nC=Storage Gateway is not an SFTP Server\nD=NLB can be used with SFTP, but EC2 is single"
      },
      {
        "date": "2023-01-14T06:59:00.000Z",
        "voteCount": 12,
        "content": "Option B is the correct answer. Migrating the SFTP server to AWS Transfer for SFTP will improve the reliability and scalability of the SFTP solution. AWS Transfer for SFTP is a fully managed SFTP service that enables the company to transfer files directly into and out of Amazon S3 using the SFTP protocol. By using this service, the company can offload the management of the SFTP server to AWS, which will provide high availability, scalability, and security. The company can then update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname, which will ensure that the SFTP server is reachable on the DNS."
      },
      {
        "date": "2023-01-14T06:59:00.000Z",
        "voteCount": 4,
        "content": "Option A, C and D do not provide the same level of scalability and reliability as AWS Transfer for SFTP. While placing the EC2 instance behind a load balancer can help improve availability, it will not necessarily improve scalability, and it would still require the company to manage the SFTP server. Option C , migrating the SFTP server to a file gateway in AWS Storage Gateway, would not necessarily improve the scalability and reliability of the SFTP solution, as it would still require the company to manage the SFTP server."
      },
      {
        "date": "2023-12-23T20:05:00.000Z",
        "voteCount": 1,
        "content": "How about the cron job?"
      },
      {
        "date": "2024-08-31T03:07:00.000Z",
        "voteCount": 1,
        "content": "B. Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname."
      },
      {
        "date": "2023-07-02T10:10:00.000Z",
        "voteCount": 1,
        "content": "B of course"
      },
      {
        "date": "2023-06-18T09:18:00.000Z",
        "voteCount": 2,
        "content": "keyword = AWS Transfer for SFTP\nthen B"
      },
      {
        "date": "2023-03-26T05:39:00.000Z",
        "voteCount": 3,
        "content": "B is the way to go.."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/95273-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T07:02:00.000Z",
        "voteCount": 14,
        "content": "The correct answer is B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command. This approach allows the solutions architect to export the application as an image in OVF format, which preserves the software and configuration settings, and then import it into Amazon EC2 using the EC2 import command."
      },
      {
        "date": "2024-01-06T14:38:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html"
      },
      {
        "date": "2023-01-14T07:02:00.000Z",
        "voteCount": 8,
        "content": "Option A is incorrect because it uses AWS DataSync and FSx for Windows File Server to replicate the data store, but it doesn't preserve the software and configuration settings of the application.\n\nOption C is incorrect because it uses AWS Storage Gateway to export a CIFS share, but it doesn't preserve the software and configuration settings of the application.\n\nOption D is incorrect because it uses AWS Systems Manager and AWS Backup to create a snapshot of the VM, but it doesn't preserve the software and configuration settings of the application."
      },
      {
        "date": "2024-08-31T03:07:00.000Z",
        "voteCount": 1,
        "content": "B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command."
      },
      {
        "date": "2024-01-05T08:36:00.000Z",
        "voteCount": 2,
        "content": "A = SMB share cannot host VMware datastore. Also, installing agent modify configuration settings\nB = correct\nC = not clear how the backup copy is created and what format is used to allow then creating an AMI from it\nD = hybrid activation allows SSM to manage on-premise / other cloud VM but doesn't enable AWS Backup. This instead requires a backup gateway to backup VMware environment https://aws.amazon.com/blogs/storage/backup-and-restore-on-premises-vmware-virtual-machines-using-aws-backup/"
      },
      {
        "date": "2023-08-16T00:37:00.000Z",
        "voteCount": 3,
        "content": "The only thing that is missing from the B answer is that the OVF file has to be transformed to a OVA file : https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html."
      },
      {
        "date": "2023-08-09T03:24:00.000Z",
        "voteCount": 2,
        "content": "what the B is wrong is that the VM format, should be OVA or VMDK or VHD, not OVF"
      },
      {
        "date": "2023-08-08T02:19:00.000Z",
        "voteCount": 1,
        "content": "I prefer B I do not know why the correct is D."
      },
      {
        "date": "2023-07-02T11:36:00.000Z",
        "voteCount": 1,
        "content": "it's a B"
      },
      {
        "date": "2023-05-05T21:50:00.000Z",
        "voteCount": 3,
        "content": "https://www.learnitguide.net/2023/01/how-to-migrate-vmware-vm-to-aws-ec2.html"
      },
      {
        "date": "2023-08-09T03:23:00.000Z",
        "voteCount": 1,
        "content": "It said the VM fomat is OVA or VMDK, not OVF"
      },
      {
        "date": "2023-04-10T17:36:00.000Z",
        "voteCount": 1,
        "content": "I vote to B. Why the admin has selected D as Answer."
      },
      {
        "date": "2023-03-26T05:41:00.000Z",
        "voteCount": 2,
        "content": "B is the answer - OVF."
      },
      {
        "date": "2023-02-28T15:56:00.000Z",
        "voteCount": 4,
        "content": "Use VM Import/Export. B is correct . https://aws.amazon.com/ec2/vm-import/"
      },
      {
        "date": "2023-02-28T15:58:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html\nPrerequisites\n    Create an Amazon S3 bucket for storing the exported images or choose an existing bucket. The bucket must be in the Region where you want to import your VMs. For more information about S3 buckets, see the Amazon Simple Storage Service User Guide.\n\n    Create an IAM role named vmimport. For more information, see Required service role.\n\nIf you have not already installed the AWS CLI on the computer you'll use to run the import commands, see the AWS Command Line Interface User Guide."
      },
      {
        "date": "2023-01-31T03:11:00.000Z",
        "voteCount": 1,
        "content": "I vote B\nhttps://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/95275-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.<br><br>The application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application\u2019s architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T09:52:00.000Z",
        "voteCount": 24,
        "content": "A: create Docker image and save it to ECR\nB: run this image on Fargate\n\nNo answer should have Lambda the will be time out"
      },
      {
        "date": "2023-01-16T13:30:00.000Z",
        "voteCount": 8,
        "content": "You are correct, both options A and B involve creating a Docker image of the application code and running it on Amazon Elastic Container Service (ECS) using either Fargate or EC2 as the launch type. These options would allow for more control over the resources allocated to the application and potentially prevent timeout errors. Option A is necessary to create the image and store it in a registry, and option B is necessary to run the image on Fargate which is a managed container orchestration service that eliminates the need for provisioning and scaling of the underlying infrastructure."
      },
      {
        "date": "2023-01-16T13:30:00.000Z",
        "voteCount": 14,
        "content": "The correct answer is A and B.\n\nA. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).\n\n- This step is necessary to package the application code in a container and make it available for running on ECS.\n\nB. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.\n\n- This step is necessary to run the containerized application on Fargate, which is a fully managed container orchestration service that eliminates the need to provision and scale the underlying infrastructure."
      },
      {
        "date": "2023-01-16T13:30:00.000Z",
        "voteCount": 6,
        "content": "Option C and E are not correct because they don't address the problem of timeout errors. AWS Step Functions and Amazon Elastic File System (EFS) are services that can be used to coordinate and manage workflows and file storage respectively, but they don't help with the specific problem of the Lambda function timing out.\n\nOption D is not correct because AWS Fargate is a serverless compute engine for containers that eliminates the need for provisioning and scaling the underlying infrastructure.\nIt means that the company does not have to manage the underlying infrastructure, which is what the company wants."
      },
      {
        "date": "2024-08-31T03:08:00.000Z",
        "voteCount": 1,
        "content": "A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).\nB. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3."
      },
      {
        "date": "2024-08-12T02:28:00.000Z",
        "voteCount": 1,
        "content": "To be honest, I don't trust the examtopic answers anymore, we should only rely on most voted ones"
      },
      {
        "date": "2024-03-17T01:01:00.000Z",
        "voteCount": 1,
        "content": "AB, ECR + ECS Margate"
      },
      {
        "date": "2023-12-28T17:53:00.000Z",
        "voteCount": 1,
        "content": "A: create Docker image and save it to ECR\nB: run this image on Fargate"
      },
      {
        "date": "2023-12-15T10:50:00.000Z",
        "voteCount": 1,
        "content": "A: create docker image and store in on ECR\nB: run it on a AWS-managed infrastructure (as required)"
      },
      {
        "date": "2023-10-27T18:56:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A and B. But Lambda function should be replaced with EventBridge."
      },
      {
        "date": "2023-10-16T08:30:00.000Z",
        "voteCount": 1,
        "content": "B - 100%\nC OR E ??"
      },
      {
        "date": "2023-08-08T02:27:00.000Z",
        "voteCount": 1,
        "content": "I think is AB"
      },
      {
        "date": "2023-07-02T11:39:00.000Z",
        "voteCount": 1,
        "content": "it's AB"
      },
      {
        "date": "2023-06-27T11:10:00.000Z",
        "voteCount": 1,
        "content": "AB\nits correct!"
      },
      {
        "date": "2023-06-18T09:24:00.000Z",
        "voteCount": 1,
        "content": "A + B \nA , basic dockerized the aplication and use Elastic Container Register \nB , deploy how serverless with fargate without overhead managament infrastructure"
      },
      {
        "date": "2023-03-26T05:43:00.000Z",
        "voteCount": 2,
        "content": "A + B."
      },
      {
        "date": "2023-03-17T16:31:00.000Z",
        "voteCount": 2,
        "content": "A+B makes sense to me"
      },
      {
        "date": "2023-02-28T19:30:00.000Z",
        "voteCount": 4,
        "content": "Based on Serverless solutions used, need to go with Fargate in combination with either ECS/EC2.As company does not want to manage infra, we go for because Fargate-ECS combo as Fargate-EC2 needs more maintenance .That means D is out. E is obviously out EFS does not contribute to lambda invocation timeouts.\nC is wrong because, increased concurrency (more lambda versions) won't solve timeouts.\nThat leaves A and B as right answers."
      },
      {
        "date": "2023-02-15T07:26:00.000Z",
        "voteCount": 2,
        "content": "C is not right, question clearly said no involve infrastructure, EC2 is a infrastructure, Lamda time out 15 mins."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/95276-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company\u2019s production OU.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom SCP in AWS Control Tower. Apply the SCP to the production OU."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T07:19:00.000Z",
        "voteCount": 19,
        "content": "The correct answer is B. AWS Control Tower provides a set of \"strongly recommended guardrails\" that can be enabled to implement governance and policy enforcement. One of these guardrails is \"Encrypt Amazon RDS instances\" which will detect RDS DB instances that are not encrypted at rest. By enabling this guardrail and applying it to the production OU, the company will be able to enforce encryption for RDS instances in the production environment.\n\nOption A is incorrect because mandatory guardrails are pre-defined by AWS and cannot be customized.\nOption C is incorrect because AWS Config does not provide mandatory guardrails for RDS instances.\nOption D is incorrect because AWS Control Tower does not provide a feature called custom SCP (Service Control Policy), it uses guardrails instead."
      },
      {
        "date": "2023-01-25T07:38:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
      },
      {
        "date": "2023-01-29T13:51:00.000Z",
        "voteCount": 1,
        "content": "The only thing is that this option talks about guardrails, while the article talks about controls, not mandatory."
      },
      {
        "date": "2024-08-31T03:09:00.000Z",
        "voteCount": 1,
        "content": "B. Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU."
      },
      {
        "date": "2024-07-01T04:48:00.000Z",
        "voteCount": 1,
        "content": "The keyword in the question is detect which indicates Config.\n\n \"The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company\u2019s production OU.\""
      },
      {
        "date": "2024-02-10T08:28:00.000Z",
        "voteCount": 1,
        "content": "Option B is correct because AWS Control Tower\u2019s strongly recommended guardrails include checks for best practices and additional security measures that are not enforced by default but are highly recommended. Among these, there is likely a guardrail that can detect unencrypted RDS DB instances, aligning with the company\u2019s requirement. Applying this guardrail to the production OU will ensure that all RDS DB instances in that OU are checked for encryption at rest."
      },
      {
        "date": "2024-01-05T08:49:00.000Z",
        "voteCount": 2,
        "content": "A = Mandatory controls are owned by AWS Control Tower, and they apply by default to every OU on your landing zone and they can't be deactivated\nB = correct https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted\nC = You cannot create new mandatory controls as they are owned by AWS Control Tower\nD = You can create custom SCP in AWS Control Tower as part of the Customizations for AWS Control Tower https://docs.aws.amazon.com/controltower/latest/userguide/cfcn-set-up-custom-scps.html However this requires a lot of work"
      },
      {
        "date": "2024-01-05T08:53:00.000Z",
        "voteCount": 2,
        "content": "Note on D, the question is asking to detect and not to mandate, thus D would not meet requirement"
      },
      {
        "date": "2023-11-13T22:37:00.000Z",
        "voteCount": 1,
        "content": "check  masetromain's comment"
      },
      {
        "date": "2023-07-16T06:45:00.000Z",
        "voteCount": 4,
        "content": "A. No, because mandatory controls are owned by AWS Control Tower, and they apply to every OU on your landing zone. These controls are applied by default when you set up your landing zone, and they can't be deactivated. Moreover, none of them address RDS encrypted at rest. \n\nB. Yes, because Strongly recommended controls are owned by AWS Control Tower. They are based on best practices for well-architected multi-account environments. These controls are not enabled by default, and they can be deactivated through the AWS Control Tower console or the control APIs. Moreover, three of them are RDS detective controls\n\nC. No, because AWS Config does not create mandatory guardrails; AWS Config has managed and custom rules\n\nD. No, because SCPs are created in AWS Orgs and are not designed to detect Amazon RDS DB instances that are not encrypted at rest."
      },
      {
        "date": "2023-07-02T11:41:00.000Z",
        "voteCount": 1,
        "content": "It's. B"
      },
      {
        "date": "2023-06-18T09:29:00.000Z",
        "voteCount": 1,
        "content": "A seems but previous exist rule\nthen B is more apropiate in this case \nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
      },
      {
        "date": "2023-05-31T03:53:00.000Z",
        "voteCount": 2,
        "content": "C - using AWS Config for detective action"
      },
      {
        "date": "2023-04-08T00:56:00.000Z",
        "voteCount": 3,
        "content": "Option B suggests enabling an appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower and applying it to the production OU. While AWS Control Tower provides a set of pre-packaged guardrails that enforce best practices for security, operations, and compliance, there is no guarantee that there is a pre-packaged guardrail specifically for detecting Amazon RDS DB instances that are not encrypted at rest.\n\nIn contrast, option C  creates a custom rule in AWS Config that specifically checks for Amazon RDS DB instances that are not encrypted at rest. This provides more flexibility and control in ensuring that the company\u2019s specific requirement is met."
      },
      {
        "date": "2023-04-14T22:09:00.000Z",
        "voteCount": 1,
        "content": "It's incorrect ideally you only apply to the OU and not to an individual account, therefore this needs to be discounted."
      },
      {
        "date": "2023-03-26T05:45:00.000Z",
        "voteCount": 2,
        "content": "Enable the appropriate guardrail"
      },
      {
        "date": "2023-03-04T16:35:00.000Z",
        "voteCount": 4,
        "content": "Mandatory controls are owned by AWS Control Tower, and they apply to every OU on your landing zone. These controls are applied by default when you set up your landing zone, and they can't be deactivated.\nThe solution requirement falls under a proactive(Recommended Control). \nhttps://docs.aws.amazon.com/controltower/latest/userguide/rds-rules.html#ct-rds-pr-16-description\nOptional controls are OU specific."
      },
      {
        "date": "2023-02-28T20:46:00.000Z",
        "voteCount": 3,
        "content": "Tip - As this detective guardrail is available, answer is B. But if the guardrail is not available in that predefined list, the answer would be --C https://aws.amazon.com/blogs/mt/aws-control-tower-detective-guardrails-as-an-aws-config-conformance-pack/"
      },
      {
        "date": "2023-02-15T07:29:00.000Z",
        "voteCount": 2,
        "content": "question is asking for detection, not mandate"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/95278-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company\u2019s engineers rely heavily on SSH access to the instances for troubleshooting.<br><br>The company\u2019s existing architecture includes the following:<br><br>\u2022\tA VPC with private and public subnets, and a NAT gateway.<br>\u2022\tSite-to-Site VPN for connectivity with the on-premises environment.<br>\u2022\tEC2 security groups with direct SSH access from the on-premises environment.<br><br>The company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.<br><br>Which strategy should a solutions architect use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer\u2019s devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer\u2019s devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T07:24:00.000Z",
        "voteCount": 20,
        "content": "The correct answer is D. This strategy uses IAM roles and AWS Systems Manager to provide secure and auditable SSH access to the instances. The IAM role is attached to all the EC2 instances and has the AmazonSSMManagedInstanceCore managed policy attached, which allows the instances to be managed by Systems Manager. The engineers then install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager. This approach provides secure and auditable access to the instances without the need for IP-based security group rules or additional infrastructure."
      },
      {
        "date": "2023-01-14T07:24:00.000Z",
        "voteCount": 4,
        "content": "Option A uses EC2 Instance Connect to provide secure and auditable SSH access to the instances, but it requires additional infrastructure and configuration. \n\nOption B provides auditing of commands run by the engineers, but it relies on IP-based security group rules, which can be difficult to manage and may not be as secure as using IAM roles. \n\nOption C uses AWS Config and Firewall Manager to automatically remediate changes to security group rules, but it still relies on IP-based security group rules and does not provide an auditable method of access to the instances."
      },
      {
        "date": "2023-01-14T07:25:00.000Z",
        "voteCount": 4,
        "content": "For option A to work, the following additional infrastructure and configuration would be required:\n\nThe EC2 Instance Connect service needs to be enabled in the AWS account and the appropriate IAM permissions would need to be granted to the engineers.\n\nThe EC2 instances would need to have the EC2 Instance Connect agent installed and configured.\n\nThe engineers would need to install the EC2 Instance Connect CLI on their devices and have the necessary credentials to authenticate with AWS.\n\nIn addition, the company would need to update their processes and procedures to ensure that engineers are only using EC2 Instance Connect to access the instances and that all access is being logged and audited."
      },
      {
        "date": "2023-12-21T08:16:00.000Z",
        "voteCount": 2,
        "content": "The key factor is that Option A explains to remove the port 22 inbound SSH access security group, they would need to keep that present: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-prerequisites.html"
      },
      {
        "date": "2023-03-02T12:10:00.000Z",
        "voteCount": 11,
        "content": "A is wrong because Instance connect does not provided auditing\nB is wrong because it mentions OS audit logs. we need to audit SSH trafic\nC is wrong because we want to audit not remediate as asked in question. config service is to record \nusing predefined rules and remediate as well\n\nD is correct because,\nBy attaching the AmazonSSMManagedInstanceCore policy to an IAM role, EC2 instances can be controlled and monitored through the Systems Manager service, enabling capabilities such as remote instance management, patching, and compliance reporting. (ChatGPT response its answers are brief and helpful sometimes)"
      },
      {
        "date": "2024-09-05T20:19:00.000Z",
        "voteCount": 1,
        "content": "The explanation for A is wrong.\nAWS EC2 Instance Connect does support auditing."
      },
      {
        "date": "2024-08-31T03:10:00.000Z",
        "voteCount": 1,
        "content": "D. Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager."
      },
      {
        "date": "2024-03-17T01:07:00.000Z",
        "voteCount": 1,
        "content": "D, use SSM"
      },
      {
        "date": "2024-02-10T08:40:00.000Z",
        "voteCount": 1,
        "content": "Option D is the best strategy because it leverages AWS Systems Manager Session Manager, which allows for secure instance management without the need for SSH access. By attaching an IAM role with the AmazonSSMManagedInstanceCore policy to EC2 instances, engineers can use Session Manager for shell access to instances without needing to open port 22, significantly enhancing security. Session Manager also automatically logs session activity to S3 or CloudWatch Logs, providing the required command auditing capability. This eliminates the need for direct SSH access and offers a centralized, secure, and audited method for engineers to access and run commands on instances."
      },
      {
        "date": "2023-12-23T20:33:00.000Z",
        "voteCount": 1,
        "content": "It required to increase security around ssh access, why so many people voted on D?"
      },
      {
        "date": "2024-02-28T10:49:00.000Z",
        "voteCount": 1,
        "content": "Cloudwatch agent does not provide auditable logs for SSH sessions; it only provides metrics about CPU/Memory/Network Packets/etc; nothing about what user started session at what time and ran certain trackable API calls while in that session."
      },
      {
        "date": "2023-10-16T19:24:00.000Z",
        "voteCount": 2,
        "content": "The answer is D. Option A is wrong because EC2 Instance Connect requires the host security group to permit SSH traffic inbound. https://repost.aws/questions/QUnV4R9EoeSdW0GT3cKBUR7w/what-is-the-difference-between-ec2-instance-connect-and-session-manager-ssh-connections"
      },
      {
        "date": "2023-07-02T11:44:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2023-06-18T09:31:00.000Z",
        "voteCount": 1,
        "content": "keyword = AWS Systems Manager Session Manager\nthen D"
      },
      {
        "date": "2023-03-26T05:47:00.000Z",
        "voteCount": 2,
        "content": "D for sure."
      },
      {
        "date": "2023-03-04T16:43:00.000Z",
        "voteCount": 2,
        "content": "Why its NOT A\nTo connect using the Amazon EC2 console, the instance must have a public IPv4 address.\n\nIf the instance does not have a public IP address, you can connect to the instance over a private network using an SSH client or the EC2 Instance Connect CLI. For example, you can connect from within the same VPC or through a VPN connection, transit gateway, or AWS Direct Connect.\n\nEC2 Instance Connect does not support connecting using an IPv6 address.\ngoing with D:"
      },
      {
        "date": "2023-02-19T17:36:00.000Z",
        "voteCount": 2,
        "content": "Need to be able to audit the commands ran on the machine."
      },
      {
        "date": "2023-02-15T14:34:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why it can't be A for this one. Why is AWS Systems Manager Session better than EC2 Instance Connect? They both require installing something on the instances."
      },
      {
        "date": "2023-02-19T17:35:00.000Z",
        "voteCount": 1,
        "content": "Could option A audit the commands ran on the server, as required by the question? I knew D certainly can."
      },
      {
        "date": "2023-03-04T13:23:00.000Z",
        "voteCount": 4,
        "content": "For EC2 instance connect there are a few requirements:\n- instance has public IP (the instances in question are private)\n- you have port 22 open (A says remove port 22 inbound)"
      },
      {
        "date": "2023-02-10T20:09:00.000Z",
        "voteCount": 2,
        "content": "According to ChatGPT,\n\nYes, AWS Systems Manager Session Manager can track the commands that are executed during a session. The session is recorded in the form of a log, which can be accessed and reviewed later. The log contains information such as the start time, end time, and the user who initiated the session, as well as a record of all the commands executed during the session, including their output and exit codes. This information can be useful for auditing purposes, troubleshooting, and compliance reporting."
      },
      {
        "date": "2023-02-08T16:52:00.000Z",
        "voteCount": 3,
        "content": "provide auditing of commands run by the engineers  = B Only"
      },
      {
        "date": "2023-08-28T19:49:00.000Z",
        "voteCount": 1,
        "content": "Read docs you can audit command using SSM https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html"
      },
      {
        "date": "2023-10-09T02:49:00.000Z",
        "voteCount": 1,
        "content": "\"In addition to providing information about current and completed sessions in the Systems Manager console, Session Manager provides you with the ability to audit session activity in your AWS account using AWS CloudTrail\"\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-auditing.html\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-auditing.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/amazon/view/95284-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Budgets to create a fixed monthly budget for each developer\u2019s account as part of the account creation process.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 76,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-16T15:46:00.000Z",
        "voteCount": 18,
        "content": "Clear - BCF - SCP is preferable over IAM"
      },
      {
        "date": "2023-02-24T10:24:00.000Z",
        "voteCount": 16,
        "content": "I prefer D over C as IAM cant be applied to Account"
      },
      {
        "date": "2024-09-12T11:20:00.000Z",
        "voteCount": 1,
        "content": "Option D says apply it to the Developers accounts. Unnecessary operational overhead"
      },
      {
        "date": "2024-08-31T03:10:00.000Z",
        "voteCount": 1,
        "content": "B. Use AWS Budgets to create a fixed monthly budget for each developer\u2019s account as part of the account creation process.\nC. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.\nF. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services."
      },
      {
        "date": "2024-08-12T02:44:00.000Z",
        "voteCount": 2,
        "content": "Why Option C is Preferred to D :\nCentralized Control: SCPs provide a centralized way to manage permissions across all accounts in an organization, ensuring consistent enforcement of policies.\nScalability: SCPs are easier to manage and scale when dealing with multiple accounts, as changes to the SCP will automatically apply to all accounts under the organization.\nCompliance: SCPs help ensure compliance with organizational policies by preventing the use of restricted services across all accounts."
      },
      {
        "date": "2024-03-17T01:11:00.000Z",
        "voteCount": 2,
        "content": "BCF - SCP, budget and custom lambda to terminate services"
      },
      {
        "date": "2024-03-16T00:19:00.000Z",
        "voteCount": 1,
        "content": "BDF \ncannot apply scp in account, need to apply it in OU"
      },
      {
        "date": "2024-08-12T02:41:00.000Z",
        "voteCount": 1,
        "content": "wrong, you can apply scp to an account"
      },
      {
        "date": "2024-02-10T08:54:00.000Z",
        "voteCount": 2,
        "content": "B. Use AWS Budgets to create a fixed monthly budget for each developer\u2019s account as part of the account creation process. AWS Budgets allows you to set custom cost and usage budgets that alert you when you exceed your thresholds.\nC. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts. By creating an SCP that specifically denies access to costly AWS services, the company can prevent developers from launching such services, thereby helping to keep costs within the fixed monthly budget.\nF. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services. While AWS Budgets cannot directly terminate services when a budget is exceeded, you can configure an alert to trigger a notification. This notification can then invoke a Lambda function designed to assess and terminate services as necessary, based on the company\u2019s policies."
      },
      {
        "date": "2024-01-25T08:59:00.000Z",
        "voteCount": 1,
        "content": "Setting a monthly cost budget with a variable target amount, with each subsequent month growing the budget target by 5 percent. Then, you can configure your notifications for 80 percent of your budgeted amount and apply an action. For example, you could automatically apply a custom IAM policy that denies you the ability to provision additional resources within an account.\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\nans :bdf"
      },
      {
        "date": "2024-01-06T07:08:00.000Z",
        "voteCount": 2,
        "content": "A = SCP is used to limit permission that administrator can grant IAM users/roles, SCP cannot set a fixed monthly account usage limit\nB = correct\nC = correct\nD = it could work, but it would required more work wrt SCP\nE = Budget actions cannot terminate all kind of services, actually supports 3 types of actions 1/ apply IAM policy to IAM identities, 2/ apply SCP to an OU and 3/ terminate EC2 and RDS instances\nF = correct"
      },
      {
        "date": "2024-01-02T10:09:00.000Z",
        "voteCount": 1,
        "content": "Although, C is correct, some people here says that SCP cannot be attached to an account, but it is not true, you can, the most common option when we want to deny permissions to an account is to use an IAM policy."
      },
      {
        "date": "2023-10-09T03:13:00.000Z",
        "voteCount": 1,
        "content": "BCF. \nIn Option D, we can not apply IAM policy to an AWS Account."
      },
      {
        "date": "2023-08-17T22:54:00.000Z",
        "voteCount": 4,
        "content": "I'd go with BDF, since there's no mention of OU. As a rule of thumb, IAM policies to restrict are applied on Accounts, Users, Groups and SCP's on OU's."
      },
      {
        "date": "2023-08-21T05:10:00.000Z",
        "voteCount": 1,
        "content": "IAM policies for user ? https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies-overview.html"
      },
      {
        "date": "2023-08-21T05:45:00.000Z",
        "voteCount": 1,
        "content": "Sorry I mistake, IAM policies can applied on User."
      },
      {
        "date": "2023-08-08T03:39:00.000Z",
        "voteCount": 2,
        "content": "BCF is right.\nI think SCP is more convenient than iam.\nYou need to config the IAM to all account manually"
      },
      {
        "date": "2023-07-19T18:03:00.000Z",
        "voteCount": 2,
        "content": "prefer SCP over IAm in org accounts"
      },
      {
        "date": "2023-07-02T11:47:00.000Z",
        "voteCount": 2,
        "content": "It's a BCF"
      },
      {
        "date": "2023-06-20T22:56:00.000Z",
        "voteCount": 2,
        "content": "C - SCP would be prefer to control the services could be used in Organization's AWS accounts."
      },
      {
        "date": "2023-06-18T09:33:00.000Z",
        "voteCount": 2,
        "content": "Clear - BCF - SCP is preferable over IAM"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/amazon/view/95285-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora.<br><br>The company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 66,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T08:48:00.000Z",
        "voteCount": 21,
        "content": "The correct answer is option B. This solution uses a combination of AWS Resource Access Manager (RAM) and automated backups to migrate the Lambda functions and the Aurora database to the Target account while minimizing downtime.\n\nIn this solution, the Lambda function deployment package is downloaded from the Source account and used to create new Lambda functions in the Target account. The Aurora DB cluster is shared with the Target account using AWS RAM and the Target account is granted permission to clone the Aurora DB cluster, allowing for a new copy of the Aurora database to be created in the Target account. This approach allows for the data to be migrated to the Target account while minimizing downtime, as the Target account can use the cloned Aurora database while the original Aurora database continues to be used in the Source account."
      },
      {
        "date": "2023-01-14T08:48:00.000Z",
        "voteCount": 2,
        "content": "Option A is not the best solution because it doesn't share the Aurora DB cluster with the Target account and this would cause data inconsistencies as the Source and Target accounts would not share the same data.\n\nOption C is not the best solution because, it does not specify how the data will be migrated and it would cause downtime as the Source and Target accounts are not sharing the same data.\n\nOption D is not the best solution because it does not specify how the Lambda function will be migrated and it would cause data inconsistencies as the Source and Target accounts are not sharing the same data."
      },
      {
        "date": "2023-07-02T03:06:00.000Z",
        "voteCount": 3,
        "content": "For option A, its also not possible because automated snapshots cannot be shared..\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-share-snapshot.html"
      },
      {
        "date": "2023-08-21T22:29:00.000Z",
        "voteCount": 16,
        "content": "AWS Resource Access Manager (RAM) can only share the follow services:\n\uf06e\tAmazon Aurora \u2013 DB clusters\n\uf06e\tAmazon EC2 \u2013 capacity reservations and dedicated hosts\n\uf06e\tAWS License Manager \u2013 License configurations\n\uf06e\tAWS Outposts \u2013 Local gateway route tables, outposts, and sites\n\uf06e\tAmazon Route 53 \u2013 Forwarding rules\n\uf06e\tAmazon VPC \u2013 Customer-owned IPv4 addresses, prefix lists, subnets, traffic mirror targets, transit gateways, transit gateway multicast domains\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html"
      },
      {
        "date": "2024-08-31T03:11:00.000Z",
        "voteCount": 1,
        "content": "B. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster."
      },
      {
        "date": "2024-03-03T08:48:00.000Z",
        "voteCount": 2,
        "content": "A is viable, but as AWS RAM can share Aurora clusters, B is faster. However, AWS RAM can't share lambdas, so C and D are out."
      },
      {
        "date": "2024-03-03T08:43:00.000Z",
        "voteCount": 1,
        "content": "B, C, and D are all out since AWS RAM cannot share either Lambdas or Aurora DB clusters. A is the only viable one - you must use a manual shapshot for the DB, share it, and redeploy any deployment package in the destination account. (The question tries to trip you up by its wording: lambda deployments can't be downloaded, but the same deployment packages used to deploy the lambdas can, for instance from S3 or from source)"
      },
      {
        "date": "2024-02-10T09:37:00.000Z",
        "voteCount": 2,
        "content": "Option B is the most accurate and efficient solution based on this AWS article content (https://aws.amazon.com/about-aws/whats-new/2019/07/amazon_aurora_supportscloningacrossawsaccounts-/). It correctly outlines the steps for Lambda migration and utilizes the Aurora DB cluster cloning feature across accounts via AWS RAM, which aligns with the article\u2019s description. This approach ensures minimal downtime and efficient migration by allowing direct cloning of the Aurora database.\n\nOption C incorrectly suggests using AWS RAM to share Lambda functions, which is not supported yet based on latest sharable AWS resources: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html"
      },
      {
        "date": "2024-01-22T23:18:00.000Z",
        "voteCount": 1,
        "content": "AWS Resource Access Manager (RAM) to share AWS Lambda functions and Aurora DB clusters with another AWS account. AWS RAM allows you to share resources that are created and managed by other AWS services with individual AWS accounts or with the accounts in an organization or organizational units (OUs) in AWS Organizations.\n\nTo share a Lambda function with another AWS account, you can delegate access to an IAM user (or all users) in the other AWS account so that they can assume a role in your account and invoke the Lambda function in your account.\n\nTo share an Aurora DB cluster with another AWS account, you can create a resource share in AWS RAM and specify the Amazon Resource Name (ARN) of the Aurora DB cluster as the resource to share. You can then specify the AWS account IDs of the accounts with which you want to share the resource."
      },
      {
        "date": "2024-01-06T07:39:00.000Z",
        "voteCount": 1,
        "content": "A = you can share snapshot to restore DB, but this will introduce some downtime\nB = correct (cloning a DB allows for very limited downtime)\nC = if you only share Lambda you are not migrating it, also it appears the Lambda is not a RAM sharable resource https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html\nD = it appears the Lambda is not a RAM sharable resource and you cannot directly share an automated snapshot, you need first to create a manual snapshot by copying the automated snapshot, and then share that copy https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-share-snapshot.html"
      },
      {
        "date": "2024-01-06T07:41:00.000Z",
        "voteCount": 1,
        "content": "A is not correct as you cannot directly share an automated snapshot, you need first to create a manual snapshot by copying the automated snapshot, and then share that copy https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-share-snapshot.html"
      },
      {
        "date": "2024-01-01T21:57:00.000Z",
        "voteCount": 2,
        "content": "There is limit on the number of resources you can share with AWS RAM.\nAWS RAM does not support direct sharing of Lambda functions between accounts.\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html"
      },
      {
        "date": "2023-07-02T11:52:00.000Z",
        "voteCount": 2,
        "content": "it's B. \nIn A - automated snapshots are not shareable"
      },
      {
        "date": "2023-06-20T11:32:00.000Z",
        "voteCount": 1,
        "content": "Option B minimizes downtime, compared to A, where we only share a snapshot of the cluster. For C we do not migrate the lambdas, we just share them, which is not the idea of the exercise."
      },
      {
        "date": "2023-06-18T09:35:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is option B. This solution uses a combination of AWS Resource Access Manager (RAM) and automated backups to migrate the Lambda functions and the Aurora database to the Target account while minimizing downtime."
      },
      {
        "date": "2023-06-18T09:35:00.000Z",
        "voteCount": 2,
        "content": "in case the letter A use only snapshot not sync the complete data and is posible lost data in the process"
      },
      {
        "date": "2023-06-15T04:14:00.000Z",
        "voteCount": 1,
        "content": "They just want to migrate the Lambda and Aurora DB, they dont care about the app itself"
      },
      {
        "date": "2023-05-07T10:40:00.000Z",
        "voteCount": 3,
        "content": "The question is about migration and not sharing, so the answer is how to use a RAM feature to help you on the migration. In option D they are not migrating anything, both Lambda and Aurora are being shared with the Target account and not migrated. In option C is a similar situation, the Lambda is not being migrated. Option A seems a good option but might cause a larger downtime. Hence option D is more appropriate because you can use the cluster share with the Target account and clone the database cluster into it. In my view this answer should contemplate in which moment the cutoff from Source to Target would occur."
      },
      {
        "date": "2023-04-02T18:49:00.000Z",
        "voteCount": 2,
        "content": "You can share the following Amazon Aurora resources by using AWS RAM."
      },
      {
        "date": "2023-03-26T05:55:00.000Z",
        "voteCount": 2,
        "content": "B is the way forward"
      },
      {
        "date": "2023-03-02T13:20:00.000Z",
        "voteCount": 4,
        "content": "AWS RAM can share ec2 instances,  lambdas, DB clusters, RDS, event Redshift clusters.\nRefer AWS SA video here - https://www.youtube.com/watch?v=KL9SICG52zY\nIf company would not have had critical data, answer C is good. as existing app should not be down, we have to download lambda and then share. so answer is B. other wise you can stop app and share with RAM (Resource shares)"
      },
      {
        "date": "2023-04-13T10:57:00.000Z",
        "voteCount": 2,
        "content": "However, if on migration the AWS RMS already will stop the Aurora I don\u00b4t see a problem use this window to migrate Lambda also?"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/amazon/view/95287-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed.<br><br>The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T08:55:00.000Z",
        "voteCount": 21,
        "content": "The correct answer is A, migrating the data processing script to an AWS Lambda function and using an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects. This solution meets the company's requirements of high availability and scalability, as well as reducing long-term management overhead, and is likely to be the most cost-effective option.\n\nOption B involves creating an SQS queue and configuring S3 to send event notifications to it. The data processing script would then poll the SQS queue and process the S3 objects that the SQS message identifies. While this option also provides high availability and scalability, it is less cost-effective than using Lambda, as it requires additional resources such as an SQS queue and an EC2 Auto Scaling group."
      },
      {
        "date": "2023-09-04T18:15:00.000Z",
        "voteCount": 2,
        "content": "Agree. Also, it says the company does not wanna manage long-term overhead, which points to serverless."
      },
      {
        "date": "2023-10-11T13:35:00.000Z",
        "voteCount": 1,
        "content": "SQS is out of the question because the script already has a built in logic that will prevent it to reprocess a message that's already been processed"
      },
      {
        "date": "2023-01-14T08:55:00.000Z",
        "voteCount": 5,
        "content": "Option C, migrating the data processing script to a container image and running it on an EC2 instance, would still require the company to manage the underlying EC2 instances and may not be as cost-effective as using Lambda.\n\nOption D, migrating the data processing script to a container image that runs on Amazon ECS on AWS Fargate, would still require the company to manage the underlying infrastructure and may not be as cost-effective as using Lambda. Additionally, it introduces additional complexity by adding a Lambda function that calls the Fargate RunTask API operation."
      },
      {
        "date": "2024-03-27T03:49:00.000Z",
        "voteCount": 1,
        "content": "ECS in Fargate mode you don't need to manage anything underling infra!\nYou're totally forgot about cost, for sure running an ECS Fargate has lower cost than running a Lambda for 5 minutes every 10 minutes!\nAlso the function to trigger the ECS workload (in option D), running for milliseconds (as need only to notify the doc upload in S3), so it's more correct the D answer.\nAsk to any Gen AI model, you will have mine answer with more details :)"
      },
      {
        "date": "2024-07-19T12:17:00.000Z",
        "voteCount": 1,
        "content": "Use an S3 event notification to invoke the Lambda function to process the objects"
      },
      {
        "date": "2023-01-15T12:06:00.000Z",
        "voteCount": 7,
        "content": "A is correct, it provide HA, scale, less management. Task only need 5 minutes\nB: enen more complex\nC: container still run on one EC2, not scale\nd: need container, Farget and Lambda. Complex than A"
      },
      {
        "date": "2024-08-31T03:11:00.000Z",
        "voteCount": 1,
        "content": "A. Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects."
      },
      {
        "date": "2024-08-12T04:37:00.000Z",
        "voteCount": 1,
        "content": "instead of scheduled 10 min with multiple files processing (each takes 5 minutes) it will be event driven with lambda each time a file is uploaded --&gt; Answer A"
      },
      {
        "date": "2024-07-18T07:05:00.000Z",
        "voteCount": 2,
        "content": "A, the company wants to reduce management overhead not costs, we should stay with the question requirement, it doesn't said anything about cost, probably D will be cheaper but  the solution must resolve the question necessity and is reduce long-term management overhead"
      },
      {
        "date": "2024-06-17T14:21:00.000Z",
        "voteCount": 1,
        "content": "I vote D because the service is from EC2 to lambda and work is processing data. Without given how big is the data we can't assume that the data is always below the lambda ephemeral storage limit 0.5GB. Nowadays, a file can easily break 0.5GB.\nWhile D is still EC2 based so whatever previous EC2 can do farget can do as well."
      },
      {
        "date": "2024-06-12T16:40:00.000Z",
        "voteCount": 2,
        "content": "The answer is A:\n\nAWS Pricing Calculator \n(using:\n10,000 request per month,\n300,000 ms which = 5 minutes\n128 MB of Memory\n512 MB of Storage\n)\n\nAmount of memory allocated: 128 MB x 0.0009765625 GB in a MB = 0.125 GB\nAmount of ephemeral storage allocated: 512 MB x 0.0009765625 GB in a MB = 0.5 GB\nPricing calculations\n10,000 requests x 300,000 ms x 0.001 ms to sec conversion factor = 3,000,000.00 total compute (seconds)\n0.125 GB x 3,000,000.00 seconds = 375,000.00 total compute (GB-s)\n375,000.00 GB-s x 0.0000166667 USD = 6.25 USD (monthly compute charges)\n10,000 requests x 0.0000002 USD = 0.00 USD (monthly request charges)\n0.50 GB - 0.5 GB (no additional charge) = 0.00 GB billable ephemeral storage per function\nLambda costs - Without Free Tier (monthly): 6.25 USD\n\nFor those thinking D is the cheaper option, do you really believe ECS Fargate would be cheaper?"
      },
      {
        "date": "2024-03-27T03:45:00.000Z",
        "voteCount": 2,
        "content": "Ok i was thinking between A and D.\nI'm pretty sure which is D our answer, see the details.\n\nThe requirements are:\n- COST as much as possible low\n- OPERATIONS as much as possible managed.\n\nSo at the first reading, the A option seems to be the correct option (because it's totally AWS managed), but here we're totally forgot the cost.\nRunning a Lambda function, for 5 minutes every 10 minutes, it's very very more expensive than a simple ECS task running continously.\n\nFinally, ECS in fargate mode is totally AWS managed, so we will have lower cost, and a serverless and HA environment, which auto-scale if we need more processing at time.\n\nFor me, option D is the correct answer."
      },
      {
        "date": "2024-03-17T01:19:00.000Z",
        "voteCount": 1,
        "content": "A, use lambda function is much cost-effective than use ECS Margate"
      },
      {
        "date": "2024-02-10T09:49:00.000Z",
        "voteCount": 1,
        "content": "Option A is the most cost-effective and efficient solution. AWS Lambda allows for running code in response to triggers such as S3 event notifications without the need to manage servers, thereby directly addressing the requirement to reduce long-term management overhead. Since the script is only needed when new files are uploaded and takes about 5 minutes to process each file, Lambda\u2019s ability to scale automatically and its billing model based on actual compute time used make it an ideal solution. Lambda can process files immediately upon upload, maximizing efficiency and minimizing idle time.\nOption D proposes using Amazon ECS on AWS Fargate with Lambda to trigger tasks. This solution introduces container orchestration, which can improve scalability and reduce some management overhead. However, it is not as cost-effective as directly invoking a Lambda function to process files, considering the lightweight nature of the task and the added complexity of managing container orchestration and Lambda functions together."
      },
      {
        "date": "2024-02-01T10:31:00.000Z",
        "voteCount": 2,
        "content": "100% the answer is D. \n5 minutes to process EACH FILE? And the EC2 instance is processing files 60% of the time? \nLambda would be crazy expensive in this scenario. ECS/Fargate = cheaper for sure. See link in @covabix879 comment for proof of this.\n\nGreyeye said something rather ridiculous: \"If you get 1000 images, you will see 1000 tasks. That is not economical or cheap.\"\n\nHow can 1x EC2 instance running a script every 10 minutes process 1000 images with each one taking 5 minutes? Even if the script processed images in parallel, e.g. one image per vCPU at a time, that instance would need 500 vCPUs! For the EC2 instance to be idle 40% of the time, it would need 833 vCPUs. That's ridiculous. \n\nBut even if 1000 images suddenly appeared, the Lambda solution would still result in 1000 Lambdas all firing and running for 5 minutes each. Which is going to be more expensive than ECS/Fargate."
      },
      {
        "date": "2024-01-12T00:55:00.000Z",
        "voteCount": 1,
        "content": "A = correct\nB = does not reduce long-term management overhead\nC = does not reduce long-term management overhead\nD = does not reduce long-term management overhead\n\nNote: D is a cheap options as mentioned by other here below could be cheaper than A. However, in addition to maintaining the script code it requires to maintain the container image and the lambda"
      },
      {
        "date": "2023-11-13T22:57:00.000Z",
        "voteCount": 1,
        "content": "in the real world it might be D, but with provided details and keeping in mind lambda retries in case of A, I would vote for A."
      },
      {
        "date": "2023-10-25T10:38:00.000Z",
        "voteCount": 1,
        "content": "D is more complex and overload for administration. Hence Vote for A"
      },
      {
        "date": "2023-10-01T05:27:00.000Z",
        "voteCount": 5,
        "content": "https://blogs.perficient.com/2021/06/17/aws-cost-analysis-comparing-lambda-ec2-fargate/ Even Fargate running continuosly is cheaper than Lambda running half of the time. So long running work load not cost effective with Lambda ( Every 10 minutes run for 5 minutes. So half of the time lambda is running) Therefore Fargate is the most cost-effective solution."
      },
      {
        "date": "2023-09-10T13:00:00.000Z",
        "voteCount": 2,
        "content": "running lambda for 5 minutes is not cost effective, so answer is D"
      },
      {
        "date": "2023-08-19T03:51:00.000Z",
        "voteCount": 1,
        "content": "I vote A\n\nD will invoke a new Fargate task per every PUT command. \nIf you get 1000 images, you will see 1000 tasks. That is not economical or cheap.\n\nif D was invoking a new task by other means like EventBridge, this would have been a lot cheaper."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/amazon/view/95288-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch the application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALSet up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALEnable health checks to ensure high availability between Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. Create an Amazon Route 53 hosted zone. Create a record for the ALB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T09:02:00.000Z",
        "voteCount": 19,
        "content": "The correct answer is C. Choice C meets the requirements for the application to be highly available and to dynamically scale to meet user traffic, as well as implementing a disaster recovery environment in the us-west-1 Region through active-passive failover.\n\nIn choice C, the company creates a VPC in us-east-1 and a VPC in us-west-1, and sets up an Application Load Balancer (ALB) and Auto Scaling group in both VPCs. The ALB extends across multiple Availability Zones in each VPC, and the Auto Scaling group deploys the EC2 instances across these Availability Zones. The Auto Scaling group is placed behind the ALB, which allows for automatic scaling of the instances to meet user traffic.\n\nAn Amazon Route 53 hosted zone is also created, with separate records for each ALB. Health checks are enabled for each record, and a failover routing policy is configured. This allows for active-passive failover between the two regions, ensuring high availability for the application."
      },
      {
        "date": "2023-01-14T09:02:00.000Z",
        "voteCount": 6,
        "content": "Choice A, B, and D do not fully meet the requirements of the disaster recovery environment in the us-west-1 Region and the failover routing policy because they do not include the necessary configurations for active-passive failover.\n\nIn choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.\n\nChoice B, the VPCs in us-east-1 and us-west-1 are separate, and the configuration is replicated in both regions but there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage."
      },
      {
        "date": "2023-01-14T09:03:00.000Z",
        "voteCount": 6,
        "content": "Choice D is similar to choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.\n\nChoice C is the correct answer as it includes all the necessary components for a disaster recovery environment in the us-west-1 region. It creates separate VPCs, Application Load Balancer, and Auto Scaling Group in both regions, and it enables health checks and configure a failover routing policy for each record. This ensures that in the event of an outage, the application can automatically failover to the us-west-1 region with minimal downtime."
      },
      {
        "date": "2023-01-30T13:33:00.000Z",
        "voteCount": 5,
        "content": "active-passive failover==&gt;a failover routing policy within route 53"
      },
      {
        "date": "2024-08-31T03:12:00.000Z",
        "voteCount": 1,
        "content": "C. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record."
      },
      {
        "date": "2023-07-02T11:59:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2023-03-26T06:00:00.000Z",
        "voteCount": 2,
        "content": "C for DR"
      },
      {
        "date": "2023-03-02T15:59:00.000Z",
        "voteCount": 5,
        "content": "Active-Passive failover with primary and secondary records in Route53\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\nhttps://d1tcczg8b21j1t.cloudfront.net/strapi-assets/32_Route_53_health_checks_4_64165fc533.png"
      },
      {
        "date": "2023-03-02T16:03:00.000Z",
        "voteCount": 3,
        "content": "VPC Peering is good for fully accessing all resources in a shared env but thats not asked here, so A and D gets eliminated. B does not mention the weighted routing config enable ment although setup is good. So answer is C"
      },
      {
        "date": "2023-01-15T12:07:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/amazon/view/95289-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the company could improve specifically in terms of access to the AWS Management Console. The company\u2019s IT support workers currently access the console for administrative tasks, authenticating with named IAM users that have been mapped to their job role.<br><br>The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console by using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement this functionality.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD Connector to connect to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company\u2019s Active Directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company\u2019s Active Directory.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 79,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T13:35:00.000Z",
        "voteCount": 22,
        "content": "https://www.examtopics.com/discussions/amazon/view/69172-exam-aws-certified-solutions-architect-professional-topic-1/\n\nYou are correct, I apologize for the oversight. To meet the requirements of the IT support workers, option D would be the correct solution:\n\nThis option will first enable all features in AWS Organizations, then create and configure an AD Connector to connect to the company's on-premises Active Directory. Then, it will configure IAM Identity Center (AWS SSO) and set the AD Connector as the identity source, allowing the IT support workers to access the console using their existing Active Directory credentials. Finally, it will create permission sets and map them to the existing groups within the company's Active Directory. This solution will also be cost-effective as it does not involve creating a new directory in AWS Directory Service."
      },
      {
        "date": "2023-03-18T15:05:00.000Z",
        "voteCount": 17,
        "content": "D is the correct answer.. B is wrong answer \n\nFrom aws documentation:\nQ: Which AWS accounts can I connect to IAM Identity Center?\n\nYou can add any AWS account managed using AWS Organizations to IAM Identity Center. You need to enable all features in your organizations to manage your accounts single sign-on."
      },
      {
        "date": "2023-12-27T10:43:00.000Z",
        "voteCount": 1,
        "content": "Source: https://aws.amazon.com/iam/identity-center/faqs/#product-faqs#iam-identity-center-faqs#identity-sources-and-applications-support"
      },
      {
        "date": "2024-08-31T03:13:00.000Z",
        "voteCount": 1,
        "content": "D. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company\u2019s Active Directory."
      },
      {
        "date": "2024-03-17T01:32:00.000Z",
        "voteCount": 1,
        "content": "B, Turn on the IAM Identity Center feature in Organizations... similar to D, but without enabling directy the SSO, you can't configure it..."
      },
      {
        "date": "2024-09-04T01:41:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-02-10T10:18:00.000Z",
        "voteCount": 1,
        "content": "Option D is the best because AWS FAQs asked the following question and answered: \"Which AWS accounts can I connect to IAM Identity Center?\nYou can add any AWS account managed using AWS Organizations to IAM Identity Center. You need to enable all features in your organizations to manage your accounts single sign-on.\" Link: https://aws.amazon.com/iam/identity-center/faqs/#product-faqs#iam-identity-center-faqs#identity-sources-and-applications-support.\nWith the clarification that enabling all features in AWS Organizations is necessary for integrating with IAM Identity Center, Option D becomes the most accurate and compliant solution. It correctly combines the need to enable all features in AWS Organizations with the use of an AD Connector for a direct connection to the company\u2019s on-premises Active Directory, which remains the most cost-effective way to leverage existing Active Directory credentials for AWS console access."
      },
      {
        "date": "2024-02-01T10:43:00.000Z",
        "voteCount": 1,
        "content": "Most cost effective is D.\n\nBut C is also technically a valid solution that meets all the other requirements. A two way trust means AD users in the on-premise AD can be added to AD groups in the AWS-managed AD."
      },
      {
        "date": "2024-01-14T03:00:00.000Z",
        "voteCount": 3,
        "content": "A = you do not turn on AWS IdC feature only in AWS Orgs. It is either Consolidation billing or All features\nB = same as above\nC = requirements is to login users based on-premise AD, for this there is no need to AWS Managed AD with a local domain/directory and 2-way trust. AD Connector is enough and cheaper\nD = correct"
      },
      {
        "date": "2024-01-05T11:24:00.000Z",
        "voteCount": 1,
        "content": "I love such questions, while both B and D seems reasonable, I thinking more about B because of this https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html"
      },
      {
        "date": "2023-12-17T10:40:00.000Z",
        "voteCount": 3,
        "content": "you can absolutely use an AD Connector as the identity source for AWS IAM Identity Center without turning on all features in your AWS organization. In fact, it's the most cost-effective and recommended approach if you only need single sign-on functionality with your existing on-premises Active Directory"
      },
      {
        "date": "2023-11-23T05:28:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/prereq-orgs.html\n\n```If you've already set up AWS Organizations and are going to add IAM Identity Center to your organization, make sure that all AWS Organizations features are enabled. When you create an organization, enabling all features is the default.```"
      },
      {
        "date": "2023-11-13T23:08:00.000Z",
        "voteCount": 1,
        "content": "see  dev112233xx's answer"
      },
      {
        "date": "2023-09-27T05:33:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html#:~:text=if%20you've%20already%20set%20up%20aws%20organizations%2C%20make%20sure%20that%20all%20features%20are%20enabled."
      },
      {
        "date": "2023-07-19T18:52:00.000Z",
        "voteCount": 1,
        "content": "i think it's b because having all the features enabled is not a requirement, otherwise it could incour in more charges. the features are not enabled by default , you have to go one by one or select all to enable them"
      },
      {
        "date": "2024-01-14T03:20:00.000Z",
        "voteCount": 2,
        "content": "Actually not. AWS Org has 2 feature modes: All features enabled (default) and Consolidated billing. AWS Orgs is free of charge regardless feature mode select see Billing section in the https://aws.amazon.com/organizations/faqs/"
      },
      {
        "date": "2023-07-02T12:04:00.000Z",
        "voteCount": 1,
        "content": "It's D.\nB would work if was supported"
      },
      {
        "date": "2023-05-13T10:37:00.000Z",
        "voteCount": 3,
        "content": "After reading all comments i concur with D. Reason being , requirement is no duplication fs users so it all stay at one place, thats what they want. So rule out all the 2-way trust options. Why not B? because there is no way in AWS organisations, you can only enable IAM identity center. The available feature sets are only two : All features, or only consolidated billing. Check here https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html#feature-set-cb-only"
      },
      {
        "date": "2023-05-07T12:08:00.000Z",
        "voteCount": 4,
        "content": "The options where they turn on only the AWS SSO feature in Organizations must be excluded (A and B). Because it is a requirement to have all features enabled in the organizations.\nReference from https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html\nPrerequisites for using AWS IAM Identity Center or former AWS SSO:\n\u201cYour AWS account must be managed by AWS Organizations. If you have not set up an organization, you don\u2019t have to. When you enable IAM IC, you will choose whether to have AWS create an organization for you.\nIf you already set up AWS Organizations, make sure that all features are enabled.\u201d\nBetween C and D, you do not need to create and configure a new AWS Managed Microsoft AD since you already have an AD present in the on premises, so there is no reason to expend more on this solution. Hence the response is D."
      },
      {
        "date": "2023-05-07T10:32:00.000Z",
        "voteCount": 3,
        "content": "I think, the correct is \"B\". Because, when you create an organization, enabling all features is the default, according this link: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html\n\n\"When you create an organization, enabling all features is the default. With all features enabled, you can use the advanced account management features available in AWS Organizations such as integration with supported AWS services and organization management policies.\"\n\nThis would rule out the option \"D\""
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/amazon/view/95290-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.<br><br>Users who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app\u2019s performance for these uploads.<br><br>Which solutions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the app to add random prefixes to the files before uploading."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-30T13:43:00.000Z",
        "voteCount": 12,
        "content": "Transfer Accelerator + Multi-part uploads for files more 500MB"
      },
      {
        "date": "2023-04-05T07:12:00.000Z",
        "voteCount": 7,
        "content": "Explanation for this .\n\nB: Configuring an S3 bucket in each Region to receive the uploads and using S3 Cross-Region Replication to copy the files to the distribution S3 bucket may improve data durability and availability, but it does not address the issue of slow uploads from Australia.\n\nC: Amazon Route 53 with latency-based routing can route the uploads to the nearest S3 bucket Region based on network latency, but it cannot guarantee faster upload speeds or better reliability.\n\nE: Adding random prefixes to the files before uploading will not improve upload performance or reliability.\n\nThence,  I select A and D."
      },
      {
        "date": "2024-08-31T03:13:00.000Z",
        "voteCount": 1,
        "content": "A. Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.\nD. Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3."
      },
      {
        "date": "2024-01-14T03:49:00.000Z",
        "voteCount": 1,
        "content": "A = correct (improve upload performance)\nB = this could work along with C to improve performance, but this will not fix upload failure for files &gt;5GB as you need multi-part upload\nC = se answer B\nD = correct (required to fix upload failures for &gt;5GB files)\nE = this could help with throttling which is not clearly stated as an issue"
      },
      {
        "date": "2023-08-23T00:34:00.000Z",
        "voteCount": 1,
        "content": "Answer: A, D? Maybe. But I prefer D and E. Let me explain why:\n\nRequirement is: \"A solutions architect must improve the app\u2019s performance for these uploads.\"\n\nShould we change S3 or the app? (or both?) \n\nDepending on how you interpret this question, you might think on the app, then it should be D and E, seriously. And it DOES make sense. Bear with me here. If you break the files into chunks, you will still have to upload them, let's say 10GB. And here comes the option E, which helps improving uploads with PARALELLISM, and you didn't touch S3 to fix that, just the app :)\n\nB and C would also work and would address the issue with users in Australia but it would change their design. I am not sure this is required, but in the real world, it's good to have options ;)\n\nAll in all, I personally would go with D, E, but AD and BC would also work."
      },
      {
        "date": "2023-07-02T12:07:00.000Z",
        "voteCount": 2,
        "content": "its AD"
      },
      {
        "date": "2023-06-20T12:09:00.000Z",
        "voteCount": 1,
        "content": "A and D satisfy the requirement"
      },
      {
        "date": "2023-05-21T12:33:00.000Z",
        "voteCount": 1,
        "content": "Transfer Accelerator + Multi-part uploads for files more 500MB\nQuestion similar to AWS Certified Solutions Architect Associate"
      },
      {
        "date": "2023-03-26T06:08:00.000Z",
        "voteCount": 2,
        "content": "AD all day"
      },
      {
        "date": "2023-03-15T03:23:00.000Z",
        "voteCount": 2,
        "content": "B is not suitable here, since it wants to improve upload experience, not download"
      },
      {
        "date": "2023-01-30T10:13:00.000Z",
        "voteCount": 2,
        "content": "I like AD but I am unsure. If the users in US don't complain about issues, it must be because multi-part upload is already enabled, otherwise it would fail 50% of the times. If only Australia users complain, it must be something else... Maybe A+B is a better option, although B is not the most cost efficient certainly."
      },
      {
        "date": "2023-01-15T12:11:00.000Z",
        "voteCount": 1,
        "content": "AD is correct"
      },
      {
        "date": "2023-01-14T09:14:00.000Z",
        "voteCount": 4,
        "content": "https://www.examtopics.com/discussions/amazon/view/74177-exam-aws-certified-solutions-architect-professional-topic-1/\n\nThe correct answers would be A and D.\n\nA. Enabling S3 Transfer Acceleration on the S3 bucket and configuring the app to use the Transfer Acceleration endpoint for uploads will improve the app's performance for users in Australia by providing a fast and secure way to transfer large files over the Internet.\n\nD. Configuring the app to break the video files into chunks and using a multipart upload to transfer files to Amazon S3, will improve the app's performance for users in Australia by allowing them to upload large files in parallel, which can increase upload speed and reduce the risk of upload failures."
      },
      {
        "date": "2023-01-14T09:14:00.000Z",
        "voteCount": 1,
        "content": "B. Configuring an S3 bucket in each Region to receive the uploads and using S3 Cross-Region Replication to copy the files to the distribution S3 bucket is not the most cost-effective solution for this specific use case.\n\nC. Setting up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region is not a solution that would improve the performance of the uploads specifically for users in Australia.\n\nE. Modifying the app to add random prefixes to the files before uploading will not improve the app's performance for users in Australia."
      },
      {
        "date": "2023-03-06T22:26:00.000Z",
        "voteCount": 2,
        "content": "yes, it will. Other options are more important, but sure random (rsp. any hash that distributes well) prefixes improve performance a lot."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/amazon/view/95291-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the connections to the database and could not re-establish the connections. After a restart of the application, the application re-established the connections.<br><br>A solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurora reader endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update the connection settings in the application to point to the Athena endpoint"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-02T21:06:00.000Z",
        "voteCount": 9,
        "content": "Amazon RDS Proxy is a fully managed database proxy service for Amazon Relational Database Service (RDS) that makes applications more scalable, resilient, and secure. It allows applications to pool and share connections to an RDS database, which can help reduce database connection overhead, improve scalability, and provide automatic failover and high availability."
      },
      {
        "date": "2023-01-15T14:29:00.000Z",
        "voteCount": 7,
        "content": "B is correct.\nC: Aurora is useless, Proxy is pointing to existing RDS"
      },
      {
        "date": "2024-08-31T03:14:00.000Z",
        "voteCount": 1,
        "content": "B. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint."
      },
      {
        "date": "2024-04-02T23:56:00.000Z",
        "voteCount": 1,
        "content": "C is wrong since RDS proxy for Aurora cluster only support reader endpoint, where in question it doesn't mention the read-only as requirement"
      },
      {
        "date": "2024-01-14T04:00:00.000Z",
        "voteCount": 5,
        "content": "A = using Aurora MySQL Serverless will not fix the issue, also serverless V1 is not great with HA. If you are running  a single instance (no read replicas) it will attempt to create a new DB Instance in the same AZ\nB = correct (RDS Proxy in addition to pooling connections, makes applications more resilient to database failures by automatically connecting to a standby DB instance while preserving application connections and detects failover and routes requests to standby instance up to 66% faster failover time)\nC = Creating and migrating to Aurora cluster is not needed, RDS Proxy is enough\nD = this requires a lot of work"
      },
      {
        "date": "2023-07-02T13:34:00.000Z",
        "voteCount": 1,
        "content": "it's a B"
      },
      {
        "date": "2023-06-18T09:45:00.000Z",
        "voteCount": 1,
        "content": "keyword = RDS proxy"
      },
      {
        "date": "2023-03-26T06:09:00.000Z",
        "voteCount": 1,
        "content": "Create an RDS proxy."
      },
      {
        "date": "2023-02-15T08:03:00.000Z",
        "voteCount": 1,
        "content": "proxy will be a buffer"
      },
      {
        "date": "2023-01-14T09:16:00.000Z",
        "voteCount": 4,
        "content": "The correct solution is B. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.\n\nAn RDS proxy is a service that allows you to pool and share connections to an RDS database. By using an RDS proxy, your application can automatically reconnect to the database after a failover event, without the need to restart the application.\n\nSolution A, migrating to Aurora Serverless, may not solve the problem because Aurora Serverless does not support Multi-AZ.\nSolution C and D are not the correct solutions because it does not solve the problem of reconnecting to the database after a failover event."
      },
      {
        "date": "2023-03-02T20:59:00.000Z",
        "voteCount": 5,
        "content": "What?? Aurora does not support Multi AZ ? its a blunder !"
      },
      {
        "date": "2023-06-01T05:46:00.000Z",
        "voteCount": 1,
        "content": "was about to point this"
      },
      {
        "date": "2023-06-02T03:11:00.000Z",
        "voteCount": 7,
        "content": "they are copying the answers from chatgpt"
      },
      {
        "date": "2023-09-05T23:54:00.000Z",
        "voteCount": 1,
        "content": "masetromain ~&gt; X \nGPTromain ~&gt; O lol"
      },
      {
        "date": "2023-12-27T22:46:00.000Z",
        "voteCount": 1,
        "content": "Even if the person is copying from chatgpt, they are saving your time and giving some pointers."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/amazon/view/95292-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T09:19:00.000Z",
        "voteCount": 23,
        "content": "The correct solution is C. Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.\n\nAWS IoT Core is a fully managed service that enables secure, bi-directional communication between internet-connected devices and the AWS Cloud. It supports the MQTT protocol and includes built-in device authentication and access control. By using AWS IoT Core, the company can easily provision and manage the X.509 certificates for each device, and connect the devices to the service with minimal operational overhead."
      },
      {
        "date": "2023-01-14T09:19:00.000Z",
        "voteCount": 6,
        "content": "Option A, setting up Amazon MQ queues and connecting each device to a queue, would require significant operational overhead to manage the queues and ensure that each device is properly authenticated and connected.\nOption B and D, using a Network Load Balancer (NLB) with a Lambda authorizer or an Amazon API Gateway HTTP API with a mutual TLS certificate authorizer and running an MQTT broker on EC2 instances, would also introduce more operational complexity and overhead compared to using AWS IoT Core."
      },
      {
        "date": "2024-08-12T05:05:00.000Z",
        "voteCount": 1,
        "content": "AWS IoT Core: This service is specifically designed for managing IoT devices and supports the MQTT protocol natively. It provides built-in support for device authentication using X.509 certificates."
      },
      {
        "date": "2024-03-17T05:33:00.000Z",
        "voteCount": 1,
        "content": "C, use IoT Core"
      },
      {
        "date": "2024-02-10T10:31:00.000Z",
        "voteCount": 1,
        "content": "Option C is the most suitable solution as AWS IoT Core is specifically designed for IoT scenarios, including device management and secure communication. AWS IoT Core natively supports MQTT, a lightweight communication protocol ideal for IoT devices. It allows devices to connect securely with an individual X.509 certificate for authentication, significantly reducing operational overhead compared to managing a custom MQTT broker or other intermediate services. AWS IoT Core also simplifies device management and scaling, making it the best choice for the described use case."
      },
      {
        "date": "2024-01-20T14:02:00.000Z",
        "voteCount": 3,
        "content": "I don\u2019t like C, but C might be the preferred answer. \nThere are thousands of devices. If C is the real answer, there should be a way to automatically create IOT thing and provision certificate. The answer seems implying to create IOT thing and provision certificates manually. If IoT core doesn\u2019t have this automation feature, this definitely is not the right answer in real life. \nIf there is this automation way and the question designer is expecting the exam taker to know this detail, that might be too specific for the exam takers.\nD is ugly, and usually is not a correct answer in most question designs. But it provides a feasible way in the real life comparing with C."
      },
      {
        "date": "2023-08-03T08:24:00.000Z",
        "voteCount": 1,
        "content": "\u7b54\u6848\u662fC\nhttps://aws.amazon.com/cn/iot-core/faqs/?nc=sn&amp;loc=5&amp;dn=2"
      },
      {
        "date": "2023-07-02T13:36:00.000Z",
        "voteCount": 1,
        "content": "it's C"
      },
      {
        "date": "2023-03-26T06:10:00.000Z",
        "voteCount": 1,
        "content": "I choose C"
      },
      {
        "date": "2023-03-17T13:54:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/iot/latest/developerguide/attach-to-cert.html\n\nIt is C,  - you have to do this through IOT core, for the devices you need an AWS IOT \"thing\" and then provision a certificate for the thing. from there connect the device."
      },
      {
        "date": "2023-03-09T15:23:00.000Z",
        "voteCount": 1,
        "content": "-The AWS IoT Device SDKs support device communications using the MQTT\n-Device connections to AWS IoT use X.509 client certificates \nhttps://docs.aws.amazon.com/iot/latest/developerguide/iot-connect-devices.html"
      },
      {
        "date": "2023-03-09T15:30:00.000Z",
        "voteCount": 2,
        "content": "Sorry I meant \"C\""
      },
      {
        "date": "2023-01-30T13:46:00.000Z",
        "voteCount": 2,
        "content": "C is correct (less op overhead than A)"
      },
      {
        "date": "2023-01-15T14:49:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/amazon/view/95293-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM role that the engineers use for access.<br><br>What should the solutions architect do to create the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers\u2019 IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the IAM policy for the engineers\u2019 IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the IAM policy for the engineers\u2019 IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers\u2019 IAM role to only allow access to their own AWS CloudFormation stack."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-02T22:11:00.000Z",
        "voteCount": 17,
        "content": "Tricky one. Question has a hint -\"to enforce the new restriction on the IAM role\" (note its not IAM policy as mentioned in option B) Creating a policy with approved resources first and assuming/applying that role to engineers will enforce. So C is correct. (B lacks enforcement, B is incorrect)"
      },
      {
        "date": "2023-05-07T15:09:00.000Z",
        "voteCount": 10,
        "content": "C is correct not B , AWS CloudFormation makes calls to create, modify, and delete those resources on their behalf. To separate permissions between a user and the AWS CloudFormation service, use a service role. AWS CloudFormation uses the service role's policy to make calls instead of the user's policy. For more information, see AWS CloudFormation service role . check this out . https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html\nOption B would allow engineers to provision resources using other methods outside of CloudFormation, which would not comply with the new company policy. This would make it difficult to enforce the new restriction on the IAM role that the engineers use for access."
      },
      {
        "date": "2024-08-31T03:15:00.000Z",
        "voteCount": 1,
        "content": "C. Update the IAM policy for the engineers\u2019 IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation."
      },
      {
        "date": "2024-03-17T05:38:00.000Z",
        "voteCount": 1,
        "content": "C, use the IAM service role to execute the stack"
      },
      {
        "date": "2024-02-12T18:16:00.000Z",
        "voteCount": 2,
        "content": "Option C is the most effective solution. It involves updating the engineers\u2019 IAM role to only allow actions related to AWS CloudFormation, effectively preventing direct provisioning or modification of AWS resources outside of CloudFormation. By creating a service role (with permissions to provision approved resources) that CloudFormation assumes when executing templates, you enforce the provisioning of only approved resources through CloudFormation. This setup provides a clear separation of permissions: engineers can manage CloudFormation stacks but cannot directly create resources unless defined in a CloudFormation template and permitted by the service role.\n\nOption B suggests updating the IAM policy to allow only the provisioning of approved resources and CloudFormation actions. This approach could theoretically work by explicitly listing allowed actions for specific AWS services in the IAM policy. However, it might be challenging to maintain and could inadvertently permit actions outside of CloudFormation, depending on the policy\u2019s specificity."
      },
      {
        "date": "2024-01-14T23:58:00.000Z",
        "voteCount": 2,
        "content": "A = doesn't prevent to have a CloudFromation template with non-approved resources deployed\nB = this doesn't prevent engineers to provision resources from console or cli\nC = correct\nD = doesn't prevent to provision non-approved resources or to provision only via CloudFormation"
      },
      {
        "date": "2023-12-06T17:24:00.000Z",
        "voteCount": 1,
        "content": "B would be created generally in organization. C is fine , but more restriction , the user can only use the cloud formation stack sets only which is not good for organization level."
      },
      {
        "date": "2023-11-13T23:46:00.000Z",
        "voteCount": 1,
        "content": "with B engineer will be able to directly provision resources without using of CF"
      },
      {
        "date": "2023-08-24T04:08:00.000Z",
        "voteCount": 1,
        "content": "The two contenders are Option B and C.\nOption B would allow the users to provision the approved resources without using CloudFormation (as the Users\u2019 IAM role would permission that). So, this violates the requirement.\nOption C would ensure that Only Cloudformation can provision the resources. So, that\u2019s the correct answer."
      },
      {
        "date": "2023-08-08T19:37:00.000Z",
        "voteCount": 1,
        "content": "I prefer C, because you need to give permission to cloud formation"
      },
      {
        "date": "2023-07-02T13:38:00.000Z",
        "voteCount": 1,
        "content": "C no doubt"
      },
      {
        "date": "2023-03-26T06:12:00.000Z",
        "voteCount": 2,
        "content": "C. Update the IAM policy for the engineers\u2019 IAM role with permissions to only allow AWS CloudFormation actions."
      },
      {
        "date": "2023-02-22T16:30:00.000Z",
        "voteCount": 3,
        "content": "C IAM policy is allowing to provision of approved resources."
      },
      {
        "date": "2023-01-30T10:57:00.000Z",
        "voteCount": 3,
        "content": "B does not enfore CF, otherwise it would work."
      },
      {
        "date": "2023-01-28T08:39:00.000Z",
        "voteCount": 3,
        "content": "C\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#use-iam-to-control-access\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
      },
      {
        "date": "2023-01-17T16:20:00.000Z",
        "voteCount": 4,
        "content": "You have to use a service role"
      },
      {
        "date": "2023-01-16T13:57:00.000Z",
        "voteCount": 2,
        "content": "C. Update the IAM policy for the engineers\u2019 IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.\n\nThis option is also correct, it is a way to restrict the access of engineers to only be able to perform AWS CloudFormation actions and provision only approved resources. By giving only permissions to the IAM role used by engineers for CloudFormation and creating a separate IAM role with permissions to provision approved resources and then assigning that role to CloudFormation during stack creation, we ensure that engineers can only provision the approved resources using CloudFormation."
      },
      {
        "date": "2023-01-16T13:59:00.000Z",
        "voteCount": 1,
        "content": "Both options B and C are correct.\n\nOption B: Update the IAM policy for the engineers\u2019 IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.\n\nOption C: Update the IAM policy for the engineers\u2019 IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.\n\nBoth options will enforce the new restriction on the IAM role that the engineers use for access, by limiting their access only to approved resources and only allowing them to provision resources using AWS CloudFormation. The specif"
      },
      {
        "date": "2023-11-23T01:26:00.000Z",
        "voteCount": 1,
        "content": "B works but is inappropriate.\nYou fail to consider that you NEED to use CFn for resource provisioning.\nOption B does not meet the requirement to limit this."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/amazon/view/95294-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can be deleted.<br><br>The solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.<br><br>Which storage strategy is the MOST cost-effective and meets the design requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T09:26:00.000Z",
        "voteCount": 23,
        "content": "The most cost-effective and efficient solution that meets the design requirements would be option B, Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.\n\nDynamoDB is a NoSQL key-value store designed for high scale and performance. It is fully managed by AWS and can easily handle millions of small records per minute. Additionally, with the TTL feature, you can set an expiration time for each record, so that the data can be automatically deleted after the specified time period."
      },
      {
        "date": "2023-01-14T09:26:00.000Z",
        "voteCount": 6,
        "content": "Option A, storing each incoming record as a single .csv file in an Amazon S3 bucket, would not be a good option because it would be difficult to retrieve individual records from the .csv files, and will likely increase the cost of data retrieval.\n\nOption C, storing each incoming record in a single table in an Amazon RDS MySQL database, would be a more expensive option as RDS is typically more expensive than DynamoDB. Additionally, running a cron job to delete old data could lead to additional operational overhead.\n\nOption D, storing incoming records in batches in an S3 bucket, would be a less efficient option as it would require additional processing and parsing of the data to retrieve individual records."
      },
      {
        "date": "2023-07-15T15:49:00.000Z",
        "voteCount": 6,
        "content": "A. No, because millions of writes to a single .csv file would cause read and write latency\n\nB. Yes, because DynamoDB can support peaks of more than 20 million requests per second.\n\nC. No, because creating nightly cron is unnecessary, and a relation database isn't designed to ingest millions of small records per minute\n\nD. No, because S3 supports 210,000 PUT requests per minute (3,500 requests per second * 60 seconds per min) which is far less than 1,000,000+ writes per minute"
      },
      {
        "date": "2024-08-31T03:15:00.000Z",
        "voteCount": 1,
        "content": "D. Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days."
      },
      {
        "date": "2024-06-07T12:20:00.000Z",
        "voteCount": 2,
        "content": "Obviously it is DynamoDB. Although as a side node I would say it is  probably a very bad choice as it would be astronomically expensive for millions of writes per minute\u2026. A Kinesis Data Streams would make much more sense especially that the data is only needed for 3 months\u2026"
      },
      {
        "date": "2024-03-17T05:41:00.000Z",
        "voteCount": 1,
        "content": "B, dynamodb is the best option"
      },
      {
        "date": "2024-02-12T19:15:00.000Z",
        "voteCount": 1,
        "content": "For small records less than 4 KB, DynamoDB can efficiently handle the ingestion of millions o records per minute from devices around the world, meeting the application's design requirements for low-latency data access. Additionally, DynamoDB's Time to Live (TTL) feature allows for automatic deletion of items after a specific period, aligning with the requirement to store data for only 120 days."
      },
      {
        "date": "2024-01-15T00:10:00.000Z",
        "voteCount": 1,
        "content": "A = S3 is not great with small files and searching for data based on index (a common pattern is to store object metadata in a database like DDB, OpenSearch or RDS/Aurora). Many small files can lead to high costs for retrieval\nB = correct\nC = single-table design, high volume write/retrieval os small object and no need for complex query are better served and cost less with DDB rather than RDS\nD = more efficient than A, but still S3 metadata search feature is limited"
      },
      {
        "date": "2023-11-13T23:52:00.000Z",
        "voteCount": 1,
        "content": "see uC6rW1aB's answer"
      },
      {
        "date": "2023-09-19T08:30:00.000Z",
        "voteCount": 1,
        "content": "B is the best for cost-effective.\nD is more cost for S3 request"
      },
      {
        "date": "2023-09-04T01:36:00.000Z",
        "voteCount": 3,
        "content": "Ref: https://aws.amazon.com/dynamodb/pricing/on-demand/\nDynamoDB read requests can be either strongly consistent, eventually consistent, or transactional. A strongly consistent read request of up to 4 KB requires one read request unit. For items larger than 4 KB, additional read request units are required."
      },
      {
        "date": "2023-09-04T01:44:00.000Z",
        "voteCount": 4,
        "content": "for a US East write object price: \nS3 Standard put object per thound cost $0.005  -&gt; 1 million put cost $5 ( per minutes in this situation )\nDynamo DB 1 million write cost $1.25  is a lot of cheaper"
      },
      {
        "date": "2023-08-27T18:34:00.000Z",
        "voteCount": 1,
        "content": "Dinamo DB is at least 5X more expensive than S3 for this use case. There are million of writing and each is 4K, total disk space is 10-15TB."
      },
      {
        "date": "2023-08-31T19:46:00.000Z",
        "voteCount": 2,
        "content": "D - S3 metadata search feature does not exist"
      },
      {
        "date": "2023-08-26T12:49:00.000Z",
        "voteCount": 1,
        "content": "Although both B and D are correct, Option D is more cost effective."
      },
      {
        "date": "2023-07-03T19:29:00.000Z",
        "voteCount": 1,
        "content": "Going with D as it's more cost effective. Question didn't ask for more efficient."
      },
      {
        "date": "2023-10-27T22:27:00.000Z",
        "voteCount": 3,
        "content": "B satisfies the requirement but D is not. The keyword here is Low latency - \u201c a durable location where it can be retrieved with low latency\u201d"
      },
      {
        "date": "2023-07-02T13:44:00.000Z",
        "voteCount": 2,
        "content": "D is more cost effective, evenif more complex"
      },
      {
        "date": "2023-06-26T07:28:00.000Z",
        "voteCount": 2,
        "content": "While B is viable, it seems like it's a massively expensive option - millions of writes per minute is a lot of WCU. Similarly, C would require a beefy database to support that many writes, may or may not be cheaper than the DDB option. But in a question asking for most cost effective, scalable writes from many sources screams an S3-based solution to me, which leaves A and D. Too many small files (A) and S3's performance will degrade, and millions of objects per minute seems like it would tax S3's ability to index buckets. Nothing in D is impossible to implement; though it's not the simplest solution, it's by far the cheapest."
      },
      {
        "date": "2023-06-14T00:56:00.000Z",
        "voteCount": 1,
        "content": "I think it is A.\nI'm not English native speaker, but I read it the way that each incoming record will be stored in separate file, thus the retrieval of a single record would be fast based on it's key. S3 is by far the cheapest option of all."
      },
      {
        "date": "2023-04-20T22:53:00.000Z",
        "voteCount": 4,
        "content": "Most cost-effective will be D. and the following makes the size under 5TB , under the limits.\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB."
      },
      {
        "date": "2023-05-01T14:05:00.000Z",
        "voteCount": 3,
        "content": "sorry, metadata is incorrect because the following:  \"millions of small records per minute from devices all around the world. Each record is less than 4 KB in size \""
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/amazon/view/95297-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.<br><br>Which solution will provide the HIGHEST availability for the database?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-17T17:55:00.000Z",
        "voteCount": 15,
        "content": "This really should be multi-az but you could move to it w/ D.\nHere is the key to this one though; Highest Availability - the read replica is an asynchronous copy, while backup is a \"time\". Easier to do the read replica, and flip the switches than to reload from backup. Global Tables relate to DynomoDB https://disaster-recovery.workshop.aws/en/services/databases/dynamodb/dynamo-global-table.html\nLittle handy \"DR\" guide"
      },
      {
        "date": "2024-08-31T03:16:00.000Z",
        "voteCount": 1,
        "content": "D. Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source."
      },
      {
        "date": "2024-01-15T00:42:00.000Z",
        "voteCount": 1,
        "content": "A = you cannot promote an automated backup to a standalone DB (you restore a backup into a new DB instance instead). Creating a read replica could help in this scenario in case it is cross-region. This is not specified\nB = RDS does not support global table, copying a read replicas from a region to another make no sense to me\nC =  see B\nD = correct"
      },
      {
        "date": "2023-07-02T13:46:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2023-05-07T16:22:00.000Z",
        "voteCount": 2,
        "content": "There is Aurora Global Database, DynamoDB Global Tables and the question is about RDS for MySQL DB Instance. \nhttps://jayendrapatil.com/aws-aurora-global-database-vs-dynamodb-global-tables/\nSo, options B and C are not acceptable.\nOption D refers to using a cross-region replication for disaster recovery which can be found here https://disaster-recovery.workshop.aws/en/services/databases/rds/rds-cross-region.html \nFollowing article demonstrates a similar scenario using RDS for SQL Server\nhttps://aws.amazon.com/blogs/database/use-cross-region-read-replicas-with-amazon-relational-database-service-for-sql-server/\nThe design seems to be what we are looking in terms of option D.\nhttps://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2022/11/15/dbblog-2614-image001.png"
      },
      {
        "date": "2023-03-26T06:15:00.000Z",
        "voteCount": 1,
        "content": "D makes the most sense"
      },
      {
        "date": "2023-03-02T22:30:00.000Z",
        "voteCount": 3,
        "content": "No global tables concept in RDS, B,C are eliminated. A is wrong in terms of backing up Db copy to a standalone instance ? D provides read replicas for reading and also swtiches as a failiover in times of disruption and becomes primary. this is how HA can be maintained. D is correct."
      },
      {
        "date": "2023-03-02T04:40:00.000Z",
        "voteCount": 2,
        "content": "MySQL - Read Replica. In this case, this is not aurora so not the global table option and hence can not be B and C"
      },
      {
        "date": "2023-03-02T02:28:00.000Z",
        "voteCount": 2,
        "content": "I haven't found any information about a \"global table\" for RDS.\nGlobal tables are for DynamoDB. For Aurora, it's called \"global databases\".\nRDS for MySQL supports cross-region read replicas https://aws.amazon.com/fr/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/, so D has a better availability than A."
      },
      {
        "date": "2023-01-17T20:00:00.000Z",
        "voteCount": 4,
        "content": "for B,C, Amazon RDS does not support global tables yet. Only Aurora supports."
      },
      {
        "date": "2023-02-19T05:52:00.000Z",
        "voteCount": 1,
        "content": "Is Aurora not part of RDS? You can choose Aurora's compatibility with MySQL and PostreSQL)."
      },
      {
        "date": "2023-01-15T14:53:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2023-01-16T14:06:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/69438-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-01-16T14:06:00.000Z",
        "voteCount": 2,
        "content": "It is possible that some people may think that option D. Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source. is the best solution, as it also utilizes read replicas and cross-Region promotion to minimize downtime. However, it is important to consider that while this solution provides high availability, it doesn't provide the same level of automatic replication that global tables do. In case of a disruption, there is a risk of data loss during the manual switchover.\nand also with option D, you are still working with a single point of failure, the primary database, while in option B you have multiple copies of your data distributed across different regions, so in case of a failure you can switch over to one of the replicas without loss of data."
      },
      {
        "date": "2023-02-03T08:28:00.000Z",
        "voteCount": 1,
        "content": "B is not right.  Only Aurora has global tables.  RDS don't"
      },
      {
        "date": "2023-02-15T13:07:00.000Z",
        "voteCount": 1,
        "content": "Cant be B due to global tables, ReadReplicas are supported with RDS and other options of restoring from backup do not create high availability"
      },
      {
        "date": "2023-01-14T09:30:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is option B. Configuring global tables and read replicas on Amazon RDS with the cross-Region scope enabled provides the highest availability for the database. In case of disruption, the company can use AWS Lambda to copy the read replicas from one Region to another Region, ensuring that the website remains operational at all times. This solution provides automatic failover across multiple regions and allows for fast recovery in case of a disruption.\n\nOption A involves promoting an automated backup to be a standalone DB instance and creating a replacement read replica that has the promoted DB instance as its source. This solution is less efficient since it requires manual intervention and additional steps to promote the backup and create a replacement read replica."
      },
      {
        "date": "2023-03-01T11:21:00.000Z",
        "voteCount": 1,
        "content": "If the disruption is an outage that takes the Region offline completely, how could we use Lambda to copy the read replica from the Region that is no longer available to the backup to another Region?"
      },
      {
        "date": "2023-01-14T09:30:00.000Z",
        "voteCount": 1,
        "content": "Option C involves configuring global tables and automated backups on Amazon RDS. This solution is less efficient since it does not provide automatic failover across multiple regions and requires additional steps to copy the read replicas from one Region to another Region using AWS Lambda.\n\nOption D involves configuring read replicas on Amazon RDS. In the case of disruption, promoting a cross-Region and read replica to be a standalone DB instance. This solution is less efficient than Option B since it does not provide automatic failover across multiple regions and requires manual intervention to promote the read replica to a standalone instance."
      },
      {
        "date": "2023-06-19T05:11:00.000Z",
        "voteCount": 1,
        "content": "In fact global tables is a Dynamo DB thing. And RDS has Aurora Global Database. In this case Aurora is out of the question, it says RDS MySql, not Aurora (RDS) MySQL."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/amazon/view/95298-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which has a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.<br><br>Example Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.<br><br>Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Site-to-Site VPN\u2019s virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private getaway between the two VPCs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T21:21:00.000Z",
        "voteCount": 10,
        "content": "https://docs.aws.amazon.com/pt_br/whitepapers/latest/aws-vpc-connectivity-options/aws-transit-gateway-vpn.html\nTransit gateway is an AWS managed high availability and scalability regional network transit hub used to interconnect VPCs and customer networks. AWS Transit Gateway + VPN, using the Transit Gateway VPN Attachment, provides the option of creating an IPsec VPN connection between your remote network and the Transit Gateway over the internet, as shown in the following picture.\nhttps://docs.aws.amazon.com/images/whitepapers/latest/aws-vpc-connectivity-options/images/image4.png\nOption A is the correct answer since the transit gateway will allow both VPCs to connect to the on premises network.\nOption B suggests the same feature but is using the Transit Gateway in a incorrect way. The soul purpose of the gateway is to have point for interconnectivity."
      },
      {
        "date": "2024-08-31T03:16:00.000Z",
        "voteCount": 1,
        "content": "A. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks."
      },
      {
        "date": "2024-08-25T12:22:00.000Z",
        "voteCount": 1,
        "content": "After all, what is the right answer A or D ?"
      },
      {
        "date": "2024-03-17T05:50:00.000Z",
        "voteCount": 1,
        "content": "A, Transit Gateway"
      },
      {
        "date": "2024-02-12T19:30:00.000Z",
        "voteCount": 1,
        "content": "Option A is the most straightforward and effective solution. A transit gateway acts as a cloud router that simplifies network topology and connectivity between on-premises networks, VPCs, and other AWS services. By attaching both VPCs (A and B) and the Site-to-Site VPN to a single transit gateway and updating the route tables accordingly, Example Corp. can enable seamless communication between its on-premises network and both VPCs. This approach minimizes operational effort by centralizing network management and eliminating the need for complex routing configurations or multiple VPN connections.\n\nOption D proposes modifying the Site-to-Site VPN\u2019s virtual private gateway to include both VPC A and VPC B. However, a virtual private gateway cannot be directly shared or split between VPCs in the manner described. This option misunderstands the architecture of AWS networking components and their capabilities."
      },
      {
        "date": "2024-01-15T01:03:00.000Z",
        "voteCount": 1,
        "content": "A = correct\nB = if you setup a second VPN you do not need a TGW\nC = peering does not allow edge-to-edge routing (aka VPC B cannot access on-premise via VPC A and vice versa)\nD = Virtual Private Gateway is specific to a single VPC"
      },
      {
        "date": "2023-08-27T06:35:00.000Z",
        "voteCount": 2,
        "content": "reluctantly selecting option A. these answers do not take into consideration that the On-promises already has a peered connection to VPC A through the existing site to site"
      },
      {
        "date": "2023-08-08T22:53:00.000Z",
        "voteCount": 1,
        "content": "I think A is right, I do not know why other guys select D"
      },
      {
        "date": "2023-07-02T13:50:00.000Z",
        "voteCount": 1,
        "content": "surely A"
      },
      {
        "date": "2023-05-01T22:15:00.000Z",
        "voteCount": 4,
        "content": "For those that have written SAP-C02, how relevant are these questions to the real exam questions? After adequate preparation, I wanted to truly test my knowledge before dabbling into the exam and would really appreciate anyone's candid opinion.\nThanks."
      },
      {
        "date": "2023-09-12T19:41:00.000Z",
        "voteCount": 2,
        "content": "please reply to him"
      },
      {
        "date": "2023-04-28T08:38:00.000Z",
        "voteCount": 2,
        "content": "A is the best option.\n\nCreating a transit gateway and attaching Site-to-Site VPN, VPC A, and VPC B to the transit gateway would enable the on-premise servers to access VPC B with minimal operational effort. The transit gateway route tables would need to be updated with IP range routes for all the other networks to enable communication between the VPCs and the on-premises servers."
      },
      {
        "date": "2023-03-28T00:49:00.000Z",
        "voteCount": 1,
        "content": "Solution A is the only one possible solution"
      },
      {
        "date": "2023-03-28T00:49:00.000Z",
        "voteCount": 4,
        "content": "B is impossible : When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW (it\"s a 3 entities). You can however connect a S2S VPN to a TGW (onprem to TGW) (which is solution A).\nC : Does not work, there is no transitivity on AWS. S2S VPN cannot reach VPC B through VPC A\nD is impossible :  There is no magic, you cannot \"split\" router (that does not exist). VGW is attach to a single VPC. A S2S VPN cannot multiplex VPC"
      },
      {
        "date": "2023-03-28T00:49:00.000Z",
        "voteCount": 4,
        "content": "A : the best (and the only one possible) answer : When you have 2 VPC, you have multiple solution to connect to onprem : \n- Create 2 S2S VPN (1 for each VPC)\n- or Create a TGW, attach both VPC to it and attach S2S VPN to it too\n- or Create a third VPC (VPC routing), and peer VPC A with VPC routing, VPC B to VPC routing, attach a S2S VPN to VPC routing and use a NVA on VPC routing to route trafic. NVA can do transitivity.\nHere, solution A is one of the possible answer"
      },
      {
        "date": "2023-03-26T06:17:00.000Z",
        "voteCount": 1,
        "content": "A. Create a transit gateway. Attach the Site-to-Site VPN"
      },
      {
        "date": "2023-03-19T15:00:00.000Z",
        "voteCount": 1,
        "content": "A makes sense to me"
      },
      {
        "date": "2023-03-19T03:30:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2023-03-02T23:13:00.000Z",
        "voteCount": 1,
        "content": "A has this wierd wording - attaching S-S VPN ? transit gateway attaches to VPCs only not S-S vpn. A is wrong. Since VPC A and VPC B are already peered, the easiest solution to connect from the on-premises servers to VPC B would be to create another Site-to-Site VPN connection between the on-premises data center and VPC B. This would require minimal operational effort, as the existing VPN connection with VPC A can remain unchanged."
      },
      {
        "date": "2023-03-02T23:18:00.000Z",
        "voteCount": 1,
        "content": "oops this is wrong..VPN can be attached..."
      },
      {
        "date": "2023-03-02T23:25:00.000Z",
        "voteCount": 1,
        "content": "Moderator, please delete this comment.."
      },
      {
        "date": "2023-03-02T23:24:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\nWhen you create a virtual private gateway, you can specify the private Autonomous System Number (ASN) for the Amazon side of the gateway. If you don't specify an ASN, the virtual private gateway is created with the default ASN (64512). You cannot change the ASN after you've created the virtual private gateway. Due to this reason, So A is not possible (with least effort). Answer should be B."
      },
      {
        "date": "2023-03-28T00:41:00.000Z",
        "voteCount": 1,
        "content": "THe VGW for VPCA is no more needed on A because you attach the VPCA to the TGW.\nThe ASN will be on the TGW attachment with the S2S VPN.\nThis is the best solution.\nIn the meantime, B is impossible. When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW. You can however connect a S2S VPN to a TGW (onprem to TGW)."
      },
      {
        "date": "2023-02-16T16:54:00.000Z",
        "voteCount": 1,
        "content": "TGW is the solutions"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/amazon/view/95299-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company\u2019s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.<br><br>The company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.<br><br>What should the company do to modify the application to send email messages from Amazon SES?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 53,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-23T18:38:00.000Z",
        "voteCount": 16,
        "content": "B is correct.\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html\nSTARTTLS supports  ports 25, 587, and 2587\nTLSWRAPPER supports ports  465 and 2465"
      },
      {
        "date": "2023-03-03T11:17:00.000Z",
        "voteCount": 5,
        "content": "FYI Amazon SES supports STARTTLS encryption over port 587, which is the recommended port for email transmission. But existing port 25 can be configured too as in this case as the migration came from SMTP port 25"
      },
      {
        "date": "2023-01-28T08:45:00.000Z",
        "voteCount": 9,
        "content": "In this scenario, you should use Amazon SES SMTP interface to send emails because the application can use SMTP only.\nhttps://docs.aws.amazon.com/ses/latest/dg/send-email-smtp.html\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-credentials.html\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html"
      },
      {
        "date": "2024-08-31T03:17:00.000Z",
        "voteCount": 1,
        "content": "B. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES."
      },
      {
        "date": "2024-02-13T10:47:00.000Z",
        "voteCount": 2,
        "content": "Here's why option B is the correct choice:\nSTARTTLS Support: Amazon SES supports STARTTLS, a protocol command used to upgrade an existing insecure connection to a secure connection using TLS (Transport Layer Security). This is crucial since the legacy SMTP server does not support TLS, and STARTTLS can be used to initiate a secure connection.\nSMTP Credentials: Amazon SES requires authentication to send emails through its SMTP interface. This is achieved by using SMTP credentials, which are different from AWS access keys. SMTP credentials can be obtained from the Amazon SES console and are used to authenticate with the Amazon SES SMTP endpoint.\nOperational Simplicity: This approach allows the application to continue using SMTP for sending emails, which aligns with the application's existing capabilities. By using STARTTLS, the application can upgrade its connection to Amazon SES to a secure one, ensuring compliance with security best practices without significant changes to the application's email sending functionality."
      },
      {
        "date": "2024-02-01T21:10:00.000Z",
        "voteCount": 2,
        "content": "Terrible Q. All answers are wrong.\nA is wrong because you cannot send emails through SES SMTP using SMTP credentials derived from temporary STS tokens (ie IAM roles). Must use an IAM user access keys to derive creds.\nB is wrong because the question imposes a constraint that prevents us from selecting an answer that requires upgrading or modifying the application itself. Could you just offload SMTP STARTTLS/AUTH to the local sendmail/postfix daemon? Maybe, if it were Linux, but what if it's Windows? Cygwin? WSL?\nC &amp; D - wrong, for a similar rationale as B. \n\nBut the question designer OBVIOUSLY doesn't know that IAM roles can't be used for SES SMTP auth, because these questions are written by inexperienced, unqualified people who are not themselves architects or engineers."
      },
      {
        "date": "2024-02-01T21:13:00.000Z",
        "voteCount": 3,
        "content": "To be fair, the question says this:\n\n\"The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.\"\n\nThe question doesn't say the application cannot handle STARTTLS or SMTP AUTH. In theory, if an application claims to support SMTP, then it should support all features of SMTP, which includes STARTTLS and AUTH.  It only says the legacy SMTP server cannot handle TLS. So I suppose perhaps B is correct after all :-)"
      },
      {
        "date": "2024-01-15T01:16:00.000Z",
        "voteCount": 2,
        "content": "A = this sends email via SES API while application can use SMTP only\nB = correct\nC = this sends email via SES API while application can use SMTP only\nD = this sends email via SES SDK (API) while application can use SMTP only"
      },
      {
        "date": "2024-01-15T07:30:00.000Z",
        "voteCount": 2,
        "content": "Need to correct my comment on A. This is a TLS Wrapper (A) vs STARTTLS (B), where STARTTLS allows initiating an encrypted connection by first establishing an unencrypted connection. While TLS Wrapper is a means of initiating an encrypted connection without first establishing an unencrypted connection (it's the client's responsibility to connect to the endpoint using TLS, and to continue using TLS for the entire conversation). As our app con only work with SMTP we should go for B"
      },
      {
        "date": "2023-11-26T04:57:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B.\nA: We are unable to obtain authentication information.\nC,D: Does not meet SMTP requirements.\n\nB: This is the correct procedure.\n\nhttps://repost.aws/knowledge-center/ses-set-up-connect-smtp\nhttps://docs.aws.amazon.com/ses/latest/dg/security-protocols.html"
      },
      {
        "date": "2023-10-02T02:49:00.000Z",
        "voteCount": 2,
        "content": "Here's why option B is the correct choice:\n\nSMTP Protocol: The legacy SMTP server uses the SMTP protocol, and Amazon SES provides an SMTP interface for sending email, which is suitable for your application.\n\nSTARTTLS: Using STARTTLS ensures that your communication with Amazon SES is encrypted, which is a best practice for secure email transmission.\n\nSMTP Credentials: Amazon SES SMTP credentials are required to authenticate your application with Amazon SES when sending emails. These credentials include an SMTP username and password."
      },
      {
        "date": "2023-10-02T02:50:00.000Z",
        "voteCount": 4,
        "content": "Option A mentions TLS Wrapper, which isn't a standard approach when using Amazon SES for sending email. Amazon SES supports STARTTLS for secure communication.\n\nOption C suggests using the SES API, which is a valid approach but requires code modifications to use the SES API instead of SMTP. Since your application can only use SMTP, this option might involve significant code changes.\n\nOption D mentions using AWS SDKs and IAM users, which is more suitable for programmatic access to SES but not for legacy SMTP applications that can only send via SMTP.\n\nTherefore, Option B is the most appropriate choice for configuring your application to send email messages from Amazon SES while preserving the SMTP protocol and ensuring secure communication."
      },
      {
        "date": "2023-08-09T00:18:00.000Z",
        "voteCount": 1,
        "content": "I selecte A"
      },
      {
        "date": "2023-07-02T14:00:00.000Z",
        "voteCount": 1,
        "content": "It's B - to preserve SMTP protocol"
      },
      {
        "date": "2023-06-18T10:16:00.000Z",
        "voteCount": 1,
        "content": "B because is \"legacy\" app then use properties to set SMTP \nkeyword === Obtain Amazon SES SMTP credentials"
      },
      {
        "date": "2023-05-10T23:12:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/query-and-visualize-aws-cost-and-usage-data-using-amazon-athena-and-amazon-quicksight/"
      },
      {
        "date": "2023-05-07T21:50:00.000Z",
        "voteCount": 3,
        "content": "Option A states that the company would require to do more changes in the application than a replatform migration strategy where we are supposed to migrate the application with minimal changes. In Option A using the TLS wrapper would require an additional layer of software (stunnel) to be installed and configured on the EC2 instance, which may introduce additional complexity and management overhead.\nIn option B, we need to configure the application to connect to SES using STARTLS using SMTP credentials, since the legacy SMTP server does not support TLS encryption. This would require minimal change to the application."
      },
      {
        "date": "2023-04-15T07:07:00.000Z",
        "voteCount": 2,
        "content": "To set up a STARTTLS connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 25, 587, or 2587, issues an EHLO command, and waits for the server to announce that it supports the STARTTLS SMTP extension. The client then issues the STARTTLS command, initiating TLS negotiation. When negotiation is complete, the client issues an EHLO command over the new encrypted connection, and the SMTP session proceeds normally\n\nTo set up a TLS Wrapper connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 465 or 2465. The server presents its certificate, the client issues an EHLO command, and the SMTP session proceeds normally."
      },
      {
        "date": "2023-03-26T06:18:00.000Z",
        "voteCount": 1,
        "content": "B. Configure the application to connect to Amazon SES by using STARTTLS."
      },
      {
        "date": "2023-03-22T11:04:00.000Z",
        "voteCount": 3,
        "content": "B , https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html"
      },
      {
        "date": "2023-03-19T15:39:00.000Z",
        "voteCount": 2,
        "content": "B is wrong because STARTTLS uses port 25 and EC2 instances can\u2019t send outbound traffic through port 25 (you must ask AWS to allow port 25)"
      },
      {
        "date": "2023-06-05T17:19:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html \nsays:\n\"Amazon Elastic Compute Cloud (Amazon EC2) throttles email traffic over port 25 by default. To avoid timeouts when sending email through the SMTP endpoint from EC2,submit a Request to Remove Email Sending Limitations\"\n\nAnd the question explicitly says:\n\"The company has lifted the SES limits.\""
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/amazon/view/95305-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.<br><br>The acquiring company\u2019s finance team needs a solution to report on costs for all the companies through a self-managed application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T10:33:00.000Z",
        "voteCount": 14,
        "content": "The correct solution is A.\n\nCreating an AWS Cost and Usage Report for the organization and defining tags and cost categories in the report will allow for detailed cost reporting for the different companies that have been consolidated into one organization. By creating a table in Amazon Athena and an Amazon QuickSight dataset based on the Athena table, the finance team will be able to easily query and generate reports on the costs for all the companies. The dataset can then be shared with the finance team for them to use for their reporting needs.\n\nOption B is not correct because it does not provide a way to query and generate reports on the costs for all the companies. \n\nOption C is not correct because it only provides spending information from the AWS Price List Query API and does not provide detailed cost reporting for the different companies. \n\nOption D is not correct because it only uses the AWS Price List Query API and does not provide a way to query and generate reports on the costs for all the companies."
      },
      {
        "date": "2023-02-10T21:37:00.000Z",
        "voteCount": 7,
        "content": "I can customize reporting in Cost Explorer but cannot find how to do templates."
      },
      {
        "date": "2024-08-31T03:17:00.000Z",
        "voteCount": 1,
        "content": "A. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team."
      },
      {
        "date": "2024-01-15T07:39:00.000Z",
        "voteCount": 2,
        "content": "A = correct\nB = there isn't specialized templates in AWS Cost Explorer. It provides default reports, but also enables you to change the filters and constraints used to create the reports. You can save the reports that you made as a bookmark, download the CSV file, or save them as a report\nC &amp; D = AWS Price List provides a catalog of the products and prices for AWS services that you can purchase on AWS, not cost of your resources"
      },
      {
        "date": "2023-08-09T01:23:00.000Z",
        "voteCount": 1,
        "content": "I prefer A"
      },
      {
        "date": "2023-07-02T14:02:00.000Z",
        "voteCount": 1,
        "content": "its n A"
      },
      {
        "date": "2023-06-20T21:36:00.000Z",
        "voteCount": 1,
        "content": "I vote A mostly because there is no template option in Cost Explorer and A is the only other option which covers the scenario"
      },
      {
        "date": "2023-05-10T23:14:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/big-data/query-and-visualize-aws-cost-and-usage-data-using-amazon-athena-and-amazon-quicksight/"
      },
      {
        "date": "2023-03-26T06:19:00.000Z",
        "voteCount": 1,
        "content": "A. Create an AWS Cost and Usage Report for the organization."
      },
      {
        "date": "2023-01-15T15:22:00.000Z",
        "voteCount": 2,
        "content": "A is correct\nB: no such template for cost exporer\nCD: Price List Query API is for list price, not for usage"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/amazon/view/95308-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company\u2019s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.<br><br>The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.<br><br>Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResize the MySQL General Purpose SSD storage to 6 TB to improve the volume\u2019s IOPS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS X-Ray to analyze and debug application issues and add more API servers to match the load.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T10:55:00.000Z",
        "voteCount": 20,
        "content": "C and E are the correct answers.\n\nOption C: Leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data would help to resolve the issues with the API servers being consistently overloaded. By using Kinesis, the data can be ingested and processed in real-time, allowing the API servers to handle the increased load. Using Lambda to process the data can also help to improve the overall performance and scalability of the platform.\n\nOption E: Re-architecting the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance would help to resolve the issues with high write latency. DynamoDB is a NoSQL database that is designed for high performance and scalability, making it a good fit for this use case. Additionally, DynamoDB supports auto-scaling, which can help to ensure that the database can handle the expected growth in the number of sensors."
      },
      {
        "date": "2023-02-28T13:13:00.000Z",
        "voteCount": 6,
        "content": "I disagree with option E. Re-architecting the database tier from RDS to DynamoDB is not possible. RDS is a SQL database, and DynamoDB is a NoSQL database. \n\nThe correct one should be C and B"
      },
      {
        "date": "2024-03-11T01:08:00.000Z",
        "voteCount": 3,
        "content": "That is why it says to \"Re-architect the DB tier\"."
      },
      {
        "date": "2023-06-07T04:40:00.000Z",
        "voteCount": 2,
        "content": "if it was read operations yes but the issue is write latency.  also rds proxy is used to handle the write operations"
      },
      {
        "date": "2023-06-07T04:41:00.000Z",
        "voteCount": 1,
        "content": "also rds proxy is not used (sorry typo) to handle write operations properly"
      },
      {
        "date": "2023-03-10T00:14:00.000Z",
        "voteCount": 2,
        "content": "I agree with you.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\nAurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.\n\nAurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB)."
      },
      {
        "date": "2023-03-17T18:48:00.000Z",
        "voteCount": 1,
        "content": "Naw, you can migrate: https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/\n\nPlus, with DynamoDB it scales, don't need to add read replica complexity and it also supports IoT out of the box - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.WhyDynamoDB.html\nThis is for IoT sensors that send data and I don't need to store forever so, DynamoDB for this use case is better and cheaper allowing scale"
      },
      {
        "date": "2023-05-11T14:07:00.000Z",
        "voteCount": 1,
        "content": "I think this is the big point in this question and that DynamoDB is being position by AWS for IoT very hard. Although is technically possible to migrate with DMS from SQL to DynamoDB, is hard, but harder yet is the change of model inside the application or service."
      },
      {
        "date": "2023-04-05T07:22:00.000Z",
        "voteCount": 1,
        "content": "While options C and E may also provide some benefits, they may not address the underlying issues with the overloaded API servers and high write latency in the database. Therefore, options B and D are the best combination for resolving the issues and enabling growth as new sensors are provisioned."
      },
      {
        "date": "2023-01-14T10:55:00.000Z",
        "voteCount": 3,
        "content": "Option A, Resizing the MySQL General Purpose SSD storage to 6 TB to improve the volume\u2019s IOPS will not solve the problem, as the problem is not just related to storage size but also high write latency.\n\nOption B, Re-architecting the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and adding read replicas would help to improve the read performance, but it won't help in reducing write latency.\n\nOption D, Using AWS X-Ray to analyze and debug application issues and adding more API servers to match the load, would help in identifying the problem and resolving it, but it will not help in reducing the load on the servers."
      },
      {
        "date": "2024-08-31T03:18:00.000Z",
        "voteCount": 1,
        "content": "C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\nE. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance."
      },
      {
        "date": "2024-07-19T12:12:00.000Z",
        "voteCount": 2,
        "content": "What discards B is \"Add read replicas\", the problem is writing the new data in the DB, adding Read replicas will increase the cost and this is not what question requests \"maintain cost\""
      },
      {
        "date": "2024-06-17T19:08:00.000Z",
        "voteCount": 1,
        "content": "Write performance will be improved by switch RDS to Aurora. RDS to Aurora is smooth transition without too much on the application side. \nAnswer E will application side not just backend DB."
      },
      {
        "date": "2024-04-03T07:02:00.000Z",
        "voteCount": 3,
        "content": "Option CE and BC.  The only reason I choose  E over B because said SO.  Per AWS, DynamoDB is suitable for IoT ( Sensor data and log ingestion)  \n\nhttps://docs.aws.amazon.com/whitepapers/latest/best-practices-for-migrating-from-rdbms-to-dynamodb/suitable-workloads.html"
      },
      {
        "date": "2024-03-17T06:00:00.000Z",
        "voteCount": 1,
        "content": "CE, kinesis + lambda &amp; Dynamodb"
      },
      {
        "date": "2024-03-03T08:37:00.000Z",
        "voteCount": 1,
        "content": "Switching from RDS mysql to aurora will improve performance, by up to 10 times, which could solve the write issue.   Switching from relationship database to nosql is not practical, need re-engineering whole application. plus, the performance improvement of nosql are around data read, not data write ( creating/updating indexes is a huge effort)"
      },
      {
        "date": "2024-02-13T12:19:00.000Z",
        "voteCount": 1,
        "content": "* B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. Aurora provides several benefits over standard RDS MySQL, including better performance, scalability, and availability. It automatically grows storage as needed, up to 128 TB, potentially providing better write performance. Aurora also supports up to 15 read replicas with very low replication latency, improving read performance significantly and reducing the load on the primary database instance."
      },
      {
        "date": "2024-02-13T12:19:00.000Z",
        "voteCount": 1,
        "content": "* C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\nBy using Amazon Kinesis Data Streams, the company can collect, process, and analyze real-time, streaming data so that it can react to new information quickly. This service allows for the ingestion of a large amount of data generated by IoT sensors. AWS Lambda can then be used to process this data in real-time, which can help to offload the work from the API servers, reducing their load. This setup can scale automatically with the number of incoming data records, providing a more efficient and cost-effective solution to handle growth."
      },
      {
        "date": "2024-01-15T07:42:00.000Z",
        "voteCount": 2,
        "content": "A = does not fix permanently (who knows that 6TB is enough?)\nB = going from RDS to Aurora will not fix the issue\nC = correct\nD = this could work, but it is not cost efficient (more EC2 instance along the line)\nE = correct"
      },
      {
        "date": "2023-11-08T07:03:00.000Z",
        "voteCount": 1,
        "content": "The requirement is to resolve high write latency while Aurora is a good fit for structured datasets but option B has indicated read replica as a direction for resolution against a question seeking resolution in write latency. So Option B is definitely out."
      },
      {
        "date": "2023-08-17T08:12:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS FeaturesAmazon RDS supports multiple database engines, including Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server, and PostgreSQL.Amazon RDS allows you to scale your database instances\u2019 storage size and performance.Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud.Amazon RDS provides a cost-effective way to manage relational databases in the cloud.DynamoDB FeaturesPrimarily, DynamoDB features flexibility, scalability, and performance. It offers high availability out of the box with no need for setup or configuration. DynamoDB automatically replicates your data across multiple Availability Zones within a Region to give you fault tolerance and high availability."
      },
      {
        "date": "2023-08-17T08:12:00.000Z",
        "voteCount": 1,
        "content": "c and E  ans 100 %"
      },
      {
        "date": "2023-08-12T08:08:00.000Z",
        "voteCount": 1,
        "content": "CE \nhttps://aws.amazon.com/dynamodb/iot/"
      },
      {
        "date": "2023-07-25T12:16:00.000Z",
        "voteCount": 1,
        "content": "b-c-b-c-b-c-b-c"
      },
      {
        "date": "2023-07-02T14:07:00.000Z",
        "voteCount": 1,
        "content": "CE for sure. classic IoT use case"
      },
      {
        "date": "2023-06-20T14:07:00.000Z",
        "voteCount": 1,
        "content": "Rds MySQL to Aurora to scale automatically and stay relational"
      },
      {
        "date": "2023-06-20T06:34:00.000Z",
        "voteCount": 1,
        "content": "Options A, D, and E are not the most suitable choices for resolving the issues and enabling growth while keeping the platform cost-efficient in this scenario:\n\nA. Resizing the MySQL General Purpose SSD storage to 6 TB might increase the volume's IOPS, but it won't address the underlying scalability and performance issues caused by the growing number of sensors and high write latency.\n\nD. While using AWS X-Ray for analyzing and debugging application issues can help optimize performance, it alone won't be sufficient to handle the increased workload caused by the growing number of sensors.\n\nE. Re-architecting the database tier to use Amazon DynamoDB instead of RDS MySQL would require significant changes to the application and might not be cost-efficient, considering the already established use of RDS MySQL. DynamoDB is a NoSQL database and requires a different data modeling approach compared to a relational database like MySQL."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/amazon/view/95309-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.<br>The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the API Gateway Regional endpoints to edge-optimized endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 53,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 26,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T11:00:00.000Z",
        "voteCount": 22,
        "content": "A and C are correct answers.\nA. Enable S3 Transfer Acceleration on the S3 bucket and ensure that the web application uses the Transfer Acceleration signed URLs will accelerate the uploads of documents to S3 bucket, this will help to reduce the latency for users outside of Europe.\nC. Change the API Gateway Regional endpoints to edge-optimized endpoints will help the company to improve the latency by caching the responses of the API Gateway closer to the users."
      },
      {
        "date": "2024-04-12T12:01:00.000Z",
        "voteCount": 1,
        "content": "A is wrong because why would you enable transfer acceleration when transfer acceleration uses the cloudfront distribution system. it makes no sense"
      },
      {
        "date": "2024-07-19T12:17:00.000Z",
        "voteCount": 1,
        "content": "S3 Global acceleration is used to upload files, so, the users can upload faster the documents in any part of the world"
      },
      {
        "date": "2023-06-19T05:40:00.000Z",
        "voteCount": 2,
        "content": "A is wrong because the users of S3 are the lambda functions, not the end user. \"The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\""
      },
      {
        "date": "2023-09-07T12:25:00.000Z",
        "voteCount": 2,
        "content": "Users of S3 are not lambda, lambda is used only for writing to serverless database. Also, Aurora serverless global database only writes in one cluster and the other region cluster are used only for reads. So no matter from which location you upload, the metadata will be written to cluster in Central Europe . If it was Global DynnamoDB table then it could have helped to reduce latency."
      },
      {
        "date": "2024-01-17T06:36:00.000Z",
        "voteCount": 1,
        "content": "\"web app uses CloudFront distribution for delivery with Amazon S3 as the origin\" and \"Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket\" these 2 sentences let me think that users are not uploading via CluodFront into the S3 bucket at its origin, rather docs are uploaded from the Lambda"
      },
      {
        "date": "2023-01-14T11:00:00.000Z",
        "voteCount": 5,
        "content": "B. Creating an accelerator in AWS Global Accelerator and attaching it to the CloudFront distribution will not help in this scenario as it only helps to route the traffic to the optimal endpoint based on the location of the user.\nD. Provisioning the entire stack in two other locations that are spread across the world and using global databases on the Aurora Serverless cluster will help to reduce the latency but it would be more complex to implement and manage.\nE. Adding an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database will not help in this scenario because it is only used to improve connection management and load balancing for Amazon RDS databases, but not for Aurora Serverless databases."
      },
      {
        "date": "2023-01-14T11:00:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/69470-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-23T03:14:00.000Z",
        "voteCount": 1,
        "content": "Complexity is not evidence against option D.\nFurthermore, option D is correct because the question statement also suggests that costs can be incurred.\nOn the other hand, A is not a method to eliminate geographical factors."
      },
      {
        "date": "2024-07-19T12:16:00.000Z",
        "voteCount": 1,
        "content": "AC will improve latency using AWS edge locations worldwide, adding 2 locations will only benefit those 2 locations"
      },
      {
        "date": "2024-07-06T23:46:00.000Z",
        "voteCount": 2,
        "content": "A, C for sure.\nB is wrong; AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection."
      },
      {
        "date": "2024-04-05T04:31:00.000Z",
        "voteCount": 2,
        "content": "A and C for me are the correct answers.\nD is not so usefull as we are recreating the entire stack and increase a lot the costs. As first approach, A and C are the most appropriate"
      },
      {
        "date": "2024-03-25T00:33:00.000Z",
        "voteCount": 3,
        "content": "Aurora serverless does not support global database. search DB instance class requirements in https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-getting-started.html"
      },
      {
        "date": "2024-05-25T03:52:00.000Z",
        "voteCount": 1,
        "content": "it does in V2.\n\n[] https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html#aurora-serverless-v2.advantages : Using Aurora Serverless v2 - Advantages of Aurora Serverless v2"
      },
      {
        "date": "2024-03-21T10:14:00.000Z",
        "voteCount": 2,
        "content": "By elimination: B is pointless, as CF already does geo proximity. D is impossible as global DBs aren't supported by Aurora Serverless. E doesn't really help.\n\nRemaining: A and C, which are sensible and will do the trick."
      },
      {
        "date": "2024-03-17T06:14:00.000Z",
        "voteCount": 2,
        "content": "AC, s3 transfer acceleration + edge-optimised api gateway"
      },
      {
        "date": "2024-01-17T07:01:00.000Z",
        "voteCount": 2,
        "content": "This is tricky. Here is my take having in mind that the question is \"The company must improve latency outside of Europe\",.\n\nA = Transfer Acceleration improves upload/downlad time, but we have already CloudFront that can also be used to speedup upload. This will not further improve. Also I don't know how to combine TA with CF\nB = This will not help and also I don't know how to combine GA with CF\nC = correct\nD = correct\nE = RDS Proxy do not improve latency"
      },
      {
        "date": "2024-03-01T13:57:00.000Z",
        "voteCount": 1,
        "content": "Looks like D is wrong because you don't use global databases on the Aurora Serverless cluster. That is just not a feature given by Aurora Serverless (even v2). However, it does support using Aurora Serverless in global databases. \"The secondary clusters\" in the link below is a reference to Aurora Global Database.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.how-it-works.html#aurora-serverless.ha:~:text=You%20can%20use%20Aurora%20Serverless%20v2%20capacity%20in%20the%20secondary%20clusters%20so%20they%27re%20ready%20to%20take%20over%20during%20disaster%20recovery."
      },
      {
        "date": "2024-03-04T07:57:00.000Z",
        "voteCount": 1,
        "content": "In addition, we are more likely to get latency from Lambda functions loading documents into S3 from API Gateway calls than we are from Lambda functions loading metadata into Aurora Serverless DB. \n\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/"
      },
      {
        "date": "2023-12-28T00:02:00.000Z",
        "voteCount": 3,
        "content": "Tricky Tricky.\n\nA. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs. -&gt; Wrong. No such thing like TA signed URLs.\nB. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution. -&gt; Wrong GA does not support CF.\nC. Change the API Gateway Regional endpoints to edge-optimized endpoints.\nD. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.\nE. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database. -&gt; Wrong. It is not related with latency."
      },
      {
        "date": "2024-01-03T09:37:00.000Z",
        "voteCount": 1,
        "content": "Yes there is, https://stackoverflow.com/questions/37437782/aws-transfer-acceleration-with-pre-signed-urls-using-javascript-sdk"
      },
      {
        "date": "2024-01-13T03:51:00.000Z",
        "voteCount": 1,
        "content": "Sorry. I was wrong. Answer is A C.\nserverless does not support global database and RDS proxy."
      },
      {
        "date": "2024-01-13T03:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations"
      },
      {
        "date": "2024-05-25T03:52:00.000Z",
        "voteCount": 1,
        "content": "it does in V2.\n[] https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html#aurora-serverless-v2.advantages : Using Aurora Serverless v2 - Advantages of Aurora Serverless v2"
      },
      {
        "date": "2023-12-13T05:09:00.000Z",
        "voteCount": 2,
        "content": "A and C makes sense.\nA is clear as what masetromain has explained. \nC, An edge-optimized API endpoint typically routes requests to the nearest CloudFront Point of Presence (POP). It certaintly improve the latency of traffic originating from Europe as the traffic will now be directed to the nearest POP instead of the origin API Gateway."
      },
      {
        "date": "2023-11-14T00:16:00.000Z",
        "voteCount": 1,
        "content": "see Sab answer"
      },
      {
        "date": "2023-10-08T03:20:00.000Z",
        "voteCount": 1,
        "content": "\"The company must improve latency outside of Europe.\"\nThen in where are you going to provision an additional stack? It only says \"outside of Europe.\"\nUSA? Asia? Where?\nYou have to consider an overall latency. \nI'll go for AC"
      },
      {
        "date": "2023-09-09T06:09:00.000Z",
        "voteCount": 1,
        "content": "AD\nIssue is minimize latency for \"users uploading documents\"\nIts NOT an issue with the latency of website being delivered to the users.\n\nGlobal Accelerator - Is used to decrease latency in having the user request delivered using AWS backbone network to the point of Origin\nBut it doesnt accelerate delivery of uploaded files into S3 .... so A is a better option.\n\nRDS Proxy is used to decrease the time in establishing the DB connectivity ... It keeps few DB connections on warm-by condition. Option D doesn't help in reducing cross-Region latency\n\nAPI Gateway edge point will reduce the latency in serving the website closer to ur location. But here question is about uploading document.\n\nAurora Serverless Global - can be used for uploading meta-data reducing latency time."
      },
      {
        "date": "2023-09-05T01:27:00.000Z",
        "voteCount": 2,
        "content": "On a global scale, and particularly for users outside of Europe, the API Gateway and S3 access operations are the most likely components to introduce significant latency.\nFor the API Gateway, changing from regional endpoints to edge-optimized endpoints would bring API calls closer to global users.\nFor S3, enabling Transfer Acceleration would speed up the uploading and downloading of files.\nTherefore, based on the provided system overview, these two components are the most likely areas needing optimization to reduce latency."
      },
      {
        "date": "2023-08-28T00:33:00.000Z",
        "voteCount": 2,
        "content": "even though option D is complex, it would decrease the latency outside eu region."
      },
      {
        "date": "2023-08-14T10:20:00.000Z",
        "voteCount": 1,
        "content": "S3 Transfer Acceleration primarily improves upload speeds to an S3 bucket and doesn't significantly affect the latency of the web application itself."
      },
      {
        "date": "2023-08-10T04:24:00.000Z",
        "voteCount": 1,
        "content": "I prefer CD. \nA is not right. You already get a cloudfront, what is the acceleration used for."
      },
      {
        "date": "2023-09-30T22:42:00.000Z",
        "voteCount": 1,
        "content": "in S3 hosted web application you upload directly to s3 using s3 url"
      },
      {
        "date": "2024-01-17T06:48:00.000Z",
        "voteCount": 1,
        "content": "You can (and should) avoid enabling S3 web site hosting working with CloudFront. You can simply configure the S3 as the distribution origin"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/amazon/view/95376-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.<br><br>The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure S3 Intelligent-Tiering on the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:12:00.000Z",
        "voteCount": 14,
        "content": "The correct answer is A. Configure S3 Intelligent-Tiering on the S3 bucket.\n\nAmazon S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers based on changing access patterns. Objects that are accessed frequently are stored in the frequent access tier and objects that are accessed infrequently are stored in the infrequent access tier. This allows for cost optimization without requiring manual intervention. This makes it an ideal solution for the scenario described, as it can automatically move objects that are infrequently accessed after 30 days to a lower-cost storage tier while still maintaining millisecond retrieval availability."
      },
      {
        "date": "2023-01-15T01:12:00.000Z",
        "voteCount": 3,
        "content": "Option B is not correct as it only moves data to S3 Glacier Deep Archive after 30 days, which would still require additional steps to retrieve the data.\nOption C is not correct because Amazon Elastic File System (Amazon EFS) is a file storage service for use with Amazon EC2 instances, it does not provide a cost-effective solution for storing and retrieving large amounts of data.\nOption D is not correct because adding a Cache-Control: max-age header only controls the caching behavior of the objects and does not address the cost optimization requirements."
      },
      {
        "date": "2023-01-29T21:15:00.000Z",
        "voteCount": 1,
        "content": "Option D works for the reduction cost on retrieval request"
      },
      {
        "date": "2023-01-30T06:23:00.000Z",
        "voteCount": 1,
        "content": "take the test then tell us if your answers are valid, if they are share them with us ;)"
      },
      {
        "date": "2024-02-25T03:25:00.000Z",
        "voteCount": 1,
        "content": "A is right\nB S3 Glacier Deep Archive after 30 days is not correct, retrieval takes time so incorrect."
      },
      {
        "date": "2023-10-25T11:13:00.000Z",
        "voteCount": 1,
        "content": "millisecond retrieval availability"
      },
      {
        "date": "2023-10-08T03:21:00.000Z",
        "voteCount": 2,
        "content": "A, no brainer"
      },
      {
        "date": "2023-09-05T01:43:00.000Z",
        "voteCount": 1,
        "content": "A. Configure S3 Intelligent-Tiering on the S3 bucket: This option would automatically move objects to different storage tiers based on their access patterns. For objects that are infrequently accessed, this would help to reduce storage costs. For those that continue to be accessed frequently, they would remain in a higher-cost but faster-access tier. This should be the option that meets the requirements.\n\nB. Configure an S3 Lifecycle policy to transition image and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days: This option would significantly lower storage costs, but the retrieval time for Glacier Deep Archive could take several hours, which does not meet the millisecond retrieval requirement."
      },
      {
        "date": "2023-08-10T04:28:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2023-07-25T00:50:00.000Z",
        "voteCount": 1,
        "content": "B is wrong due to the Glacier Deep Archive part which is not warranted by the question.\n\nC is wrong due to the cost of EFS and because it would require some kind of EC2 instance.\n\nD would help caching the objects on proxies and clients, but other than that..."
      },
      {
        "date": "2023-07-02T14:20:00.000Z",
        "voteCount": 1,
        "content": "A of course"
      },
      {
        "date": "2023-06-21T05:24:00.000Z",
        "voteCount": 1,
        "content": "I was hesitating between A and D and D looks like a really good option but it's missing one part - we do not do anything with the storage class in this option - we only update the cache TTL which would possibly reduce some costs, however, we keep paying the same price for storage. Hence I switched to A"
      },
      {
        "date": "2023-03-26T06:29:00.000Z",
        "voteCount": 1,
        "content": "A - easy question"
      },
      {
        "date": "2023-03-19T16:32:00.000Z",
        "voteCount": 1,
        "content": "A - S3 Intelligent-Tiering can fit the requirement"
      },
      {
        "date": "2023-03-04T12:59:00.000Z",
        "voteCount": 4,
        "content": "First half of question drags you to answer B but SA found that some media is being used even after downloads. so data is being accessed in unknown patterns. Way to go is Intelligent tier."
      },
      {
        "date": "2023-03-04T13:00:00.000Z",
        "voteCount": 1,
        "content": "*I meant even after 30 days (not downloads in above comment)"
      },
      {
        "date": "2023-02-20T00:42:00.000Z",
        "voteCount": 1,
        "content": "This is my open. The question ask us maintains millisecond retrieval ability. It means we can't use cold storage (So, A, B is not answer). EFS is expensive and not durable. If we use client cache (Ignore client's volume), we can reduce network costs(actually s3's storage costs is really cheap). It means that we can reduce costs too."
      },
      {
        "date": "2023-02-20T00:44:00.000Z",
        "voteCount": 2,
        "content": "There are lots of wrong types. Please forgive me. English is not familiar with me yet."
      },
      {
        "date": "2023-02-23T23:05:00.000Z",
        "voteCount": 2,
        "content": "The keyword is millisecond retrieval time, which rules everything out except A."
      },
      {
        "date": "2023-02-15T08:32:00.000Z",
        "voteCount": 1,
        "content": "bc A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days."
      },
      {
        "date": "2023-01-30T14:21:00.000Z",
        "voteCount": 2,
        "content": "typico A S3 Intelligent-Tiering"
      },
      {
        "date": "2023-01-29T21:22:00.000Z",
        "voteCount": 1,
        "content": "D it will reduce the cost on retrieval requests"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/amazon/view/95377-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year.<br><br>A solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-17T19:17:00.000Z",
        "voteCount": 14,
        "content": "Storage class: After you configure a filter, you'll start seeing data analysis based on the filter in the Amazon S3 console in 24 to 48 hours. However, storage class analysis observes the access patterns of a filtered data set for 30 days or longer to gather information for analysis before giving a result\n\nStorage Lens: All S3 Storage Lens metrics are retained for a period of 15 months. However, metrics are only available for queries for a specific duration, which depends on your metrics selection. This duration can't be modified. Free metrics are available for queries for a 14-day period, and advanced metrics are available for queries for a 15-month period.\n\nYou have to upgrade regardless to query up to 12 months"
      },
      {
        "date": "2023-01-28T08:53:00.000Z",
        "voteCount": 7,
        "content": "Both B and C are good.\nI guess AWS wants clients to use S3 Storage Lens... Hence I vote C."
      },
      {
        "date": "2023-01-30T14:24:00.000Z",
        "voteCount": 3,
        "content": "agree with u gess aws want us to know about Lens"
      },
      {
        "date": "2024-07-08T04:48:00.000Z",
        "voteCount": 1,
        "content": "C, for sure."
      },
      {
        "date": "2024-05-22T08:46:00.000Z",
        "voteCount": 1,
        "content": "B ..S3 Storage Class Analysis is specifically designed to help you analyze storage access patterns. It monitors the access patterns of objects and provides insights into when it is appropriate to transition objects to different storage classes ."
      },
      {
        "date": "2024-03-17T06:15:00.000Z",
        "voteCount": 1,
        "content": "C, S3 Storage Lens offers comprehensive visibility into storage usage and activity trends across the AWS Organization, facilitating informed decisions on cost optimization and storage efficiency"
      },
      {
        "date": "2024-02-13T12:55:00.000Z",
        "voteCount": 1,
        "content": "Option C refers to using Amazon S3 Storage Lens, which provides organization-wide visibility into object storage usage and activity trends. By upgrading to include advanced metrics and recommendations, users can access detailed insights that help optimize storage costs across their S3 resources. S3 Storage Lens offers dashboard views and metrics that can directly inform on the appropriate storage class based on actual usage patterns, making it a comprehensive solution for the stated requirements."
      },
      {
        "date": "2024-01-23T17:52:00.000Z",
        "voteCount": 1,
        "content": "Amazon S3 Storage Class Analysis:\n\nAmazon S3 provides a Storage Class Analysis tool that helps you analyze access patterns to your S3 objects over time. You can enable it on your S3 bucket to collect data on object access patterns."
      },
      {
        "date": "2024-01-23T17:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is B."
      },
      {
        "date": "2024-01-17T08:15:00.000Z",
        "voteCount": 1,
        "content": "To me here the key sentence is \"review data trends for the past 12 months\"\nA = CUR provides detailed usage data but it is not the best tool for this job\nB = S3 storage class analysis provides recommendation for Standard and Standard IA storage classes, but does not provide data trends\nC = correct\nD = Access Analyzer provides visibility for buckets that are configured to allow access to anyone on the internet or other AWS accounts"
      },
      {
        "date": "2023-12-17T02:21:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer, because it suffices a bucket analysis --&gt; https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html\nC instead is a solution for a more organization-wide analysis of bucket --&gt; https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html"
      },
      {
        "date": "2023-11-14T00:20:00.000Z",
        "voteCount": 1,
        "content": "see  AMohanty answer"
      },
      {
        "date": "2023-09-17T23:28:00.000Z",
        "voteCount": 2,
        "content": "S3 Storage Class Analysis enables you to monitor access patterns across objects to help you decide when to transition data to the right storage class to optimize costs."
      },
      {
        "date": "2024-01-03T10:14:00.000Z",
        "voteCount": 1,
        "content": "Storage Class is only used for recommendation for Standard to Standard IA"
      },
      {
        "date": "2023-09-09T08:46:00.000Z",
        "voteCount": 3,
        "content": "C\nStorage Class is only used for recommendation for Standard to Standard IA"
      },
      {
        "date": "2023-09-05T02:05:00.000Z",
        "voteCount": 2,
        "content": "Option B: Amazon S3's Storage Class Analysis function is mainly used to analyze the access patterns of objects in S3 buckets so that you can transfer these objects to the most cost-effective storage class. However, this feature does not provide detailed historical data for the past 12 months; it is more about observing current usage patterns and making the best storage class decisions based on those patterns.\n\nIf you need detailed storage trends and object status over the past 12 months, option C (using Amazon S3 Storage Lens) may be a better choice. Amazon S3 Storage Lens provides comprehensive storage analysis, including historical trends and advanced metrics, which may be more suitable for analyzing long-term data and storage conditions."
      },
      {
        "date": "2023-07-03T20:17:00.000Z",
        "voteCount": 1,
        "content": "I choose C.\nB. Storage class analysis only provides recommendations for Standard to Standard IA classes. The company uses a variety of storage classes."
      },
      {
        "date": "2023-07-02T14:45:00.000Z",
        "voteCount": 1,
        "content": "a hard one ... I guess C, but could be B :/"
      },
      {
        "date": "2023-06-07T05:24:00.000Z",
        "voteCount": 1,
        "content": "By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. \n\nSo it meet the qn objective of identify the appropriate storage class for the objects"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/amazon/view/95378-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company\u2019s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:19:00.000Z",
        "voteCount": 16,
        "content": "The correct answer is C. Use AWS Organizations and AWS CloudFormation StackSets.\nAWS Organizations allows the management of multiple AWS accounts as a single entity and AWS CloudFormation StackSets allows creating, updating, and deleting stacks across multiple accounts and regions in an organization. This solution allows creating a single CloudFormation template that can be deployed across multiple accounts and regions, and also allows for the management of access and permissions for the different accounts through the use of IAM roles and policies in the management account."
      },
      {
        "date": "2023-01-15T01:19:00.000Z",
        "voteCount": 6,
        "content": "Option A and D both use AWS CloudFormation, but do not take into account the management of multiple accounts and regions. Option B uses AWS Organizations but doesn't include the use of CloudFormation StackSets, which is necessary for managing deployments across multiple accounts and regions."
      },
      {
        "date": "2024-01-03T10:20:00.000Z",
        "voteCount": 1,
        "content": "I agree with what you say here, C is a good choice, but in B they mention Control Tower which is also used to manage multiple accounts, couldn't it be a correct answer also?"
      },
      {
        "date": "2024-01-17T08:41:00.000Z",
        "voteCount": 3,
        "content": "A = cloud work but it is hard\nB = Control Tower cannot manage stack deployments across accounts\nC = correct\nD = nested stack allows to provision resources by using different CloudFormation templates"
      },
      {
        "date": "2023-10-02T03:22:00.000Z",
        "voteCount": 1,
        "content": "Option C is the most suitable. Here's why:\n\nAWS Organizations: AWS Organizations helps you centrally manage multiple AWS accounts, which is especially useful when dealing with multiple Regions and accounts. You can organize your accounts into an organizational structure, apply policies across accounts, and manage billing.\n\nAWS CloudFormation StackSets: StackSets is a CloudFormation feature that enables you to deploy CloudFormation stacks across multiple accounts and Regions with a single CloudFormation template. This simplifies the process of deploying and managing infrastructure consistently across your organization."
      },
      {
        "date": "2023-07-02T14:46:00.000Z",
        "voteCount": 2,
        "content": "C no doubt"
      },
      {
        "date": "2023-06-18T10:27:00.000Z",
        "voteCount": 1,
        "content": "keywords =  AWS Organizations &amp;&amp; AWS CloudFormation StackSets."
      },
      {
        "date": "2023-05-08T08:46:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\nCloud Formation Stack Sets allow you to roll out Cloud Formation stacks over multiple AWS accounts and in multiple Regions with just a couple of clicks. When we launched Stack Sets, grouping accounts was primarily for billing purposes. Since the launch of AWS Organizations, you can centrally manage multiple AWS accounts across diverse business needs including billing, access control, compliance, security, and resource sharing."
      },
      {
        "date": "2023-03-26T06:31:00.000Z",
        "voteCount": 2,
        "content": "Use AWS Organizations and AWS CloudFormation StackSets"
      },
      {
        "date": "2023-01-30T14:28:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/amazon/view/95379-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company\u2019s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:20:00.000Z",
        "voteCount": 17,
        "content": "same question of \"Questions #84\""
      },
      {
        "date": "2023-10-31T11:15:00.000Z",
        "voteCount": 5,
        "content": "These Site Moderators getting lazy boy!"
      },
      {
        "date": "2023-07-02T14:47:00.000Z",
        "voteCount": 2,
        "content": "C. a dup question"
      },
      {
        "date": "2023-05-08T09:13:00.000Z",
        "voteCount": 1,
        "content": "This question is duplicated in the Exam Topics site. Question 85 is the same as Question 84"
      },
      {
        "date": "2023-05-04T17:22:00.000Z",
        "voteCount": 1,
        "content": "C:\nhttps://sanderknape.com/2017/07/cloudformation-stacksets-automated-cross-account-region-deployments/#:~:text=A%20StackSet%20is%20a%20set,deploying%20to%20multiple%20accounts%2Fregions."
      },
      {
        "date": "2023-04-04T09:43:00.000Z",
        "voteCount": 3,
        "content": "Thought that my internet was intertupted. then i was wrong =)))"
      },
      {
        "date": "2023-02-01T09:58:00.000Z",
        "voteCount": 2,
        "content": "This is repeated :-("
      },
      {
        "date": "2023-02-01T09:42:00.000Z",
        "voteCount": 3,
        "content": "Duplicate question with #84"
      },
      {
        "date": "2023-01-15T15:39:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/amazon/view/95380-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:<br><br>\u2022\tIt should allow changes to be released several times every hour.<br>\u2022\tIt should be able to roll back the changes as quickly as possible.<br><br>Which design will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRoll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:24:00.000Z",
        "voteCount": 16,
        "content": "The correct answer is B. Specifying AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application and swapping the staging and production environment URLs. This approach allows the company to deploy updates several times an hour and quickly roll back changes as needed.\n\nOption A, Deploying a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances, while it may provide a way to roll back changes by replacing instances with previous versions, it may not allow for rapid deployment of updates multiple times per hour."
      },
      {
        "date": "2023-01-15T01:24:00.000Z",
        "voteCount": 4,
        "content": "Option C, Using AWS Systems Manager to re-provision the infrastructure for each deployment. Updating the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and using Amazon Route 53 weighted routing to point to the new environment, would require more time-consuming steps and may not be able to roll back changes as quickly.\n\nOption D, Rolling out the application updates as part of an Auto Scaling event using prebuilt AMIs. Using new versions of the AMIs to add instances and phasing out all instances that use the previous AMI version with the configured termination policy during a deployment event, while it may be a way to roll back changes, it doesn't allow for rapid deployment of updates multiple times per hour."
      },
      {
        "date": "2024-01-03T10:36:00.000Z",
        "voteCount": 1,
        "content": "Good explanation, but concerning option C it is not quite right, you say that 'may not be able to roll back changes as quickly.', but since it is using Route 53 weighted configuration, in case of failure of the new instances, you just need to change again the weighted configuration to point 100% to the old instances while you replace again the new instances by old instances."
      },
      {
        "date": "2024-07-14T01:06:00.000Z",
        "voteCount": 1,
        "content": "B, for sure.\nUsing AWS Elastic Beanstalk environment Swap.\nhttps://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html"
      },
      {
        "date": "2024-01-17T08:55:00.000Z",
        "voteCount": 1,
        "content": "A = replacing existing EC2 instances does not allow for roll back the changes as quickly as possible\nB = correct (tough Beanstalk is not the best service for releasing several times every hour)\nC = could work, but here you are combining SSM and user data to achieve what beanstalk does natively\nD = this would not work as you need to build AMIs (AMI Builder not mentioned) and also rapid rollback is better achieved avoiding termination of old AMI version"
      },
      {
        "date": "2023-07-02T14:50:00.000Z",
        "voteCount": 1,
        "content": "probably B"
      },
      {
        "date": "2023-05-08T09:22:00.000Z",
        "voteCount": 1,
        "content": "Imagine the cost for replacing AMIs and EC2 or re-provision infrastructure several times per day. Although cost effectiveness is not part the requirement in the question. the only option that seems correct is B."
      },
      {
        "date": "2023-03-26T06:32:00.000Z",
        "voteCount": 1,
        "content": "B. Specify AWS Elastic Beanstalk"
      },
      {
        "date": "2023-01-22T22:40:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/amazon/view/95381-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own security group.<br><br>The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 59,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:27:00.000Z",
        "voteCount": 31,
        "content": "The correct combination of steps to meet these requirements is B and C.\n\nB. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port. This allows the instances to make outbound connections to the DB cluster on the default Aurora port.\n\nC. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port. This allows connections to the DB cluster from the EC2 instances on the default Aurora port."
      },
      {
        "date": "2023-01-15T01:27:00.000Z",
        "voteCount": 3,
        "content": "A. Adding an inbound rule to the EC2 instances' security group would allow incoming connections to the instances on the default Aurora port, but it would not allow the instances to connect to the DB cluster.\n\nD. Adding an outbound rule to the DB cluster's security group would allow the DB cluster to make outbound connections to the EC2 instances on the default Aurora port, but it would not allow connections to the DB cluster from the instances.\n\nE. Adding an outbound rule to the DB cluster's security group specifying the EC2 instances' security group as the destination over the ephemeral ports would allow the DB cluster to make outbound connections to the instances on ephemeral ports, but it would not allow connections to the DB cluster from the instances on the default Aurora port."
      },
      {
        "date": "2023-09-19T09:06:00.000Z",
        "voteCount": 2,
        "content": "Security group is stateful. So you just need to set up inbound"
      },
      {
        "date": "2023-06-20T12:43:00.000Z",
        "voteCount": 3,
        "content": "why we should add an outbound rule to the EC2 instances' security group??? it is already allowed by default in the EC2 security group becauce all outbound ports are allowed by default."
      },
      {
        "date": "2023-11-25T01:01:00.000Z",
        "voteCount": 1,
        "content": "wow..then in that case your EC2 instance can talk to anything. No SG rule is required. You need to establish a connectivity route first."
      },
      {
        "date": "2024-01-17T08:57:00.000Z",
        "voteCount": 1,
        "content": "it is the other way around, all connection are denied and you can only allow connection. You need outbound from EC2 to Aurora to allow the app initiate a connection to the database instance"
      },
      {
        "date": "2023-02-15T23:48:00.000Z",
        "voteCount": 12,
        "content": "To provide the application with least privilege access to the Aurora DB cluster, the solutions architect should add inbound rules to both the security groups.\n\nFor the EC2 instances' security group, an inbound rule should be added that allows traffic from the DB cluster's security group over the default Aurora port. This will allow the EC2 instances to communicate with the Aurora DB cluster.\n\nFor the Aurora DB cluster's security group, an inbound rule should be added that allows traffic from the EC2 instances' security group over the default Aurora port. This will allow the Aurora DB cluster to communicate with the EC2 instances.\n\nBy default all outbound rules are open, it's only the ingress that needs to allow traffic."
      },
      {
        "date": "2023-02-21T19:15:00.000Z",
        "voteCount": 7,
        "content": "B&amp;C after doing a recreate in the AWS Console, stand corrected."
      },
      {
        "date": "2023-02-22T16:54:00.000Z",
        "voteCount": 3,
        "content": "To provide the application with least privilege access to the Amazon Aurora DB Cluster, the solutions architect should take the following steps:\n\nAdd an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port (port 3306). This will allow the EC2 instances to connect to the Aurora DB Cluster.\n\nAdd an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port (port 3306). This will allow the EC2 instances to send traffic to the Aurora DB Cluster."
      },
      {
        "date": "2024-10-08T23:38:00.000Z",
        "voteCount": 1,
        "content": "NB. The DB cluster doesn't need to initiate connections to the EC2 instances."
      },
      {
        "date": "2024-03-17T06:18:00.000Z",
        "voteCount": 2,
        "content": "BC, ec2 -&gt; bd; ec2 outbound rule to allow access to bd; db inbound rule to allow access from ec2"
      },
      {
        "date": "2024-02-03T11:16:00.000Z",
        "voteCount": 3,
        "content": "Tricky question. They say with least privileges, so I think they don't want to use default (allow-all) rule, but limit as much as possible and allow only specific traffic to DB)\n\n\"By default, a security group includes an outbound rule that allows all outbound traffic. We recommend that you remove this default rule and add outbound rules that allow specific outbound traffic only.\"\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/vpc-security-groups.html"
      },
      {
        "date": "2024-01-18T10:12:00.000Z",
        "voteCount": 1,
        "content": "CE\n- A and B are nonsense, since they talk about aurora port on ec2 SGs. In SG you always put rules on the local ports.\n- C obvious\n- E over D, always ephemeral on outbound, but at the condition we replace the existing all open rule"
      },
      {
        "date": "2024-01-03T10:48:00.000Z",
        "voteCount": 2,
        "content": "I believe that C is enough, we don't need to define the outbound from EC2 to DB, but since we have to choose two, the only other option that is correct is B. And someone say below that have tested this configuration, so I hope he tested defining only what is mentioned in C, to see if it is enough or not. It would be nice."
      },
      {
        "date": "2023-12-04T15:18:00.000Z",
        "voteCount": 1,
        "content": "Answer - B&amp; C\nOutbound rule to the EC2 SG with DB SG as destination \nInbound rule to the DB SG with EC2 SG as source"
      },
      {
        "date": "2023-11-29T19:02:00.000Z",
        "voteCount": 1,
        "content": "Security Groups are stateful, that means you don't need to specify an outbound rule if you have an inbound rule that permit access to the resource. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html#security-group-basics \n\nIn other hand, the outbound traffic rules typically don't apply to DB clusters. Outbound traffic rules apply only if the DB cluster acts as a client. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.RDSSecurityGroups.html#Overview.RDSSecurityGroups.VPCSec.\n\nBecause of that B, D and E are wrong answers"
      },
      {
        "date": "2023-09-05T04:57:00.000Z",
        "voteCount": 2,
        "content": "By default, AWS Security Groups allow all outbound traffic. Therefore, in most cases, there's no need to configure outbound rules unless you have specific security requirements.\n\nAdd an inbound rule to the EC2 instance's security group, setting the DB cluster's security group as the source over Aurora's default port. This enables interaction between the DB Cluster and the EC2 instances. Corresponds to Option A.\n\nAdd an inbound rule to the DB Cluster's security group, setting the EC2 instance's security group as the source over Aurora's default port. This allows the EC2 instances to interact with the DB Cluster. Corresponds to Option C."
      },
      {
        "date": "2023-09-05T05:06:00.000Z",
        "voteCount": 1,
        "content": "By the way, the outbound rules are unnecessary in this case because the database cluster does not need to access any data from the application. The database cluster only needs to receive traffic from the application so that the application can read and write to the database."
      },
      {
        "date": "2023-11-29T19:00:00.000Z",
        "voteCount": 1,
        "content": "my two cents.\nAgree AC are the correct answer.  \nSecurity Groups are stateful, that means you don't need to specify an outbound rule if you have an inbound rule that permit access to the resource.  https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html#security-group-basics\nIn other hand, the outbound traffic rules typically don't apply to DB clusters. Outbound traffic rules apply only if the DB cluster acts as a client.  https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.RDSSecurityGroups.html#Overview.RDSSecurityGroups.VPCSec"
      },
      {
        "date": "2023-08-23T09:16:00.000Z",
        "voteCount": 1,
        "content": "By default, all outbound rules are allow"
      },
      {
        "date": "2023-08-31T20:12:00.000Z",
        "voteCount": 1,
        "content": "Don't provide wrong answer. Answer is B,C"
      },
      {
        "date": "2023-11-25T01:06:00.000Z",
        "voteCount": 1,
        "content": "you are providing the wrong answer. The correct answer is AC. Inbound rules are supposed to be added."
      },
      {
        "date": "2023-08-31T20:11:00.000Z",
        "voteCount": 1,
        "content": "The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster."
      },
      {
        "date": "2023-07-02T14:53:00.000Z",
        "voteCount": 2,
        "content": "BC of course"
      },
      {
        "date": "2023-11-25T01:07:00.000Z",
        "voteCount": 1,
        "content": "AC is correct."
      },
      {
        "date": "2023-06-19T06:05:00.000Z",
        "voteCount": 2,
        "content": "It is outbound from the clients to the db server listening port. And inbound to the db server listening ports from the clients."
      },
      {
        "date": "2023-05-31T06:32:00.000Z",
        "voteCount": 1,
        "content": "\"My choice relays on the fact that the security groups are stateful, so we only need to allow the outbound traffic for the ec2 instances to pass and the return will also be allowed. Same for the RDS. This combination is also based on the standard traffic flow initiated from instance to DB\""
      },
      {
        "date": "2023-04-26T12:45:00.000Z",
        "voteCount": 3,
        "content": "My choice relays on the fact that the security groups are stateful, so we only need to allow the outbound traffic for the ec2 instances to pass and the return will also be allowed. Same for the RDS. This combination is also based on the standard traffic flow initiated from instance to DB."
      },
      {
        "date": "2023-03-26T06:36:00.000Z",
        "voteCount": 2,
        "content": "BC gets my vote"
      },
      {
        "date": "2023-03-17T19:36:00.000Z",
        "voteCount": 3,
        "content": "Look at the traffic - from the instances EC2 -&gt; DB Cluster I need to go to it as the destination and port (outbound, nothing more or less); so that DB responses needs to see my Security group (since they are shared) coming inbound on that port; any other port deny. \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/working-with-security-groups.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/amazon/view/95382-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold.<br><br>Which solution is the MOST cost-effective way to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment. and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:32:00.000Z",
        "voteCount": 23,
        "content": "B. Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.\n\nThis option is the most cost-effective because it utilizes the organization's management account to set budgets and configure alerts for all accounts in the organization, rather than having to configure budgets and alerts individually in each account. Additionally, using Cost Explorer in the management account allows the cloud governance team to view the consolidated spending for all accounts in the organization and create reports for each business unit. This eliminates the need to access each individual account to view costs and create reports."
      },
      {
        "date": "2023-01-15T01:32:00.000Z",
        "voteCount": 4,
        "content": "Option A is not the most cost-effective solution because it requires configuring budgets and reports in multiple accounts, which increases the complexity and cost of managing the cloud spending for each business unit.\n\nOption C is not the most cost-effective solution because it requires the cloud governance team to access the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit, which increases the complexity and cost of managing the cloud spending for each business unit.\n\nOption D is not the most cost-effective solution because it requires creating an AWS Lambda function to process AWS Cost and Usage Reports, which increases the complexity and cost of managing the cloud spending for each business unit."
      },
      {
        "date": "2023-07-02T14:55:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-21T06:07:00.000Z",
        "voteCount": 2,
        "content": "\"configure budget alerts that are grouped by application, environment, and owner\" - I just literally tried to create a budget alert and I am not able to see any option for grouping by tags. Another nonsense question"
      },
      {
        "date": "2023-08-09T02:19:00.000Z",
        "voteCount": 1,
        "content": "Billing &gt; Budgets &gt; Create budget &gt; Customize (advanced) &gt; Budget scope &gt; Filter specific AWS cost dimensions"
      },
      {
        "date": "2023-06-18T10:34:00.000Z",
        "voteCount": 1,
        "content": "keyword = AWS Budgets in the organization's management\nother more overhead each by account"
      },
      {
        "date": "2023-04-16T17:33:00.000Z",
        "voteCount": 4,
        "content": "B\ncentralized solution = management account\nsend notifications for any cloud spending that exceeds a set threshold = AWS Budgets\nhttps://aws.amazon.com/blogs/mt/manage-cost-overruns-part-1/"
      },
      {
        "date": "2023-03-26T06:39:00.000Z",
        "voteCount": 1,
        "content": "B. Configure AWS Budgets in the organization's management account"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/amazon/view/95383-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.<br><br>How can the company prevent users from accidentally deleting data in this way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a stack policy that disallows the deletion of RDS and EBS resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config rules to prevent deleting RDS and EBS resources."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-17T19:57:00.000Z",
        "voteCount": 17,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\nWith the DeletionPolicy attribute you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.\nRetain\nCloudFormation keeps the resource without deleting the resource or its contents when its stack is deleted. You can add this deletion policy to any resource type. When CloudFormation completes the stack deletion, the stack will be in Delete_Complete state; however, resources that are retained continue to exist and continue to incur applicable charges until you delete those resource"
      },
      {
        "date": "2024-10-08T23:59:00.000Z",
        "voteCount": 1,
        "content": "By adding the DeletionPolicy attribute to the CloudFormation template for RDS and EBS resources, you can specify actions to be taken when a stack is deleted. Setting the DeletionPolicy to Retain ensures that the RDS and EBS resources are not deleted when the CloudFormation stack is deleted."
      },
      {
        "date": "2024-02-13T13:20:00.000Z",
        "voteCount": 1,
        "content": "Option A is the correct approach because CloudFormation allows you to specify a DeletionPolicy attribute for resources within your templates. This attribute can prevent resources like Amazon RDS databases and Amazon EBS volumes from being deleted when the stack is deleted. You can set the DeletionPolicy to \u201cRetain\u201d for specific resources, ensuring they are not automatically removed alongside the stack."
      },
      {
        "date": "2023-12-26T08:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"
      },
      {
        "date": "2023-07-02T15:00:00.000Z",
        "voteCount": 2,
        "content": "A, basic DeletionPolicy use case"
      },
      {
        "date": "2023-09-01T05:31:00.000Z",
        "voteCount": 2,
        "content": "Yes but should be supplemented with deletion protection on the database."
      },
      {
        "date": "2023-06-21T06:16:00.000Z",
        "voteCount": 2,
        "content": "Although that I would preferably use both A and B - this is an exam and the truth is in the wording - \"important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted\" - we don't care if the resources are deleted but the data, which makes me believe they want us to set up a deletion policy at a resource level to \"Retain\""
      },
      {
        "date": "2023-06-04T10:10:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\nStack policies are a powerful feature of AWS CloudFormation that allows you to control fine-grained permissions for resources within a stack. By configuring a stack policy that disallows the deletion of RDS and EBS resources, you can prevent users from accidentally deleting these critical resources and the associated data.\n\nOption A (Modifying CloudFormation templates with DeletionPolicy attribute) is not the best solution in this case. While the DeletionPolicy attribute can be used to control resource behavior during stack deletion, it is not applicable to Amazon RDS instances or Amazon EBS volumes."
      },
      {
        "date": "2024-05-25T10:12:00.000Z",
        "voteCount": 1,
        "content": "&gt; the DeletionPolicy attribute [...] is not applicable to Amazon RDS instances or Amazon EBS volumes.\nThis statement is false. From https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\nRetain\n[...] You can add this deletion policy to any resource type.\n\nSnapshot\nResources that support snapshots include:\n[...]\n- AWS::EC2::Volume\n[...]\n- AWS::RDS::DBInstance"
      },
      {
        "date": "2023-06-19T06:12:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A, not because what you say is wrong, but because the question states that the stacks can be deleted, you cannot prevent the deletion of the stack (as required by the question). So the DeletionPolicy will let you delete the stack and retain or take a snapshot of the Database/BUCKET/... (whichever is applicable). You will not lose any data in that case and the stack would have been succesfully deleted."
      },
      {
        "date": "2023-05-08T20:13:00.000Z",
        "voteCount": 3,
        "content": "Check the differences and use cases where to use a stack policy or add a deletion policy (retain):\nStack policy and deletion policy are both ways to protect resources created by CloudFormation stacks, but they have different functions.\nStack policy is a feature that allows you to specify a JSON policy document that restricts what actions can be taken on a CloudFormation stack. Stack policies are used to prevent accidental or intentional updates or deletions of critical resources in your stack, by specifying which resources can be modified and by whom. Stack policies can be used to allow specific teams or individuals to modify specific resources in a stack while preventing them from modifying others."
      },
      {
        "date": "2023-05-08T20:14:00.000Z",
        "voteCount": 1,
        "content": "Deletion policy, on the other hand, is a property of certain AWS resources that determines what happens to the resource when the stack is deleted. The deletion policy can be set to one of three values: \"Delete\", \"Retain\", or \"Snapshot\". When the deletion policy is set to \"Delete\", the resource is deleted when the stack is deleted. When the deletion policy is set to \"Retain\", the resource is not deleted when the stack is deleted, but must be deleted manually. When the deletion policy is set to \"Snapshot\", the resource is deleted when the stack is deleted, but a snapshot of the resource is retained.\nIn summary, stack policies are used to control what changes can be made to a stack, while deletion policies are used to determine what happens to resources when a stack is deleted."
      },
      {
        "date": "2023-04-18T00:24:00.000Z",
        "voteCount": 1,
        "content": "ption B, which suggests configuring a stack policy that disallows the deletion of RDS and EBS resources, is better in this scenario. While using DeletionPolicy attribute (Option A) can be helpful for preserving and backing up the resource, it does not address the problem of accidental deletion of resources or control access to delete the resource.\n\nOn the other hand, a Stack Policy can be used to prevent accidental deletion of resources by specifying which actions can be performed on the resources within in the stack, thereby adding an essential layer of protection.\n\nBy implementing a Stack Policy, a company can limit updating the resources in the stack, control who can make changes to the stack, and prevent accidental deletion of resources. Therefore, configuring a Stack Policy is necessary and more satisfactory to protect data from accidental deletion while using AWS CloudFormation."
      },
      {
        "date": "2023-04-19T10:00:00.000Z",
        "voteCount": 1,
        "content": "You are correct about the process of the UPDATE stack action. What happens to the resources created by the CloudFormation stack when the stack itself is deleted?"
      },
      {
        "date": "2023-03-26T06:40:00.000Z",
        "voteCount": 2,
        "content": "A for sure"
      },
      {
        "date": "2023-03-08T14:58:00.000Z",
        "voteCount": 1,
        "content": "A stack policy is a document that defines the update and deletion actions that can be performed on resources in a CloudFormation stack. By default, all resources in a CloudFormation stack can be deleted by users with appropriate permissions. However, you can use a stack policy to restrict the deletion of certain resources, such as Amazon RDS databases or Amazon EBS volumes.\n\nIn this case, the company can create a stack policy that explicitly disallows the deletion of any RDS or EBS resources in the production CloudFormation stack. This will prevent users from accidentally deleting important data stored in these resources."
      },
      {
        "date": "2023-03-04T21:17:00.000Z",
        "voteCount": 1,
        "content": "For RDS instances, you can set the \"DeletionPolicy\" attribute to \"Retain\". This will ensure that when the stack is deleted, the RDS instance will not be deleted and its data will be retained. \n\nFor EBS volumes, you can use the \"DeletionPolicy\" attribute in combination with the \"SnapshotId\" attribute to create a snapshot of the volume before deleting it. This will allow you to restore the data later if need\n\nYaml examples for RDS and EBS :\n\nResources:\n  MyDB:\n    Type: AWS::RDS::DBInstance\n    Properties:\n      # RDS instance properties go here\n    DeletionPolicy: Retain\n\nResources:\n  MyVolume:\n    Type: AWS::EC2::Volume\n    Properties:\n      # Volume properties go here\n    DeletionPolicy: Snapshot\n    SnapshotId: my-snapshot-id"
      },
      {
        "date": "2023-02-16T17:36:00.000Z",
        "voteCount": 1,
        "content": "Clear A"
      },
      {
        "date": "2023-02-14T15:04:00.000Z",
        "voteCount": 2,
        "content": "AC1984 do your homework.\nStack policy can protect against deletion but not against actual entire CFN stack template being deleted. DeletionPolicy = if I was to delete the entire CFN stack, the CFN process will delete all elements and skip over RDS and EBS due to protections. 20 second Google search could of confirmed this."
      },
      {
        "date": "2023-02-13T22:39:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"
      },
      {
        "date": "2023-02-13T22:37:00.000Z",
        "voteCount": 1,
        "content": "B. Configure a stack policy that disallows the deletion of RDS and EBS resources.\nA stack policy is a JSON-based document that defines the actions that can be performed on a CloudFormation stack, and can be used to prevent users from accidentally deleting critical resources. By configuring a stack policy that disallows the deletion of RDS and EBS resources, the company can prevent users from accidentally deleting important data stored in those resources.\n\nOption A (adding a DeletionPolicy attribute) does not prevent users from deleting the resources, but rather determines what happens to the resources when the stack is deleted. Option C (modifying IAM policies) is not sufficient because it only affects the permissions of specific users or groups, and does not prevent accidental deletions. Option D (using AWS Config rules) can help detect deletions of RDS and EBS resources, but it does not prevent them from being deleted."
      },
      {
        "date": "2023-03-03T01:24:00.000Z",
        "voteCount": 1,
        "content": "\"Option A (adding a DeletionPolicy attribute) does not prevent users from deleting the resources, but rather determines what happens to the resources when the stack is deleted.\" This is actually what the question is asking !"
      },
      {
        "date": "2023-02-11T11:26:00.000Z",
        "voteCount": 1,
        "content": "I go for A because I assume that the CF stack is allowed to be deleted in some deployment scenarios."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/amazon/view/95384-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has VPC flow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound traffic that comes from public IP address 198.51.100.2 destined for a private Amazon EC2 instance.<br><br>A solutions architect must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 203.0.<br><br>Which set of steps should the solutions architect take to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interlace. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance\u2019s elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 63,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 33,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-24T08:16:00.000Z",
        "voteCount": 21,
        "content": "I would go with option B. Source will be public IP like 198.51.100.2."
      },
      {
        "date": "2023-03-08T15:11:00.000Z",
        "voteCount": 18,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/\n\nRefer Reason 1\n\nRun the query below.\nfilter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')\n| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr\n| limit 10\nNote: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC. In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.\n\nQuery results show traffic on the NAT gateway private IP from the public IP, but not traffic on other private IPs in the VPC. These results confirm that the incoming traffic was unsolicited. However, if you do see traffic on the private instance's IP, then follow the steps under Reason #2."
      },
      {
        "date": "2023-03-17T20:11:00.000Z",
        "voteCount": 2,
        "content": "For those that are choosing D - this is why D is incorrect and needs to be B"
      },
      {
        "date": "2024-07-19T13:14:00.000Z",
        "voteCount": 1,
        "content": "destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\""
      },
      {
        "date": "2024-07-03T01:01:00.000Z",
        "voteCount": 1,
        "content": "Of course it is D. What useful info will you get from B? You need to check original request which in case of NAT is always EC2, not something in the internet."
      },
      {
        "date": "2024-06-17T20:05:00.000Z",
        "voteCount": 1,
        "content": "I vote B. Because the network traffic to check is unsolicited inbound connection. IT is initiated from the internet to internal EC2. The source is public IP address and the target is internal IP."
      },
      {
        "date": "2024-06-04T13:08:00.000Z",
        "voteCount": 1,
        "content": "To determine whether the traffic represents unsolicited inbound connections from the internet, use the Amazon CloudWatch console. Select the log group that contains the NAT gateway\u2019s elastic network interface and the private instance\u2019s elastic network interface. Run a query to filter with the destination address set as \u201clike 203.0\u201d and the source address set as \u201clike 198.51.100.2\u201d. This approach helps you analyze the VPC flow logs to identify if the inbound traffic to the private EC2 instance is expected return traffic or unsolicited. The stats command can be used to filter the sum of bytes transferred by the source address and the destination address, providing insight into the traffic patterns and ensuring network security."
      },
      {
        "date": "2024-04-07T01:14:00.000Z",
        "voteCount": 2,
        "content": "the solution architect want to check if it's unsolicited traffic or not, so we need to check the if the request is sent by us. which means 198.51.100.2 should be the destination."
      },
      {
        "date": "2024-03-17T06:21:00.000Z",
        "voteCount": 2,
        "content": "B, CloudWatch &amp; destination address 203.0"
      },
      {
        "date": "2024-03-11T04:32:00.000Z",
        "voteCount": 3,
        "content": "The question is \"Solutions architect must determine whether the traffic represents unsolicited inbound connections from the internet\". The NAT gateway does not allow any inbound traffic from an internet other than response to a traffic it sent out to internet which came from a VPC resource (eg, EC2). So to find out if the inbound traffic to NAT Gateway from internet IP 198.51.100.2 is unsolicit or not, check the vpc flowlog to see if there was an original request from source IP 203.0 to destination 198.51.100.2. This is what option D says."
      },
      {
        "date": "2024-02-13T13:27:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct because VPC flow logs are stored in Amazon CloudWatch Logs. Analyzing these logs in CloudWatch allows you to filter and examine specific traffic patterns, such as traffic coming from a public IP address to a private instance. The query specified in this option correctly aims to identify traffic from the public IP (198.51.100.2) to the private IP range of the VPC (beginning with 203.0), which aligns with the requirement to investigate unsolicited inbound connections."
      },
      {
        "date": "2024-01-23T18:09:00.000Z",
        "voteCount": 3,
        "content": "Action = ACCEPT for inbound traffic that comes from public IP address 198.51.100.2 --&gt; Destination"
      },
      {
        "date": "2024-01-19T00:06:00.000Z",
        "voteCount": 4,
        "content": "B is what \"the company is seeing\", so D to see if it was first initiated from EC2."
      },
      {
        "date": "2023-12-01T13:16:00.000Z",
        "voteCount": 2,
        "content": "I would say this question is wrong, even we ignore the 203.0 is a public IP.\nBoth B and D can do the job.\nWith B: if the return value is bigger than 0, that means the traffic was initiated from internal so that NAT GW wouldn't drop that traffic. While, if the return is 0, that means the traffic was dropped by NAT after ACCEPTed, which means it was not initiated from internal.\nWith D: if the return value is bigger than 0, obviously the traffic was initiated from internal. If the return value is 0, that means the traffic was initiated from internet."
      },
      {
        "date": "2024-01-17T09:30:00.000Z",
        "voteCount": 1,
        "content": "You need first to query traffic from public IP to private IP, check if the NAT Gateway is the only private IP. If not then you query traffic (from private IP to public IP) OR (from public IP to private IP) and this will show bi-directional traffic allowing you to determine whether the private instance or external public IP address is the initiator. Thus B and not D"
      },
      {
        "date": "2023-11-14T00:46:00.000Z",
        "voteCount": 3,
        "content": "see kiran15789's answer"
      },
      {
        "date": "2023-09-09T10:29:00.000Z",
        "voteCount": 1,
        "content": "D\nAt NAT GW VPC flow logs will destination be VPC Private IP or will it be NAT GW IP"
      },
      {
        "date": "2023-08-05T06:41:00.000Z",
        "voteCount": 2,
        "content": "I was inclined towards Reason #2 in https://repost.aws/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway.\n\nHowever, the striking point is the VPC CIDR 203.0.... which is not a private addressing and not sure if we require a NAT gateway here at all for translation &amp; check if the traffic was initiated through NAT gateway. Does the definition of unsolicited connection means any inbound connection other than the traffic initiated from VPC via NAT gateway will not be considered as solicited.\n\nTough one from the unclear definition in the question, it would be Reason 1 (Option B) if the traffic is mentioned as dropped in the question but needs to be analyzed for whether this is unsolicited. \n\nOr if question states inbound traffic is not permitted, but still it is seen and needs to be analyzed then D). Again, point to be noted is why outbound traffic from '203.0...'  needs to go via NAT gateway."
      },
      {
        "date": "2023-08-01T04:47:00.000Z",
        "voteCount": 2,
        "content": "Correct D.\nYou need to open the Amazon CloudWatch console, select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface, run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\", and run the stats command to filter the sum of bytes transferred by the source address and the destination address."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/amazon/view/95385-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.<br><br>Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).<br><br>What is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 57,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T10:03:00.000Z",
        "voteCount": 26,
        "content": "Answer is A\nKeyword is \"The S3 buckets have millions of objects\"\nIf there are million of objects then you should use Batch operations. \nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/"
      },
      {
        "date": "2023-03-19T15:46:00.000Z",
        "voteCount": 1,
        "content": "good point, changing my answer to A"
      },
      {
        "date": "2024-10-09T01:42:00.000Z",
        "voteCount": 1,
        "content": "S3 Batch Operations can be used to efficiently apply changes to a large number of objects in a bucket, including copying and encrypting them in place. This is ideal for retroactively encrypting millions of existing objects without needing to manually handle them one by one.\n"
      },
      {
        "date": "2024-03-11T05:21:00.000Z",
        "voteCount": 1,
        "content": "I understand S3 Batch operations is required. But why no one is choosing SSE-KMS?"
      },
      {
        "date": "2024-03-17T16:42:00.000Z",
        "voteCount": 2,
        "content": "Because the question states the company wants to use SSE-S3, nowhere does it mention SSE-KMS"
      },
      {
        "date": "2024-03-08T08:27:00.000Z",
        "voteCount": 2,
        "content": "To encrypt your existing unencrypted Amazon S3 objects, you can use Amazon S3 Batch Operations. You provide S3 Batch Operations with a list of objects to operate on, and Batch Operations calls the respective API to perform the specified operation. You can use the Batch Operations Copy operation to copy existing unencrypted objects and write them back to the same bucket as encrypted objects. A single Batch Operations job can perform the specified operation on billions of objects.   https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html"
      },
      {
        "date": "2024-01-19T03:27:00.000Z",
        "voteCount": 4,
        "content": "A = correct (see https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/)\nB = KMS is for SSE-KMS not for the requested SSE-S3\nC = CLI is less efficient than S3 Batch\nD = see answer B"
      },
      {
        "date": "2023-12-22T12:59:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2023-11-25T01:29:00.000Z",
        "voteCount": 4,
        "content": "Correct answer should be A. But this question seem too old to be true now since SSE-S3 based encryption is by default enabled and can't be disabled (you can change however) since Jan 2023."
      },
      {
        "date": "2023-09-30T23:34:00.000Z",
        "voteCount": 1,
        "content": "Since SSE-S3 does not support cross-account replication, answer should be D"
      },
      {
        "date": "2023-09-13T09:35:00.000Z",
        "voteCount": 3,
        "content": "In a cross-account scenario, where the source and destination buckets are owned by different AWS accounts, you can use a KMS key to encrypt object replicas. However, the KMS key owner must grant the source bucket owner permission to use the KMS key.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html#replication-kms-cross-acct-scenario\n\nS3 Batch operation:\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/"
      },
      {
        "date": "2023-09-05T06:18:00.000Z",
        "voteCount": 1,
        "content": "S3 Batch operation is the MOST operationally efficient way for millions objects"
      },
      {
        "date": "2023-07-15T04:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2023-07-02T15:17:00.000Z",
        "voteCount": 1,
        "content": "A more efficient"
      },
      {
        "date": "2023-06-21T06:36:00.000Z",
        "voteCount": 1,
        "content": "I vote for A. Batch operations is better for such a high number of objects"
      },
      {
        "date": "2023-05-08T21:11:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/\nThe launch of S3 default encryption feature automate the wok of encrypting new objects, and you asked for similar, straightforward ways to encrypt existing objects in your buckets. While tools and scripts exist to do this work, each one requires some development work to set up. S3 batch operations gives you a solution for encrypting large number of archived files. \nThis can also be done by CLI, Option C, however, the same article refers to Batch Operations in case you have a large bucket with millions of objects. \nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/ \nOption A should be the most efficient, even though it has more operational cost to implement but the question is the about efficiency, it would take to much time to complete this using CLI (Option C)."
      },
      {
        "date": "2023-03-26T07:10:00.000Z",
        "voteCount": 1,
        "content": "A is much more efficient"
      },
      {
        "date": "2023-03-09T17:21:00.000Z",
        "voteCount": 1,
        "content": "A and C seems to be correct but using batch requires more steps.\nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/"
      },
      {
        "date": "2023-03-04T22:55:00.000Z",
        "voteCount": 2,
        "content": "C is wrong. How can S3 copy encrypt ? A is correct. Refer how S3 batch operations are used to encrypt here -https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/amazon/view/95386-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.<br><br>The company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier Deep Archive.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 49,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T10:00:00.000Z",
        "voteCount": 20,
        "content": "The correct answer is C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.\n\nThis solution allows you to use Amazon Athena and the AWS Glue Data Catalog to query and analyze the data in an S3 bucket. Amazon Athena is a serverless, interactive query service that allows you to analyze data in S3 using SQL. The AWS Glue Data Catalog is a managed metadata repository that can be used to store and retrieve table definitions for data stored in S3. Together, these services can provide a cost-effective way to query and analyze large amounts of unstructured data. Additionally, by using an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive, you can retain the data indefinitely for compliance reasons while also reducing storage costs."
      },
      {
        "date": "2023-01-17T10:01:00.000Z",
        "voteCount": 9,
        "content": "The other options are not correct because:\nA. Using S3 Select is good for filtering data in S3, but it may not be a suitable solution for querying and analyzing large amounts of data.\n\nB. Amazon Redshift Spectrum can be used to query data stored in S3, but it may not be as cost-effective as using Amazon Athena for querying unstructured data\n\nD. Using Amazon Redshift Spectrum with S3 Intelligent-Tiering could be a good solution, but S3 Intelligent-Tiering is designed to optimize storage costs based on access patterns and it would not be the best solution for compliance reasons as S3 Intelligent-Tiering will move data to other storage classes according to access patterns."
      },
      {
        "date": "2023-11-28T03:28:00.000Z",
        "voteCount": 4,
        "content": "This is a nonsense explanation.\nIn the first place, Redshift cannot handle unstructured data."
      },
      {
        "date": "2024-02-11T10:52:00.000Z",
        "voteCount": 2,
        "content": "Amazon Redshift is designed for structured data. However, Amazon Redshift Spectrum enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required."
      },
      {
        "date": "2023-01-28T09:15:00.000Z",
        "voteCount": 7,
        "content": "Generally, unstructured data should be converted structured data before querying them. AWS Glue can do that.\nhttps://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html\nhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html"
      },
      {
        "date": "2024-10-06T04:53:00.000Z",
        "voteCount": 1,
        "content": "B, C seem both acceptable. The reason C is selected is because redshift spectrum need Glue Data Catalog as well which is not mentioned there."
      },
      {
        "date": "2024-03-17T06:27:00.000Z",
        "voteCount": 1,
        "content": "C, aws glue + amazon athena"
      },
      {
        "date": "2024-01-31T02:00:00.000Z",
        "voteCount": 1,
        "content": "Many comments were not convincing of not using Redshift Spectrum.. the only reason I see it to exclude that option is a Redshift Spectrum MUST have a Redshift Cluster available to start the query to S3.."
      },
      {
        "date": "2024-03-04T08:40:00.000Z",
        "voteCount": 1,
        "content": "This question is actually pretty difficult since both Redshift Spectrum and AWS Glue + Athena could query unstructured data. Redshift Spectrum and Athena actually cost about the same per TB. However, with Athena, you could lower the cost by compressing the data. Glue doesn't seem to cost that much either. \n\nhttps://aws.amazon.com/redshift/pricing/\nhttps://aws.amazon.com/athena/pricing/\nhttps://aws.amazon.com/glue/pricing/"
      },
      {
        "date": "2024-01-19T02:58:00.000Z",
        "voteCount": 2,
        "content": "A = S3 Select good for filtering an retrieve subset of data, not enough to analyze\nB = need a Redshift instance that is expensive\nC = correct (Glue Data Catalog can help putting some structure to data and Athena is good for both query and analytics, transition to Deep Archive after 1 year)\nD = see answer B + Intelligent-Tiering not the best option here"
      },
      {
        "date": "2024-01-09T02:55:00.000Z",
        "voteCount": 1,
        "content": "redshift spectrum vs athena: https://www.upsolver.com/blog/aws-serverless-redshift-spectrum-athena\n\nBoth are good solutions to query s3 data. However, redshift spectrum is useful for joining S3 data with other data in Redshift, and if the data is only in S3, it would be preferable to choose athena."
      },
      {
        "date": "2023-12-22T13:02:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer as Data needs to be queried and Analyzed."
      },
      {
        "date": "2023-12-06T22:53:00.000Z",
        "voteCount": 1,
        "content": "Athena and aws glue is more cost , so better go with A . and what is the purpose for aws glue here. AWS glue is for ETL purpose unnecessary"
      },
      {
        "date": "2023-11-25T02:02:00.000Z",
        "voteCount": 1,
        "content": "C correct: S3 copy command in AWS CLI is less operational processes than the batch operation."
      },
      {
        "date": "2023-09-05T06:35:00.000Z",
        "voteCount": 2,
        "content": "In this particular scenario, using Amazon Athena and AWS Glue Data Catalog might be a better fit due to the large amount of data stored in S3 buckets and growing every day. Athena can query data across an entire S3 bucket or across multiple buckets, which is useful when parsing multiple files and large amounts of data."
      },
      {
        "date": "2023-08-06T12:13:00.000Z",
        "voteCount": 3,
        "content": "Answer: C\n\nCriminally tricky question. S3 Select does the same thing as Athena but there are some differences. The key here is \"...a large amount of unstructured data...\"\nIf wasn't this, S3 Select hands down."
      },
      {
        "date": "2023-08-06T12:14:00.000Z",
        "voteCount": 2,
        "content": "Using an Olabiba to explain the differences between the two:\n\n1. Query Capability: Amazon Athena is a fully managed interactive query service that allows you to run SQL queries directly on your data in S3. It supports complex queries, joins, aggregations, and even nested data structures. Athena is designed for ad-hoc querying and analysis of large datasets.\n\nOn the other hand, S3 Select is a feature of Amazon S3 that allows you to retrieve a subset of data from an object using SQL expressions. It is primarily used for selective retrieval of specific data within an object, rather than running complex queries across multiple objects."
      },
      {
        "date": "2023-08-06T12:15:00.000Z",
        "voteCount": 2,
        "content": "2. Data Format: Amazon Athena supports various data formats such as CSV, JSON, Parquet, Avro, and more. It can automatically infer the schema of your data or you can provide a schema explicitly. Athena can handle structured, semi-structured, and unstructured data.\n\nS3 Select, on the other hand, is limited to querying CSV, JSON, and Parquet files. It requires the data to be in a specific format and does not support nested data structures."
      },
      {
        "date": "2023-08-06T12:15:00.000Z",
        "voteCount": 3,
        "content": "3. Performance: Amazon Athena is optimized for running queries on large datasets and can parallelize the query execution across multiple nodes. It automatically scales resources based on the query complexity and data size, providing fast and efficient query performance.\n\nS3 Select, on the other hand, is designed for retrieving a subset of data from an object. It can significantly reduce the amount of data transferred over the network and improve query performance by only retrieving the necessary data.\n\n4. Cost: Both Amazon Athena and S3 Select have different pricing models. Amazon Athena charges based on the amount of data scanned by your queries, while S3 Select charges based on the amount of data selected and returned by your queries. The cost will depend on the size of your data and the complexity of your queries."
      },
      {
        "date": "2023-07-12T08:02:00.000Z",
        "voteCount": 1,
        "content": "its a C , true question!"
      },
      {
        "date": "2023-07-02T15:20:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-05-30T08:48:00.000Z",
        "voteCount": 1,
        "content": "redshift spectrum can run sql queries directly on s3"
      },
      {
        "date": "2023-06-29T16:55:00.000Z",
        "voteCount": 1,
        "content": "Not the best for cost."
      },
      {
        "date": "2023-03-26T07:11:00.000Z",
        "voteCount": 3,
        "content": "C is the best choice for unstructured data"
      },
      {
        "date": "2023-03-04T23:16:00.000Z",
        "voteCount": 4,
        "content": "S3 select only to select few parts of the data and here its lot of unstructured data. So A is wrong. Use Athena console to create Glue crawler as referred here - \nhttps://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/amazon/view/95387-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.<br><br>The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the connection.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T01:56:00.000Z",
        "voteCount": 11,
        "content": "The correct answer is A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.\n\nThis option will meet the requirements to complete the data transfer within 3 weeks, as the Snowball Edge devices can transfer large amounts of data quickly and securely. The data will be encrypted in transit and at rest. The company's internet connection speed is not a bottleneck as the data transfer will happen on the devices and not over the internet."
      },
      {
        "date": "2023-01-15T01:56:00.000Z",
        "voteCount": 3,
        "content": "Option B is not a cost-effective solution, as setting up and maintaining a 10 Gbps Direct Connect connection can be quite expensive, especially if it's only needed for a one-time data transfer.\n\nOption C is not a cost-effective solution, as creating a VPN connection between the on-premises storage and the nearest AWS region would require significant networking configuration and maintenance, and would likely be more expensive than using Snowball Edge devices.\n\nOption D is not a cost-effective solution, as deploying an AWS Storage Gateway file gateway on premises would require additional hardware and ongoing maintenance costs, and may not be necessary for a one-time data transfer."
      },
      {
        "date": "2024-01-19T04:53:00.000Z",
        "voteCount": 2,
        "content": "A = correct\nB = takes a month or more to setup DX\nC = this would take more than 3 weeks for transferring data\nD = this would take more than 3 weeks for transferring data"
      },
      {
        "date": "2023-12-22T13:04:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-01T03:20:00.000Z",
        "voteCount": 1,
        "content": "wish all the questions were like this. happy days :)"
      },
      {
        "date": "2023-08-10T13:00:00.000Z",
        "voteCount": 4,
        "content": "as we know snowball storage optimized NVMe up to 210 TB &lt;3 A is the best and easy answer"
      },
      {
        "date": "2023-08-10T13:01:00.000Z",
        "voteCount": 1,
        "content": "like several sorry for any confision :)"
      },
      {
        "date": "2023-08-31T07:33:00.000Z",
        "voteCount": 1,
        "content": "several thanks too :)"
      },
      {
        "date": "2023-07-02T15:21:00.000Z",
        "voteCount": 1,
        "content": "A - basic snowball use case"
      },
      {
        "date": "2023-06-21T06:48:00.000Z",
        "voteCount": 1,
        "content": "Given the deadline (3 weeks) and the amount of data I would use Snowball Edge"
      },
      {
        "date": "2023-03-26T07:13:00.000Z",
        "voteCount": 3,
        "content": "A obviously"
      },
      {
        "date": "2023-03-05T10:32:00.000Z",
        "voteCount": 4,
        "content": "Around 8 devices and snowball (actually a Rectangular box)\nSnowball Edge Storage Optimized device is equipped with up to 80 terabytes (TB) of storage capacity, as well as 40 vCPUs and 80 GB of memory for running compute-intensive applications. It also includes an optional GPU for accelerated computing workloads.\n\nBuilt-in security features such as tamper-resistant enclosures, an E Ink shipping label, and 256-bit encryption for data at rest and in transit."
      },
      {
        "date": "2023-01-30T15:22:00.000Z",
        "voteCount": 1,
        "content": "3 weeks + cost effective ==&gt; Snowball Edge Storage"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/amazon/view/95390-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database.<br><br>When forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an API.<br><br>A solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time to market, and minimize tong-term operational overhead.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's APL. Host the new application tier on EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T02:03:00.000Z",
        "voteCount": 14,
        "content": "The correct answer is D. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.\n\nThis solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead. Amazon Textract and Amazon Comprehend are fully managed and serverless services that can perform OCR and extract relevant data from the forms, which eliminates the need to develop custom libraries or train and host models. Using AWS Step Functions and Lambda allows for easy automation of the process and the ability to scale as needed."
      },
      {
        "date": "2023-01-15T02:03:00.000Z",
        "voteCount": 3,
        "content": "Option A:\nThis option would require significant development and maintenance effort and would not take advantage of fully managed services, resulting in increased operational overhead.\n\nOption B:\nThis option is similar to option A in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.\n\nOption C:\nThis option is similar to option B in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead."
      },
      {
        "date": "2024-03-17T06:33:00.000Z",
        "voteCount": 1,
        "content": "D. This solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead"
      },
      {
        "date": "2023-12-22T13:10:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-07-02T15:24:00.000Z",
        "voteCount": 1,
        "content": "D - basic use case for textract"
      },
      {
        "date": "2023-06-21T06:51:00.000Z",
        "voteCount": 1,
        "content": "An easy one - if AWS has a service for something - do not reinvent the wheel - use Textract and Comprehend"
      },
      {
        "date": "2023-06-11T10:28:00.000Z",
        "voteCount": 1,
        "content": "D : Managed AWS Services"
      },
      {
        "date": "2023-03-26T07:14:00.000Z",
        "voteCount": 1,
        "content": "Amazon Textract.."
      },
      {
        "date": "2023-03-05T14:23:00.000Z",
        "voteCount": 1,
        "content": "Textract can analyze different types of documents such as forms, invoices, receipts, and tables, and can extract information such as text, tables, and key-value pairs.\n\nComprehend provides a set of APIs that can be used to analyze text data in real-time. The service can identify the language of the text, extract entities such as people, organizations, and locations, and detect the sentiment expressed in the text. It can also extract key phrases that summarize the meaning of the text, and can classify the text into predefined categories."
      },
      {
        "date": "2023-03-03T02:42:00.000Z",
        "voteCount": 1,
        "content": "D : Managed AWS Services"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/amazon/view/95391-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T02:10:00.000Z",
        "voteCount": 19,
        "content": "Option A is the correct answer. In this solution, the company creates an Amazon Machine Image (AMI) of the web server VM, which can be used to launch EC2 instances that are identical to the on-premises web servers. The company then creates an EC2 Auto Scaling group that uses the AMI and an Application Load Balancer (ALB) to provide automatic scaling and high availability for the web front end. The company also replaces the on-premises messaging queue (RabbitMQ) with Amazon MQ, which is a managed message broker service that is fully compatible with RabbitMQ. Finally, the company uses Amazon Elastic Kubernetes Service (EKS) to host the order-processing backend, which allows them to run their existing Kubernetes cluster in the AWS cloud without making any major changes to the application. This approach allows the company to lift and shift their existing platform with minimal operational overhead."
      },
      {
        "date": "2023-01-15T02:10:00.000Z",
        "voteCount": 3,
        "content": "Option B, using a custom AWS Lambda runtime and Amazon API Gateway, would require significant changes to the application and may not be compatible with the current codebase. \n\nOption C, installing Kubernetes on a fleet of different EC2 instances, would also require significant changes to the application and may not be compatible with the current codebase. \n\nOption D, using Amazon Simple Queue Service (Amazon SQS) instead of Amazon MQ, would not provide the same level of messaging capabilities as Amazon MQ and may not be sufficient for the needs of the order-processing platform."
      },
      {
        "date": "2023-03-03T02:47:00.000Z",
        "voteCount": 10,
        "content": "Your justification for option C is wrong.\nOption C is valid, as Kubernetes on EC2 is very similar as the existing Kubernetes environment on-premises. But EKS is a safe bet and reduces operational overhead, while keeping the same API as previously. Hence, A is a better choice."
      },
      {
        "date": "2024-01-31T02:26:00.000Z",
        "voteCount": 6,
        "content": "AWS exams got more 'sarcastic' with the ways of formulating questions.. E.g here: _'A company is refactoring its on-premises order-processing platform in the AWS Cloud' \nBUT '\nThe company does not want to make any major changes to the application.\n\nReplatforming and Rehosting is not real refactoring.. but the closest answer as an architect with least operational overhead is A obvisouly.. aws questions sometimes can be ultra vague"
      },
      {
        "date": "2024-06-11T20:53:00.000Z",
        "voteCount": 1,
        "content": "answer is A"
      },
      {
        "date": "2024-03-17T06:34:00.000Z",
        "voteCount": 1,
        "content": "A, is the only option to don't involve a rearchitectured solution"
      },
      {
        "date": "2024-01-03T11:44:00.000Z",
        "voteCount": 3,
        "content": "A better explanation to choose between option A and D is that Amazon MQ respondes to the requirement of not changing the app, because it accepts the same protocol as RabbitMQ (Supports AMQP, MQTT, STOMP, OpenWire, and JMS) while SQS has its own API, so it would need more changes to the app."
      },
      {
        "date": "2023-12-22T13:17:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-12T13:04:00.000Z",
        "voteCount": 1,
        "content": "a bunch of keywords for this migration here : \nKubernetes == EKS\nRabbitMQ == Amazon MQ\nA fleet of VM ==  AMI + ec2 instances\n\nThe answer A proposes all thoses points, so it's perfect here."
      },
      {
        "date": "2023-07-02T15:26:00.000Z",
        "voteCount": 1,
        "content": "A no doubt"
      },
      {
        "date": "2023-03-26T07:16:00.000Z",
        "voteCount": 1,
        "content": "A is the best choice."
      },
      {
        "date": "2023-02-20T03:13:00.000Z",
        "voteCount": 2,
        "content": "Option A is re-hosting or mybe re-platforming. The question says the purpose is re-factoring, then it's B."
      },
      {
        "date": "2023-02-21T19:37:00.000Z",
        "voteCount": 6,
        "content": "It says the company does not want to make changes to the application in the problem statement. B would require significant code changes to the application."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/amazon/view/95392-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.<br><br>The solutions architect created the following IAM policy and attached it to an IAM role:<br><br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image7.png\"><br><br>During tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.<br><br>Which action must the solutions architect add to the IAM policy to meet all the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkms:GenerateDataKey\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkms:GetKeyPolicy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkms:GetPublicKey",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkms:Sign"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T02:16:00.000Z",
        "voteCount": 15,
        "content": "A. kms:GenerateDataKey\n\nThe solutions architect needs to add the \"kms:GenerateDataKey\" action to the IAM policy in order to generate a data key for client-side encryption. Without this action, the IAM role does not have the necessary permissions to generate a data key, which causes the error message when attempting to upload a new object."
      },
      {
        "date": "2023-01-15T02:18:00.000Z",
        "voteCount": 2,
        "content": "The other options are not correct because they are not required for this use case. kms:GetKeyPolicy allows for the retrieval of the key policy for a CMK but it does not have any relation to client-side encryption of S3 objects, kms:GetPublicKey allows for the retrieval of the public key of a CMK, but it does not have any relation to client-side encryption of S3 objects and kms:Sign allows for signing a message using a CMK but it does not have any relation to client-side encryption of S3 objects."
      },
      {
        "date": "2024-01-19T05:00:00.000Z",
        "voteCount": 3,
        "content": "A = correct (you encrypt data with KMS Data Key and not KMS Key directly, unless data is &lt; 4K)\nB = getting the policy would allow to get the data key needed for encryption\nC = client side encryption uses symmetric key not asymmetric keys\nD = sign allows for signing messages, API calls, etc."
      },
      {
        "date": "2023-12-22T13:22:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-07-02T15:27:00.000Z",
        "voteCount": 1,
        "content": "A - need data key for client-side encr"
      },
      {
        "date": "2023-05-16T12:56:00.000Z",
        "voteCount": 4,
        "content": "I don't understand since it's client side encryption, it means both encryption and key and tools are maintained in client side before submitting to aws s3, why we need add kms:GenerateDatakey ? We don't need kms to do anything since it's client-side encryption all is done outside of aws."
      },
      {
        "date": "2023-08-29T01:58:00.000Z",
        "voteCount": 9,
        "content": "When you want to do the client side encryption, your files are most likely above 4K in size. So, you would be performing envelope encryption.\nFor that, you need a data key.\nYou ask KMS to generate and give you the data key, supplying the kms CMK.\nKMS would generate a new data key, encrypt it with the CMK and return you both the encrypted and plain data key. AWS would never retain the data key; they will immediately discard it.\nYou would now encrypt your data using the plain data key and immediately delete the plain data key (unencrypted). You store the encrypted data key that you got from KMS along with the encrypted data, which is then uploaded to s3. Note that AWS does NOT know about the data key at this point; only you know. KMS just holds the kms CMK that was used to encrypt the data key.\nSo, you need access to KMS to decrypt the data key before using that decrypted data key to unencrypt your data.\nSimilarly AWS cannot read your data, even though it has the KMS CMK and also the encrypted data key stored in s3.\nThis is why you need the generateDataKey permission. Hope this helps."
      },
      {
        "date": "2023-08-29T02:00:00.000Z",
        "voteCount": 1,
        "content": "Of course the answer is A"
      },
      {
        "date": "2023-06-19T09:08:00.000Z",
        "voteCount": 2,
        "content": "Indeed, the question says client side encryption but the answer is all about S3-KMS."
      },
      {
        "date": "2023-03-26T07:17:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-01-28T09:18:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/kms/latest/cryptographic-details/client-side-encryption.html"
      },
      {
        "date": "2023-01-23T17:21:00.000Z",
        "voteCount": 1,
        "content": "I Vote A.\nhttps://repost.aws/ja/knowledge-center/s3-large-file-encryption-kms-key\nAdding kms:GenerateDataKey  is necessary."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/amazon/view/95394-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.<br><br>How should a solutions architect configure the web ACLs to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-05T16:54:00.000Z",
        "voteCount": 20,
        "content": "AWS WAF allows you to create web ACL (Access Control List) rules in \"Count\" mode, which allows you to monitor traffic without actually blocking it. In Count mode, AWS WAF counts the number of requests that match a particular rule, but doesn't take any action to block those requests.\n\nCount mode can be useful in several ways:\n\n    Testing new rules: You can create new rules and test them in Count mode before enabling them to block traffic. This allows you to evaluate the effectiveness of your rules without risking false positives or false negatives.\n\n    Analyzing traffic: You can use Count mode to analyze traffic patterns and identify potential security threats. By monitoring the number of requests that match a particular rule, you can detect patterns that may indicate an attack or vulnerability.\n\n    Compliance reporting: Count mode can be used for compliance reporting, where you need to demonstrate that certain rules are being enforced. By counting the number of requests that match a rule, you can provide evidence that your security policies are being followed."
      },
      {
        "date": "2023-01-15T02:20:00.000Z",
        "voteCount": 6,
        "content": "https://www.examtopics.com/discussions/amazon/view/74273-exam-aws-certified-solutions-architect-professional-topic-1/\n\nThe correct answer is A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.\n\nThis approach allows for monitoring of the incoming traffic and its behavior before taking any action that can affect the legitimate traffic. By setting the action to count, the web ACL will only log the requests that match the conditions of the rules, but it will not block them. This way, the company can analyze the requests and check for any false positives. Once they identify and correct any false positives, they can gradually change the action of the web ACL rules from count to block, thus improving the security posture of the application without adversely affecting legitimate traffic."
      },
      {
        "date": "2023-01-15T02:21:00.000Z",
        "voteCount": 1,
        "content": "Option B is not correct because using only rate-based rules can lead to false positives and blocking of legitimate traffic. Option C is not correct because using only AWS managed rule groups can limit the flexibility and specificity of the web ACLs. Option D is not correct because using only custom rule groups with action set to allow can lead to security vulnerabilities."
      },
      {
        "date": "2024-03-17T06:39:00.000Z",
        "voteCount": 1,
        "content": "A, configure the rules on COUNT"
      },
      {
        "date": "2023-08-19T05:58:00.000Z",
        "voteCount": 1,
        "content": "vote A"
      },
      {
        "date": "2023-07-02T15:29:00.000Z",
        "voteCount": 1,
        "content": "Its an A"
      },
      {
        "date": "2023-03-26T07:19:00.000Z",
        "voteCount": 1,
        "content": "A. Set the action of the web ACL rules to Count. Enable AWS WAF logging."
      },
      {
        "date": "2023-01-28T09:20:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/waf/latest/developerguide/web-acl-testing.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/amazon/view/95397-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.<br><br>The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company\u2019s on-premises network. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.<br><br>The solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new customer-managed prefix list in the security team\u2019s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team\u2019s AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T10:02:00.000Z",
        "voteCount": 11,
        "content": "C. Create a new customer-managed prefix list in the security team\u2019s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.\n\nThis solution meets the requirements with the least amount of operational overhead as it requires the security team to create and maintain a single customer-managed prefix list, and share it with the organization using AWS Resource Access Manager. The owners of each AWS account are then responsible for allowing the prefix list in their security groups, which eliminates the need for the security team to manually notify each account owner when changes are made. This solution also eliminates the need for a separate AWS Lambda function in each account, reducing the overall complexity of the solution."
      },
      {
        "date": "2023-01-17T10:03:00.000Z",
        "voteCount": 2,
        "content": "Option A is not correct because it requires setting up an SNS topic in the security team's AWS account, and deploying an AWS Lambda function in each AWS account. This increases the operational overhead as it requires setting up and maintaining the SNS topic, and deploying and configuring the Lambda function in each account.\n\nOption B is not correct because it requires creating new customer-managed prefix lists in each AWS account within the organization, which increases the operational overhead as it requires the security team to create and maintain multiple prefix lists.\n\nOption D is not correct because it requires creating an IAM role in each account in the organization, which increases the operational overhead as it requires the security team to set up and maintain multiple roles. Additionally, it also deploys an AWS Lambda function in the security team\u2019s AWS account, which increases complexity and operational overhead."
      },
      {
        "date": "2023-09-09T00:05:00.000Z",
        "voteCount": 8,
        "content": "masetromain is ChatGPT and might have outdated answers since it doesnt know aws latest update to services"
      },
      {
        "date": "2024-03-23T08:57:00.000Z",
        "voteCount": 1,
        "content": "Human cost is major overhead. I will go A. This is one time setup."
      },
      {
        "date": "2024-03-17T16:57:00.000Z",
        "voteCount": 1,
        "content": "Centralised management and standard use case for prefix lists and RAM"
      },
      {
        "date": "2023-12-22T13:40:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-02T15:31:00.000Z",
        "voteCount": 1,
        "content": "C - basic RAM use case"
      },
      {
        "date": "2023-06-19T09:17:00.000Z",
        "voteCount": 1,
        "content": "Typical use case for RAM. It is the typical question that leads you to the solution without even finishing reading the question."
      },
      {
        "date": "2023-06-18T12:02:00.000Z",
        "voteCount": 1,
        "content": "KEYWORD = AWS Resource Access Manager\nThen C"
      },
      {
        "date": "2023-05-30T08:22:00.000Z",
        "voteCount": 1,
        "content": "operational overhead"
      },
      {
        "date": "2023-03-26T08:13:00.000Z",
        "voteCount": 2,
        "content": "Prefix lists + RAM"
      },
      {
        "date": "2023-03-05T18:06:00.000Z",
        "voteCount": 5,
        "content": "Prefix lists + Resource Access Manager RAM is the solution."
      },
      {
        "date": "2023-02-01T14:42:00.000Z",
        "voteCount": 1,
        "content": "Clearly"
      },
      {
        "date": "2023-01-30T15:38:00.000Z",
        "voteCount": 1,
        "content": "Create a new customer-managed prefix list in the security team\u2019s AWS account"
      },
      {
        "date": "2023-01-28T09:21:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html"
      },
      {
        "date": "2023-01-15T16:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct. The prefix list is managed by security team and shared with other accounts. Other accounts can directly use it."
      },
      {
        "date": "2023-01-15T02:25:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D.\n\nOption D creates an IAM role in each account in the organization which grants permissions to update security groups. Then, it deploys an AWS Lambda function in the security team\u2019s AWS account, this lambda function is able to assume the IAM roles in each account and update the security groups with the new IP CIDR ranges. This solution allows the security team to easily distribute and update the common set of IP CIDR ranges across all accounts with minimal operational overhead.\n\nOption A, uses an SNS topic, where the security team would need to notify all account owners every time an update is made to the allow list and would require the developers in each account to run a Lambda function which updates the security group. This solution would require a lot of manual work, and is not automated."
      },
      {
        "date": "2023-01-15T02:25:00.000Z",
        "voteCount": 1,
        "content": "Option B, requires the security team to notify the owners of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups, this solution would not provide a centralized control of the IP CIDR ranges and would require a lot of manual work.\n\nOption C, uses a customer-managed prefix list in the security team\u2019s AWS account. But, it still requires the owners of each account to allow the new customer-managed prefix list ID in their security groups, this solution would not provide a centralized control of the IP CIDR ranges and would require a lot of manual work."
      },
      {
        "date": "2023-03-05T17:32:00.000Z",
        "voteCount": 1,
        "content": "Create an IAM role in each account in the organization. this does not add up to operational overhead right."
      },
      {
        "date": "2023-06-02T10:33:00.000Z",
        "voteCount": 1,
        "content": "It's ChatGPT talking"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/amazon/view/95398-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is hosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises office network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts.<br><br>A solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.<br><br>What is the MOST cost-effective solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure required routing that allows access to internal applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 67,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 61,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T07:51:00.000Z",
        "voteCount": 32,
        "content": "C.\nHave you guys worked in a place where the configuration of B works?\n\nThe question clearly ask to design something scalable, and on C, the Transit Gateway serves as a network transit hub, allowing VPN connections to access resources across multiple VPCs in different AWS accounts.\nVPC peering connections do not support transitive peering relationships, which means that if a user is connected to one VPC via AWS Client VPN, they cannot access resources in another VPC that's connected via a peering connection."
      },
      {
        "date": "2023-08-31T20:40:00.000Z",
        "voteCount": 12,
        "content": "The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts =&gt; no need transit gw"
      },
      {
        "date": "2023-12-20T01:27:00.000Z",
        "voteCount": 11,
        "content": "The question asks a scalable Client VPN solution (i.e. no openvpn on an EC2 instance or something like that), and asks for the most cost-effective. So AWS Client VPN is the scalable option. Reusing the current VPC peering is the most cost-effective compared to the far more expensive transit gateway solution.\nI do agree that the peering does not support transitive peering. But for AWS Client VPN you get an ENI in the main account VPC and using the ENI you can access the VPCs over the VPC peering. So that does really work (in contrast to the Site-To-Site VPN): https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html"
      },
      {
        "date": "2024-08-28T21:18:00.000Z",
        "voteCount": 1,
        "content": "Most cost effective - Transit gateway option is more costlier then B"
      },
      {
        "date": "2023-01-15T02:27:00.000Z",
        "voteCount": 22,
        "content": "https://www.examtopics.com/discussions/amazon/view/80782-exam-aws-certified-solutions-architect-professional-topic-1/\n\nB. Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications is the MOST cost-effective solution that meets these requirements. This solution allows employees to connect to the main AWS account using a Client VPN endpoint, and then use peering connections established with other AWS accounts to access the internal applications. This eliminates the need for additional Client VPN endpoints in each AWS account, reducing costs. \n\nOption A, creating a Client VPN endpoint in each AWS account, would be more expensive as it would require multiple endpoints. \n\nOption C, creating a transit gateway, would also add unnecessary costs. \n\nOption D, connecting the Client VPN endpoint to the Site-to-Site VPN, may not provide a scalable solution for remote employees."
      },
      {
        "date": "2024-09-14T11:43:00.000Z",
        "voteCount": 1,
        "content": "Always find possible solutions first. Then look for cost effective. A cost effective option that does not solve the requirements is by default the most expensive option.\n\nRequirement: most scalable option"
      },
      {
        "date": "2024-09-01T22:32:00.000Z",
        "voteCount": 1,
        "content": "B. Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications."
      },
      {
        "date": "2024-08-01T10:55:00.000Z",
        "voteCount": 1,
        "content": "C introduces additional costs."
      },
      {
        "date": "2024-07-22T04:19:00.000Z",
        "voteCount": 2,
        "content": "Answer is B, right now you have VPC Peering from main VPC to others account VPC, you can re-use that configuration, also transit-gateway has a cost based on connections and traffic and the solution must be MOST cost-effective"
      },
      {
        "date": "2024-05-13T11:47:00.000Z",
        "voteCount": 1,
        "content": "The current AWS S2S VPN works fine without a TGW for app access\nOnly requirement is to have users use the Client VPN\nSo just deploy it in the main account, where the S2S VPN terminates and users will have the same level of access they have from onprem using VPC peerings.\nThat's the most cost effective way (even though client VPN is one expensive service for what it does...)"
      },
      {
        "date": "2024-04-27T08:06:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer(Also gives scalable solution), \n\naws transit gateway bit high cost"
      },
      {
        "date": "2024-04-23T03:56:00.000Z",
        "voteCount": 1,
        "content": "marszalekm gave the key AWS doc\nhttps://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html"
      },
      {
        "date": "2024-04-17T12:36:00.000Z",
        "voteCount": 1,
        "content": "Cost Effective with Peering Working"
      },
      {
        "date": "2024-04-05T07:18:00.000Z",
        "voteCount": 2,
        "content": "For me the question clearly wants to test our scalability solution. And for sure C is the best answer in my mind"
      },
      {
        "date": "2024-04-04T06:55:00.000Z",
        "voteCount": 2,
        "content": "Option B: - the question is asking for \" MOST cost-effective\" solution.  AWS Transit gateway charged for the number of connections that you make to the Transit Gateway per hour and the amount of traffic that flows through AWS Transit Gateway. AWS Site-to-Site VPN connection pricing still applies in addition to AWS Transit Gateway VPN attachment pricing.\n\nhttps://aws.amazon.com/transit-gateway/pricing/\n\nAWS Client VPN is a fully-managed remote access VPN solution used by your remote workforce to securely access resources within both AWS and your on-premises network. Fully elastic, it automatically scales up, or down, based on demand.\nhttps://aws.amazon.com/vpn/client-vpn/"
      },
      {
        "date": "2024-04-04T06:57:00.000Z",
        "voteCount": 1,
        "content": "We (AWS) recommend this configuration if you need to give clients access to the resources inside an on-premises network only.\nhttps://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-onprem.html"
      },
      {
        "date": "2024-03-31T03:22:00.000Z",
        "voteCount": 1,
        "content": "No need TGW..."
      },
      {
        "date": "2024-03-17T07:00:00.000Z",
        "voteCount": 2,
        "content": "C, This setup leverages a central point of access through the transit gateway, minimizing the need to manage multiple VPN endpoints across accounts and simplifying network administration"
      },
      {
        "date": "2024-03-16T10:01:00.000Z",
        "voteCount": 2,
        "content": "it's C"
      },
      {
        "date": "2024-03-08T09:09:00.000Z",
        "voteCount": 2,
        "content": "Not B -  Not Transit Gateway, not the most cost-effective. In AWS Transit Gateway you are charged for the number of connections that you make to the Transit Gateway per hour and the amount of traffic that flows through AWS Transit Gateway.  -  https://aws.amazon.com/transit-gateway/pricing/"
      },
      {
        "date": "2024-03-08T06:55:00.000Z",
        "voteCount": 3,
        "content": "In the case of client VPN, your device is considered as an ENI inside the VPC, due to which you get the private IP from the VPC CIDR Block. If your VPC is peered with another VPC, then just edit the route tables appropriately just like you do, and your client VPN's ENI will be able to communicate with the peered VPC resources."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/amazon/view/95399-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in error rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly invoking an AWS Lambda function.<br><br>A solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Step Functions state machine to pass events to the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon EventBridge rule to pass events to the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T02:31:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is A. Using an Amazon Simple Queue Service (SQS) queue to store events and invoke the Lambda function is a good solution to decouple the third-party service calls and ensure that all the calls are eventually completed. SQS is a fully managed, reliable, and highly scalable message queuing service that allows applications to send, store, and receive messages between distributed components. By sending the third-party service calls to an SQS queue, it allows the application to continue processing without waiting for the third-party services to respond, which can result in faster response times and lower error rates."
      },
      {
        "date": "2023-01-15T02:31:00.000Z",
        "voteCount": 1,
        "content": "Other options like AWS Step Functions state machine, Amazon EventBridge, and Amazon Simple Notification Service (SNS) topic are not appropriate for this use case. AWS Step Functions is a service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Amazon EventBridge is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated SaaS applications, and AWS services. Amazon SNS is a fully managed messaging service for both application-to-application and application-to-person (A2P) communication. These services are not focused on providing message queues and would not be the best fit for this use case."
      },
      {
        "date": "2024-10-09T04:10:00.000Z",
        "voteCount": 1,
        "content": "SQS Queue = Decoupling the service calls + Eventual completion + Error handling and retries (DLQ)\n\n"
      },
      {
        "date": "2024-09-01T22:36:00.000Z",
        "voteCount": 1,
        "content": "A. Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function."
      },
      {
        "date": "2024-03-16T21:35:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-12-22T13:52:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-11-24T05:29:00.000Z",
        "voteCount": 1,
        "content": "SQS support dead letter queue and retry if the event processed fails"
      },
      {
        "date": "2023-07-02T15:38:00.000Z",
        "voteCount": 1,
        "content": "A no brainer"
      },
      {
        "date": "2023-05-09T21:42:00.000Z",
        "voteCount": 2,
        "content": "step functions would not help on the decoupling if you are not using an asynchronous element in this architecture which is SQS. the application need to have the ability to move out from synchronous calls to the third party services. correct answer is A."
      },
      {
        "date": "2023-03-31T06:32:00.000Z",
        "voteCount": 1,
        "content": "A : SQS QUEUE"
      },
      {
        "date": "2023-03-26T10:27:00.000Z",
        "voteCount": 2,
        "content": "SQS for decoupling"
      },
      {
        "date": "2023-02-22T17:16:00.000Z",
        "voteCount": 2,
        "content": "SQS ---&gt; Lambda is the correct option"
      },
      {
        "date": "2023-01-30T15:41:00.000Z",
        "voteCount": 1,
        "content": "decouple ==&gt; SQS"
      },
      {
        "date": "2023-01-28T09:36:00.000Z",
        "voteCount": 2,
        "content": "The application needs to pass the initiative to the next step. That means the application does not wait the response from the Lambda function, it should have the responsibility only to call the Lambda function. To do so, the application only throw the job information to Amazon SQS queue and finish. After that, AWS Lambda function can pull the job information from SQS queue and start processing actively.\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html"
      },
      {
        "date": "2023-01-26T08:40:00.000Z",
        "voteCount": 1,
        "content": "I vote for C - use Step Functions with its callback feature to throttle the third party api call."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/amazon/view/95416-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.<br><br>The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T10:05:00.000Z",
        "voteCount": 24,
        "content": "The correct answer is D. Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight role to create a trust relationship with the new IAM role in the sales account.\n\nThis solution meets the requirements by allowing the marketing team to access the data in the S3 bucket in the sales account through assuming an IAM role, which eliminates the need to copy the data or share the KMS key, and also eliminates the need to modify the S3 bucket policy or create a KMS grant. This solution allows to use the same access to the bucket without duplicating data and re-encrypting it."
      },
      {
        "date": "2023-01-17T10:05:00.000Z",
        "voteCount": 4,
        "content": "A. Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket is not correct because it would create unnecessary data duplication and increased storage costs.\n\nB. Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sales account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because it does not provide a secure way to share the KMS key between accounts and also it would create unnecessary data duplication and increased storage costs."
      },
      {
        "date": "2023-01-17T10:06:00.000Z",
        "voteCount": 2,
        "content": "C. Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because the Sales team's S3 bucket is in a different account, so the Marketing team cannot update the policy on the Sales team's S3 bucket."
      },
      {
        "date": "2023-06-21T11:09:00.000Z",
        "voteCount": 7,
        "content": "The catch is in the answers - \"Update the S3 bucket policy in the marketing account\". We don't need to access a bucket in the marketing but the sales account."
      },
      {
        "date": "2024-10-09T04:25:00.000Z",
        "voteCount": 1,
        "content": "Creating an IAM role in the sales account that grants access to the S3 bucket and allowing the marketing account (QuickSight) to assume that role.\n"
      },
      {
        "date": "2024-09-01T22:58:00.000Z",
        "voteCount": 1,
        "content": "C. Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket."
      },
      {
        "date": "2024-08-05T19:31:00.000Z",
        "voteCount": 3,
        "content": "There must be a typo in C.\n\nIn the context of option D, if Amazon QuickSight needs to access data in an S3 bucket in a different AWS account, and the setup involves assuming multiple roles, this approach could be problematic. QuickSight would not be able to assume the role in the sales account while simultaneously using its own role in the marketing account."
      },
      {
        "date": "2024-08-28T23:36:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-08-05T19:32:00.000Z",
        "voteCount": 3,
        "content": "In C, \"Update the S3 bucket policy in the marketing account\" should be changed to \"Update the S3 bucket policy in the sales account\""
      },
      {
        "date": "2024-08-01T09:15:00.000Z",
        "voteCount": 1,
        "content": "What is QuickSight rote? It can't be D. I'm assuming there is no typo, so C is wrong too. B is wrong because you can't grant that permission with SCPs.\n\nA would work provided that the replication permissions are set up correctly. It's not great because I don't think it's necessary to duplicate the data, but it's the only viable option we are given."
      },
      {
        "date": "2024-08-05T19:27:00.000Z",
        "voteCount": 1,
        "content": "Dude, do you have any idea of what Petabytes amount of data mean? No one would do that in real life if there's other options"
      },
      {
        "date": "2024-06-27T04:19:00.000Z",
        "voteCount": 3,
        "content": "C should be correct if change typo from market account to sales account for S3  bucket policy statement."
      },
      {
        "date": "2024-06-09T10:55:00.000Z",
        "voteCount": 3,
        "content": "Agree with other answers on C. This question is clearly a typo, and \"marketing\" should be changed to \"sales\" in C. The resolution for this scenario is even stated in the AWS Knowledge base, and the solution is identical when replacing \"marketing\" with \"sales\": https://repost.aws/knowledge-center/quicksight-cross-account-s3"
      },
      {
        "date": "2024-04-23T06:38:00.000Z",
        "voteCount": 2,
        "content": "It should be C and there should be a misspelling in \"Update the S3 bucket policy in the marketing account\" when it's referring to sales account"
      },
      {
        "date": "2024-03-05T08:06:00.000Z",
        "voteCount": 2,
        "content": "I think this is a great question with poorly phrased answers. If I have to choose between C and D, it would be neither since they both do not provide complete answers. Let me explain:\n\nFor C, you are updating the S3 bucket policy for the marketing account, when you should be  doing that for the sales account. So, C is wrong. However, if that were fixed to the sales account, everything would make sense, since the sales account would be providing the right policy, granting the correct KMS key permission, and the marketing account would be tweaking its permission in QuickSight. \n\nFor D, it is wrong simply because it says nothing about providing KMS key grant. Not only do you have to establish trust policy in the QuickSight role to access S3 bucket, you have to allow Decrypt to happen. You have to explicitly spell this out (read the permission part in the link below).\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
      },
      {
        "date": "2024-03-05T08:11:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/quicksight-cross-account-s3"
      },
      {
        "date": "2024-02-25T04:29:00.000Z",
        "voteCount": 1,
        "content": "Option C: Update the S3 bucket policy in the \"marketing account\" ....lol"
      },
      {
        "date": "2024-02-14T16:35:00.000Z",
        "voteCount": 2,
        "content": "The answer is C. Update the S3 bucket policy in the sales account to grant access to the QuickSight role in the marketing account. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.\n\nOption C correctly identifies the need to update the S3 bucket policy to grant access specifically to the QuickSight IAM role in the marketing account, which directly addresses the requirement for cross-account access to S3 data. Additionally, creating a KMS grant for the encryption key to allow decrypt access by the QuickSight role aligns with best practices for secure, cross-account access to encrypted S3 data. This approach minimizes operational overhead by using existing roles and permissions without the need for replication or additional resource sharing mechanisms."
      },
      {
        "date": "2024-02-03T12:58:00.000Z",
        "voteCount": 2,
        "content": "the question is badly formulated.. with all given options missing each a spec .. none of the answers are fully convincing"
      },
      {
        "date": "2024-01-12T01:50:00.000Z",
        "voteCount": 4,
        "content": "All answers are wrong:\n\nA. No KMS, not necessary replication \nB. No IAM\nD. No KMS\n\n\nBut the most likely answer is C.\n\n\"Update the S3 bucket policy in the marketing account\"\nThe question was never asked marketing s3 team bucket and all the data store in sales team S3 bucket.\nI think it's a typing error (marketing-&gt; sales)."
      },
      {
        "date": "2023-12-22T14:38:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-12-21T03:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is D - see: https://medium.com/codebyte/cross-account-s3-object-copying-with-kms-encrypted-buckets-5ebabef8aa03"
      },
      {
        "date": "2023-12-04T20:38:00.000Z",
        "voteCount": 1,
        "content": "none of the answers is correct.\nA: no mention of role granted to quicksight and no permission to KMS is granted. \nB: SCP is not designed for this.\nC: no mentioning of creating S3 bucket in marketing account. QuickSight role is created in Marketing account, while the dycryption access is required in sales account.\nD: Lack of dycryption access to KMS is grated to the new created IAM role in sales account."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/amazon/view/95417-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions architect must design a heterogeneous database migration on AWS.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-10T13:32:00.000Z",
        "voteCount": 8,
        "content": "This question quietly smell weird to me but no problem answer is C \nExp : AWS Schema Conversion Tool (SCT) can automatically convert the database schema from Microsoft SQL Server to Amazon RDS for MySQL. This allows for a smooth transition of the database schema without any manual intervention. AWS DMS can then be used to migrate the data from the on-premises databases to the newly created Amazon RDS for MySQL instance. This service can perform a one-time migration of the data or can set up ongoing replication of data changes to keep the on-premises and AWS databases in sync."
      },
      {
        "date": "2024-10-09T04:35:00.000Z",
        "voteCount": 1,
        "content": "Schema conversion required!"
      },
      {
        "date": "2024-09-01T23:36:00.000Z",
        "voteCount": 1,
        "content": "C. Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS."
      },
      {
        "date": "2024-03-08T12:34:00.000Z",
        "voteCount": 3,
        "content": "This process becomes easier with services like AWS DMS and AWS Schema Conversion Tool (AWS SCT), which help you migrate your commercial database to an open-source database on AWS with minimal downtime.\n\nIn heterogeneous database migrations, the source and target databases engines are different, as in Oracle to Amazon Aurora, or Oracle to PostgreSQL, MySQL, or MariaDB migrations. The schema structure, data types, and database code in the source and target databases can be quite different, so the schema and code must be transformed before the data migration starts. For this reason, heterogeneous migration is a two-step process:\n\nStep 1. Convert the source schema and code to match that of the target database. You can use AWS SCT for this conversion.\n\nStep 2. Migrate data from the source database to the target database. You can use AWS DMS for this process. \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/migration-oracle-database/heterogeneous-migration.html"
      },
      {
        "date": "2023-12-22T16:01:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-08-18T11:57:00.000Z",
        "voteCount": 3,
        "content": "My 2 cents, Heterogeneous database migration and SCT go with each other"
      },
      {
        "date": "2023-07-02T19:15:00.000Z",
        "voteCount": 1,
        "content": "C of course"
      },
      {
        "date": "2023-06-18T18:06:00.000Z",
        "voteCount": 1,
        "content": "keyword =  AWS Schema Conversion Tool"
      },
      {
        "date": "2023-05-09T22:18:00.000Z",
        "voteCount": 2,
        "content": "The question is about heterogenous database migration so in this case we need to convert the DB to a new schema. \nTherefore, answer is C"
      },
      {
        "date": "2023-03-26T10:56:00.000Z",
        "voteCount": 1,
        "content": "Use the AWS Schema Conversion Tool"
      },
      {
        "date": "2023-03-05T20:59:00.000Z",
        "voteCount": 1,
        "content": "For heterogenous DBs, SCT is apt."
      },
      {
        "date": "2023-02-06T07:14:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/database/migrating-a-sql-server-database-to-a-mysql-compatible-database-engine/"
      },
      {
        "date": "2023-02-04T01:05:00.000Z",
        "voteCount": 2,
        "content": "heterogenous -&gt; frmo onee DB engine to another"
      },
      {
        "date": "2023-02-02T08:53:00.000Z",
        "voteCount": 2,
        "content": "Straightforward - C"
      },
      {
        "date": "2023-01-30T15:48:00.000Z",
        "voteCount": 3,
        "content": "C is the answer"
      },
      {
        "date": "2023-01-15T06:24:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is C. Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.\n\nAWS Schema Conversion Tool (SCT) can automatically convert the database schema from Microsoft SQL Server to Amazon RDS for MySQL. This allows for a smooth transition of the database schema without any manual intervention.\n\nAWS DMS can then be used to migrate the data from the on-premises databases to the newly created Amazon RDS for MySQL instance. This service can perform a one-time migration of the data or can set up ongoing replication of data changes to keep the on-premises and AWS databases in sync."
      },
      {
        "date": "2023-01-15T06:25:00.000Z",
        "voteCount": 2,
        "content": "Option A is not correct because while Amazon RDS for MySQL supports SQL Server databases, it is not a good fit for migrating business-critical applications. The data model and architecture are different and would require significant re-engineering.\n\nOption B is not correct because AWS Snowball Edge Storage Optimized devices are used for transferring large amounts of data to and from AWS, but they do not support SQL Server.\n\nOption D is not correct because AWS DataSync can only transfer files and folders, it does not support SQL Server databases."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/amazon/view/95421-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.<br><br>After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the production account, create a new IAM policy that allows read and write access to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the development account, create a new IAM policy that allows read and write access to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T10:08:00.000Z",
        "voteCount": 13,
        "content": "The correct answer is A, C, and E.\n\nA: In the production account, creating a new IAM policy that allows read and write access to the S3 bucket is correct because it allows the design team to upload and update the static assets in the S3 bucket in the production account.\n\nC: In the production account, creating a role and attaching the new policy to the role, and defining the development account as a trusted entity is correct because it allows the design team from the development account to assume the role and access the S3 bucket in the production account, while limiting their access to only the specific resources and actions defined in the policy."
      },
      {
        "date": "2023-01-17T10:08:00.000Z",
        "voteCount": 4,
        "content": "E: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account is correct because it allows the users in the group to assume the role created in the production account, which gives them access to the S3 bucket in the production account.\n\nThe other choices are not correct because:\n\nB: In the development account, creating a new IAM policy that allows read and write access to the S3 bucket is not correct because the design team needs to access the S3 bucket in the production account, not the development account."
      },
      {
        "date": "2023-01-17T10:08:00.000Z",
        "voteCount": 2,
        "content": "D: In the development account, creating a role, attaching the new policy to the role and defining the production account as a trusted entity is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not create a role in the development account.\n\nF: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not the development account."
      },
      {
        "date": "2023-03-18T18:22:00.000Z",
        "voteCount": 10,
        "content": "Step 1: Create a role in the Production Account; create the role in the Production account and specify the Development account as a trusted entity. You also limit the role permissions to only read and write access to the productionapp bucket. Anyone granted permission to use the role can read and write to the productionapp bucket.\nStep 2: Grant access to the role Sign in as an administrator in the Development account and allow the AssumeRole action on the UpdateApp role in the Production account.\n\nSo, recap, production account you create the policy for S3, and you set development account as a trusted entity. Then on the development account you allow the sts:assumeRole action on the role in production account. \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
      },
      {
        "date": "2024-09-01T23:40:00.000Z",
        "voteCount": 1,
        "content": "A. In the production account, create a new IAM policy that allows read and write access to the S3 bucket.\nD. In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.\nE. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account."
      },
      {
        "date": "2024-03-21T10:20:00.000Z",
        "voteCount": 1,
        "content": "ACE. F is a trap."
      },
      {
        "date": "2023-12-22T16:06:00.000Z",
        "voteCount": 1,
        "content": "A, C and E"
      },
      {
        "date": "2023-11-10T18:43:00.000Z",
        "voteCount": 1,
        "content": "BCE\nNeed to provide Account in Dev S3 Read Write Access\nWe define the permissions of the user in the Account it was created in"
      },
      {
        "date": "2023-07-02T19:18:00.000Z",
        "voteCount": 1,
        "content": "ACE in this case"
      },
      {
        "date": "2023-06-15T08:04:00.000Z",
        "voteCount": 1,
        "content": "ACE is the correct choice of course"
      },
      {
        "date": "2023-05-16T19:57:00.000Z",
        "voteCount": 2,
        "content": "Vote for ACE"
      },
      {
        "date": "2023-03-26T22:35:00.000Z",
        "voteCount": 3,
        "content": "ACE is the best choice"
      },
      {
        "date": "2023-03-05T21:19:00.000Z",
        "voteCount": 2,
        "content": "Make Dev account as trusted entity. create a role in prod account. attache IAM policy of prod account and let development account assume this role to access prod s3 bucket."
      },
      {
        "date": "2023-02-04T01:31:00.000Z",
        "voteCount": 1,
        "content": "I think it's clear"
      },
      {
        "date": "2023-02-01T10:36:00.000Z",
        "voteCount": 2,
        "content": "ACE is correct answer"
      },
      {
        "date": "2023-01-30T15:55:00.000Z",
        "voteCount": 2,
        "content": "ACE  should works"
      },
      {
        "date": "2023-01-15T16:24:00.000Z",
        "voteCount": 2,
        "content": "ACE is my answer"
      },
      {
        "date": "2023-01-15T06:32:00.000Z",
        "voteCount": 2,
        "content": "A, D, and E are the correct steps that would meet the requirements.\n\nA. In the production account, create a new IAM policy that allows read and write access to the S3 bucket. This will allow the design team to read and write to the S3 bucket that holds the assets in the production account.\n\nD. In the development account, create a role. Attach the new policy to the role. Define the production account as a trusted entity. This will allow the design team to assume a role in the development account that has permissions to access the S3 bucket in the production account.\n\nE. In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account. This will allow the users in the design team group to assume the role created in step D and access the S3 bucket in the production account."
      },
      {
        "date": "2023-01-15T06:32:00.000Z",
        "voteCount": 1,
        "content": "Option B is not required because the design team needs to access the S3 bucket in the production account, not in the development account.\n\nOption C is not required because the design team needs to access the S3 bucket in the production account and this can be done by assuming a role in the development account.\n\nOption F is not required because the design team needs to access the S3 bucket in the production account and this can be done by assuming a role in the development account that is trusted by the production account."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/amazon/view/95425-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.<br><br>A solutions architect must mitigate the performance issues before the company launches the application to production.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing environment\u2019s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-24T00:54:00.000Z",
        "voteCount": 22,
        "content": "I think AWS wants you to know is the below.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html"
      },
      {
        "date": "2024-10-09T04:47:00.000Z",
        "voteCount": 1,
        "content": "You can change your environment type to a single-instance or load-balanced, scalable environment by editing your environment's configuration.\n\n"
      },
      {
        "date": "2024-09-01T23:41:00.000Z",
        "voteCount": 1,
        "content": "C. Modify the existing environment\u2019s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes."
      },
      {
        "date": "2024-01-19T05:56:00.000Z",
        "voteCount": 3,
        "content": "A = you don't need to create a new application (instead you could create a new environment in the existing application)\nB = traffic-split is used to deploy a new version of the app, not to scale out\nC = correct\nD = rebuild does not allow to change environment configuration https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-management-rebuild.html"
      },
      {
        "date": "2023-12-26T19:50:00.000Z",
        "voteCount": 3,
        "content": "You can change the existing environment from single instance to load balanced.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html"
      },
      {
        "date": "2023-12-22T16:11:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-12-17T02:06:00.000Z",
        "voteCount": 1,
        "content": "C here https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.EditConfig.html"
      },
      {
        "date": "2023-11-14T23:41:00.000Z",
        "voteCount": 2,
        "content": "you can change existing Beanstalk environment type from a single instance to load-balanced"
      },
      {
        "date": "2023-08-12T00:09:00.000Z",
        "voteCount": 1,
        "content": "I prefer C"
      },
      {
        "date": "2023-07-26T21:15:00.000Z",
        "voteCount": 1,
        "content": "Option C is very correct. See https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html for confirmation"
      },
      {
        "date": "2023-07-02T19:22:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-05-16T20:48:00.000Z",
        "voteCount": 4,
        "content": "Anybody know why we should select all AZs?"
      },
      {
        "date": "2023-03-26T22:36:00.000Z",
        "voteCount": 1,
        "content": "Modify the existing environment\u2019s capacity configuration to use a load-balanced environment type."
      },
      {
        "date": "2023-03-18T18:27:00.000Z",
        "voteCount": 4,
        "content": "You can change your environment type to a single-instance or load-balanced, scalable environment by editing your environment's configuration. In some cases, you might want to change your environment type from one type to another. For example, let's say that you developed and tested an application in a single-instance environment to save costs. When your application is ready for production, you can change the environment type to a load-balanced, scalable environment so that it can scale to meet the demands of your customers.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html"
      },
      {
        "date": "2023-03-05T21:36:00.000Z",
        "voteCount": 2,
        "content": "A is wrong. no need to re create new EB env when the question is asking to mitigate probable performance issues based on current compute consumption of &gt;=85%"
      },
      {
        "date": "2023-02-14T11:01:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html"
      },
      {
        "date": "2023-02-04T01:41:00.000Z",
        "voteCount": 1,
        "content": "It's C. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html#using-features.managing.changetype"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/amazon/view/95427-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.<br><br>Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerforming a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T06:42:00.000Z",
        "voteCount": 15,
        "content": "B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.\n\nThis is the optimal solution as migrating the database to Amazon RDS will provide the ability to easily scale read replicas for handling increased read traffic during the end of the month. Additionally, RDS will manage the underlying infrastructure and provide automatic backups, software patching, and monitoring, which will reduce the operational overhead for the company. \n\nOption A may help but it will not be sufficient to handle the heavy load, option C and D are not efficient solutions to han"
      },
      {
        "date": "2024-09-01T23:42:00.000Z",
        "voteCount": 1,
        "content": "B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month."
      },
      {
        "date": "2024-03-17T07:38:00.000Z",
        "voteCount": 1,
        "content": "B, include read replicas"
      },
      {
        "date": "2023-12-22T16:17:00.000Z",
        "voteCount": 1,
        "content": "Option B -&gt; Reporting workload = higher read operation ==&gt; Solution RDS read replica."
      },
      {
        "date": "2023-10-20T09:38:00.000Z",
        "voteCount": 1,
        "content": "I go with D"
      },
      {
        "date": "2023-09-07T00:25:00.000Z",
        "voteCount": 1,
        "content": "I vote D\nTo solve heavy IO issue, I think both option B and D both works.But the question demands for to \"handle the month-end load with the LEAST impact on performance\" , Option B create the new read replicas during end of month seems too complicated, you'll need to seperate read/write traffic from application at the end of the month."
      },
      {
        "date": "2023-08-31T18:36:00.000Z",
        "voteCount": 2,
        "content": "Reporting is also an important hint. Only read operations are needed here; so read replicas would server the purpose"
      },
      {
        "date": "2023-08-10T13:44:00.000Z",
        "voteCount": 1,
        "content": "all other sections not applicable i guess specially D its so funny. Each month none of technical person doesnt want to do like this task."
      },
      {
        "date": "2023-07-02T19:23:00.000Z",
        "voteCount": 1,
        "content": "B of cpourse"
      },
      {
        "date": "2023-06-18T18:09:00.000Z",
        "voteCount": 2,
        "content": "it slows down during the final three days of each month due to month-end reporting\nthen \nhight read in database == solution add read replicas \nB"
      },
      {
        "date": "2023-07-05T13:18:00.000Z",
        "voteCount": 2,
        "content": "month end reporting is to submit the financial data, aka write the new data to DB"
      },
      {
        "date": "2023-03-26T22:38:00.000Z",
        "voteCount": 1,
        "content": "Performing a one-time migration"
      },
      {
        "date": "2023-01-30T16:00:00.000Z",
        "voteCount": 2,
        "content": "B is the best solution"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/amazon/view/95428-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.<br><br>Which solution will meet these requirements with the LEAST code changes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T06:46:00.000Z",
        "voteCount": 18,
        "content": "The correct answer would be A, as migrating the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container and storing container images in Amazon Elastic Container Registry (Amazon ECR) would minimize the code changes and administrative overhead required to maintain the servers. This option would allow the company to use the Application Load Balancer (ALB) to interact with the application and the ECS task execution role permission to access the ECR image repository.\n\nOption B would require the application code to be migrated to a container that runs in AWS Lambda, which would require more code changes. \n\nOption C would require migrating the application to Amazon Elastic Kubernetes Service (Amazon EKS) which would require more administrative overhead. \n\nOption D would require configuring Lambda to use an Application Load Balancer (ALB), which is not a native feature of Lambda."
      },
      {
        "date": "2023-02-04T01:52:00.000Z",
        "voteCount": 1,
        "content": "B does not say anything about Lambda. Where have you red that?"
      },
      {
        "date": "2023-02-04T01:53:00.000Z",
        "voteCount": 1,
        "content": "You are right, I mixed A with B"
      },
      {
        "date": "2023-05-13T16:00:00.000Z",
        "voteCount": 1,
        "content": "There is another problem with Option B, it suggest using EKS with managed node groups and not Fargate, which breaks the requirement for reducing administrative overhead"
      },
      {
        "date": "2023-01-15T06:48:00.000Z",
        "voteCount": 4,
        "content": "This solution allows for the existing application code to be packaged into a container, which can then be deployed to ECS on Fargate. The use of AWS App2Container will help automate the containerization process, minimizing the need for code changes. Additionally, by using ECR to store container images, the application can continue to use the same images and dependencies that it currently relies on. The use of an Application Load Balancer (ALB) to interact with the application further simplifies the migration process by allowing the use of the existing application's endpoint."
      },
      {
        "date": "2023-03-18T18:55:00.000Z",
        "voteCount": 8,
        "content": "AWS App2Container (A2C) is a command line tool to help you lift and shift applications that run in your on-premises data centers or on virtual machines, so that they run in containers that are managed by Amazon ECS, Amazon EKS, or AWS App Runner.\n\nMoving legacy applications to containers is often the starting point toward application modernization. There are many benefits to containerization:\n\u2022\tReduces operational overhead and infrastructure costs\n\u2022\tIncreases development and deployment agility\n\u2022\tStandardizes build and deployment processes across an organization\nhttps://docs.aws.amazon.com/app2container/latest/UserGuide/what-is-a2c.html\nAWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers. AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\nhttps://aws.amazon.com/fargate/"
      },
      {
        "date": "2024-09-01T23:43:00.000Z",
        "voteCount": 1,
        "content": "A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
      },
      {
        "date": "2024-03-17T07:41:00.000Z",
        "voteCount": 1,
        "content": "A, ECS Fargate"
      },
      {
        "date": "2024-01-31T04:56:00.000Z",
        "voteCount": 2,
        "content": "If the keyword 'Java' has not been mentioned, Answer A would have been considered as A2C (App2Container) is valid only for Java and .Net web applications"
      },
      {
        "date": "2024-01-19T05:59:00.000Z",
        "voteCount": 1,
        "content": "A = correct\nB = migrating app to container to be executed in a Lambda requires more code changes\nC = EKS with managed node group requires more operations than ECS with Fargate\nD = see B"
      },
      {
        "date": "2023-12-22T16:23:00.000Z",
        "voteCount": 1,
        "content": "Option A. Option C EKS not not valid because as using API Gateway is not needed and may require more code changes."
      },
      {
        "date": "2023-11-14T23:57:00.000Z",
        "voteCount": 1,
        "content": "in the case of fargate capacity provider you should grant permissions to access ecr to task execution role, otherwise to ec2 instance roles which you run containers on"
      },
      {
        "date": "2023-10-16T20:06:00.000Z",
        "voteCount": 1,
        "content": "Sorry is A"
      },
      {
        "date": "2023-10-16T20:04:00.000Z",
        "voteCount": 1,
        "content": "C on eks because of complex VM dependecies"
      },
      {
        "date": "2023-10-16T20:04:00.000Z",
        "voteCount": 1,
        "content": "D because of complex vm dependencies"
      },
      {
        "date": "2023-07-02T19:26:00.000Z",
        "voteCount": 1,
        "content": "it's an A"
      },
      {
        "date": "2023-06-21T11:58:00.000Z",
        "voteCount": 1,
        "content": "Did anyone notice that part \"has complex dependencies on VMs that are in the company's data center.\"? If the application has complex dependencies on VMs then how do we migrate it to containers or lambda? Another awkward question."
      },
      {
        "date": "2023-04-27T12:51:00.000Z",
        "voteCount": 1,
        "content": "I still select A, but as someone that has migrated Java applications to AWS using AWS App2Container and RedHat S2i, this is a lot of pain."
      },
      {
        "date": "2023-03-26T22:40:00.000Z",
        "voteCount": 1,
        "content": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container."
      },
      {
        "date": "2023-03-09T02:18:00.000Z",
        "voteCount": 2,
        "content": "least code chansges"
      },
      {
        "date": "2023-02-15T03:30:00.000Z",
        "voteCount": 2,
        "content": "Fargate, Modernize stack"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/amazon/view/95432-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T10:13:00.000Z",
        "voteCount": 20,
        "content": "The correct answer is D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints. This solution meets the requirement of having a failover to another region by having a copy of the Lambda function and API Gateway endpoint in a different region, and using Route 53's failover routing policy to route traffic between the two regions.\n\nOption A is not correct because it only creates an additional API Gateway endpoint in us-west-2 and relies on Route 53's failover routing policy to direct traffic to the correct endpoint. But it does not deploy the Lambda function to the new region and this makes the failover incomplete."
      },
      {
        "date": "2023-03-16T08:17:00.000Z",
        "voteCount": 10,
        "content": "You always use ChatGPT to paste answers. Most of the time ChatGPT gives wrong answers do you know this?"
      },
      {
        "date": "2023-01-17T10:13:00.000Z",
        "voteCount": 3,
        "content": "Option B is not correct because it uses a SQS queue as a buffer between the API Gateway and the Lambda function, but this does not provide failover to another region. In addition, it would also increase the latency of the system as the SQS will act as an additional layer.\n\nOption C is not correct because it deploys the Lambda function to the us-west-2 Region and creates an API Gateway endpoint in the same region. But it uses AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints. However, this is not a failover solution as both regions will be active and serving traffic at the same time."
      },
      {
        "date": "2024-09-01T23:51:00.000Z",
        "voteCount": 1,
        "content": "D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints."
      },
      {
        "date": "2024-03-17T07:44:00.000Z",
        "voteCount": 1,
        "content": "D, deploy everything in the second region and configure the failover routing policy"
      },
      {
        "date": "2023-12-22T16:26:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-08-31T19:07:00.000Z",
        "voteCount": 1,
        "content": "Refer https://aws.amazon.com/blogs/architecture/implementing-multi-region-disaster-recovery-using-event-driven-architecture/"
      },
      {
        "date": "2023-07-02T19:28:00.000Z",
        "voteCount": 1,
        "content": "clearly D"
      },
      {
        "date": "2023-03-26T22:41:00.000Z",
        "voteCount": 1,
        "content": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region"
      },
      {
        "date": "2023-03-18T19:13:00.000Z",
        "voteCount": 2,
        "content": "Currently, the default API endpoint type in API Gateway is the edge-optimized API endpoint, which enables clients to access an API through an Amazon CloudFront distribution. This typically improves connection time for geographically diverse clients. By default, a custom domain name is globally unique and the edge-optimized API endpoint would invoke a Lambda function in a single region in the case of Lambda integration. You can\u2019t use this type of endpoint with a Route 53 active-active setup and fail-over.\n\nThe new regional API endpoint in API Gateway moves the API endpoint into the region and the custom domain name is unique per region. This makes it possible to run a full copy of an API in each region and then use Route 53 to use an active-active setup and failover.\nhttps://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/"
      },
      {
        "date": "2023-03-05T22:21:00.000Z",
        "voteCount": 1,
        "content": "B is wrong, cannot direct traffic to SQS Queue ? it does not even mention posting messages to queue."
      },
      {
        "date": "2023-01-30T16:09:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-01-15T17:03:00.000Z",
        "voteCount": 4,
        "content": "D is correct\nA is not because the Lambda is in us-ease-1 but api gateway is in us-west-2. cannot cross regions"
      },
      {
        "date": "2023-01-15T07:01:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A.\n\nIn this solution, an API Gateway endpoint is created in the us-west-2 Region. This new endpoint is configured to direct traffic to the Lambda function in us-east-1. If a failure occurs in the us-east-1 Region, Amazon Route 53's failover routing policy automatically routes traffic to the us-west-2 Region. This ensures that traffic is directed to a healthy endpoint, providing failover support for the application.\n\nB, C and D does not meet the requirement of having failover routing policy.\n\nIn B, SQS is not a failover mechanism, it is a messaging service and it does not provide failover routing.\n\nIn C, Global Accelerator and Application Load Balancer does not provide failover routing.\n\nIn D, While creating a second endpoint in the us-west-2 Region and using Amazon Route 53 to route traffic to it, it still does not provide failover routing."
      },
      {
        "date": "2023-09-24T14:45:00.000Z",
        "voteCount": 1,
        "content": "D CLEARLY States:    Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.  You claimed it did not ,  and the moderator ALLOWED IT  ?!? !?"
      },
      {
        "date": "2023-09-24T14:39:00.000Z",
        "voteCount": 1,
        "content": "Gateway VPC endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC.  \n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html  \n\n==&gt; IN CONTRAST \nThese are the ENDPOINTS for  API Gateway:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html\nGateway endpoint DOES NOT DIRECT TRAFFIC PERIOD"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/amazon/view/95442-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.<br><br>The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to make sure that other departments cannot share the RI discounts.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the AWS Billing and Cost Management console. use the organization\u2019s management account 10 turn off RI Sharing for the HR departments production AWS account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-09T02:18:00.000Z",
        "voteCount": 8,
        "content": "Management account --&gt;Billing Dashboard --&gt; Billing preferences, this option is there to choose enable/disable RI discounts sharing"
      },
      {
        "date": "2024-09-02T00:04:00.000Z",
        "voteCount": 1,
        "content": "C. In the AWS Billing and Cost Management console. use the organization\u2019s management account 10 turn off RI Sharing for the HR departments production AWS account."
      },
      {
        "date": "2024-03-12T00:31:00.000Z",
        "voteCount": 2,
        "content": "It is indeed C."
      },
      {
        "date": "2024-03-04T07:02:00.000Z",
        "voteCount": 2,
        "content": "RI sharing is done for the whole Org. It's all or nothing, and it's done in the Billing and Cost Management console in the Org account."
      },
      {
        "date": "2024-08-30T03:08:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-03-10T02:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html\nC"
      },
      {
        "date": "2024-02-14T21:31:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct because AWS allows the management of RI sharing settings at the account level within the AWS Billing and Cost Management console. By turning off RI sharing in the HR department\u2019s production account, the RI benefits (such as the discounted rate) are applied only to instances within that account, preventing other accounts, even within the same organization, from accessing these discounts. This directly addresses the requirement.\n\nOption C suggests using the organization\u2019s management account to turn off RI sharing for the HR department\u2019s production AWS account. While the management account controls many aspects of AWS Organizations, including consolidated billing, RI sharing preferences are managed at the individual account level within the Billing and Cost Management console, not directly through the management account for specific accounts."
      },
      {
        "date": "2023-12-25T04:53:00.000Z",
        "voteCount": 1,
        "content": "option C"
      },
      {
        "date": "2023-12-22T16:31:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-02T19:31:00.000Z",
        "voteCount": 3,
        "content": "surely C"
      },
      {
        "date": "2023-03-26T22:43:00.000Z",
        "voteCount": 1,
        "content": "C is the way to go"
      },
      {
        "date": "2023-03-06T06:40:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html"
      },
      {
        "date": "2023-03-05T23:01:00.000Z",
        "voteCount": 3,
        "content": "Management account --&gt;Billing Dashboard --&gt; Billing preferences, this option is there to choose enable/disable RI discounts sharing \nhttps://us-east-1.console.aws.amazon.com/billing/home#/preferences"
      },
      {
        "date": "2023-03-02T11:43:00.000Z",
        "voteCount": 3,
        "content": "How can you restrict access from AWS billing console? Can you show me please??\nOption D is the correct solution because an SCP (Service Control Policy) can be created in the AWS Organizations service to restrict access to specific resources or actions across the entire organization or specific OUs. In this case, an SCP can be created to restrict other departments from accessing the RIs purchased by the HR department's production account. This ensures that the discounts are not shared with other departments."
      },
      {
        "date": "2023-03-05T23:01:00.000Z",
        "voteCount": 5,
        "content": "Bro, Go to Management account --&gt;Billing Dashboard --&gt; Billing preferences, this option is there to choose enable/disable RI discounts sharing \nhttps://us-east-1.console.aws.amazon.com/billing/home#/preferences"
      },
      {
        "date": "2023-08-15T06:27:00.000Z",
        "voteCount": 1,
        "content": "initially i thought the same....but the catch here is that RIs are purchased in HR Prod department\nSo, we have to work on disabling discount sharing wrt that account so that IT IS NOT SHARED W OTHERS and this actions can only be performed from Management account"
      },
      {
        "date": "2023-08-18T13:42:00.000Z",
        "voteCount": 2,
        "content": "Restricting the access to RI's is not the ask in the question, only \"restricting the RI discounts\" from HR to other departments is the ask, and that you could be done by Management Account (as identified by others in this forum). Hope that helps!"
      },
      {
        "date": "2023-02-01T23:07:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-01-15T08:08:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is C.\n\nIn this solution, the organization\u2019s management account can be used to turn off RI sharing for the HR department's production AWS account in the AWS Billing and Cost Management console. This will ensure that the other departments cannot share the RI discounts and the HR department can use the RIs for their new system without any interruption."
      },
      {
        "date": "2023-01-15T08:08:00.000Z",
        "voteCount": 3,
        "content": "A, B and D does not meet the requirement of turning off RI sharing for the HR department's production AWS account.\n\nIn A, Turning off RI sharing in the HR department's production account will not prevent other departments from sharing the RI discounts.\n\nIn B, Removing the HR department's production AWS account from the organization may cause issues in consolidated billing and it does not prevent other departments from sharing the RI discounts.\n\nIn D, Creating an SCP in the organization to restrict access to the RIs is not necessary because the management account can directly turn off the RI sharing, it also does not prevent other departments from sharing the RI discounts."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/amazon/view/95443-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.<br><br>The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive.<br><br>How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSuspend the Auto Scaling group\u2019s HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSuspend the Auto Scaling group\u2019s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-30T16:12:00.000Z",
        "voteCount": 10,
        "content": "The correct answer is D."
      },
      {
        "date": "2023-03-06T10:24:00.000Z",
        "voteCount": 9,
        "content": "Disabling health check wont let SA know which instance is un healthy. So A is certainly wrong. D is correct."
      },
      {
        "date": "2024-09-02T00:08:00.000Z",
        "voteCount": 1,
        "content": "D. Suspend the Auto Scaling group\u2019s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy."
      },
      {
        "date": "2024-03-17T07:53:00.000Z",
        "voteCount": 1,
        "content": "D, stop the autoscaling process"
      },
      {
        "date": "2024-01-27T02:53:00.000Z",
        "voteCount": 3,
        "content": "Why not B? Can the ASG override the Ec2 termination protection?"
      },
      {
        "date": "2023-12-22T16:35:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-15T00:33:00.000Z",
        "voteCount": 2,
        "content": "you can stop auto-scaling processes, here you need to stop termination, you need health checks to know which instance to check"
      },
      {
        "date": "2023-08-31T19:19:00.000Z",
        "voteCount": 4,
        "content": "If ASG terminates the instances because they are unhealthy there is no way we can login to the instance using session manager or otherwise to investigate the problem. So, suspend the termination."
      },
      {
        "date": "2023-07-02T19:33:00.000Z",
        "voteCount": 1,
        "content": "d of course"
      },
      {
        "date": "2023-06-18T12:41:00.000Z",
        "voteCount": 1,
        "content": "keyword == Auto Scaling group\u2019s Terminate process."
      },
      {
        "date": "2023-09-12T06:37:00.000Z",
        "voteCount": 1,
        "content": "Have you cleared the exam?"
      },
      {
        "date": "2023-03-26T22:45:00.000Z",
        "voteCount": 2,
        "content": "Suspend the Auto Scaling group\u2019s Terminate process."
      },
      {
        "date": "2023-03-18T19:32:00.000Z",
        "voteCount": 4,
        "content": "Amazon EC2 Auto Scaling stops marking instances unhealthy as a result of EC2 and Elastic Load Balancing health checks. Your custom health checks continue to function properly. After you suspend HealthCheck, if you need to, you can manually set the health state of instances in your group and have ReplaceUnhealthy replace them.\nSuspending the Terminate process doesn't prevent the successful termination of instances using the force delete option with the delete-auto-scaling-group command.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/incident-manager.html\nWe want the health checks to continue failing, just stop terminating to identify root cause"
      },
      {
        "date": "2023-03-02T12:36:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\nIf you do not want instances to be replaced, we recommend that you suspend the ReplaceUnhealthy and HealthCheck process for individual Auto Scaling groups. For more information, see Suspend and resume a process for an Auto Scaling group. \nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html"
      },
      {
        "date": "2023-03-18T19:34:00.000Z",
        "voteCount": 5,
        "content": "That does not solve, it removes the healthcheck process, but also removes the ones that are being marked as unhealthy. The issue now is that one it is tagged as unhealthy they are being terminated. So, any that are already marked get terminated and you just removed the health checks to find remaining. you can't troubleshoot what you don't know."
      },
      {
        "date": "2023-01-15T08:12:00.000Z",
        "voteCount": 7,
        "content": "https://www.examtopics.com/discussions/amazon/view/51249-exam-aws-certified-solutions-architect-professional-topic-1/\n\nThe correct answer is D.\n\nIn this solution, the architect can suspend the Auto Scaling group's Terminate process, which will prevent the instances marked as unhealthy from being terminated. This will allow the architect to log in to the instance using Session Manager and troubleshoot the issue without losing access to the instance."
      },
      {
        "date": "2023-01-15T08:12:00.000Z",
        "voteCount": 4,
        "content": "Option A is incorrect because suspending the HealthCheck scaling process will not prevent instances from being terminated.\n\nOption B is incorrect because enabling EC2 instance termination protection will not prevent instances from being terminated by Auto Scaling group.\n\nOption C is incorrect because setting the termination policy to OldestInstance on the Auto Scaling group will not prevent instances marked as unhealthy from being terminated."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/amazon/view/95444-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations.<br><br>Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T08:16:00.000Z",
        "voteCount": 18,
        "content": "The correct answer is A.\n\nIn this solution, AWS Firewall Manager is used to manage AWS WAF rules across accounts in the organization. An AWS Systems Manager Parameter Store parameter is used to store account numbers and OUs to manage. This parameter can be updated as needed to add or remove accounts or OUs. An Amazon EventBridge rule is used to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. This solution allows for easy management of AWS WAF rules across multiple accounts with minimal operational overhead."
      },
      {
        "date": "2023-01-15T08:16:00.000Z",
        "voteCount": 3,
        "content": "Option B does not meet the requirement of being able to add or remove accounts or OUs from managed AWS WAF rule sets as needed.\n\nOption C is not the best approach as it requires manual configuration of the cross-account IAM roles and assume-role calls in the Lambda function, increasing the operational overhead.\n\nOption D does not meet the requirement of providing a centralized management console to manage the WAF rules across multiple accounts."
      },
      {
        "date": "2023-01-24T22:12:00.000Z",
        "voteCount": 6,
        "content": "https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/"
      },
      {
        "date": "2024-09-21T06:12:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B. I would have said A like everyone else, but correct answer was provided in Udemy practice exam.\n\nThanks to Organization structure, Config rules apply automatically to newly added accounts (fulfills requirements: least amount of operational overhead (as opposed to A - manually maintaining accounts and OU list).\n\nAs often, AWS exam answers are partially off-track, a real-life deployment would be a clever combination of both A &amp; B answers, using FW manager, Config and Cloudformation.\nhttps://aws.amazon.com/blogs/security/use-aws-firewall-manager-to-deploy-protection-at-scale-in-aws-organizations/"
      },
      {
        "date": "2024-09-02T00:13:00.000Z",
        "voteCount": 1,
        "content": "A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account."
      },
      {
        "date": "2023-12-22T16:41:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-08-31T19:30:00.000Z",
        "voteCount": 5,
        "content": "AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations\nFirewall Manager supports wide variety of services, including:\n\u25cf AWS WAF\n\u25cf VPC Security Groups\n\u25cf AWS Network Firewall\n\u25cf Route53 DNS Firewall\n\u25cf AWS Shield Advanced\n\u25cf Palo Alto Cloud Next-generation firewalls\nThe Prerequisites are: AWS Organizations + AWS Config."
      },
      {
        "date": "2023-08-15T05:38:00.000Z",
        "voteCount": 2,
        "content": "I have to say A is right.\nplease take  a look at this:\nhttps://aws.amazon.com/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/"
      },
      {
        "date": "2023-07-02T19:41:00.000Z",
        "voteCount": 1,
        "content": "A is a good option"
      },
      {
        "date": "2023-06-18T12:41:00.000Z",
        "voteCount": 2,
        "content": "keyword == AWS Firewall Manager"
      },
      {
        "date": "2023-06-04T04:22:00.000Z",
        "voteCount": 2,
        "content": "the correct answer is A https://docs.aws.amazon.com/solutions/latest/automations-for-aws-firewall-manager/architecture-overview.html"
      },
      {
        "date": "2023-05-13T17:23:00.000Z",
        "voteCount": 2,
        "content": "This is a complex question. But I voted A because the Firewall manager seems to be the correct way to centralize the rules across accounts. \nBelow are some interesting references I could find\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/4cbaea3b-ceba-48e3-bd56-eca138f7a66c/en-US \nhttps://aws.amazon.com/blogs/security/use-aws-firewall-manager-vpc-security-groups-to-protect-applications-hosted-on-ec2-instances/ \n \nhttps://aws.amazon.com/blogs/security/automatically-updating-aws-waf-rule-in-real-time-using-amazon-eventbridge/"
      },
      {
        "date": "2023-03-26T22:46:00.000Z",
        "voteCount": 1,
        "content": "Use AWS Firewall Manager to manage AWS WAF rules"
      },
      {
        "date": "2023-03-06T13:04:00.000Z",
        "voteCount": 1,
        "content": "Not D, KMS to store account numbers ?"
      },
      {
        "date": "2023-01-30T16:13:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/amazon/view/95447-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.<br><br>The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise.<br><br>What should the solutions architect recommend to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-30T16:17:00.000Z",
        "voteCount": 19,
        "content": "a little bit confused between A and D but as said by others members D doesn't adress the The question of \"data must not travel across the Internet\"==&gt; A is the answer"
      },
      {
        "date": "2023-04-20T08:10:00.000Z",
        "voteCount": 6,
        "content": "B and D are out because you need the VPC endpoints.\nC is out because you cannot enable rotation in Parameter Store (https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_parameterstore.html)"
      },
      {
        "date": "2024-09-02T00:14:00.000Z",
        "voteCount": 1,
        "content": "A. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC."
      },
      {
        "date": "2024-08-13T06:09:00.000Z",
        "voteCount": 1,
        "content": "A is better than D because it remove the complexity of management of the secret to connect to the DB and replaces it with the IAM DB authentication. In addition S3 endpoint GW is better to prevent traffic going through internet."
      },
      {
        "date": "2024-03-28T13:33:00.000Z",
        "voteCount": 1,
        "content": "Key is data must not travel accros the internet mean use VPC gateway"
      },
      {
        "date": "2024-03-17T07:59:00.000Z",
        "voteCount": 1,
        "content": "A, \"data must no travel across the internet\". This setup ensures internal network use only, meeting the security and networking requirements efficiently"
      },
      {
        "date": "2024-02-27T16:39:00.000Z",
        "voteCount": 2,
        "content": "The data must not travel across the Internet."
      },
      {
        "date": "2024-02-15T09:09:00.000Z",
        "voteCount": 1,
        "content": "Option D offers a comprehensive solution by leveraging AWS Secrets Manager for storing and automatically rotating database credentials, which directly addresses the concern of minimizing the impact if credentials become compromised. Changing the Lambda function to retrieve credentials from Secrets Manager enhances security by not storing credentials within environment variables. Enforcing HTTPS for S3 data transfers ensures the data in transit is encrypted. While deploying a gateway VPC endpoint for S3 (as mentioned in other options) is a best practice to keep traffic within the AWS network, enforcing HTTPS also contributes to securing data transfers without explicitly stating the need to avoid Internet travel. Secrets Manager inherently provides secure access to secrets without needing to travel across the Internet when accessed from AWS services within the same region.\n\nOption A does not address the requirement for securing and rotating database credentials stored as Lambda environment variables."
      },
      {
        "date": "2023-12-22T16:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A as S3 VPC is endpoint is needed to avoid data going over internet."
      },
      {
        "date": "2023-09-23T23:08:00.000Z",
        "voteCount": 1,
        "content": "AWS Secrets Manager is meant for this job, why go with any other option"
      },
      {
        "date": "2023-09-23T23:13:00.000Z",
        "voteCount": 6,
        "content": "My bad its A \nD is not addressing this point \n The data must not travel across the Internet"
      },
      {
        "date": "2023-08-15T05:50:00.000Z",
        "voteCount": 2,
        "content": "I prefor A"
      },
      {
        "date": "2023-07-24T12:09:00.000Z",
        "voteCount": 3,
        "content": "A \n\nhttps://aws.amazon.com/pt/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/"
      },
      {
        "date": "2023-07-03T13:16:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/pt_br/secretsmanager/latest/userguide/vpc-endpoint-overview.html"
      },
      {
        "date": "2023-07-02T19:45:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-05-13T17:35:00.000Z",
        "voteCount": 1,
        "content": "I was about to chose D however just enforcing the HTTP will not avoid the data to travel across internet. You will need the option where the gateway VPC endpoint is deployed for access the S3. The answer is A.\nA will also solve the issue related to authenticate the lambda to aurora without needing to store passwords, refer to - https://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/"
      },
      {
        "date": "2023-05-01T00:04:00.000Z",
        "voteCount": 2,
        "content": "However, Option A is not the best choice for the given scenario because:\n\nIt doesn't address the requirement to minimize the impact of compromised database credentials. IAM database authentication eliminates traditional user credentials, but it doesn't implement password rotation for the remaining IAM credentials.\n\nWhile the VPC endpoint keeps traffic within the AWS network, it doesn't enforce encryption during data transfers to Amazon S3.\n\nOption D, on the other hand, addresses both the requirement of minimizing the impact of compromised credentials through password rotation using AWS Secrets Manager and ensuring encrypted data transfers to Amazon S3 by enforcing HTTPS. That's why Option D is the better choice for this scenario."
      },
      {
        "date": "2023-05-13T17:34:00.000Z",
        "voteCount": 3,
        "content": "I was also choosing D however just enforcing the HTTP will not avoid the data to travel across internet. You will need the option where the gateway VPC endpoint is deployed for access the S3. The answer is A"
      },
      {
        "date": "2023-03-26T22:48:00.000Z",
        "voteCount": 2,
        "content": "A for sure due to VPC endpoints."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/amazon/view/95448-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.<br><br>While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance types account for a high proportion of the costs. The solutions architect finds out that the company\u2019s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.<br><br>The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers\u2019 IAM accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T08:21:00.000Z",
        "voteCount": 15,
        "content": "The correct answer is C.\n\nIn this solution, a new IAM policy is created that specifies the allowed instance types. This policy is then attached to an IAM group that contains the IAM accounts for the developers. This will ensure that the developers can only launch instances of the specified types, thus limiting the costs associated with the creation and termination of large instances."
      },
      {
        "date": "2023-01-15T08:21:00.000Z",
        "voteCount": 10,
        "content": "A. Creating a desired-instance-type managed rule in AWS Config is not a sufficient solution, as it only identifies when an instance is launched with an unauthorized type, it does not prevent it.\n\nB. Creating a launch template that specifies the instance types that are allowed is not a sufficient solution, because it limits the instances types that can be launched in the EC2 console, but it does not prevent the launch of instances through the AWS SDK, AWS CLI, or other AWS services.\n\nD. Using EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image is not a direct solution to the problem of limiting the instance types that only the developers can launch. It can be useful for creating standardize images for the developers, but it does not provide the necessary control mechanism to limit the instance types."
      },
      {
        "date": "2024-01-26T03:29:00.000Z",
        "voteCount": 6,
        "content": "{\n            \"Sid\": \"limitedSize\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"ec2:RunInstances\",\n            \"Resource\": \"arn:aws:ec2:*:*:instance/*\",\n            \"Condition\": {\n                \"ForAnyValue:StringNotLike\": {\n                    \"ec2:InstanceType\": [\n                        \"*.nano\",\n                        \"*.small\",\n                        \"*.micro\",\n                        \"*.medium\"\n                    ]\n                }\n            }\n        }"
      },
      {
        "date": "2024-09-02T00:17:00.000Z",
        "voteCount": 1,
        "content": "C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers"
      },
      {
        "date": "2024-01-18T01:11:00.000Z",
        "voteCount": 1,
        "content": "\"an IAM group that contains the IAM accounts\" ???"
      },
      {
        "date": "2024-01-23T13:30:00.000Z",
        "voteCount": 1,
        "content": "yes, in IAM group you have user IAM accounts."
      },
      {
        "date": "2024-06-05T21:30:00.000Z",
        "voteCount": 1,
        "content": "You have IAM users...Not IAM \"accounts\". Bad wording here..."
      },
      {
        "date": "2023-12-22T16:56:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-02T19:47:00.000Z",
        "voteCount": 1,
        "content": "Its a C"
      },
      {
        "date": "2023-06-21T19:52:00.000Z",
        "voteCount": 2,
        "content": "The only technical achievable choices are A and C. However A will only identify the issue and will not prevent it. Even if we set up a remediation rule to terminate the instances immediately - that will cause more issues for the developers and unclear signals that something is wrong with the testing. So A remains the only possible option."
      },
      {
        "date": "2023-06-27T10:26:00.000Z",
        "voteCount": 1,
        "content": "C is the correct solution remained. Typo mistake in the comments."
      },
      {
        "date": "2023-06-17T14:04:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-cc-c-c-cc-c-c-c-c-cc-"
      },
      {
        "date": "2023-03-26T22:49:00.000Z",
        "voteCount": 1,
        "content": "IAM policy.."
      },
      {
        "date": "2023-01-30T16:20:00.000Z",
        "voteCount": 3,
        "content": "answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/amazon/view/95451-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.<br><br>Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule in each account to find resources with missing tags.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Inspector in the organization to find resources with missing tags.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag."
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BDE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-06T17:33:00.000Z",
        "voteCount": 7,
        "content": "If config rule is added (A) it can be seen in AWS Config aggregator (E) Using SCP in as aws organization is used here in question. So, A,B,E"
      },
      {
        "date": "2023-03-06T17:34:00.000Z",
        "voteCount": 4,
        "content": "If there are no organizations used, D can be used to prevent EC2 run instances too,\nC is for vulnerabilities checking..F for all security issues consolidated.."
      },
      {
        "date": "2024-09-02T00:18:00.000Z",
        "voteCount": 1,
        "content": "A. Create an AWS Config rule in each account to find resources with missing tags.\nB. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\nE. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag."
      },
      {
        "date": "2024-03-17T08:02:00.000Z",
        "voteCount": 1,
        "content": "ABE, SCP + Config + Config Aggregator"
      },
      {
        "date": "2024-03-04T07:32:00.000Z",
        "voteCount": 3,
        "content": "B and E handle the requirements in a centralised manner, giving least operational overhead, without anything needing to be added. The question is plainly wrongly stated. If three options have to be selected, then A is the least absurd one."
      },
      {
        "date": "2024-02-15T09:23:00.000Z",
        "voteCount": 2,
        "content": "A. Create an AWS Config rule in each account to find resources with missing tags.AWS Config can evaluate the configuration of your AWS resources and identify resources that do not comply with specified requirements, such as missing specific tags. This helps in identifying existing resources with the issue.\n B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.Service Control Policies (SCPs) can enforce permissions across all accounts in an organization. By creating an SCP that denies launching EC2 instances without the required Project tag, you can prevent the problem from occurring in the future at the organization level.\nE. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.An AWS Config aggregator can aggregate compliance data from multiple accounts and regions. This allows for centralized visibility of instances lacking the required tags, making it easier to address and resolve the issue across the entire organization."
      },
      {
        "date": "2024-01-27T10:25:00.000Z",
        "voteCount": 1,
        "content": "A is not needed if you have D. Correct answer is BDE."
      },
      {
        "date": "2024-01-27T10:27:00.000Z",
        "voteCount": 1,
        "content": "I meant E, not D"
      },
      {
        "date": "2024-02-15T09:25:00.000Z",
        "voteCount": 1,
        "content": "It is not D. Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing. \nIAM policies do not directly support conditional denies based on tag presence during the resource creation process in the same way SCPs do. This enforcement is better handled at the organization level with SCPs."
      },
      {
        "date": "2023-12-22T17:02:00.000Z",
        "voteCount": 1,
        "content": "Option A, B and E"
      },
      {
        "date": "2023-10-25T22:46:00.000Z",
        "voteCount": 3,
        "content": "Inspector checks for Vulnerabilities but not the tags."
      },
      {
        "date": "2023-07-02T19:52:00.000Z",
        "voteCount": 2,
        "content": "its ABE"
      },
      {
        "date": "2023-04-23T00:32:00.000Z",
        "voteCount": 3,
        "content": "A.  AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents. These documents define the actions to be performed on noncompliant AWS resources evaluated by AWS Config Rules. You can associate SSM documents by using AWS Management Console or by using APIs.\n\nAWS Config provides a set of managed automation documents with remediation actions. You can also create and associate custom automation documents with AWS Config rules.\n\nTo apply remediation on noncompliant resources, you can either choose the remediation action you want to associate from a prepopulated list or create your own custom remediation actions using SSM documents. AWS Config provides a recommended list of remediation action in the AWS Management Console.\n\nIn the AWS Management Console, you can either choose to manually or automatically remediate noncompliant resources by associating remediation actions with AWS Config rules. With all remediation actions, you can either choose manual or automatic remediation."
      },
      {
        "date": "2023-04-07T01:17:00.000Z",
        "voteCount": 4,
        "content": "A. Create an AWS Config rule in each account to find resources with missing tags.\nBy creating an AWS Config rule in each account, you can check if resources are missing tags or have tags that are not conforming to your organization's standards. You can also use AWS Config to automatically remediate non-compliant resources by applying tags. This can help ensure that resources are properly tagged for cost allocation purposes. Here is the AWS Config documentation for creating rules:\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html"
      },
      {
        "date": "2023-04-07T01:18:00.000Z",
        "voteCount": 6,
        "content": "E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.\nBy creating an AWS Config aggregator, you can collect a list of EC2 instances across multiple accounts in the organization that are missing the required Project tag. This can help you identify instances that need to be tagged properly for cost allocation. Here is the AWS Config documentation for creating aggregators:\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-aggregator.html"
      },
      {
        "date": "2024-01-27T10:28:00.000Z",
        "voteCount": 2,
        "content": "So what is the point of having A if you have E at an Org level?"
      },
      {
        "date": "2024-05-26T06:26:00.000Z",
        "voteCount": 1,
        "content": "AWS Config aggregator does not run any rules on its own. Instead, it collects the data from the \"source accounts\" where AWS Config is enabled. \nA to get the list of EC2 instances in each account.\nE to aggregate the lists from all accounts in one place.\nB to disallow creating non-compliant EC2 instances.\nSee https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html."
      },
      {
        "date": "2023-04-07T01:17:00.000Z",
        "voteCount": 5,
        "content": "B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\nBy creating a Service Control Policy (SCP) in the organization, you can enforce a deny action for EC2 instances that do not have the required Project tag. This can prevent users from launching instances that are not tagged correctly and ensure that new instances are tagged properly for cost allocation. Here is the AWS Organizations documentation for creating SCPs:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2023-03-26T22:51:00.000Z",
        "voteCount": 1,
        "content": "ABE is the better choice"
      },
      {
        "date": "2023-03-17T09:14:00.000Z",
        "voteCount": 4,
        "content": "what's the value of A and E together- it's either or ? the outcome is the same - thoughts?"
      },
      {
        "date": "2024-01-27T10:29:00.000Z",
        "voteCount": 1,
        "content": "Fully agree, BDE"
      },
      {
        "date": "2024-01-27T10:33:00.000Z",
        "voteCount": 2,
        "content": "Did some research..\n\nAggregators provide a read-only view into the source accounts and regions that the aggregator is authorized to view. Aggregators do not provide mutating access into the source account or region. For example, this means that you cannot deploy rules through an aggregator or pull snapshot files from the source account or region through an aggregator.\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html#multi-account-multi-region-data-aggregation\n\nSo ABE seems correct"
      },
      {
        "date": "2023-02-21T09:20:00.000Z",
        "voteCount": 1,
        "content": "ABE makes sense"
      },
      {
        "date": "2023-02-13T04:41:00.000Z",
        "voteCount": 1,
        "content": "Config, SCP and IAM policy may not require in each account but it says to select three options  so going with ABE"
      },
      {
        "date": "2023-02-04T02:57:00.000Z",
        "voteCount": 1,
        "content": "BE makes sense"
      },
      {
        "date": "2023-01-28T04:00:00.000Z",
        "voteCount": 2,
        "content": "the best way to deploy config rules accross accounts= SCP"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/amazon/view/95454-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.<br><br>The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:<br>\u2022\tManaged AWS services to minimize operational complexity.<br>\u2022\tA buffer that automatically scales to match the throughput of data and requires no ongoing administration.<br>\u2022\tA visualization tool to create dashboards to observe events in near-real time.<br>\u2022\tSupport for semi-structured JSON data and dynamic schemas.<br><br>Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-06T20:26:00.000Z",
        "voteCount": 14,
        "content": "Amazon Kinesis Data Firehose (A) allows you to buffer events in two ways: through buffering size or buffering time. With buffering size, you can configure the maximum size of the buffer in MB or the maximum number of records in the buffer. Once the buffer is full, it will automatically deliver the data to the destination\n\nAmazon ES (D) has its ability to receive events from various sources in real-time. Amazon ES can ingest data from a variety of sources, such as Amazon Kinesis Data Firehose, Amazon CloudWatch Logs, and Amazon S3, making it a powerful tool for organizations looking to analyze and visualize real-time streaming data. (Kibana dashboards)"
      },
      {
        "date": "2023-04-07T01:27:00.000Z",
        "voteCount": 12,
        "content": "Option B includes using an Amazon Kinesis data stream to buffer events, which is a valid solution for a streaming data use case. However, it requires more ongoing administration compared to using Amazon Kinesis Data Firehose, which is a fully managed service. Additionally, the use of Amazon Kinesis Data Firehose allows the company to take advantage of built-in data transformation and processing capabilities, which can reduce the amount of code required to implement the solution. Therefore, I selected option A over option B as it better meets the requirement of minimizing operational complexity."
      },
      {
        "date": "2024-09-02T00:19:00.000Z",
        "voteCount": 1,
        "content": "A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.\nD. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards."
      },
      {
        "date": "2024-05-05T13:45:00.000Z",
        "voteCount": 1,
        "content": "\"A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\" \nI think buffer, here, means a solution that will reliably hold information for further successful processing. I don't think it means to buffer and batch process the events so I don't agree with other people's comments in regards to buffer. \n\nThat said, my concern is with \"automatically scales to match the throughput of data\". Firehose does it automatically. Kinesis can also do automatically if on-demand mode is chosen. \n\nAlso, \"Support for semi-structured JSON data and dynamic schemas.\" Dynamic Schemas? Firehose or Data stream don't do that. Firehose does do dynamic partitioning and JSON deserializing. I guess that's what the question meant?"
      },
      {
        "date": "2024-04-04T08:33:00.000Z",
        "voteCount": 1,
        "content": "Option A NOT Option B -  Amazon Data Firehose buffers incoming streaming data in memory to a certain size (buffering size) and for a certain period of time (buffering interval) before delivering it to the specified destinations. \n\nhttps://docs.aws.amazon.com/firehose/latest/dev/buffering-hints.html"
      },
      {
        "date": "2024-03-12T01:09:00.000Z",
        "voteCount": 1,
        "content": "On second thought: A because B requires manual shard configuration."
      },
      {
        "date": "2024-03-12T01:06:00.000Z",
        "voteCount": 1,
        "content": "Also, Streams is more real-time."
      },
      {
        "date": "2024-10-05T01:06:00.000Z",
        "voteCount": 1,
        "content": "It says \"Near Real Time\" not \"Real Time\" so Firehouse is the better option between the 2"
      },
      {
        "date": "2024-03-12T01:04:00.000Z",
        "voteCount": 1,
        "content": "B rather than A because B integrates the lambda functionality for transformation of the data, which must be done as an added step in A, thereby increasing operational overhead."
      },
      {
        "date": "2024-02-15T10:25:00.000Z",
        "voteCount": 2,
        "content": "A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.Amazon Kinesis Data Firehose provides a fully managed service for effortlessly loading streaming data into AWS services such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. It scales automatically to match the throughput of data and requires no ongoing administration. AWS Lambda can be used in conjunction with Kinesis Data Firehose to process and transform the data before it\u2019s loaded into the destination, supporting dynamic schemas and semi-structured JSON data. Additionally, Amazon Kinesis Data Firehose has built-in buffering capabilities and can be used to observe events in near-real time, making it a more appropriate choice for the given scenario."
      },
      {
        "date": "2024-02-15T10:25:00.000Z",
        "voteCount": 1,
        "content": "D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards. Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, secure, operate, and scale Elasticsearch to search, analyze, and visualize data in real-time. Kibana is an open-source visualization tool designed to work with Elasticsearch, providing powerful and easy-to-use features to create dashboards that can visualize data in near-real-time."
      },
      {
        "date": "2024-01-31T06:51:00.000Z",
        "voteCount": 2,
        "content": "ElasticSearch is the ex name of new OpenSearch"
      },
      {
        "date": "2024-01-19T07:12:00.000Z",
        "voteCount": 1,
        "content": "I choose Data Stream (KDS) over Data Firehose (KDF) in this scenario:\n- KDS allows to you store events up to 1 year, allowing to achieve buffering with no constraints on size and with a very large time limit. KDS support on-demand capacity mode\n- KDF transport mechanism is based on buffering, but here buffering is limited on size (max 128MiB) and time (up to 900 sec)"
      },
      {
        "date": "2023-12-22T17:06:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2023-09-04T22:45:00.000Z",
        "voteCount": 2,
        "content": "BD\nQuestion states near-Real time \nThats the differentiating factor between Kinesis data stream and Firehose\n\nI would go for B and D"
      },
      {
        "date": "2023-09-12T05:51:00.000Z",
        "voteCount": 1,
        "content": "but about \"\u2022 Managed AWS services to minimize operational complexity.\"\ni believe Kinesis Firehose is managed solution whereas DataStream required operational overhead"
      },
      {
        "date": "2023-07-02T19:55:00.000Z",
        "voteCount": 1,
        "content": "AD for unstructured data"
      },
      {
        "date": "2023-03-26T22:53:00.000Z",
        "voteCount": 1,
        "content": "AD is my vote"
      },
      {
        "date": "2023-03-05T02:51:00.000Z",
        "voteCount": 1,
        "content": "A,D seem correct. https://www.examtopics.com/discussions/amazon/view/47625-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-01-15T17:24:00.000Z",
        "voteCount": 1,
        "content": "AD are correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/amazon/view/95455-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company\u2019s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.<br><br>A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-06T20:58:00.000Z",
        "voteCount": 13,
        "content": "VPC endpoints to mitigate NAT gateway huge data transfer costs especially in Kinesis usecase where large data is passed thru\n\nWith a VPC endpoint policy, you can define rules to control access to the VPC endpoint. You can specify the source IP address or IP address range that is allowed to access the endpoint, as well as the type of traffic that is allowed, such as HTTP, HTTPS, or custom TCP ports. You can also specify the resources that can be accessed through the VPC endpoint, such as an Amazon S3 bucket or an Amazon DynamoDB table."
      },
      {
        "date": "2023-06-21T20:14:00.000Z",
        "voteCount": 7,
        "content": "B is a distractor. You don't need IAM permissions to use a service via an endpoint. You only need to set up proper routing to that endpoint"
      },
      {
        "date": "2024-09-21T01:46:00.000Z",
        "voteCount": 1,
        "content": "Access Permissions are still required for most AWS services, including Kinesis Data Streams, even when accessed via a VPC endpoint. The endpoint allows traffic to the service, but your application or users still need IAM permissions to interact with the service. Without proper IAM permissions, even if the routing is set up correctly, the service will not authorize actions like reading from or writing to a Kinesis stream."
      },
      {
        "date": "2024-09-02T00:20:00.000Z",
        "voteCount": 1,
        "content": "D. Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications."
      },
      {
        "date": "2024-04-26T05:43:00.000Z",
        "voteCount": 1,
        "content": "D without any doubt."
      },
      {
        "date": "2024-03-17T08:05:00.000Z",
        "voteCount": 1,
        "content": "D, VPC endpoint"
      },
      {
        "date": "2024-03-17T08:04:00.000Z",
        "voteCount": 1,
        "content": "D, VPC endpoint"
      },
      {
        "date": "2023-12-22T17:10:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-10-20T21:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. \nAn endpoint policy is a resource-based policy that you attach to a VPC endpoint to control which AWS principals can use the endpoint to access an AWS service.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html"
      },
      {
        "date": "2023-07-12T20:16:00.000Z",
        "voteCount": 1,
        "content": "It's a d"
      },
      {
        "date": "2023-06-18T12:48:00.000Z",
        "voteCount": 2,
        "content": "reduce cost ==  interface VPC endpoint"
      },
      {
        "date": "2023-06-18T12:48:00.000Z",
        "voteCount": 1,
        "content": "A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category."
      },
      {
        "date": "2023-04-18T08:07:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nIt's not B because user's/applications doesn't need permissions to use an endpoint: https://docs.aws.amazon.com/vpc/latest/privatelink/security_iam_id-based-policy-examples.html"
      },
      {
        "date": "2023-05-18T17:50:00.000Z",
        "voteCount": 1,
        "content": "No. in your document it says \"By default, users and roles don't have permission to create or modify AWS PrivateLink resources\". Users and roles don't have permissions so they do need permissions to use an interface endpoint"
      },
      {
        "date": "2023-03-26T22:54:00.000Z",
        "voteCount": 1,
        "content": "D is the best choice."
      },
      {
        "date": "2023-03-02T07:33:00.000Z",
        "voteCount": 1,
        "content": "If this is a cost-saving question is very hard to answer, you pay for both, and depending on the region one can be cheaper than the other. There is a cost for a NAT GW and also for a VPCendpoint per AZ plus the traffic you generate over them. In my experience, because you need a VPCendpoint for each service NAT-GW is cheaper."
      },
      {
        "date": "2024-05-26T06:57:00.000Z",
        "voteCount": 1,
        "content": "I agree that both NAT GW and interface VPC endpoints can become expensive. I believe that's why the question mentioned that most applications use KDS. I assume that it's the biggest middleware service and you will not need VPC endpoints for other services.\n\nPricing (based on Ohio):\nNAT GW: 0.045 $/h + 0.045 $/GB\nInterface VPC Endpoint: 0.01 $/h + 0.01 $/GB (lowered if more data transferred)\n\nIn the final setup the company will still pay for NAT GW (hourly fee) but the transfer cost (most of it) will be moved to VPCE, which gives:\nfor 1 GB per month\nNAT GW: (24*30)h*0.045$/h + 1GB*0.045$/GB =  32.445$   &gt;    (24*30)h*0.01$/h + 1GB*0.01$/GB = 7.21$\nfor 1000GB per month\nNAT GW: (24*30)h*0.045$/h + 1000GB*0.045$/GB =  77.4$   &gt;    (24*30)h*0.01$/h + 1000GB*0.01$/GB = 17.2$"
      },
      {
        "date": "2023-02-22T20:27:00.000Z",
        "voteCount": 3,
        "content": "Allowing traffic from the application using the VPC endpoint is key to bypassing NAT Gateway."
      },
      {
        "date": "2023-02-11T14:50:00.000Z",
        "voteCount": 1,
        "content": "Which is which?\n\nA VPC endpoint policy is an IAM resource policy that you attach to a VPC endpoint. It determines which principals can use the VPC endpoint to access the endpoint service. The default VPC endpoint policy allows all actions by all principals on all resources over the VPC endpoint.  https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html#vpc-endpoints-policies"
      },
      {
        "date": "2023-02-04T03:49:00.000Z",
        "voteCount": 3,
        "content": "B seems correct too."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/amazon/view/95457-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.<br><br>The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-06T21:52:00.000Z",
        "voteCount": 14,
        "content": "https://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png\nB is wrong as it says, two DX Gateways contradictory\nC is wrong as it says to configure DXG to route traffic. infact Transit gateway peering need to be done between two transit gateways of each reigon.\nA is wrong because Private VIF is not apt in mentioned config of the question. Public VIF is correct (Transit public VIF)\nIf you are using a single DX Gateway"
      },
      {
        "date": "2023-03-06T22:00:00.000Z",
        "voteCount": 4,
        "content": "Whichever option has this text is correct - \"Peer the transit gateways with each other to support cross-Region routing\""
      },
      {
        "date": "2024-09-21T01:52:00.000Z",
        "voteCount": 1,
        "content": "While transit gateway peering can enable cross-Region VPC communication, it is not necessary when you are using a Direct Connect gateway. A Direct Connect gateway already provides the capability to route traffic across multiple Regions without needing to peer the transit gateways directly."
      },
      {
        "date": "2024-09-02T00:24:00.000Z",
        "voteCount": 1,
        "content": "D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing."
      },
      {
        "date": "2024-04-24T21:19:00.000Z",
        "voteCount": 1,
        "content": "It can be both A or D based on AWS documentation: https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/hybrid-network-connections.html"
      },
      {
        "date": "2024-03-21T10:34:00.000Z",
        "voteCount": 1,
        "content": "Don't let \"No single points of failure can exist on the network\" mislead you into thinking that you need two DCGWs. DCGWs are not part of the region they connect to. Therefore, no SPOF translates to a double DC connection to a single DCGW. Hence, D."
      },
      {
        "date": "2024-03-17T08:08:00.000Z",
        "voteCount": 1,
        "content": "D, this approach ensures high availability and robust network connectivity across the specified AWS regions and the on-premises data center."
      },
      {
        "date": "2024-02-25T17:50:00.000Z",
        "voteCount": 2,
        "content": "Answer D - As per from AWS \nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-more-than-3.html"
      },
      {
        "date": "2023-12-22T17:43:00.000Z",
        "voteCount": 1,
        "content": "Choice is between C and D. Better the two D is the right option."
      },
      {
        "date": "2023-12-07T00:39:00.000Z",
        "voteCount": 3,
        "content": "D is correct ref architecture https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html"
      },
      {
        "date": "2023-12-04T17:43:00.000Z",
        "voteCount": 1,
        "content": "Answer D. Peer the transit gateways for cross-region routing."
      },
      {
        "date": "2023-11-15T01:30:00.000Z",
        "voteCount": 1,
        "content": "to connect to transit gateways through the dx gateway you should use transit VIF"
      },
      {
        "date": "2023-09-04T06:58:00.000Z",
        "voteCount": 2,
        "content": "I agree 'D' is a good answer to the problem, but isn't the DXGW a single point of failure?\n\nQuestion says \"No single points of failure can exist on the network.\""
      },
      {
        "date": "2023-07-02T20:08:00.000Z",
        "voteCount": 1,
        "content": "it's D"
      },
      {
        "date": "2023-05-23T16:47:00.000Z",
        "voteCount": 1,
        "content": "Would it be C for the answer? A Direct Connect gateway supports communication between attached transit virtual interfaces and associated transit gateways only and may enable a virtual private gateway to another virtual private gateway. https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-gateways.html"
      },
      {
        "date": "2023-05-23T16:52:00.000Z",
        "voteCount": 1,
        "content": "Actually, D is a proper answer."
      },
      {
        "date": "2023-05-16T21:54:00.000Z",
        "voteCount": 4,
        "content": "I agree with option D \nRefer to the diagram below which explains in detail the use of Transit VIF and Public VIF. Also demonstrates the necessity for peering the transit gateways to allow the cross-region routing. \nhttps://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png\nThe only options that are using the cross-region routing are A and D. Option A mentions the use of Private VIF and not the Transit VIF. Hence A is incorrect."
      },
      {
        "date": "2023-05-16T21:55:00.000Z",
        "voteCount": 3,
        "content": "Refer to the following article\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html"
      },
      {
        "date": "2023-04-09T04:20:00.000Z",
        "voteCount": 3,
        "content": "Transit VIF required to connect to Transit Gateway, and Transit peering is required to connect multi regions... \nHere is the full diagram:\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html"
      },
      {
        "date": "2023-03-26T22:57:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/amazon/view/95458-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user. The company has a multi-Region AWS CloudTrail trail in the AWS account.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Step Functions state machine to remove access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Simple Notification Service (Amazon SNS) to notify the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Pinpoint to notify the security team."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "ABE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T13:10:00.000Z",
        "voteCount": 13,
        "content": "Event Bus (EventBridge) system to receive event notification (Option A). Step function can get triggered with workflow of doing steps like removing access and sending email etc..(Option D, E)\n\nEventBridge enables you to create event rules that match events from different sources, such as AWS services, SaaS applications, custom applications, and other AWS accounts. Once an event rule is triggered, EventBridge can route the event to one or more targets, such as AWS Lambda functions, Amazon SNS topics, Amazon SQS queues, or custom HTTP endpoints.\n\nAWS Step Functions supports several AWS services, such as AWS Lambda, Amazon Simple Notification Service (SNS), and Amazon Simple Queue Service (SQS). You can use these services to trigger actions and pass data between steps in your state machine.\n\nPinpoint is chat system which question did not ask, F is wrong. Not C as"
      },
      {
        "date": "2023-05-04T02:47:00.000Z",
        "voteCount": 1,
        "content": "I agree with this."
      },
      {
        "date": "2023-03-12T15:17:00.000Z",
        "voteCount": 1,
        "content": "this explanation makes sense to me."
      },
      {
        "date": "2024-09-02T00:27:00.000Z",
        "voteCount": 1,
        "content": "A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.\n    D. Invoke an AWS Step Functions state machine to remove access.\n    E. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team."
      },
      {
        "date": "2024-04-04T08:52:00.000Z",
        "voteCount": 1,
        "content": "Option ADE:  Most people agree with option AE.    There can be situations where human intervention is required before the workflow can progress. For example, approving a substantial credit increase may require human approval \n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-security-automation.html"
      },
      {
        "date": "2024-08-30T03:37:00.000Z",
        "voteCount": 1,
        "content": "just ADE"
      },
      {
        "date": "2024-03-17T14:36:00.000Z",
        "voteCount": 1,
        "content": "Why not BCE? or ACE?\n\nHow to use Step Function to remove permission?"
      },
      {
        "date": "2024-02-11T12:58:00.000Z",
        "voteCount": 2,
        "content": "Poorly constructed answer choices, but ADE is the least worst option."
      },
      {
        "date": "2024-02-02T19:06:00.000Z",
        "voteCount": 1,
        "content": "I picked ADE. EventBridge, Lambda / Step Function, and SNS are required.\nBDE: No. CloudTrail can't trigger Step Function directly.\nABE: No. This solution can't remove the user access automatically.\nChoosing B alone without A can't directly trigger Lambda / Step functions to remove the user access. C can't compare with D. F is not relevant."
      },
      {
        "date": "2024-01-27T22:07:00.000Z",
        "voteCount": 3,
        "content": "Eventbridge is not needed. Cloudtrail can send notifications to SNS directly\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html"
      },
      {
        "date": "2024-01-27T22:16:00.000Z",
        "voteCount": 2,
        "content": "Also, if you select ADE how would the event ever trigger SNS to send the notification?"
      },
      {
        "date": "2024-05-26T10:31:00.000Z",
        "voteCount": 1,
        "content": "What do you mean? SNS topic is one of the (many) allowed targets for EventBridge.\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html\n\nRegarding \"Eventbridge is not needed\" - it's only true for notifications because CloudTrail integrates with SNS. CloudTrail alone cannot trigger any automation tools like Lambda or Step Function. That's why EventBridge is better in this case. You can add both targets to the same rule."
      },
      {
        "date": "2024-01-24T21:50:00.000Z",
        "voteCount": 1,
        "content": "Step function is a process/workflow orchestrator. Usually process/workflow orchestrator doesn\u2019t do actual task, cause the objective of a orchestrator is to maintain the stage of a process/workflow. Instead, the orchestrator call a service to complete the task and update the stage. \nSo the task of removing access should be done by a Lambda function. Since lambda function is not an option, the only applicable option is C, while ECS introduces too much administration overhead, and is a very bad choice for this task."
      },
      {
        "date": "2023-12-22T17:49:00.000Z",
        "voteCount": 1,
        "content": "A, D and E"
      },
      {
        "date": "2023-07-02T20:13:00.000Z",
        "voteCount": 1,
        "content": "ADE. have to assume the step function calls lambda or some such to actually perform action"
      },
      {
        "date": "2023-05-18T11:17:00.000Z",
        "voteCount": 2,
        "content": "I've chosen the EventBridge option (A) because I really was not able to find a way to set Cloudtrail to trigger SNS on it's own. The rest 2 are common sense"
      },
      {
        "date": "2024-01-27T22:09:00.000Z",
        "voteCount": 1,
        "content": "Here you go https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html"
      },
      {
        "date": "2023-04-07T01:40:00.000Z",
        "voteCount": 2,
        "content": "A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.\n\nB. Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.\n\nE. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team."
      },
      {
        "date": "2023-04-07T01:40:00.000Z",
        "voteCount": 2,
        "content": "By creating an Amazon EventBridge rule, the company can detect the CreateUser event in CloudTrail and use it to trigger actions such as sending notifications or invoking AWS Lambda functions.\n\nConfiguring CloudTrail to send a notification for the CreateUser event to an Amazon SNS topic allows the security team to receive a notification whenever a new IAM user is created.\n\nUsing Amazon SNS, the security team can receive the notification and approve or deny the new IAM user creation. If the security team denies the creation, access can be automatically removed using AWS Lambda or AWS Step Functions.\n\nTherefore, these three steps will allow the company to meet its requirements for user creation approval and access removal."
      },
      {
        "date": "2023-03-26T22:59:00.000Z",
        "voteCount": 1,
        "content": "ADE is right"
      },
      {
        "date": "2023-02-10T12:32:00.000Z",
        "voteCount": 1,
        "content": "ADE Step Functions works."
      },
      {
        "date": "2023-02-04T11:20:00.000Z",
        "voteCount": 1,
        "content": "I like ACE better. I am not sure Step Functions would work."
      },
      {
        "date": "2023-02-11T15:32:00.000Z",
        "voteCount": 1,
        "content": "According to ChatGPT, AWS Step Functions can interact with AWS APIs in a few different ways. One example is below.\n\nDirectly invoking AWS APIs using the \"Task\" state in Step Functions. This state type allows you to run an AWS Lambda function, which can interact with AWS APIs as part of its logic."
      },
      {
        "date": "2023-01-15T17:31:00.000Z",
        "voteCount": 1,
        "content": "ADE are correct"
      },
      {
        "date": "2023-01-15T08:56:00.000Z",
        "voteCount": 3,
        "content": "This is the correct answer because it follows these steps:\n\n- A: The first step is to create an EventBridge rule that listens for the specific API call to create a new IAM user. This will trigger the next step in the process.\n\n- D: The next step is to use an AWS Step Functions state machine to remove access for the new IAM user. This ensures that access is removed automatically, as required by the security team.\n\n- E: Finally, use Amazon SNS to notify the security team that a new user has been created and access has been removed. This allows the security team to review and approve the user as necessary.\n\nOption B is not correct because CloudTrail alone is not able to remove access for the new user.\n\nOption C is not correct because it is not specified in the question that the company is using Amazon Elastic Container Service and AWS Fargate technology.\n\nOption F is not correct because the question specifies that the company should use Amazon SNS to notify the security team, not Amazon Pinpoint."
      },
      {
        "date": "2023-03-12T15:15:00.000Z",
        "voteCount": 1,
        "content": "\"the question specifies that the company should use Amazon SNS \" -&gt; no, it does not specify anything like that. \n\"because it is not specified in the question that the company is using Amazon Elastic Container\"-&gt; so?  is it specified that they use step function., can't find that either.\nThe question must have changed, it does not match your  explanations."
      },
      {
        "date": "2023-05-16T17:25:00.000Z",
        "voteCount": 7,
        "content": "He just copied the answer from chatgpt for every question, really made me sick"
      },
      {
        "date": "2023-06-02T18:57:00.000Z",
        "voteCount": 3,
        "content": "it is annoying, I don't bother with reading them even if the answer they picked is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/amazon/view/95459-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.<br><br>The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other.<br><br>Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to accounts and between accounts."
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T09:04:00.000Z",
        "voteCount": 15,
        "content": "The correct answer would be options A, C and D, because they address the requirements outlined in the question.\n\nA. Deploying a landing zone environment using AWS Control Tower and enrolling accounts in an organization in AWS Organizations allows for a centralized management of access to all accounts and applications.\n\nC. Creating transit gateways and transit gateway VPC attachments in each account and configuring appropriate route tables allows for private network traffic, and ensures that the production account and shared network account have connectivity to all accounts, while the development and staging accounts have access only to each other.\n\nD. Setting up and enabling AWS IAM Identity Center (AWS Single Sign-On) and creating appropriate permission sets with required MFA for existing accounts allows for multi-factor authentication at login and specific roles to be assigned to user groups."
      },
      {
        "date": "2023-01-15T09:04:00.000Z",
        "voteCount": 4,
        "content": "The other options are not correct because:\n\nB. Enabling AWS Security Hub in all accounts to manage cross-account access and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.\n\nE. Enabling AWS Control Tower in all accounts to manage routing between accounts and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution."
      },
      {
        "date": "2023-01-15T09:04:00.000Z",
        "voteCount": 3,
        "content": "F. Creating IAM users and groups and configuring MFA for all users and setting up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts does not address the requirement of creating separate accounts for development, staging, production, and shared network. Additionally, it does not address the requirement of keeping the traffic on a private network."
      },
      {
        "date": "2024-09-02T00:29:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.\nC. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.\nD. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts."
      },
      {
        "date": "2024-03-11T08:00:00.000Z",
        "voteCount": 2,
        "content": "A, C and D are right answers. Option C is though not clear. Transit gateway needs to be created in shared network account and tgw vpc attachment in all accounts. But option C says \"create tgw and tgw vpc attachment in all accounts\", which is a bit confusing"
      },
      {
        "date": "2024-08-01T11:02:00.000Z",
        "voteCount": 1,
        "content": "Yes, you probably only need one TGW in the shared account"
      },
      {
        "date": "2023-12-22T17:56:00.000Z",
        "voteCount": 1,
        "content": "A, C and D"
      },
      {
        "date": "2023-12-04T17:51:00.000Z",
        "voteCount": 1,
        "content": "Answer - ACD"
      },
      {
        "date": "2023-07-02T20:42:00.000Z",
        "voteCount": 1,
        "content": "ACD easy"
      },
      {
        "date": "2023-06-21T20:59:00.000Z",
        "voteCount": 2,
        "content": "ACD seems like the only technically achievable solution. B and E appear to be completely wrong and for F - I am not sure whether Cognito will do the job but for sure it would be extremely hard to implement that way."
      },
      {
        "date": "2023-04-07T02:00:00.000Z",
        "voteCount": 4,
        "content": "Option E is not the most appropriate choice because it suggests enabling AWS Control Tower in all accounts to manage routing between accounts. However, AWS Control Tower is not primarily designed for managing routing between accounts; it is intended to set up and govern a secure, multi-account AWS environment. The transit gateways and VPC attachments in Option C are better suited for managing routing and connectivity between accounts."
      },
      {
        "date": "2023-03-26T23:01:00.000Z",
        "voteCount": 1,
        "content": "ACD are the best choice"
      },
      {
        "date": "2023-02-14T16:39:00.000Z",
        "voteCount": 3,
        "content": "By Elimination Rule"
      },
      {
        "date": "2023-01-15T17:37:00.000Z",
        "voteCount": 3,
        "content": "ACD are correct."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/amazon/view/95464-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size.<br><br>The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key.<br><br>What should a solutions architect do to reduce costs with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops instances based on me tag, day, and time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates, instances based on the lag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that restores the instances from their last backup based on the tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag. day, and time."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T10:23:00.000Z",
        "voteCount": 10,
        "content": "The correct answer is B. Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort.\n\nThis approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.\n\nOption A would require the instances to be stopped and started once a day, which could result in instances being stopped while they are in use or not being stopped when they are not in use.\n\nOption C would terminate instances during non-business hours and restore them again in the morning, which could lead to data loss or longer start up times.\n\nOption D would terminate or restore instances every hour, which could lead to unnecessary costs as well as data loss or longer start up times."
      },
      {
        "date": "2023-02-05T08:39:00.000Z",
        "voteCount": 7,
        "content": "this is easy. I wish I'll have several of this in the exam."
      },
      {
        "date": "2024-10-10T00:10:00.000Z",
        "voteCount": 1,
        "content": "Stopping instances rather than terminating them ensures that the environment's state can be quickly restored the next day without needing to manage backups or restorations, making it operationally efficient."
      },
      {
        "date": "2024-09-02T00:30:00.000Z",
        "voteCount": 1,
        "content": "B. Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag."
      },
      {
        "date": "2024-01-27T23:48:00.000Z",
        "voteCount": 1,
        "content": "Voted B, but C seems to be more cost effective. Any idea to why it wouldn't work?"
      },
      {
        "date": "2024-02-10T18:48:00.000Z",
        "voteCount": 1,
        "content": "C will terminate the instance which may potentially the work on the disk"
      },
      {
        "date": "2023-12-22T18:00:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-02T20:46:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-21T21:24:00.000Z",
        "voteCount": 1,
        "content": "A cannot complete the requirement since it runs once a day and we need to stop the non-prod instances in the eveninig and start them in the morning. A would potentially work if we set up the rule to run every hour and then determine the appropriate action based on the time of the day. C and D are nonsense to me"
      },
      {
        "date": "2023-05-18T00:48:00.000Z",
        "voteCount": 1,
        "content": "Can anyone explain why B has less operational effort than A\uff1f"
      },
      {
        "date": "2023-06-06T03:44:00.000Z",
        "voteCount": 1,
        "content": "cuz we have to schedule Eventbridge to run twice a day [STOP trigger and START trigger]....Option A mentions about \"ONCE\" which could only be either stop or start so option B is most appropiate"
      },
      {
        "date": "2023-04-09T05:31:00.000Z",
        "voteCount": 4,
        "content": "B is correct\nThe keyword here is whether you terminate or stop the instance. ofc you don't want to terminate. stop is enough and company don't pay when the instance is in stop state."
      },
      {
        "date": "2023-03-26T23:04:00.000Z",
        "voteCount": 2,
        "content": "B is the easy choice"
      },
      {
        "date": "2023-01-15T17:41:00.000Z",
        "voteCount": 5,
        "content": "B is correct. Stop the instance that preserver all data. \nC: is incorrect because it terminate instance that will loss data"
      },
      {
        "date": "2023-05-17T14:17:00.000Z",
        "voteCount": 2,
        "content": "with the addition to the fact that to recreate those DBs from scratch would take a long time."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/amazon/view/95465-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the same production account.<br><br>The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The premium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various Regions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate that the Lambda function is never invoked.<br><br>What could be the cause of the error messages for these customers?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Lambda function reached its concurrency limit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Lambda function its Region limit for concurrency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe company reached its API Gateway account limit for calls per second.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe company reached its API Gateway default per-method limit for calls per second."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T01:20:00.000Z",
        "voteCount": 12,
        "content": "API Gateway has a limit of 10k requests per second, per account, per region \nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html"
      },
      {
        "date": "2023-01-15T10:27:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is C. The company reached its API Gateway account limit for calls per second. This is because Amazon API Gateway has a default account-level limit of 10,000 requests per second (RPS) and a default per-method limit of 5,000 RPS. If the company's premium tier customers are making more than 10,000 requests per second in total across all API methods and regions, they would be receiving the error message of 429 Too Many Requests. This indicates that the API Gateway account is reaching its capacity limit, and the Lambda function is not being invoked because API Gateway is blocking the requests before they reach the Lambda function. \n\nThe other choices are not correct because the Lambda function's concurrency limit and region limit for concurrency would not affect the API Gateway's request rate limit, and the API Gateway's default per-method limit is 5,000 RPS which is less than the premium tier's 3,000 calls per second."
      },
      {
        "date": "2023-01-15T10:28:00.000Z",
        "voteCount": 3,
        "content": "Option A is incorrect because the error message is not related to the Lambda function reaching its concurrency limit.\n\nOption B is incorrect because the error message is not related to the Lambda function reaching its region limit for concurrency.\n\nOption D is incorrect because the error message is not related to the company reaching its API Gateway default per-method limit for calls per second, but it's related to the account level limit."
      },
      {
        "date": "2024-09-02T00:31:00.000Z",
        "voteCount": 1,
        "content": "C. The company reached its API Gateway account limit for calls per second."
      },
      {
        "date": "2024-08-01T11:42:00.000Z",
        "voteCount": 1,
        "content": "I'm going to argue the problem is that the source of the errors is Lambda reaching it's regional concurency limit.\n\nBy default, Lambda has a regional limit of 1000 concurrent invocations. The premium tier allows 3000 requests/s. Depending on the number of premium customers and the average duration of a call it may be that the concurency limit is reached or not. We don't know for sure, but it is certainly plausible. If Lambda hits the concurrency limit, it also returns 429. So how do we know where the error is coming from?\n\nThe key is in what exactly fails: \"multiple API methods during peak usage hours\". Notice it is not ALL API calls. If the gateway was rate limiting then we would see blocks of random requests being denied until the rate bucket empties to allow new ones. But if the Lambda concurrency is hit then it means no new execution environments of Lambda are created, but the ones that exist keep processing. So the behaviour is that some functions will continue to operate, while others will start thorttling with 429. This behaviour better matches with \"multiple API methods\" failing."
      },
      {
        "date": "2024-08-30T03:41:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-05-04T08:24:00.000Z",
        "voteCount": 1,
        "content": "C correct. This is Gateway API response"
      },
      {
        "date": "2024-04-12T07:43:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-12-22T18:13:00.000Z",
        "voteCount": 3,
        "content": "429 is API Gateway API throttle default limit."
      },
      {
        "date": "2023-07-02T20:48:00.000Z",
        "voteCount": 1,
        "content": "C of course"
      },
      {
        "date": "2023-04-09T05:34:00.000Z",
        "voteCount": 3,
        "content": "C\n429 error indicates that API calls per second was exceeded ... it's not a Lambda issue"
      },
      {
        "date": "2023-03-26T23:05:00.000Z",
        "voteCount": 1,
        "content": "Company reached its limit"
      },
      {
        "date": "2023-01-30T16:46:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-01-15T17:43:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/amazon/view/95529-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.<br><br>The company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the web application behind a Network Load Balancer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Application Load Balancer in front of the security tool instances",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a transit gateway to facilitate communication between VPCs."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 77,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 72,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T07:55:00.000Z",
        "voteCount": 21,
        "content": "Option B, deploying the web application behind a Network Load Balancer, is not relevant to integrating the third-party security tool with AWS technology.\n\nOption C, deploying an Application Load Balancer in front of the security tool instances, is not necessary because a Gateway Load Balancer is already being used to redirect traffic to the security tool.\n\nOption E, provisioning a transit gateway to facilitate communication between VPCs, is not relevant to integrating the third-party security tool with AWS technology or inspecting packets in and out of the VPC.\n\nIn summary, options A and D are the best choices because address the specific requirements stated in the scenario while options B, C and E do not."
      },
      {
        "date": "2024-04-28T17:48:00.000Z",
        "voteCount": 1,
        "content": "DE is correct, the question clearly mention which combination\n- GWLB and provision transit gateway is solution"
      },
      {
        "date": "2023-05-22T09:16:00.000Z",
        "voteCount": 2,
        "content": "Correct for GLB---&gt; https://www.youtube.com/watch?v=-j2smz_VCH4"
      },
      {
        "date": "2023-05-18T14:21:00.000Z",
        "voteCount": 19,
        "content": "Based on the scenario in question, the requirement is that the security tool will run in an auto scaling group in a dedicated VPC this cannot be changed. This will break Option A. If we look at the usage for the Gateway Load Balancer which is the key for the solution where application cannot have performance hits if you are inspecting the traffic, so you need to TAP the traffic to move into another third-party tool. In the references you will find below the transit gateway will facilitate the VPC-to-VPC communication and as you can see, the security appliances VPC is a segregated from the application VPC, so again, option A is NOT valid.\nhttps://catalog.workshops.aws/networking/en-US/gwlb \nhttps://www.fortinet.com/blog/business-and-technology/highly-scalable-fortigate-next-generation-firewall-security-on-aws-gateway-load-balancer-service"
      },
      {
        "date": "2024-10-12T10:21:00.000Z",
        "voteCount": 1,
        "content": "In AD, it mention that will be deployed in the existing VPC. however, in DE, it does not mention that the security tool is deployed in another VPC. It only mention transit gateway between VPCs."
      },
      {
        "date": "2024-10-05T02:20:00.000Z",
        "voteCount": 1,
        "content": "it says it needs to inspect traffic coming in and out of THE VPC not multiple VPC's. This statement disqualifies E"
      },
      {
        "date": "2024-09-02T01:33:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC\nD. Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool"
      },
      {
        "date": "2024-08-31T13:42:00.000Z",
        "voteCount": 1,
        "content": "D and E make the most sense if your architecture involves multiple VPCs where traffic needs to be centrally managed and inspected. This combination addresses both the direct need for packet inspection and the broader network management requirements.\n\nA and E could be considered if the application and security tool deployment are straightforward and confined to a single or connected VPCs. However, managing traffic flow effectively to the security tools might require additional configuration that can complicate the setup.\n\nSince there is only once VPC, AD"
      },
      {
        "date": "2024-08-06T04:28:00.000Z",
        "voteCount": 2,
        "content": "It has to be AD.\n\nI've taken the Udemy Course from stephane maarek and his course described this kind of scenario"
      },
      {
        "date": "2024-08-03T21:11:00.000Z",
        "voteCount": 2,
        "content": "DE cannot be the answer. The combination doesn't describe how to deploy the security tools on the cloud."
      },
      {
        "date": "2024-06-12T09:14:00.000Z",
        "voteCount": 1,
        "content": "DE is the answer. Transit -&gt; GWLB -&gt; Inspection tool"
      },
      {
        "date": "2024-08-30T03:42:00.000Z",
        "voteCount": 1,
        "content": "just AD"
      },
      {
        "date": "2024-06-06T22:38:00.000Z",
        "voteCount": 1,
        "content": "AD is correct as the requirement is to use the Security tool to inspect traffic coming in and out of the VPC. So, you need to deploy the security tool on EC2 instances and provision a Gateway loadbalancer to load balance the traffic. With a GLB, you can deploy, manage, and scale virtual appliances, such as intrusion detection and prevention, firewalls, and deep packet inspection systems. It creates a single entry and exit point for all appliance traffic and scales your virtual appliances with demand. You can also exchange traffic across virtual private cloud (VPC) boundaries."
      },
      {
        "date": "2024-05-02T09:38:00.000Z",
        "voteCount": 1,
        "content": "AD for me"
      },
      {
        "date": "2024-04-26T07:12:00.000Z",
        "voteCount": 2,
        "content": "DE without doubts guys. \nGLB is just for this reason. Deploy the security tool into another ASG will only increase the cost and it's crazy, the performance isn't the same as the GLB (which operates at Lv. 3 of networking)."
      },
      {
        "date": "2024-04-24T23:11:00.000Z",
        "voteCount": 2,
        "content": "Based on MRamos comment"
      },
      {
        "date": "2024-03-25T19:44:00.000Z",
        "voteCount": 2,
        "content": "Not A. A does not make sense for D"
      },
      {
        "date": "2024-03-17T08:22:00.000Z",
        "voteCount": 3,
        "content": "AD - ec2 + asg + gateway load balancer"
      },
      {
        "date": "2024-03-07T17:22:00.000Z",
        "voteCount": 2,
        "content": "I answered AD and searched through these comments' links to seek to understand.\nFirst, the most convincing case is that as @rbm2023 answered, it is not pattern to put security tool in the existing VPC. The financial company in the question is also looking to only have their application migrate into a dedicated VPC. \n\nSecond, solution A sounds good and according to this link below, you can use ASG with GWLB. I think key is the fine print of Customer wanting their own dedicated VPC and the pattern of using TWG in front. (However, it is possible to do without)\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/"
      },
      {
        "date": "2024-02-26T06:05:00.000Z",
        "voteCount": 2,
        "content": "D and E \nRead the question, it says dedicated VPC"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/amazon/view/95531-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.<br><br>The company needs to design a new data analysis solution that can deliver faster and optimize costs.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshlft for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T20:09:00.000Z",
        "voteCount": 11,
        "content": "IOT Core communication supports protocols MQTT, HTTPS, MQTT over WSS, and LoRaWAN (but not FTP/SFTP ) so C should be wrong.\n\nRules Engine: AWS IoT Core provides a rules engine that allows users to define and execute business logic on the data generated by their IoT devices. This enables users to automate actions such as sending notifications, triggering alarms, or updating device settings based on real-time data.\n\nIntegration with other AWS Services: AWS IoT Core integrates with other AWS services such as AWS Lambda, AWS Kinesis, and AWS S3, allowing users to easily process and store their IoT data, as well as build complex IoT applications using a range of AWS services."
      },
      {
        "date": "2023-01-16T07:31:00.000Z",
        "voteCount": 5,
        "content": "A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.\n\nThis solution meets the requirement of faster analysis and cost optimization by using AWS IoT Core to collect data from the IoT sensors in real-time and then using AWS Glue and Amazon Athena for efficient data analysis.\n\nOption B and D do not optimize the cost of data analysis as they involve use of expensive services like AWS Fargate and Snowball Edge respectively. Option C does not make use of real-time data collection and may not be optimal for faster analysis."
      },
      {
        "date": "2024-09-02T02:28:00.000Z",
        "voteCount": 1,
        "content": "A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis."
      },
      {
        "date": "2024-03-17T08:31:00.000Z",
        "voteCount": 1,
        "content": "A, IoT Core"
      },
      {
        "date": "2023-12-23T12:38:00.000Z",
        "voteCount": 1,
        "content": "Option A is best(fastest) and most cost effective."
      },
      {
        "date": "2023-11-30T03:54:00.000Z",
        "voteCount": 1,
        "content": "Everytime the exam shows IOT sensor, think  of IOT Core and aws glue"
      },
      {
        "date": "2023-10-26T14:02:00.000Z",
        "voteCount": 4,
        "content": "How can A satisfy this requirement? \"relational database for analysis\"\nThe only option is B with relational database for analysis."
      },
      {
        "date": "2024-08-31T18:48:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2023-11-23T08:55:00.000Z",
        "voteCount": 1,
        "content": "\"The company needs to design a new data analysis solution that can deliver faster and optimize costs.\""
      },
      {
        "date": "2023-09-07T05:47:00.000Z",
        "voteCount": 1,
        "content": "Option A: AWS IoT Core + Lambda\nSpeed: Near real-time data collection and analysis.\nFlexibility: Ability to adapt to different data formats from multiple vendors.\nOption C: AWS Transfer for SFTP\nSpeed: There may be network delays and waiting for all data to be sent.\nDevelopment needs: The sensor code needs to be updated, which increases the development workload.\nAll things considered, option A is better than option C in terms of speed and flexibility, and is especially suitable for real-time or near-real-time requirements."
      },
      {
        "date": "2023-07-03T09:19:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-05-20T05:30:00.000Z",
        "voteCount": 2,
        "content": "I go for A on the elimination principle although neither of the answers does not seem to fully cover the requirements. I am not sure what is the \"vendors' proprietary formats\" and not sure why they assume it's csv. Also there is a requirement to load the data in relational database which excludes B. For A we need to assume that S3 covers this requirement."
      },
      {
        "date": "2023-04-09T06:17:00.000Z",
        "voteCount": 4,
        "content": "A is correct, even though it's not clear from the question if the sensors protocol is MQTT or HTTPS.\nbut i can't find other suitable answer so i guess A is the correct one."
      },
      {
        "date": "2023-03-26T23:19:00.000Z",
        "voteCount": 2,
        "content": "Connect the IoT sensors to AWS IoT Core."
      },
      {
        "date": "2023-02-14T16:50:00.000Z",
        "voteCount": 4,
        "content": "A by Elimination rule"
      },
      {
        "date": "2023-02-05T09:28:00.000Z",
        "voteCount": 4,
        "content": "I m not convinced about A. It kind of requires changes in the sensors to be compatible with AWS IoT Core."
      },
      {
        "date": "2023-04-20T13:10:00.000Z",
        "voteCount": 1,
        "content": "I agree with you here. We don't know if IoT Core supports it, so moving the application to AWS Fargate will guarantee compatibility."
      },
      {
        "date": "2023-01-29T16:17:00.000Z",
        "voteCount": 4,
        "content": "i'll go for A"
      },
      {
        "date": "2023-01-16T06:16:00.000Z",
        "voteCount": 4,
        "content": "A is correct.\nB: it is appliance, impossible to install on Fargate\nC: device not use FTP protocol\nD: snowball is not real time"
      },
      {
        "date": "2023-02-05T09:27:00.000Z",
        "voteCount": 1,
        "content": "In B, we don't try to port appliances to Fargate, but only the app that parses the informtion from the appliances into JSON.\nI am doubting about A. Unless you would reprogrm the sensors they would not know how to connect to AWS IoT Core."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/amazon/view/95532-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.<br><br>The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises data center.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an internet gateway. Attach the internet gateway to subnets. Allow internet traffic through the gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the transit gateway with other accounts. Attach VPCs to the transit gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision VPC peering as necessary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDF",
        "count": 27,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T07:34:00.000Z",
        "voteCount": 11,
        "content": "B and D and F are correct.\nB: Creating a Direct Connect gateway and a transit gateway in the central network account will allow the company to connect its on-premises data center to the resources in AWS.\nD: Sharing the transit gateway with other accounts will allow the company to communicate with all the VPCs in multiple accounts.\nF: Provisioning only private subnets and opening necessary routes on the transit gateway and customer gateway will allow the company to route its cloud resources to the internet through its on-premises data center.\n\nA is incorrect because it would be redundant to use both a Direct Connect gateway and a transit gateway.\nC is incorrect because it is not necessary to provision an internet gateway, since the company wants to route traffic through their on-premises data center.\nE is incorrect because VPC peering may not be necessary if the company is using a transit gateway to connect all the VPCs."
      },
      {
        "date": "2024-09-02T02:29:00.000Z",
        "voteCount": 1,
        "content": "B. Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF.\nD. Share the transit gateway with other accounts. Attach VPCs to the transit gateway.\nF. Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center."
      },
      {
        "date": "2023-12-23T12:54:00.000Z",
        "voteCount": 1,
        "content": "BDF is most scalable solution."
      },
      {
        "date": "2023-12-04T18:53:00.000Z",
        "voteCount": 1,
        "content": "Answer BDF\nDGW and TGW\nShare TGW and configure VPC attachments to TGW\nOpen necessary routes for traffic routing via NAT gw on the on-prem dc"
      },
      {
        "date": "2023-08-18T15:41:00.000Z",
        "voteCount": 1,
        "content": "Very logical"
      },
      {
        "date": "2023-07-03T09:21:00.000Z",
        "voteCount": 1,
        "content": "BDF for sure"
      },
      {
        "date": "2023-06-22T03:25:00.000Z",
        "voteCount": 2,
        "content": "Standard scenario. You connect the Direct Connect Gateway to the Transit Gateway, attach the VPCs, and route the traffic through the On-premise devices"
      },
      {
        "date": "2023-05-23T09:16:00.000Z",
        "voteCount": 1,
        "content": "BDF is the right ans"
      },
      {
        "date": "2023-03-26T23:21:00.000Z",
        "voteCount": 1,
        "content": "BDF is the right combo"
      },
      {
        "date": "2023-03-07T20:56:00.000Z",
        "voteCount": 4,
        "content": "VPC Peering does not work as there are hundreds of VPCs, transit gateway is easy to configure and practical.\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html"
      },
      {
        "date": "2023-01-30T16:54:00.000Z",
        "voteCount": 4,
        "content": "B D and F"
      },
      {
        "date": "2023-01-29T16:23:00.000Z",
        "voteCount": 3,
        "content": "I agree with BD&amp;F"
      },
      {
        "date": "2023-01-16T06:18:00.000Z",
        "voteCount": 2,
        "content": "BDF are correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/amazon/view/95533-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously.<br><br>A solutions architect needs to enforce the new process in the most secure way possible.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T07:51:00.000Z",
        "voteCount": 8,
        "content": "A and D are the correct answer.\nA: By ensuring all AWS accounts are part of an organization in AWS Organizations, it allows for centralized management and control of the accounts. This can help enforce the new purchasing process by giving a dedicated team the ability to manage and enforce policies across all accounts.\nD: By creating an SCP (Service Control Policy) that denies access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions, it enforces the new centralized purchasing process. Attaching the SCP to each OU (organizational unit) within the organization ensures that all business units are adhering to the new process.\n\nB and C are not the correct answer, because AWS Config and IAM policies are used for monitoring and managing access to resources in an account, respectively. They don't enforce the new process for purchasing reserved instances.\nE is not the correct answer as this is not related to the new process for purchasing reserved instances."
      },
      {
        "date": "2024-09-02T02:31:00.000Z",
        "voteCount": 1,
        "content": "A. Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.\nD. Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization."
      },
      {
        "date": "2023-12-23T13:32:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2023-12-17T14:44:00.000Z",
        "voteCount": 1,
        "content": "A+D achieve the goal of denying access to purchase and to modify Reserved Instances to all OUs. The dedicated team can still perform these actions if they are part of the management account.\n\nC, E don't actually do anything, as in, actually control anything at all. B will trigger on the wrong thing to be alarmed about, if triggering an alarm was the goal."
      },
      {
        "date": "2023-09-05T07:02:00.000Z",
        "voteCount": 1,
        "content": "A and D : is the best way"
      },
      {
        "date": "2023-07-03T09:25:00.000Z",
        "voteCount": 1,
        "content": "AD. A so can use SCP"
      },
      {
        "date": "2023-05-20T05:51:00.000Z",
        "voteCount": 3,
        "content": "I was not confident about enabling all features because I was messing \"features\" and \"services\". Yes - you need to enable all features, otherwise you cannot control the accounts in your organization. The rest is common sense"
      },
      {
        "date": "2023-03-26T23:24:00.000Z",
        "voteCount": 3,
        "content": "AD easy"
      },
      {
        "date": "2023-01-29T16:26:00.000Z",
        "voteCount": 4,
        "content": "A and D"
      },
      {
        "date": "2023-01-16T06:19:00.000Z",
        "voteCount": 2,
        "content": "AD are correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/amazon/view/95534-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.<br><br>A recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ElastiCache for Memcached in front of the database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ElastiCache for Redis in front of the database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse RDS Proxy in front of the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora MySQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora Replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS for MySQL read replica"
    ],
    "answer": "CDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDE",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-08T08:55:00.000Z",
        "voteCount": 12,
        "content": "CDE. RDS Failover typically takes 60-120 seconds, while Aurora failover completes within 30 seconds. ElastiCache is for reducing latency, not for failover."
      },
      {
        "date": "2023-04-09T07:11:00.000Z",
        "voteCount": 9,
        "content": "RDS Proxy with Aurora are the best combination for less than \"20 sec\" failover time...\nAccording to this article RDS Proxy can reduce the failover time of Aurora by 79% while it can reduce RDS failover time by only 32%: \nhttps://aws.amazon.com/blogs/database/improving-application-availability-with-amazon-rds-proxy/"
      },
      {
        "date": "2024-09-02T02:32:00.000Z",
        "voteCount": 1,
        "content": "C. Use RDS Proxy in front of the database.\nD. Migrate the database to Amazon Aurora MySQL.\nE. Create an Amazon Aurora Replica."
      },
      {
        "date": "2024-03-21T10:41:00.000Z",
        "voteCount": 2,
        "content": "A and B don't contribute to reducing response time in failover scenarios.\nD is required for faster failover.\nE is required to support D.\nF doesn't reduce failover time.\nC, finally, is the remaining option. It doesn't hurt, and can contribute to faster failover, though it is not the most important factor here - the switch to Aurora with an Aurora read replica is."
      },
      {
        "date": "2024-03-11T18:15:00.000Z",
        "voteCount": 1,
        "content": "Anyone can share why choosing E? I know we have to choose 3. isn't it weird? Aurora already has replicas natively. Why creating another one?"
      },
      {
        "date": "2023-12-23T13:35:00.000Z",
        "voteCount": 1,
        "content": "Option C, D and E"
      },
      {
        "date": "2023-12-17T15:49:00.000Z",
        "voteCount": 2,
        "content": "C+D+E provides the 'fastest' failover with the options available:\n\n- Aurora MySQL is Multi-AZ by design: During failover it will promote a Replica to primary or create a Primary instance\n- Creating a Replica provides the option to have something to failover to\n- Using an RDS Proxy further reduces failover time and provides 'transparent' failovers as well (It manages DNS changes)\n\nThe argument against Caching (options A or B) is that it doesn't accelerate failing over to a different instance. Cache misses and write operations will produce exceptions because there is no instance to query.  Moreover, there is no information in the question to choose between either caching option, i.e. Both options can be created starting from an Aurora DB Cluster settings - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/creating-elasticache-cluster-with-RDS-settings.html"
      },
      {
        "date": "2023-07-03T09:28:00.000Z",
        "voteCount": 2,
        "content": "CDE, agree with other comments"
      },
      {
        "date": "2023-04-20T13:35:00.000Z",
        "voteCount": 3,
        "content": "The trick seems to be that the RDS proxy handles DNS updates quickly. While if you don't use it, you are at the mercy of the host to update its DNS cache."
      },
      {
        "date": "2023-03-26T23:25:00.000Z",
        "voteCount": 1,
        "content": "CDE is the best choice"
      },
      {
        "date": "2023-03-20T18:59:00.000Z",
        "voteCount": 3,
        "content": "CDE. I would have said F, but the question asks for a combination of steps, so its looking for the Aurora replica and not the MySQL RDS replica"
      },
      {
        "date": "2023-04-16T05:32:00.000Z",
        "voteCount": 1,
        "content": "I agree with your logic."
      },
      {
        "date": "2023-03-08T15:57:00.000Z",
        "voteCount": 3,
        "content": "C for sure as connection pooling helps quick re connect. There is no preference for A or B cache solution based on the question. So, A,B are eliminated. so three correct options should be in others. If you choose Aurora only, three answers will be met :-) C,D,E"
      },
      {
        "date": "2023-01-30T16:57:00.000Z",
        "voteCount": 2,
        "content": "C D and  E"
      },
      {
        "date": "2023-02-16T23:01:00.000Z",
        "voteCount": 2,
        "content": "A and B are incorrect options because Amazon ElastiCache is a caching service, not a failover solution. F is also incorrect because RDS read replicas are asynchronous, which means that there may be a delay in replication, leading to the potential loss of data. Additionally, creating a read replica does not improve the failover time."
      },
      {
        "date": "2023-01-19T21:36:00.000Z",
        "voteCount": 5,
        "content": "RDS read replica auto failover takes approx 35 seconds hence, BCF does not satisfy under 20 seconds failover requirement.\nhttps://aws.amazon.com/rds/features/multi-az/#:~:text=Amazon%20RDS%20Multi%2DAZ%20with%20two%20readable%20standbys,-Automatically%20fail%20over&amp;text=Automatically%20failover%20in%20typically%20under,and%20with%20no%20manual%20intervention."
      },
      {
        "date": "2023-01-29T16:34:00.000Z",
        "voteCount": 2,
        "content": "thanks for the information about RDS read replica"
      },
      {
        "date": "2023-01-17T10:21:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D, E and C:\n\nMigrate the database to Amazon Aurora MySQL.\n- Create an Amazon Aurora Replica.\n- Use RDS Proxy in front of the database.\n- These options are correct because they address the requirement of reducing the failover time to less than 20 seconds.\n\nMigrating to Amazon Aurora MySQL and creating an Aurora replica can reduce the failover time to less than 20 seconds. Aurora has a built-in, fault-tolerant storage system that can automatically detect and repair failures. Additionally, Aurora has a feature called \"Aurora Global Database\" which allows you to create read-only replicas across multiple AWS regions which can further help to reduce the failover time.\n\nCreating an Aurora replica can also help to reduce the failover time as it can take over as the primary DB instance in case of a failure.\n\nUsing RDS proxy can also help to reduce the failover time as it can route the queries to the healthy DB instance, it also helps to balance the load across multiple DB instances."
      },
      {
        "date": "2023-01-17T10:21:00.000Z",
        "voteCount": 1,
        "content": "Option A and B, Use Amazon ElastiCache for Memcached and Redis in front of the database, are not correct as ElastiCache is a caching service, it doesn't provide a high availability solution for the underlying database.\n\nOption F, Create an RDS for MySQL read replica, is not correct as a read replica can only be used to offload read traffic from the primary instance, it doesn't provide a high availability solution for the underlying database."
      },
      {
        "date": "2023-01-16T07:56:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is B, C and F.\n\nUsing Amazon ElastiCache for Redis in front of the database (Option B) will help to reduce the failover time by caching the frequently-used data, so that it can be quickly served from the cache rather than having to be retrieved from the database during a failover.\n\nUsing RDS Proxy in front of the database (Option C) will help to reduce the failover time by managing the connections to the RDS DB instance, so that it can quickly route traffic to the new primary instance during a failover.\n\nCreating an RDS for MySQL read replica (Option F) will help to reduce the failover time by having a read-only copy of the database running in parallel with the primary instance, so that it can take over as the primary instance in the event of a failover.\n\nOption A and D are not relevant in this case as the question is asking specifically about reducing failover time for an RDS for MySQL database."
      },
      {
        "date": "2023-02-13T16:11:00.000Z",
        "voteCount": 2,
        "content": "C,  D and E Correct"
      },
      {
        "date": "2023-01-16T06:26:00.000Z",
        "voteCount": 3,
        "content": "CDE are correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/amazon/view/95538-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account.<br><br>What is the MOST secure way to allow org1 to access resources in org2?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role\u2019s Amazon Resource Name (ARN) when requesting access to perform the required tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role\u2019s Amazon Resource Name (ARN), including the external ID in the IAM role\u2019s trust policy, when requesting access to perform the required tasks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-09T07:16:00.000Z",
        "voteCount": 5,
        "content": "D\nWell.. \"external ID\" is the keyword that you should look for in such scenario."
      },
      {
        "date": "2024-09-02T02:33:00.000Z",
        "voteCount": 1,
        "content": "D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role\u2019s Amazon Resource Name (ARN), including the external ID in the IAM role\u2019s trust policy, when requesting access to perform the required tasks."
      },
      {
        "date": "2023-12-23T13:38:00.000Z",
        "voteCount": 1,
        "content": "Option D is most secure."
      },
      {
        "date": "2023-12-17T15:58:00.000Z",
        "voteCount": 2,
        "content": "Sharing credentials will always be a bad idea. In comparison to C and D, options A and B are insecure.\n\nThe reason D is the most secure option compared to C is because it addresses the confused deputy problem - https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html"
      },
      {
        "date": "2023-07-03T09:30:00.000Z",
        "voteCount": 3,
        "content": "it's D, but private link would be a better choice"
      },
      {
        "date": "2023-03-26T23:27:00.000Z",
        "voteCount": 2,
        "content": "With the external ID."
      },
      {
        "date": "2023-03-08T16:15:00.000Z",
        "voteCount": 3,
        "content": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"AWS\": \"Example Corp's AWS Account ID\"\n    },\n    \"Action\": \"sts:AssumeRole\",\n    \"Condition\": {\n      \"StringEquals\": {\n        \"sts:ExternalId\": \"1122334455-The ID that only Third party and customer knows\"\n      }\n    }\n  }\n}"
      },
      {
        "date": "2023-02-05T10:46:00.000Z",
        "voteCount": 1,
        "content": "Easy. The external ID is for sure the winner."
      },
      {
        "date": "2023-01-29T16:39:00.000Z",
        "voteCount": 2,
        "content": "D seems the correct answer"
      },
      {
        "date": "2023-01-25T23:36:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html"
      },
      {
        "date": "2023-01-16T07:58:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. This is the most secure way to allow org1 to access resources in org2 because it allows for least privilege security access. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role\u2019s Amazon Resource Name (ARN) and include the external ID in the IAM role\u2019s trust policy when requesting access to perform the required tasks. This ensures that the partner company can only access the resources that it needs and only from the specific customer account.\n\nOption A and B both involve providing the partner company with credentials, which can be easily compromised and could lead to a security breach. Option C also provides the partner company with an IAM role, but it doesn't have any restrictions on when and where the partner company can access the resources in customer account, it could be a security risk."
      },
      {
        "date": "2023-01-16T06:30:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/amazon/view/95540-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map.<br><br>The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.<br><br>The company needs the ability to allocate resources cost-effectively based on the number of running containers.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-09T07:24:00.000Z",
        "voteCount": 15,
        "content": "D is the correct answer, When you use the APIs to create a service or run a task, you must set enableECSManagedTags to true for run-task and create-service. (see link below)\nB doesn't make sense because EKS is more for complex orchestrated microservices apps, i don't think it needed in such scenario\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html"
      },
      {
        "date": "2023-11-17T03:51:00.000Z",
        "voteCount": 1,
        "content": "Stepped through that same thought process"
      },
      {
        "date": "2023-03-08T20:06:00.000Z",
        "voteCount": 6,
        "content": "EKS with Fargate is a more complex platform than ECS with Fargate. Kubernetes has a steeper learning curve than ECS, and requires more expertise to manage. ECS with Fargate is designed to be simple and easy to use, making it a good choice for organizations that want to quickly deploy containerized applications without having to manage the complexity of Kubernetes."
      },
      {
        "date": "2024-09-02T02:34:00.000Z",
        "voteCount": 1,
        "content": "D. Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task."
      },
      {
        "date": "2024-01-20T06:08:00.000Z",
        "voteCount": 3,
        "content": "A and B = between EKS and ECS if K8s is not required I go for ECS\nC = between EC2 and Fargate if nothing points you clearly to Ec2 i would go for Fargate (less overhead, could cost less)\nD = correct"
      },
      {
        "date": "2024-01-06T07:43:00.000Z",
        "voteCount": 1,
        "content": "D is a trap, even if it's tempting, but '--tags ' is not a valid option for tagging ecs tasks/services.\nB is the right answer."
      },
      {
        "date": "2024-08-31T18:55:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-01-17T08:15:00.000Z",
        "voteCount": 1,
        "content": "--tags is valid\nhttps://awscli.amazonaws.com/v2/documentation/api/latest/reference/ecs/run-task.html"
      },
      {
        "date": "2024-01-17T08:19:00.000Z",
        "voteCount": 1,
        "content": "but --enable-ecs-managed-tags is the right option instead of \"`enableECSManagedTags` to `true`\""
      },
      {
        "date": "2023-12-23T13:45:00.000Z",
        "voteCount": 3,
        "content": "D is best option with least operational overhead."
      },
      {
        "date": "2023-12-17T16:23:00.000Z",
        "voteCount": 2,
        "content": "Options A and C are more operationally complex than B and D because you will need to manage the EC2 instances and underpin the EKS cluster and the ECS service definition. And as if to make the selection easier, B and D explicitly mention using AWS Fargate in a way that works.\n\nSelecting between Options B and D boils down the interpretation of \u201ceach section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area\u201d. The only indication in the question that kind of helps is \"The third party supplies a supported Docker image from a public registry\": The custom configuration is just for processing orders in the section's area rather something in the docker image itself."
      },
      {
        "date": "2023-09-18T14:15:00.000Z",
        "voteCount": 3,
        "content": "As per the Amazon EKS documentation, the following EKS resources support tags:\n- clusters\n- managed node groups\n- Fargate profiles\n\nI think that rules out B in favour of D!\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-using-tags.html#tag-resources"
      },
      {
        "date": "2023-08-21T23:30:00.000Z",
        "voteCount": 4,
        "content": "Real-world answer - B.\nCertification answer - D."
      },
      {
        "date": "2023-07-03T09:33:00.000Z",
        "voteCount": 2,
        "content": "going with D"
      },
      {
        "date": "2023-05-18T21:21:00.000Z",
        "voteCount": 4,
        "content": "Since the question where the requirement is the least operational overhead and we are between EKS and ECS, I would go for ECS, I believe EKS has more operational overhead for deploying and for operating. Also, you would probably have to apply less steps to build this structure using ECS when comparing with EKS."
      },
      {
        "date": "2023-04-27T12:26:00.000Z",
        "voteCount": 2,
        "content": "B is correct\nAnytime you need Docker containers with a custom configuration use EKS"
      },
      {
        "date": "2023-04-16T05:28:00.000Z",
        "voteCount": 3,
        "content": "Like many have already stated, the debate is between B and D. I think B is the answer as \"each section uses its own set of DOcker Containers with a customer configuration, \" which leads me to believe that EKS orchestration is worthwhile in terms of operational overhead."
      },
      {
        "date": "2023-03-26T23:30:00.000Z",
        "voteCount": 2,
        "content": "D is easier"
      },
      {
        "date": "2023-03-17T22:15:00.000Z",
        "voteCount": 2,
        "content": "I vote for D"
      },
      {
        "date": "2023-03-09T14:32:00.000Z",
        "voteCount": 4,
        "content": "i still think is B\n\"each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.\""
      },
      {
        "date": "2023-05-05T02:27:00.000Z",
        "voteCount": 1,
        "content": "Agree and for the same reason."
      },
      {
        "date": "2023-03-09T03:48:00.000Z",
        "voteCount": 2,
        "content": "choosing D based on below tagging information\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/amazon/view/95541-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.<br><br>Which factors could cause this error? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IPv4 CIDR ranges of the two VPCs overlap\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe VPCs are not in the same Region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOne or both accounts do not have access to an Internet gateway",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOne of the VPCs was not shared through AWS Resource Access Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM role in the peer accepter account does not have the correct permissions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-07T07:38:00.000Z",
        "voteCount": 8,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-vpc-peering-error/"
      },
      {
        "date": "2024-10-01T00:59:00.000Z",
        "voteCount": 1,
        "content": "E is wrong"
      },
      {
        "date": "2024-09-02T02:35:00.000Z",
        "voteCount": 1,
        "content": "A. The IPv4 CIDR ranges of the two VPCs overlap\nE. The IAM role in the peer accepter account does not have the correct permissions"
      },
      {
        "date": "2023-12-23T13:51:00.000Z",
        "voteCount": 1,
        "content": "Option A and E"
      },
      {
        "date": "2023-11-15T09:15:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\nhttps://repost.aws/knowledge-center/cloudformation-vpc-peering-error"
      },
      {
        "date": "2023-08-19T04:19:00.000Z",
        "voteCount": 1,
        "content": "This is correct, per Appon's link"
      },
      {
        "date": "2023-07-03T09:35:00.000Z",
        "voteCount": 1,
        "content": "AE for sure"
      },
      {
        "date": "2023-05-14T07:43:00.000Z",
        "voteCount": 3,
        "content": "VPCs are not in the same Region."
      },
      {
        "date": "2023-05-14T07:47:00.000Z",
        "voteCount": 2,
        "content": "My bad, option B is incorrect."
      },
      {
        "date": "2023-03-26T23:34:00.000Z",
        "voteCount": 2,
        "content": "AE is the best choice"
      },
      {
        "date": "2023-03-08T20:35:00.000Z",
        "voteCount": 4,
        "content": "FYI, Other reasons for issue : \nIf the IAM role in the accepter account doesn't have the right permissions\n\nIf the PeerRoleArn property isn't passed correctly when you create a VPC peering connection between VPCs in different accounts\n\nIf the PeerRegion property isn't passed correctly when you're creating a VPC peering connection between VPCs in different AWS Regions"
      },
      {
        "date": "2023-01-29T16:48:00.000Z",
        "voteCount": 1,
        "content": "A and E"
      },
      {
        "date": "2023-01-16T08:40:00.000Z",
        "voteCount": 3,
        "content": "A is correct because the IPv4 CIDR ranges of the two VPCs overlap. The two VPCs have an IP range of 10.10.0.0/16 and 10.10.10.0/24, which means that they share the same 10.10.0.0 network. This causes a conflict in routing and will prevent the VPCs from being able to communicate with each other.\n\nE is correct because the IAM role in the peer accepter account does not have the correct permissions. The role must have permissions to create, modify, and delete VPC peering connections in order for the peering to be established.\n\nB, C, and D are not correct. The VPCs are in the same region, both accounts have access to an internet gateway and both VPCs are not shared through AWS Resource Access Manager."
      },
      {
        "date": "2023-06-08T18:42:00.000Z",
        "voteCount": 5,
        "content": "us-east-1 is in virginia, us-east-2 is in ohio - they are separate regions"
      },
      {
        "date": "2023-09-12T02:45:00.000Z",
        "voteCount": 7,
        "content": "stop asking to ChatGPT"
      },
      {
        "date": "2023-11-15T09:16:00.000Z",
        "voteCount": 1,
        "content": "It doesn't matter if both accounts are in the same region or not. \n&gt;&gt;&gt; The VPCs can be in different Regions (also known as an inter-Region VPC peering connection).\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
      },
      {
        "date": "2023-01-16T06:37:00.000Z",
        "voteCount": 4,
        "content": "AE is correct\nD is not correct because you cannot share VPC via RAM, subnet can"
      },
      {
        "date": "2024-03-07T18:40:00.000Z",
        "voteCount": 1,
        "content": "In this link, you can find VPC sharing being described as \"In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organization\". You can share subnets using AWS RAM. I think it is safe to conclude you can share VPCs using RAM.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html#vpc-share-prerequisites"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/amazon/view/95542-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An external audit of a company\u2019s serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task.<br><br>A solutions architect must determine which permissions each Lambda function needs.<br><br>What should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company's business requirements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company\u2019s business requirements."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T22:02:00.000Z",
        "voteCount": 13,
        "content": "Access Analyzer uses automated reasoning to analyze resource policies and detect issues such as overly permissive access or violations of organizational security policies. It works by examining the policies attached to AWS resources, such as S3 buckets, IAM roles, and KMS keys, and identifying any potential security risks or policy violations."
      },
      {
        "date": "2023-03-07T22:03:00.000Z",
        "voteCount": 7,
        "content": "fyi\nML tool - CodeGuru has two main components: CodeGuru Reviewer and CodeGuru Profiler.\n\nCodeGuru Reviewer is a code review service that uses machine learning to identify code quality issues and security vulnerabilities in your application's source code. It analyzes the code and provides recommendations for improvements based on best practices, industry standards, and AWS experience.\n\nCodeGuru Profiler is a profiling tool that uses machine learning to identify performance issues in your application code at runtime. It continuously analyzes the performance characteristics of your application code and provides recommendations for optimization."
      },
      {
        "date": "2024-09-02T02:36:00.000Z",
        "voteCount": 1,
        "content": "B. Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements."
      },
      {
        "date": "2024-01-17T08:31:00.000Z",
        "voteCount": 1,
        "content": "poor since B only works when functions are actually triggered and all the branches of the code are covered."
      },
      {
        "date": "2023-12-23T13:54:00.000Z",
        "voteCount": 1,
        "content": "Option B is obvious choice"
      },
      {
        "date": "2023-12-17T16:45:00.000Z",
        "voteCount": 1,
        "content": "When approaching questions related to access permissions, it will always help to determine who is accessing what, in this case, it is Lambda functions accessing AWS services (S3 buckets and DynamoDB table).\n\nThe choice between A,B and C,D is then based on knowing that Code Guru and Access Analyzer used an automated process to detect issues in code and to compare actual access versus permissions - least effort than C &amp; D.\n\nThat last bit is where the kicker is. The question refers to IAM execution roles with too-broad AWS IAM permissions to access AWS services and resources: You are looking for the option that tightens IAM policies rather than in AWS Lambda Function code."
      },
      {
        "date": "2023-07-03T09:36:00.000Z",
        "voteCount": 1,
        "content": "B - basic access analyzer use case"
      },
      {
        "date": "2023-06-18T13:10:00.000Z",
        "voteCount": 1,
        "content": "keyword == Access Management Access Analyzer to generate IAM"
      },
      {
        "date": "2023-06-07T10:56:00.000Z",
        "voteCount": 1,
        "content": "B definitely"
      },
      {
        "date": "2023-03-26T23:39:00.000Z",
        "voteCount": 1,
        "content": "B - Identity and Access Management Access Analyzer"
      },
      {
        "date": "2023-01-29T16:51:00.000Z",
        "voteCount": 1,
        "content": "Identity and Access Management Access Analyzer"
      },
      {
        "date": "2023-01-16T08:48:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B. Turn on AWS CloudTrail logging for the AWS account, and use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.\n\nThis is the least amount of effort as it makes use of AWS services that can automatically analyze the CloudTrail logs, generate the IAM policies, and provide a report for the review process. \nOption A and D both involve additional steps such as running scripts or using Amazon EMR, which would take more effort to set up and maintain. \nOption C is similar to option A and D but doesn't use any AWS services to help with the process."
      },
      {
        "date": "2023-01-16T06:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/amazon/view/95543-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect must analyze a company\u2019s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.<br><br>The solutions architect must analyze the environment and take action based on the findings.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T22:15:00.000Z",
        "voteCount": 16,
        "content": "AWS Compute Optimize helps analyze the usage patterns of AWS resources, such as EC2 instances and Auto Scaling groups, and makes recommendations on how to optimize them for performance and cost using machine learning algorithms.It then generates recommendations that can be used to adjust instance types, purchase options, and other parameters.It provides two types of recommendations:\n    Recommended instance types - recommends instance types that are more cost-effective and better suited to the workload requirements.\n    Recommended purchase options - recommends purchasing options, such as Reserved Instances or Savings Plans, that can help customers save money on their compute resources."
      },
      {
        "date": "2023-03-07T22:33:00.000Z",
        "voteCount": 4,
        "content": "A is wrong. \nOpsCenter, a capability of AWS Systems Manager, provides a central location where operations engineers and IT professionals can manage operational work items (OpsItems) related to AWS resources. An OpsItem is any operational issue or interruption that needs investigation and remediation. Using OpsCenter, you can view contextual investigation data about each OpsItem, including related OpsItems and related resources. You can also run Systems Manager Automation runbooks to resolve OpsItems."
      },
      {
        "date": "2023-03-07T22:22:00.000Z",
        "voteCount": 2,
        "content": "fyi Pricing looks cheap too - https://aws.amazon.com/compute-optimizer/pricing/"
      },
      {
        "date": "2024-09-02T02:37:00.000Z",
        "voteCount": 1,
        "content": "C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed."
      },
      {
        "date": "2024-02-11T18:03:00.000Z",
        "voteCount": 2,
        "content": "A - Not possible\nD - Costliest Option possible\nnow between B and C\nThe question mentions high-memory EC2 instances.\nYou cannot get memory metrics without the Cloudwatch agent installed hence C."
      },
      {
        "date": "2023-12-23T13:58:00.000Z",
        "voteCount": 1,
        "content": "Option C is most cost effective choice."
      },
      {
        "date": "2023-12-22T16:58:00.000Z",
        "voteCount": 1,
        "content": "C is incorrect : When you first opt in Compute Optimizer, it may take up to 24 hours to fully analyze the AWS resources in your account. https://aws.amazon.com/compute-optimizer/faqs/"
      },
      {
        "date": "2024-01-01T07:12:00.000Z",
        "voteCount": 1,
        "content": "You are correct that in the FAQ you've linked it says 24 hours, but in other places of the AWS documentation it says 12 hours, like here: https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-getting-recommendations.html#viewing-recommendations\nor here: https://docs.aws.amazon.com/awssupport/latest/user/compute-optimizer-with-trusted-advisor.html\nSeems like even AWS doesn't know :D So I would still go with C."
      },
      {
        "date": "2023-12-17T18:39:00.000Z",
        "voteCount": 1,
        "content": "Option A is not in the running because it will require incurring further expense to address the cost issue.\n\nOption D is expensive - the Enterprise Support plan charges a minimum flat fee minimum or a % of your AWS bill. This could be a large amount for the company's hundreds of instances.\n\nOption B is expensive - Detailed monitoring scales based on the number of metrics and the number of resources. The company has hundreds of instances so this option could potentially be more expensive than D. \n\nOption C - Compute Optimizer will provide improvement suggestions based on 14 prior days usage data from the moment it was enabled. Moreover, the default service option is free. Nothing is said about the custom metrics being used for the CloudWatch agent but it could be the most expensive of all options if mis-used. So either cost 0 or incredibly large if used carelessly."
      },
      {
        "date": "2023-07-03T09:40:00.000Z",
        "voteCount": 1,
        "content": "C. need CW agent for RAm util"
      },
      {
        "date": "2023-04-21T14:55:00.000Z",
        "voteCount": 1,
        "content": "C- Compute Optimizer is the easiest solution"
      },
      {
        "date": "2023-03-26T23:42:00.000Z",
        "voteCount": 1,
        "content": "C - cost optimizer"
      },
      {
        "date": "2023-03-26T23:43:00.000Z",
        "voteCount": 1,
        "content": "*Compute"
      },
      {
        "date": "2023-03-04T06:46:00.000Z",
        "voteCount": 2,
        "content": "C is correct - Optimzer"
      },
      {
        "date": "2023-02-26T04:03:00.000Z",
        "voteCount": 1,
        "content": "Option C may be a good solution to rightsize the EC2 instances but may incur additional cost for installing the Amazon CloudWatch agent on each of the EC2 instances.\n\nThe MOST cost-effective solution to analyze the company\u2019s Amazon EC2 instances and Amazon EBS volumes is to create a dashboard using AWS Systems Manager OpsCenter. The OpsCenter dashboard can be configured to visualize the Amazon CloudWatch metrics associated with the EC2 instances and their EBS volumes. By reviewing the dashboard periodically, usage patterns can be identified, and EC2 instances can be right-sized based on the peaks in the metrics."
      },
      {
        "date": "2023-03-07T22:29:00.000Z",
        "voteCount": 2,
        "content": "Bro, install cost is 0. Simple linux command &gt; sudo yum install amazon-cloudwatch-agent"
      },
      {
        "date": "2023-01-16T10:27:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is C. Installing the Amazon CloudWatch agent on each of the EC2 instances and turning on AWS Compute Optimizer allows the solutions architect to analyze the environment and make recommendations on the sizing of the EC2 instances in a cost-effective way. AWS Compute Optimizer analyzes the utilization of the instances and recommends the optimal instance types for the workloads. This solution is more cost-effective than creating a dashboard and reviewing it periodically, or signing up for the AWS Enterprise Support plan and waiting for Trusted Advisor recommendations."
      },
      {
        "date": "2023-01-16T06:40:00.000Z",
        "voteCount": 1,
        "content": "C is correct, with computer optimizer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/amazon/view/95544-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.<br><br>In an AWS application account, the company\u2019s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed m the application account.<br><br>The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 63,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-23T09:47:00.000Z",
        "voteCount": 15,
        "content": "Follow below link. It has both option to be used for this scenarios. But default kms key can not be used so B\nhttps://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/"
      },
      {
        "date": "2023-04-21T04:54:00.000Z",
        "voteCount": 6,
        "content": "Although I think B is the best, it is missing to mention of the trust policy in the application account."
      },
      {
        "date": "2024-01-22T07:26:00.000Z",
        "voteCount": 1,
        "content": "Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. This sounds like a trust policy to me"
      },
      {
        "date": "2024-01-22T07:47:00.000Z",
        "voteCount": 3,
        "content": "A = Secret is not a RAM sharable resource. But who can recall this full list? Thus my reasoning is, I would expect more details for sharing via RAM like enable AWS Org sharing, assign permission (actions allowed on the shared resource) and select the external principal.\nB = correct see https://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/\nC = cannot cross-account access AWS managed KMS key as you do not have control on key policy\nD = SCP can only remove permissions. Even tough an SCP doesn't prevent you from accessing a secret, you still need to have IAM user permission and/or resource based policy in place to actually access"
      },
      {
        "date": "2023-12-25T02:27:00.000Z",
        "voteCount": 1,
        "content": "option b"
      },
      {
        "date": "2023-12-23T16:25:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-08T18:39:00.000Z",
        "voteCount": 2,
        "content": "Even B is the best answer among all the options, actually B is not correct. Without permission to access the KMS key, B cannot decrypt the secret."
      },
      {
        "date": "2024-03-11T19:38:00.000Z",
        "voteCount": 1,
        "content": "I was wrong. It is using AWS managed default encryption key, so it doesn't need the permission to access KMS key. The flaw of B is trust relationship policy."
      },
      {
        "date": "2023-11-15T23:11:00.000Z",
        "voteCount": 1,
        "content": "the Secrets Manager keys cannot be shared with RAM, key policy(resource policy) for the default KMS key managed by AWS cannot be changed, role is identity and can be granted access to assume other role"
      },
      {
        "date": "2023-10-21T09:55:00.000Z",
        "voteCount": 3,
        "content": "Answer is B.\nOption A is wrong. AWS RAM can not share AWS Secrets Manager ( see shareable resources in https://docs.aws.amazon.com/ram/latest/userguide/shareable.html )"
      },
      {
        "date": "2023-09-09T01:04:00.000Z",
        "voteCount": 1,
        "content": "Both Option A and Option B give repository administrators access to the repository and eliminate the need to manually share secrets.\nOption A is a relatively simple process of sharing secrets with AWS RAM and setting up an IAM role within the DBA account.\nOption B requires creating an IAM role in two different AWS accounts and setting cross-account permissions, which is a more complicated process.\nSo, while both A and B accomplish the goal, option A is simpler and more straightforward."
      },
      {
        "date": "2023-09-11T05:08:00.000Z",
        "voteCount": 4,
        "content": "who said we can share secrets using RAM??\ni just checked under RAM and allowed sharable AWS services\nAWS Secrets Manager is NOT one of those\nAnswer is B"
      },
      {
        "date": "2023-09-02T03:34:00.000Z",
        "voteCount": 1,
        "content": "As several people have highlighted, we refer to the blog https://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/\n\nWant to provide the following comment to emphasize why \"C\" is NOT even possible.\nIn Option C, its mentioned that the default AWS Managed CMK is used by the secrets manager. \nWe cannot provide any custom permissions to the AWS Managed CMK and by extension, its not possible to allow cross account access to it.  So, only Option B is valid."
      },
      {
        "date": "2023-07-03T09:44:00.000Z",
        "voteCount": 1,
        "content": "its a b"
      },
      {
        "date": "2023-06-13T18:37:00.000Z",
        "voteCount": 1,
        "content": "Guys, you want to know the right answer? Copy paste the whole question to olabiba.ai\nThe answer is B"
      },
      {
        "date": "2023-04-29T08:40:00.000Z",
        "voteCount": 2,
        "content": "Option A is the correct answer because it meets the requirement of giving the database administrators access to the database and eliminates the need to manually share the secrets. AWS Resource Access Manager (AWS RAM) enables you to share AWS resources with other accounts within your organization or organizational units (OUs) in AWS Organizations. By using AWS RAM to share the secrets from the application account with the DBA account, you can eliminate the need for manual sharing of secrets.\n\nOption B involves creating an IAM role in the application account and another IAM role in the DBA account. The DBA-Admin role in the DBA account would need to assume the DBA-Secret role in the application account to access the secrets. This approach adds complexity and does not eliminate the need for manual sharing of secrets.\n\nIn summary, Option A is a simpler and more efficient solution that meets the requirements."
      },
      {
        "date": "2023-06-22T02:38:00.000Z",
        "voteCount": 4,
        "content": "I couldn't find any option to share Secret Manager resources via RAM, did anyone try it?"
      },
      {
        "date": "2023-04-09T07:56:00.000Z",
        "voteCount": 2,
        "content": "B is correct, D doesn't make sense! SCP doesn't give any permission.. it just defines what can be allowed. you still need an IAM role/policy"
      },
      {
        "date": "2023-03-26T23:45:00.000Z",
        "voteCount": 2,
        "content": "B is the best choice"
      },
      {
        "date": "2023-03-21T08:19:00.000Z",
        "voteCount": 4,
        "content": "Has to be B because C is not possible.\nI get that you can't share access to the default KMS key, but how does it work to share access through a cross account role? How does the role in the DBA account decrypt the secrets that are encrypted by the default key if the role doesn't have permissions to that key?"
      },
      {
        "date": "2023-03-09T12:11:00.000Z",
        "voteCount": 2,
        "content": "cross account assume role"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/amazon/view/95545-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.<br><br>Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.<br><br>A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T08:13:00.000Z",
        "voteCount": 5,
        "content": "C. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU. This will ensure that all resources deployed in the organization reside in the ap-northeast-1 Region.\n\nE. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU. This will ensure that EC2 instances deployed in the DataOps OU use only the predefined list of instance types."
      },
      {
        "date": "2023-04-07T08:14:00.000Z",
        "voteCount": 3,
        "content": "Option D is incorrect because it suggests using the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. However, the ec2:Region condition key is not a valid condition key for EC2 actions. Instead, the aws:RequestedRegion condition key should be used to restrict access to specific AWS Regions.\n\nAdditionally, applying the SCP to the root OU, the DataOps OU, and the Research OU is unnecessary because applying the SCP to the root OU alone will ensure that the restriction applies to all accounts in the organization, including those in the DataOps and Research OUs.\n\nIn summary, option D is incorrect because it suggests using an invalid condition key and because applying the SCP to multiple OUs is unnecessary."
      },
      {
        "date": "2023-12-23T16:32:00.000Z",
        "voteCount": 1,
        "content": "Option C &amp; E"
      },
      {
        "date": "2023-09-02T03:44:00.000Z",
        "voteCount": 2,
        "content": "Very straightforward"
      },
      {
        "date": "2023-07-30T02:44:00.000Z",
        "voteCount": 1,
        "content": "C for all resources region\nand E for DataOps OU launch instantce type"
      },
      {
        "date": "2023-07-03T09:46:00.000Z",
        "voteCount": 1,
        "content": "its CE"
      },
      {
        "date": "2023-03-26T23:47:00.000Z",
        "voteCount": 1,
        "content": "SCP's are the most efficient here"
      },
      {
        "date": "2023-02-01T12:14:00.000Z",
        "voteCount": 4,
        "content": "With AWS Org, consider SCP first.\nIn this scenario, Only C,D,E are mention about SCP, but D apply for all, not only the DataOps OU"
      },
      {
        "date": "2023-01-16T10:36:00.000Z",
        "voteCount": 4,
        "content": "The correct options are C and E.\n\nOption C: Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.\n\nThis option is correct because it allows the company to restrict access to all AWS regions except for ap-northeast-1. This ensures that all resources deployed in the organization must reside in the ap-northeast-1 region. By applying the SCP to the root OU, it ensures that all accounts and OUs under the root will be affected.\n\nOption E: Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.\n\nThis option is correct because it allows the company to restrict access to specific instance types, which is required for the DataOps OU. By applying the SCP to the DataOps OU, it ensures that only resources deployed in the DataOps OU will be affected by the restriction."
      },
      {
        "date": "2023-01-16T10:39:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because it only restricts access to specific instance types, but it does not restrict access to a specific region.\n\nOption B is incorrect because it is applied to IAM users rather than OUs, which would not effectively apply the restriction to all resources in the organization.\n\nOption D is incorrect because it uses the ec2:Region condition key which would not allow to restrict the instances types only in the DataOps OU.\n\nBy creating an SCP that uses the aws:RequestedRegion condition key and restricting access to all regions except ap-northeast-1 and applying it to the root OU, this ensures that all resources deployed in the organization will reside in the ap-northeast-1 Region.\n\nBy creating an SCP that uses the ec2:InstanceType condition key and restricts access to specific instance types and applying it to the DataOps OU, this ensures that all EC2 instances deployed in the DataOps OU will use the predefined list of instance types."
      },
      {
        "date": "2023-01-16T06:44:00.000Z",
        "voteCount": 1,
        "content": "CE is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/amazon/view/95546-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.<br><br>The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region.<br><br>Which combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the SQS queue with the Lambda function to other Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubscribe the SNS topic in each Region to the SQS queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubscribe the SQS queue in each Region to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the SQS queue to publish URLs to SNS topics in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the SNS topic and the Lambda function to other Regions."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-19T04:54:00.000Z",
        "voteCount": 6,
        "content": "SNS being the publisher, SQS is subscribing"
      },
      {
        "date": "2023-10-13T16:36:00.000Z",
        "voteCount": 5,
        "content": "AC. \nAmazon SNS supports cross-region deliveries. \nhttps://docs.aws.amazon.com/sns/latest/dg/sns-cross-region-delivery.html"
      },
      {
        "date": "2024-01-25T18:50:00.000Z",
        "voteCount": 2,
        "content": "SNS in Region A, SQS + Lambda in Region A &amp; B, S3 Bucket in Region A"
      },
      {
        "date": "2023-12-23T16:40:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2023-10-14T03:16:00.000Z",
        "voteCount": 3,
        "content": "Deploy the SQS queue with the Lambda function to other Regions.\nSubscribe the SQS queue in each Region to the SNS topic."
      },
      {
        "date": "2023-07-03T09:50:00.000Z",
        "voteCount": 2,
        "content": "It's an AC"
      },
      {
        "date": "2023-06-22T05:10:00.000Z",
        "voteCount": 3,
        "content": "Basically, you need to replicate it all except the bucket in the other regions. The question is explained very vaguely however"
      },
      {
        "date": "2024-04-07T03:39:00.000Z",
        "voteCount": 1,
        "content": "SNS is the publisher and must stay in same region"
      },
      {
        "date": "2023-04-26T15:39:00.000Z",
        "voteCount": 3,
        "content": "A, C is correct.\nIt looks like Fan out pattern."
      },
      {
        "date": "2023-04-19T20:24:00.000Z",
        "voteCount": 1,
        "content": "Why would need to deploy SQS with Lambda? Makes no sense! It\u2019s BE."
      },
      {
        "date": "2023-05-07T08:44:00.000Z",
        "voteCount": 2,
        "content": "It's SNS that publishes not SQS"
      },
      {
        "date": "2023-04-05T20:49:00.000Z",
        "voteCount": 1,
        "content": "What does it mean in Option A that Lambda deploys SQS?"
      },
      {
        "date": "2023-03-26T23:50:00.000Z",
        "voteCount": 2,
        "content": "AC - SQS"
      },
      {
        "date": "2023-03-07T02:54:00.000Z",
        "voteCount": 1,
        "content": "support A,C. https://www.examtopics.com/discussions/amazon/view/74009-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-02-04T10:32:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C - Deploy &amp; Subscribe SQS."
      },
      {
        "date": "2023-01-29T17:21:00.000Z",
        "voteCount": 3,
        "content": "A and C"
      },
      {
        "date": "2023-01-16T10:40:00.000Z",
        "voteCount": 4,
        "content": "Option A is correct because deploying the SQS queue with the Lambda function to other regions will allow the application to process URLs in those regions and compare differences in site localization.\n\nOption C is correct because subscribing the SQS queue in each region to the SNS topic in the existing region will allow the application to publish URLs to the existing SNS topic and have those URLs processed in other regions.\n\nOption B is incorrect because subscribing the SNS topic in each region to the SQS queue in the existing region would not allow URLs to be processed in other regions.\n\nOption D is incorrect because configuring the SQS queue to publish URLs to SNS topics in each region would not ensure that the URLs are processed in those regions.\n\nOption E is incorrect because deploying the SNS topic and Lambda function to other regions without the SQS queue would not allow the application to process URLs in those regions."
      },
      {
        "date": "2023-01-16T06:46:00.000Z",
        "voteCount": 1,
        "content": "AC is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/amazon/view/95547-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.<br><br>Which strategy should the solutions architect use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T06:49:00.000Z",
        "voteCount": 15,
        "content": "C is correct. only eventbridge can run scheduled task"
      },
      {
        "date": "2023-06-22T05:19:00.000Z",
        "voteCount": 6,
        "content": "If there wasn't a schedule element I would choose AWS Batch because it pretty much loads a container and does the job, especially since it's like a 20-minute job. However the step functions part doesn't help with the scheduling part, hence I go for C"
      },
      {
        "date": "2024-04-05T07:33:00.000Z",
        "voteCount": 1,
        "content": "Option C :  https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/run-event-driven-and-scheduled-workloads-at-scale-with-aws-fargate.html"
      },
      {
        "date": "2024-01-22T09:14:00.000Z",
        "voteCount": 1,
        "content": "A = CW Log cannot invoke lambda every 4 hours\nB = Step Function cannot invoke batch job every 4 hour (unless you use an EventBridhe scheduled event)\nC = correct (but I do not like when Fargate is mentioned as a standalone service, as it is a serverless compute option for some some services)\nD = CodeDeploy cannot run an application every 4 hours"
      },
      {
        "date": "2024-01-17T06:20:00.000Z",
        "voteCount": 1,
        "content": "none. \"highly CPU intensive\" means no Fargate. scheduling means eventbridge."
      },
      {
        "date": "2024-01-09T04:38:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\nLambda's max running time is 15 mins, cannot support up to  20mins application."
      },
      {
        "date": "2023-12-28T06:55:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/tutorials/scheduling-a-serverless-workflow-step-functions-amazon-eventbridge-scheduler/\nStep Function cannot schedule a job. Step Function needs EventBridge as the scheduler."
      },
      {
        "date": "2023-12-23T16:52:00.000Z",
        "voteCount": 1,
        "content": "B is not possible as Step Function can not be used to run scheduled a job every 4 hour"
      },
      {
        "date": "2023-09-25T05:59:00.000Z",
        "voteCount": 2,
        "content": "containers are well-suited for applications that are built in microservices architecture, where each service is a self-contained unit that performs a specific task. These types of applications are typically designed to be scalable and easy to deploy, making them a good fit for containerization.\nI feel D is the best option"
      },
      {
        "date": "2024-02-29T08:57:00.000Z",
        "voteCount": 2,
        "content": "you can\u00b4t garantee with spot instances that they're available every 4 hours, C is the answer"
      },
      {
        "date": "2023-09-09T02:39:00.000Z",
        "voteCount": 1,
        "content": "I think Both B \u3001C is missing some key point\nOption B does not explain how to AWS Step Functions to trigger an AWS Batch job regually, in this case 4 hours per run.\nOption C does not explain how to use EventBridge to call the Fargate task, which is not native support, it might involved lambda to achive."
      },
      {
        "date": "2023-07-03T09:52:00.000Z",
        "voteCount": 1,
        "content": "C. schedule -&gt; eventbridge"
      },
      {
        "date": "2023-05-19T13:37:00.000Z",
        "voteCount": 3,
        "content": "The application is a Linux binary which can be packaged into a container, then run on AWS Fargate and scheduled using Event Bridge.\n# Use a base image that matches your application's runtime environment\nFROM ubuntu:latest\n# Copy the Linux binary into the container\nCOPY myapp /usr/local/bin/myapp\n# Set the entry point to execute the binary\nENTRYPOINT [\"/usr/local/bin/myapp\"]"
      },
      {
        "date": "2023-03-26T23:51:00.000Z",
        "voteCount": 1,
        "content": "C - Fargate is the best choice here"
      },
      {
        "date": "2023-01-16T10:42:00.000Z",
        "voteCount": 4,
        "content": "C. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.\n\nAWS Fargate is a serverless compute engine for containers that allows running containerized workloads without managing the underlying EC2 instances. This eliminates the need to provision, configure, and scale clusters of virtual machines to run containers.\n\nAmazon EventBridge (formerly CloudWatch Events) allows scheduling tasks using cron or rate expressions, which can be used to invoke the Fargate task every 4 hours. This will allow for cost-effective and scalable solution, as the infrastructure is managed by AWS and the application can run in a serverless fashion, only incurring costs when the task is running."
      },
      {
        "date": "2023-01-16T10:44:00.000Z",
        "voteCount": 4,
        "content": "The other options are not appropriate in this scenario:\n\nOption A: Running the application on AWS Lambda would not be appropriate, as Lambda is designed to run event-driven, short-lived functions, and not CPU-intensive, long-running tasks.\nOption B: AWS Batch is a service for running batch jobs, and it may not be the most appropriate service for this scenario, as the application is not a batch job but a long running task.\nOption D: Using Amazon EC2 Spot Instances would not be the best option for this scenario because the application is running for up to 20 minutes and EC2 Spot instances can be terminated at any time."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/amazon/view/95548-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:<br><br>\u2022\tAmazon S3 bucket that stores game assets<br>\u2022\tAmazon DynamoDB table that stores player scores<br><br>A solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-29T17:28:00.000Z",
        "voteCount": 14,
        "content": "DynamoDB global tables  + S3 replication+Cloudfront"
      },
      {
        "date": "2023-01-16T10:49:00.000Z",
        "voteCount": 7,
        "content": "Option C is the correct answer because it meets the requirements of reducing latency, improving reliability and requiring minimal effort to implement.\n\nBy creating another S3 bucket in a new Region, and configuring S3 Cross-Region Replication between the buckets, the game assets will be replicated to the new Region, reducing latency for users accessing the assets from that region. Additionally, by creating an Amazon CloudFront distribution and configuring origin failover with two origins accessing the S3 buckets in each Region, it ensures that the game assets will be served to users even if one of the regions becomes unavailable.\n\nConfiguring DynamoDB global tables by enabling Amazon DynamoDB Streams, and adding a replica table in a new Region, will also improve reliability by allowing the player scores to be replicated and updated in multiple regions, ensuring that the scores are available even in the event of a regional failure."
      },
      {
        "date": "2023-01-16T10:49:00.000Z",
        "voteCount": 3,
        "content": "Option A is not correct because using the new table as a replica target for DynamoDB global tables will not improve reliability. The same applies for Option D, which only uses S3 Same-Region Replication, which will not reduce latency for users in other regions.\n\nOption B is not correct because configuring asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) is not the best solution for this use case. It would require additional configuration and management effort."
      },
      {
        "date": "2024-10-06T00:49:00.000Z",
        "voteCount": 1,
        "content": "Just to add for DynamoDB, indeed you will need to create replica in the new region when creating global table, making it accessible in the new region nearer to the user.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables.tutorial.html"
      },
      {
        "date": "2024-10-05T15:03:00.000Z",
        "voteCount": 1,
        "content": "Option C is correct.\nJust to clarify: AWS uses DynamoDB Streams to replicate DynamoDB Global Tables. Using the Console, it is enabled automatically. Using the CLI, you must enable it explicitly by using StreamEnabled=true."
      },
      {
        "date": "2024-01-22T08:13:00.000Z",
        "voteCount": 1,
        "content": "A = \"Configure S3 Cross-Region Replication\" but doesn't create a new bucket in another region.\nB = \"Configure S3 Same-Region Replication\" without creating a second bucket and this should be cross-region. AWS DMS with CDC is not a good fit here, global table is the right option here\nC = correct\nD = we need the new bucket in a different region"
      },
      {
        "date": "2023-12-23T16:56:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2023-12-04T19:50:00.000Z",
        "voteCount": 2,
        "content": "Answer C.\nRegarding DynamoDB Streams - \nGlobal tables use DynamoDB Streams to replicate data across different Regions. When you create a replica for a global table, a stream is created by default. Any changes to a replica are replicated to all the other replicas within the same global table within a second using DynamoDB Streams."
      },
      {
        "date": "2023-11-08T14:47:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. C added unnecessary complexities such as Amazon DynamoDB Streams and Origin Failover."
      },
      {
        "date": "2024-09-01T20:08:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-01-22T08:14:00.000Z",
        "voteCount": 1,
        "content": "Option A doesn't mention creating a new bucket in a different region"
      },
      {
        "date": "2023-11-18T06:10:00.000Z",
        "voteCount": 1,
        "content": "I initially thought it was C, but I was torn between A and C. You may be right."
      },
      {
        "date": "2023-09-09T04:41:00.000Z",
        "voteCount": 2,
        "content": "other option are incorrect.\nB: Configure S3 Same-Region Replication.---&gt; It's not meet multi-region requirement.\nC: Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. ---&gt; It's not support for this kinda failover\nD: Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between the buckets. ---&gt; It's not meet multi-region requirement."
      },
      {
        "date": "2024-01-22T08:16:00.000Z",
        "voteCount": 2,
        "content": "C is correct, Origin Group allows failover see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      },
      {
        "date": "2023-09-07T02:24:00.000Z",
        "voteCount": 1,
        "content": "option c is the easiest way to do"
      },
      {
        "date": "2023-09-01T04:27:00.000Z",
        "voteCount": 1,
        "content": "Creating an Amazon CloudFront distribution will reduce latency for global users by serving assets from the closest edge location. S3 Cross-Region Replication will ensure that game assets are available in another region, improving reliability. Creating a new DynamoDB table in a new region and using it as a replica target for DynamoDB global tables will enable multi-region replication, improving reliability."
      },
      {
        "date": "2023-08-19T05:04:00.000Z",
        "voteCount": 2,
        "content": "Option C has another differentiator - DynamoDBStreams that will assist in Reliability"
      },
      {
        "date": "2023-07-29T05:50:00.000Z",
        "voteCount": 1,
        "content": "Correct A.\nCloudFront does not support origin failover with two origins accessing the S3 buckets in each Region. According to the AWS documentationhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html, origin failover only works within the same Region, not across Regions. This means that you can only configure origin failover with two origins that are in the same Region as the CloudFront distribution. If you want to use origin failover with S3 buckets in different Regions, you need to create multiple CloudFront distributions, one for each Region, and configure them to use the same domain name with geolocation routinghttps://blog.ippon.tech/when-a-cloudfront-origin-must-fail-for-testing-high-availability/."
      },
      {
        "date": "2023-09-02T04:01:00.000Z",
        "voteCount": 1,
        "content": "Referred to your AWS doc link. I don't see any condition that states that the origins in the origin group cannot be from two different regions. Can you provide the statement from the AWS doc that you are referring to please ?"
      },
      {
        "date": "2023-07-03T09:54:00.000Z",
        "voteCount": 1,
        "content": "weird question wording, but C fit more"
      },
      {
        "date": "2023-03-26T23:53:00.000Z",
        "voteCount": 2,
        "content": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets"
      },
      {
        "date": "2023-01-16T06:51:00.000Z",
        "voteCount": 2,
        "content": "C is correct. S3 cross replicate, CloudFront, Dynamodb global database and origin failover"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/amazon/view/95549-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.<br><br>The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 49,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-09T05:08:00.000Z",
        "voteCount": 12,
        "content": "C correct\nDocumentDB only have on-demand instance but not on-demand capacity mode, the mode is for DynamoDB"
      },
      {
        "date": "2024-07-04T10:41:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer https://aws.amazon.com/documentdb/pricing/\non-demand instance is supported by DocumentDB"
      },
      {
        "date": "2024-09-01T20:09:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-03-17T09:09:00.000Z",
        "voteCount": 1,
        "content": "C, documented. No exists the on-demand capacity mode"
      },
      {
        "date": "2024-02-01T03:26:00.000Z",
        "voteCount": 3,
        "content": "'Appropriately sized instances' Means on-demand ? that is quite vague.."
      },
      {
        "date": "2024-01-22T09:09:00.000Z",
        "voteCount": 4,
        "content": "A = Aurora supports MySQL and PostgreSQL, not MongoDB. App changes are not allowed\nB = This could work but DocumentDB provides managed MongoDB instance that is preferable\nC = correct\nD = there isn't on-demand capacity mode, in 2022 launched MondoDB Elastic Cluster that eliminates the need to choose, manage or upgrade instances and allows to scale up to 4PiB storage whereas instance based scales up to 128TiB.\n\nI thing this question is pre elastic cluster as this is ambiguous between C and D"
      },
      {
        "date": "2024-01-07T08:46:00.000Z",
        "voteCount": 1,
        "content": "DocumentDB does indeed support on-demand capacity mode (Contrary to what other users say here) https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/\nOn-Demand is ideally to a use case where you have unpredictable or variable database workloads, like this case, it is not said anywhere the expected workload, so it is better to start with On-demand , and later when you know the workload you can cahnge it."
      },
      {
        "date": "2024-01-12T05:29:00.000Z",
        "voteCount": 1,
        "content": "what you have linked here is a dynamodb article not a documentDB one, documentDB does not support on-demand capacity mode - https://aws.amazon.com/documentdb/faqs/\n\n\"You can scale the compute resources allocated to your instance in the AWS Management Console by selecting the desired instance and clicking the \u201cmodify\u201d button. Memory and CPU resources are modified by changing your instance class.\""
      },
      {
        "date": "2024-01-22T08:30:00.000Z",
        "voteCount": 1,
        "content": "There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more.-,Elastic%20Clusters,-What%20is%20Amazon"
      },
      {
        "date": "2024-01-12T07:11:00.000Z",
        "voteCount": 1,
        "content": "This is DynamoDB, not DocumentDB. The choices only mention DocumentDB."
      },
      {
        "date": "2023-12-23T17:10:00.000Z",
        "voteCount": 1,
        "content": "There is no on-demand capacity mode for DocumentDB, though there is on-demand vCPU based pricing available."
      },
      {
        "date": "2024-01-22T08:30:00.000Z",
        "voteCount": 1,
        "content": "There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more.-,Elastic%20Clusters,-What%20is%20Amazon"
      },
      {
        "date": "2024-08-12T14:20:00.000Z",
        "voteCount": 1,
        "content": "DocumentDB does support on-demand capacity:\nhttps://aws.amazon.com/documentdb/pricing/#:~:text=On-demand%20instances%20let%20you%20pay%20per%20second%2C,and%20having%20to%20guess%20the%20correct%20capacity"
      },
      {
        "date": "2023-09-01T05:29:00.000Z",
        "voteCount": 3,
        "content": "Amazon DocumentDB does NOT have on-demand capacity mode, so its option C."
      },
      {
        "date": "2024-01-22T08:30:00.000Z",
        "voteCount": 1,
        "content": "There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more.-,Elastic%20Clusters,-What%20is%20Amazon"
      },
      {
        "date": "2023-08-19T05:18:00.000Z",
        "voteCount": 2,
        "content": "I was leaning towards Option C but \"Appropriately sized instances\" is vague since the question does not state the size of Mongo DB. On-demand instances serve the purpose here, they are offered by DocumentDB, see the link\nhttps://aws.amazon.com/documentdb/pricing/"
      },
      {
        "date": "2023-07-03T12:24:00.000Z",
        "voteCount": 2,
        "content": "its a c"
      },
      {
        "date": "2023-06-19T11:52:00.000Z",
        "voteCount": 2,
        "content": "c-c-c-c-c-c-c-c\n\nOn-demand capacity mode as suggested in D may not provide the same level of high availability as multi-Availability Zone deployments. So it's c-c-c-c-c-c-c for me."
      },
      {
        "date": "2023-06-11T13:50:00.000Z",
        "voteCount": 2,
        "content": "See best practices for amazon documentdb - instance sizing in docs.\nAddicionally  there is no on-demand capacity mode."
      },
      {
        "date": "2023-05-20T15:17:00.000Z",
        "voteCount": 3,
        "content": "DocumentDB does indeed support on-demand capacity mode (Contrary to what other users say here) https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/\n\nbut this mode is good for spikey workloads and does not address the high availablity requirement"
      },
      {
        "date": "2023-05-20T15:19:00.000Z",
        "voteCount": 2,
        "content": "The correct link https://www.applytosupply.digitalmarketplace.service.gov.uk/g-cloud/services/743016963590682"
      },
      {
        "date": "2023-11-17T00:36:00.000Z",
        "voteCount": 1,
        "content": "The content mentioned in your link and the original comment are both mentioning things related to DynamoDB.  Your link is even worse which is describing DynamoDB but say it is for DocumentDB. Please study hard"
      },
      {
        "date": "2023-05-19T23:32:00.000Z",
        "voteCount": 1,
        "content": "See best practices for amazon documentdb - instance sizing in docs."
      },
      {
        "date": "2023-04-21T06:52:00.000Z",
        "voteCount": 2,
        "content": "Going wit C. I still call the DocumentDB used in mode C \"on-demand mode\" because you have to select the Ec2 instance; the pricing documentation still uses that name. There is an Elastic cluster for DocumentDB. Could it be that option D \"on-demand capacity mode\" is referring to Elastic mode?"
      },
      {
        "date": "2023-04-11T01:45:00.000Z",
        "voteCount": 1,
        "content": "Amazon DocumentDB does not support an on-demand capacity mode. You can only choose from different instance classes that have fixed compute and memory resources. However, you can scale your instances up or down as needed, and you can also pause and resume your instances to save costs. Amazon DocumentDB also automatically scales your storage and I/O based on your data size and workload."
      },
      {
        "date": "2023-03-26T23:59:00.000Z",
        "voteCount": 1,
        "content": "C - there is no on-demand capacity mode."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/amazon/view/95550-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company\u2019s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.<br><br>A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.<br><br>The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 40,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-08T23:30:00.000Z",
        "voteCount": 13,
        "content": "B wrong - full permissions ? when question asks for minimum permissions.\nD wrong - anonymous user ? anonymous does not work\nE wrong - encrypt permissions ? No Strategy account needs decrypt permissions\nSo, A,C,F"
      },
      {
        "date": "2023-03-08T23:33:00.000Z",
        "voteCount": 3,
        "content": "first the source bucket needs to give grant access thru bucket policy and KMS key policy (A,C options)\nSecondly, Strategy IAM role needs to give access to read from S3 bucket and also KMS key (Option F)"
      },
      {
        "date": "2023-05-21T20:04:00.000Z",
        "voteCount": 6,
        "content": "B full permission ? X\nD anonymous? X\nE encryption not needed for strategy team"
      },
      {
        "date": "2023-12-23T17:16:00.000Z",
        "voteCount": 1,
        "content": "A, C and F"
      },
      {
        "date": "2023-08-19T05:24:00.000Z",
        "voteCount": 1,
        "content": "By rule of elimination\nBDE are wrong. God_Is_Love is spot on"
      },
      {
        "date": "2023-07-03T12:28:00.000Z",
        "voteCount": 2,
        "content": "its ACF"
      },
      {
        "date": "2023-04-08T01:50:00.000Z",
        "voteCount": 3,
        "content": "Option B suggests updating the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. This option is not ideal because it grants more permissions than necessary. The requirement is to provide users with only the minimum permissions they need to view objects in the S3 bucket.\n\nOption D suggests creating a bucket policy that includes read permissions for the S3 bucket and setting the principal of the bucket policy to an anonymous user. This option is not ideal because it would allow anyone to read objects in the S3 bucket, which could pose a security risk.\n\nOption E suggests updating the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role. This option is not necessary because the requirement is for users in the Strategy account to be able to view objects in the S3 bucket, not to encrypt them."
      },
      {
        "date": "2023-03-27T00:19:00.000Z",
        "voteCount": 2,
        "content": "ACF is the best choice"
      },
      {
        "date": "2023-03-17T22:36:00.000Z",
        "voteCount": 2,
        "content": "A. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.\nC. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.\nF. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
      },
      {
        "date": "2023-01-29T17:49:00.000Z",
        "voteCount": 3,
        "content": "A C AND F"
      },
      {
        "date": "2023-01-26T22:46:00.000Z",
        "voteCount": 3,
        "content": "https://repost.aws/knowledge-center/cross-account-access-denied-error-s3"
      },
      {
        "date": "2023-01-19T10:24:00.000Z",
        "voteCount": 4,
        "content": "A, C, and F are the correct options."
      },
      {
        "date": "2023-01-16T10:53:00.000Z",
        "voteCount": 3,
        "content": "A, C, and F are the correct options.\n\nOption A creates a bucket policy that includes read permissions for the S3 bucket and sets the principal of the bucket policy to the account ID of the Strategy account. This ensures that users in the Strategy account have the necessary permissions to access the S3 bucket.\n\nOption C updates the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. This ensures that the users in the Strategy account have the necessary permissions to decrypt the objects stored in the S3 bucket.\n\nOption F updates the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. This ensures that the users in the Strategy account have the necessary permissions to read the objects in the S3 bucket and to decrypt them using the custom KMS key.\n\nThe other options are not correct because they either grant unnecessary permissions (B, D) or grant permissions in the wrong way (E)."
      },
      {
        "date": "2023-01-16T06:59:00.000Z",
        "voteCount": 2,
        "content": "ACF is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/amazon/view/95551-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.<br><br>The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 58,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-09T10:12:00.000Z",
        "voteCount": 20,
        "content": "Almost voted D because of the Storage Gateway + SAN combination.. but seems like it's not correct since S3 events cannot trigger Batch jobs directly, you need a Lambda function! S3 events can be only Lambda,SNS or SQS.."
      },
      {
        "date": "2023-04-20T11:01:00.000Z",
        "voteCount": 3,
        "content": "Agree - The Lambda function acts as a bridge between the S3 event and AWS Batch, allowing you to trigger AWS Batch jobs in response to S3 events."
      },
      {
        "date": "2023-03-09T10:52:00.000Z",
        "voteCount": 9,
        "content": "Guys its Tricky one between C and D and answer is D! (Modernization question)\nLook at this two below blogs : \nhttps://aws.amazon.com/blogs/storage/using-aws-storage-gateway-to-modernize-next-generation-sequencing-workflows/\n\nThanks to tinyflame who made me do my research on this :-)\nYes, SAN -&gt; Storage Gateway Only\nNAS -&gt; Data Sync or Storage Gateway\nhttps://aws.amazon.com/blogs/storage/from-on-premises-to-aws-hybrid-cloud-architecture-for-network-file-shares/"
      },
      {
        "date": "2024-10-07T10:34:00.000Z",
        "voteCount": 1,
        "content": "Nope, you need S3 events to trigger Lambda. S3 events cannot trigger batch"
      },
      {
        "date": "2024-09-01T20:14:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2023-03-09T10:59:00.000Z",
        "voteCount": 4,
        "content": "On Premise NAS and file servers to  S3.  --&gt; Use DataSync solution\nOn Premise SMB or NFS file share to S3 --&gt; Use Storage/File Gateway solution"
      },
      {
        "date": "2024-03-17T15:07:00.000Z",
        "voteCount": 1,
        "content": "@God_Is_Love, both articles you've provided are NOT mentioning \"SAN\" at all. You cannot copy data from SAN using storage GW, but you do it with DataSync ran from within a server, which is connected to that SAN. Research more on what SAN is and how does it work :)"
      },
      {
        "date": "2024-08-14T13:37:00.000Z",
        "voteCount": 1,
        "content": "lambda solo dura 900 segundos me voy por la D"
      },
      {
        "date": "2024-09-01T20:14:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-05-27T18:05:00.000Z",
        "voteCount": 1,
        "content": "Currently, S3 events can only push to three different types of destinations:\nSNS topic, SQS Queue, AWS Lamba. \nYou cannot directly trigger a Batch job by S3 Event"
      },
      {
        "date": "2024-01-22T09:31:00.000Z",
        "voteCount": 2,
        "content": "A = 200GB very now and then doesn't need Snowball Edge\nB = Data Pipeline is ETL and not suitable in hybrid scenarios\nC = correct (DataSync does the job, also the app is already container based and it works well with Batch that is suited for HPC kind of workload - genomic sequencing is a typical HPC workload)\nD = even tough Storage Gateway does the job you cannot directly trigger a AWS Batch job from an S3 event, you need either a Lambda in the middle or enable EventBrdige notification and create a rule that triggers the AWS Batch Job"
      },
      {
        "date": "2024-01-17T06:57:00.000Z",
        "voteCount": 1,
        "content": "... \"The main requirement is that the data needs to be accessible over the network in a file format like NFS that DataSync supports.\""
      },
      {
        "date": "2024-01-17T06:55:00.000Z",
        "voteCount": 1,
        "content": "C - Amazon Q says \"While it does not directly support SAN (storage area network), you can use AWS DataSync to transfer data from files stored on a SAN volume to AWS storage services like Amazon S3.\""
      },
      {
        "date": "2023-12-23T17:24:00.000Z",
        "voteCount": 2,
        "content": "Option C is better option. Though D is also possible but as the jobs are already container based C would be better.\nQuestion is not clear whether containers used on-premise are docker based containers."
      },
      {
        "date": "2023-12-23T07:47:00.000Z",
        "voteCount": 2,
        "content": "Data Transfer  --- &gt; Data  Sync\nData Integration  --- &gt; Storage GW\nData Orchestration  --- &gt; Data Pipeline"
      },
      {
        "date": "2023-12-08T20:08:00.000Z",
        "voteCount": 2,
        "content": "D doesn't seem to be correct as AWS Batch is not a destination for AWS S3 events.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html"
      },
      {
        "date": "2023-09-09T05:28:00.000Z",
        "voteCount": 1,
        "content": "Option C: Use AWS DataSync to transfer data to Amazon S3. DataSync is designed for fast, easy and secure data transfer. This option also uses S3 events to trigger an AWS Lambda function, which launches an AWS Step Functions workflow and runs a Docker container using AWS Batch. This option takes into account data transfer, processing and container management, and should be the most suitable solution.\nOption D: Use AWS Storage Gateway's file gateway to transfer data to Amazon S3. Storage Gateway is suitable for hybrid cloud environments, but in this case, since the company already has a high-speed AWS Direct Connect connection, it will be more efficient to use DataSync."
      },
      {
        "date": "2023-08-21T13:21:00.000Z",
        "voteCount": 1,
        "content": "C.\nOf the given options C is probably the closest. Step Functions can be used to model the workflow. D does not specify this. DataSync can be used to transfer data [https://docs.aws.amazon.com/datasync/latest/userguide/s3-cross-account-transfer.html]."
      },
      {
        "date": "2023-08-19T05:37:00.000Z",
        "voteCount": 1,
        "content": "I choose D. My rationale - 200GB data for 1 genome sequence, Lets say DirectConnect is 1Gbps line, DataSync cannot efficiently transfer the data to get the processing under 1 day.\nAgree with God_Is_Love's hypothesis"
      },
      {
        "date": "2023-09-01T00:32:00.000Z",
        "voteCount": 1,
        "content": "S3 event can't trigger direct AWS Batch job. =&gt; C"
      },
      {
        "date": "2024-01-22T09:39:00.000Z",
        "voteCount": 1,
        "content": "Assuming DX is 1Gbps, it takes about 27 minutes to transfer 200GB. also, I don't see how Storage Gateway can speedup things. My point is that here both DataSynch and Storage Gateway  can di the job, but you cannot trigger Batch job directly from S3 object event. Thus C"
      },
      {
        "date": "2023-08-09T08:44:00.000Z",
        "voteCount": 1,
        "content": "Does the AWS DataSync support SAN?"
      },
      {
        "date": "2023-07-29T05:22:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-03T13:00:00.000Z",
        "voteCount": 1,
        "content": "C\nD would be an option if using volume gateway and lambda to trigger batch\ndatasync dont need to support NAS. agent can copy off of NFS or SMB mount of the NAS drive."
      },
      {
        "date": "2023-06-19T16:33:00.000Z",
        "voteCount": 1,
        "content": "I answered D, but Olabiba.ai says C, because:\nHere's why option C is the most suitable choice:\n\n\n\nOverall, option C provides a scalable and efficient solution for the company to process genomics data on AWS, meeting their capacity and turnaround time requirements."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/amazon/view/95555-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.<br><br>A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time.<br><br>Which solution will meet these requirements with the LEAST management overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-09T16:07:00.000Z",
        "voteCount": 15,
        "content": "EFS is Linux/Mac based, So, A,D are out.\nLustre stands for Linux cluster, So B is out. Left is C which is correct (Amazon FSx for Windows )"
      },
      {
        "date": "2024-04-23T01:59:00.000Z",
        "voteCount": 1,
        "content": "C for windows, AD and ACLs"
      },
      {
        "date": "2024-02-07T06:36:00.000Z",
        "voteCount": 1,
        "content": "C is the correct option"
      },
      {
        "date": "2023-12-23T17:30:00.000Z",
        "voteCount": 1,
        "content": "Option C as it is windows based OS."
      },
      {
        "date": "2023-09-09T05:40:00.000Z",
        "voteCount": 2,
        "content": "Option B FSx for Lustre is not for Linux POSIX-compliant\nOption C correct"
      },
      {
        "date": "2023-09-07T08:00:00.000Z",
        "voteCount": 1,
        "content": "C FSx for windows is a good fit for this"
      },
      {
        "date": "2023-07-11T17:13:00.000Z",
        "voteCount": 1,
        "content": "FSx for Lustre can only be used by Linux-based instances."
      },
      {
        "date": "2023-07-03T13:02:00.000Z",
        "voteCount": 1,
        "content": "C for windows"
      },
      {
        "date": "2023-06-11T14:03:00.000Z",
        "voteCount": 2,
        "content": "EFS and FSx for Lustre == Linux\nFSx Windows File == Windows"
      },
      {
        "date": "2023-03-22T06:40:00.000Z",
        "voteCount": 2,
        "content": "EFS and Windows is not straight forward. C is the best solution."
      },
      {
        "date": "2023-03-21T21:03:00.000Z",
        "voteCount": 3,
        "content": "Amazon FSx is built on Windows Server... Access Control Lists (ACLs)... To control user access, Amazon FSx integrates with your on-premises Microsoft Active Directory as well as with AWS Microsoft Managed AD.\nhttps://aws.amazon.com/fsx/windows/features/?nc=sn&amp;loc=2\n\nAll others don't work - forget about the \"least management\" statement - it says \"implement Windows ACLS to control...\" all others are thrown out."
      },
      {
        "date": "2023-02-26T04:57:00.000Z",
        "voteCount": 2,
        "content": "Option D suggests using an EFS file system, which is a shared file system that can be mounted on multiple EC2 instances, but this requires additional configuration to keep the content in sync across all instances.\n\nOption C is the optimal choice because Amazon FSx for Windows File Server supports Windows ACLs and seamlessly integrates with Active Directory to join instances to a domain. This option minimizes management overhead by reducing the complexity of managing multiple EFS file shares or writing scripts to synchronize content across EC2 instances."
      },
      {
        "date": "2023-02-06T11:39:00.000Z",
        "voteCount": 2,
        "content": "FSX for WIndows is the only option. The rest of options are not supported."
      },
      {
        "date": "2023-02-03T00:34:00.000Z",
        "voteCount": 2,
        "content": "FSx for Lustre can only be used by Linux-based instances."
      },
      {
        "date": "2023-01-29T17:58:00.000Z",
        "voteCount": 1,
        "content": "good answer are C or D but as it says LEAST management overhead ==&gt; D as in C we will need a user data script"
      },
      {
        "date": "2023-01-29T18:00:00.000Z",
        "voteCount": 1,
        "content": "sorry D is uncorrect as it use Elastic File System (Amazon EFS) itch is not windows so Iswitch to C"
      },
      {
        "date": "2023-07-05T02:47:00.000Z",
        "voteCount": 1,
        "content": "Also that means each instance launched from the AMI will have 2TB EBS volume.. which is not ideal"
      },
      {
        "date": "2023-01-27T10:52:00.000Z",
        "voteCount": 1,
        "content": "@masetromain is this a good exam study guide? Like how many questions were from here. Any help would be appreciated. Thank you"
      },
      {
        "date": "2023-01-26T23:37:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_join_instance.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/amazon/view/95557-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer.<br><br>The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-09T21:02:00.000Z",
        "voteCount": 12,
        "content": "SendTemplatedEmail\nSendEmail\nSendRawEmail are email api methods used in SES"
      },
      {
        "date": "2023-01-16T11:02:00.000Z",
        "voteCount": 11,
        "content": "The correct answer is D.\n\nIn this solution, the company can use Amazon SES to send email messages, which will minimize operational overhead as SES is a fully managed service that handles sending and receiving email messages. The company can store the email template on Amazon SES with parameters for the customer data and use an AWS Lambda function to call the SendTemplatedEmail API operation, passing in the customer data to replace the parameters and the email destination. This solution eliminates the need to set up and manage an SMTP server on EC2 instances, which can be costly and time-consuming.\n\nOption A and B are not correct because it requires to set up an SMTP server on EC2 instances, which is not necessary and will increase operational overhead.\nOption C is not correct because it stores the email template in Amazon SES with parameters for the customer data which is not possible."
      },
      {
        "date": "2023-06-22T08:48:00.000Z",
        "voteCount": 7,
        "content": "Ok, so according to chatgpt C is not correct because \"Option C is not correct because it stores the email template in Amazon SES with parameters for the customer data which is not possible.\"\nHowever, D says exactly the same - so D is not correct as well?\nDo not fully trust chatgp"
      },
      {
        "date": "2024-03-17T15:27:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT also is saying \"Option A and B are not correct because it requires to set up an SMTP server on EC2 instances\", but those options are \"A\" and \"C\", not \"A\" and \"B\". Seems there is some mismatch with the options."
      },
      {
        "date": "2024-05-11T01:33:00.000Z",
        "voteCount": 1,
        "content": "S3 Buckets is not needed to store template"
      },
      {
        "date": "2023-12-23T17:32:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-08-19T06:03:00.000Z",
        "voteCount": 1,
        "content": "D - Can send templated email with request parameters"
      },
      {
        "date": "2023-07-11T06:52:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDDD"
      },
      {
        "date": "2023-07-03T13:03:00.000Z",
        "voteCount": 1,
        "content": "its a d"
      },
      {
        "date": "2023-06-22T08:51:00.000Z",
        "voteCount": 1,
        "content": "I vote for B due to the fact that I cannot see an option to \"Store the email template on Amazon SES with parameters for the customer data\" Other than that it looks like a good option but it's just not working"
      },
      {
        "date": "2023-08-19T06:01:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/ses/latest/APIReference-V2/API_CreateEmailTemplate.html"
      },
      {
        "date": "2023-12-28T06:40:00.000Z",
        "voteCount": 1,
        "content": "D is correct.\nRegarding your concerns about email templates on SES with parameters see: https://docs.aws.amazon.com/ses/latest/dg/send-personalized-email-api.html"
      },
      {
        "date": "2023-06-18T13:39:00.000Z",
        "voteCount": 1,
        "content": "keyword = SendTemplatedEmail API"
      },
      {
        "date": "2023-03-27T00:24:00.000Z",
        "voteCount": 1,
        "content": "Template - easy one."
      },
      {
        "date": "2023-01-29T18:07:00.000Z",
        "voteCount": 3,
        "content": "D should be the answer"
      },
      {
        "date": "2023-01-16T07:22:00.000Z",
        "voteCount": 2,
        "content": "D is correct - https://docs.aws.amazon.com/ses/latest/APIReference/API_SendTemplatedEmail.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/amazon/view/95559-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.<br><br>The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.<br><br>Several times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not been processed property. An investigation finds no errors m the application logs.<br><br>How can the company solve this problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on termination protection tor the EC2 Instances",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the visibility timeout for the SQS queue to 3 hours",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure scale-in protection for the instances during processing\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the redrive policy and set maxReceiveCount to 0."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T11:04:00.000Z",
        "voteCount": 27,
        "content": "The correct answer is C. The company can solve the problem by configuring scale-in protection for the instances during processing. This will ensure that the instances are not terminated while they are processing videos. This will prevent the messages from moving to the dead-letter queue and ensure that videos are processed properly.\n\nOption A is incorrect because turning on termination protection for the EC2 instances will not solve the problem as it will impact the ability of the Auto Scaling group to scale instances in and out based on the number of videos in the queue.\n\nOption B is incorrect because the company has specified a visibility timeout of 1 hour, which is enough time for the instances to process a video and there is no need to update the timeout to 3 hours.\n\nOption D is incorrect because the company has set the maxReceiveCount to 1 and changing it to 0 will not solve the problem. maxReceiveCount allowed range is 1 to 1000."
      },
      {
        "date": "2023-07-20T07:04:00.000Z",
        "voteCount": 1,
        "content": "fully agree, option d is inocrrect because 0 is an invalida value for maxReceiveCount"
      },
      {
        "date": "2023-05-24T05:44:00.000Z",
        "voteCount": 8,
        "content": "ChatGPT confirms this reasoning."
      },
      {
        "date": "2023-05-22T05:42:00.000Z",
        "voteCount": 6,
        "content": "D makes sense\nI think D answer has a typo! probably they didn't copy the text properly\nhttps://repost.aws/knowledge-center/lambda-retrying-valid-sqs-messages"
      },
      {
        "date": "2024-06-09T20:41:00.000Z",
        "voteCount": 1,
        "content": "D is a typo"
      },
      {
        "date": "2024-02-25T19:21:00.000Z",
        "voteCount": 1,
        "content": "If Option D is a typo, then D"
      },
      {
        "date": "2024-01-27T15:36:00.000Z",
        "voteCount": 2,
        "content": "B.\nThe best solution for this problem is to update the visibility timeout for the SQS queue to 3 hours. This is because when the visibility timeout is set to 1 hour, it means that if the EC2 instance doesn't process the message within an hour, it will be moved to the dead-letter queue. By increasing the visibility timeout to 3 hours, this should give the EC2 instance enough time to process the message before it gets moved to the dead-letter queue. Additionally, configuring scale-in protection for the EC2 instances during processing will help to ensure that the instances are not terminated while the messages are being processed."
      },
      {
        "date": "2024-01-13T00:58:00.000Z",
        "voteCount": 4,
        "content": "Option D is a typo.\nI seen the same question in udemy but the Option D is 10"
      },
      {
        "date": "2023-12-23T17:45:00.000Z",
        "voteCount": 2,
        "content": "Option C is correct."
      },
      {
        "date": "2023-11-16T00:19:00.000Z",
        "voteCount": 1,
        "content": "setting MaxReceiveCount to 0 doesn't make and send and it impossible, because messages would be send to DLQ without any attempt to consume them from source queue"
      },
      {
        "date": "2023-09-21T11:23:00.000Z",
        "voteCount": 1,
        "content": "checked 4 AI, C is definitely not the correct answer: Option C: Configuring scale-in protection for the instances during processing will not prevent messages from being moved to the dead-letter queue if they cannot be processed on the first attempt."
      },
      {
        "date": "2023-09-02T15:39:00.000Z",
        "voteCount": 5,
        "content": "Refer https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/\nFrom the above link, \"an instance might be handling a long-running work task, perhaps pulled from an SQS queue. Protecting the instance from termination will avoid wasted work\" - This is what the question is also alluding to.\nThis is how one would make use of the functionality.\nYou  change the protection status of one or more instances by calling the SetInstanceProtection function. If you wanted to use this function to protect long-running, queue-driven worker processes from scale-in termination, you could set up your application as follows (this is pseudocode):\n\nwhile (true)\n{\n  SetInstanceProtection(False);\n  Work = GetNextWorkUnit();\n  SetInstanceProtection(True);\n  ProcessWorkUnit(Work);\n  SetInstanceProtection(False);\n}"
      },
      {
        "date": "2023-08-19T06:23:00.000Z",
        "voteCount": 2,
        "content": "Going with C only because D has value of maxReceiveCount set to 0"
      },
      {
        "date": "2023-07-17T16:50:00.000Z",
        "voteCount": 1,
        "content": "I go with C"
      },
      {
        "date": "2023-07-04T22:23:00.000Z",
        "voteCount": 2,
        "content": "B.\nAWS \"recommends setting your queue's visibility timeout to six times your function timeout\" which makes 3 hours perfect.\nsource: https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
      },
      {
        "date": "2024-03-11T12:46:00.000Z",
        "voteCount": 1,
        "content": "But this for a queue to use with lambda. Here it is EC2 in ASG"
      },
      {
        "date": "2023-07-03T13:11:00.000Z",
        "voteCount": 1,
        "content": "C more likely"
      },
      {
        "date": "2023-06-22T09:27:00.000Z",
        "voteCount": 1,
        "content": "I couldn't find any way to configure scale-in protection for the instances during processing except to do it manually, which is going to be an insane exercise. Eventually, that can be done by the application as part of the processing but I would then expect some more context in the answer."
      },
      {
        "date": "2023-05-20T16:25:00.000Z",
        "voteCount": 1,
        "content": "Option C, configuring scale-in protection for the instances during processing, is not directly related to the problem. Scale-in protection prevents instances from being terminated during an Auto Scaling event, but it does not address the issue of messages being moved to the dead-letter queue without successful processing.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-instance-protection.html\n\nI think D is tyoe and should read :\nD.\tUpdate the redrive policy and set maxReceiveCount to 10."
      },
      {
        "date": "2023-04-26T09:02:00.000Z",
        "voteCount": 2,
        "content": "D should be \"set maxReceiveCount to 10.\" It, Maybe a typo.\n\nExplanation:\nThis setting ensures that any message that failed to be processed will be sent back to the queue to be picked up by other consumers and re-processed.\n\n---\nWhy C is incorrect?\nWell, the Auto Scaling group responds to the number of messages on the queue, scale-in protection is not cost-effective when there are no messages on the SQS queue."
      },
      {
        "date": "2023-05-20T15:48:00.000Z",
        "voteCount": 2,
        "content": "there are no errors in the application logs, this leave us to believe that the instances are being terminated by the auto scaling during the processing of the videos. any workaround in the SQS layer might not sove the problem"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/amazon/view/95561-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.<br><br>The solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated user<br><br>Which solution will meet these requirements with the LEAST amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-29T18:19:00.000Z",
        "voteCount": 9,
        "content": "should be C as on the question has said 'no need for public IP\" ==&gt; private in API gateway = VPC endpoint"
      },
      {
        "date": "2023-12-10T21:24:00.000Z",
        "voteCount": 8,
        "content": "Bad question design. None of the answers is correct.\nNone of the answers mentions how to satisfy the requirement of \"All APIs need to be called with an authenticated user\".\nAnother requirement \"make the set of APIs accessible only from a VPC\". \"the set\" doesn't mean the whole set. Here \"the set\" means a part of the whole set. \nA: The set of APIs are still publicly accessible.\nB: Removing DNS entry doesn't remove the public accessibility.\nC: This is making the whole set of APIs private. If this answer can be specific to \"the set\" APIs, this could be a good answer.\nD: Using EC2 instances is always a bad answer."
      },
      {
        "date": "2024-06-25T16:12:00.000Z",
        "voteCount": 2,
        "content": "there is only set of APIs that do not require public access, you dont need all APIs private access? so it could be that the answer is A?"
      },
      {
        "date": "2024-02-01T03:57:00.000Z",
        "voteCount": 3,
        "content": "All given answers are not ideal.. the closet one is C BUT.. .when mentioning the requirement to have only 'a set of API to be private' means 'not all'.. turning the endpoint from public to private will turn all to Private ,, which is not fully correct as per the question.. I suppose the given answer or question missing an info.. or AWS starts playing with AI"
      },
      {
        "date": "2023-12-28T07:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html"
      },
      {
        "date": "2023-12-23T17:48:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-09-02T15:56:00.000Z",
        "voteCount": 1,
        "content": "Refer https://aws.amazon.com/blogs/compute/introducing-amazon-api-gateway-private-endpoints/"
      },
      {
        "date": "2023-09-02T12:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is C as explain in https://repost.aws/knowledge-center/api-gateway-vpc-connections"
      },
      {
        "date": "2023-08-19T06:29:00.000Z",
        "voteCount": 1,
        "content": "Regional to Private fits the use-case"
      },
      {
        "date": "2023-07-21T07:46:00.000Z",
        "voteCount": 1,
        "content": "the best possible answer from all the options is C"
      },
      {
        "date": "2023-07-03T13:19:00.000Z",
        "voteCount": 2,
        "content": "it's C, although it begs the questions about APIs that need to stay public..."
      },
      {
        "date": "2023-03-27T00:31:00.000Z",
        "voteCount": 1,
        "content": "C. Update the API endpoint from Regional to private in API Gateway."
      },
      {
        "date": "2023-01-16T11:05:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is C. Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.\nThis solution will meet the requirements with the least amount of effort because it utilizes the built-in features of API Gateway and VPC to restrict access to the API. With this method, no additional infrastructure or configurations are necessary.\nA and B are not correct because they would require additional infrastructure and configurations.\nD is not correct because it would require provisioning an EC2 instance and installing an Apache server, introducing additional complexity and management overhead."
      },
      {
        "date": "2023-01-16T07:54:00.000Z",
        "voteCount": 1,
        "content": "C is  correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/amazon/view/95562-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.<br><br>The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.<br><br>Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-08T02:19:00.000Z",
        "voteCount": 18,
        "content": "A: Global Accelerator can't have an s3 bucket as endpoint\nC: People are complaining about time to retreive maps. Transfert acceleration is used to accelerate PUT requests to an s3 bucket located in a distant region.\nE: An accelerator as cloudfront origin does not make much sense, because cloudfront is already using the AWS network. Global Accelerator is usually for Layer 4 networking and/or static anycast IPs"
      },
      {
        "date": "2023-01-16T11:06:00.000Z",
        "voteCount": 8,
        "content": "B is correct because it involves creating a new S3 bucket in the us-east-1 region and configuring cross-Region replication to synchronize from the existing S3 bucket in eu-west-1. This will allow users in us-east-1 to access the weather maps from a closer location, improving performance.\n\nD is correct because it involves using Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1. This will also allow users in us-east-1 to access the weather maps from a closer location, improving performance.\n\nA and E are not correct because they do not involve creating a new S3 bucket in us-east-1, which is necessary for improving performance for the users in that region. C is not correct because it involves using the S3 Transfer Acceleration endpoint, which is a different service and not necessary for this scenario."
      },
      {
        "date": "2024-04-03T22:59:00.000Z",
        "voteCount": 1,
        "content": "BD\nC using S3 Transfer Acceleration is good but this answer option itself is wrong due to the statement that pointing to a regional endpoint, where it doesn't exist. Once enable, it is just a global endpoint URL\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration-examples.html"
      },
      {
        "date": "2024-01-07T09:44:00.000Z",
        "voteCount": 1,
        "content": "If you want to improve latency , you always look for Global Accelerator fro the readings and Transfer accelerator for the updates. \nYes, it is possible to configure AWS Global Accelerator to distribute traffic from an S3 bucket in one AWS Region (eu-west-1) to endpoint groups in another AWS Region (us-east-1) for TCP ports 80 and 443. This configuration can be useful for improving the performance and availability of your S3 bucket for users in both regions.\nThis way you sabe money in the storage, you don't need to duplicate the storage. And for persons that chose option D, if you update the bucket there, those objects will not be replicated to the other region since replication works only in one way."
      },
      {
        "date": "2024-09-02T21:22:00.000Z",
        "voteCount": 1,
        "content": "just BD"
      },
      {
        "date": "2023-12-23T17:53:00.000Z",
        "voteCount": 1,
        "content": "Option B &amp; D"
      },
      {
        "date": "2023-12-10T21:31:00.000Z",
        "voteCount": 3,
        "content": "This is not a good question design. Does that mean the application use CloudFront in EU and does not use CloudFront in the US? How weird it is!!!"
      },
      {
        "date": "2023-11-30T04:44:00.000Z",
        "voteCount": 4,
        "content": "Exactly case from this blog post https://aws.amazon.com/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/"
      },
      {
        "date": "2023-07-21T08:01:00.000Z",
        "voteCount": 2,
        "content": "BD, I was initially looking at BE, I think global accelerator is used more for write requests."
      },
      {
        "date": "2023-07-03T13:25:00.000Z",
        "voteCount": 2,
        "content": "BD makes more ense"
      },
      {
        "date": "2023-07-02T12:25:00.000Z",
        "voteCount": 1,
        "content": "https://godof.cloud/dynamic-origin-s3-spa/\nUse case"
      },
      {
        "date": "2023-03-28T20:03:00.000Z",
        "voteCount": 3,
        "content": "BE- global accelerators improve performance by providing edge location for onboarding traffic."
      },
      {
        "date": "2023-03-28T20:13:00.000Z",
        "voteCount": 2,
        "content": "Q: Can I use AWS Global Accelerator for object storage with Amazon S3?\n\nA: You can use Amazon S3 Multi-Region Access Points to get the benefits of Global Accelerator for object storage. S3 Multi-Region Access Points use Global Accelerator transparently to provide a single global endpoint to access a data set that spans multiple S3 buckets in different AWS Regions. This allows you to build multi-region applications with the same simple architecture used in a single region, and then to run those applications anywhere in the world. Application requests made to an S3 Multi-Region Access Point\u2019s global endpoint automatically route over the AWS global network to the S3 bucket with the lowest network latency. This allows applications to automatically avoid congested network segments on the public internet, improving application performance and reliability."
      },
      {
        "date": "2023-03-27T00:37:00.000Z",
        "voteCount": 1,
        "content": "Ill go with BD"
      },
      {
        "date": "2023-02-26T11:22:00.000Z",
        "voteCount": 4,
        "content": "Since only one additional region we dont need global accelerators"
      },
      {
        "date": "2023-02-22T07:25:00.000Z",
        "voteCount": 1,
        "content": "S3 transfer acceleration is more efficient"
      },
      {
        "date": "2023-01-29T18:22:00.000Z",
        "voteCount": 2,
        "content": "A and E are not correct as there isn't a need to use aws global accel"
      },
      {
        "date": "2023-01-16T07:55:00.000Z",
        "voteCount": 1,
        "content": "BD is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/amazon/view/95563-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.<br><br>The solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T11:07:00.000Z",
        "voteCount": 8,
        "content": "B is correct. It can prevent the issue from happening again by monitoring the file system with the FreeStorageCapacity metric in Amazon CloudWatch and using Amazon EventBridge to invoke an AWS Lambda function to increase the capacity as required. This ensures that the file system always has enough free space to store user profiles and avoids reaching maximum capacity.\nA: Removing old user profiles may not be sufficient to create enough space and does not prevent the problem from happening again.\nC: AWS Step Functions cannot be used to increase capacity, it is a service for creating and running workflows that stitch together multiple AWS services.\nD: Creating an additional FSx for Windows File Server file system and updating user profile redirection for a portion of the users may not be sufficient to prevent the problem from happening again and does not address the current capacity issue."
      },
      {
        "date": "2023-03-09T21:53:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html\nEventBridge invoking lambda to update settings will prevent too from occurring again"
      },
      {
        "date": "2024-05-19T10:12:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't you need a cloudwatch alarm that would trigger a Lambda based on the metric going above a certain treshold?\nMetric -&gt; Lambda is a bit of a shortcut"
      },
      {
        "date": "2024-04-29T07:00:00.000Z",
        "voteCount": 1,
        "content": "It's D.\nOption B Simply do not prevent problem to happen again. It's not possible to resize the FSx Size after creation so option D is more suitable."
      },
      {
        "date": "2023-12-23T17:55:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-21T08:14:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-07-03T13:27:00.000Z",
        "voteCount": 1,
        "content": "it's B"
      },
      {
        "date": "2023-06-18T13:46:00.000Z",
        "voteCount": 1,
        "content": "keyword ==  update-file-system"
      },
      {
        "date": "2023-05-22T01:07:00.000Z",
        "voteCount": 1,
        "content": "Is it necessary to implement new cloudwatch metric? And using step functions seems to be able to increase storage capacity, according to the following reference. \nhttps://docs.aws.amazon.com/step-functions/latest/dg/supported-services-awssdk.html#supported-services-awssdk-list"
      },
      {
        "date": "2023-06-22T10:42:00.000Z",
        "voteCount": 1,
        "content": "Perhaps the metric is used to trigger the step functions"
      },
      {
        "date": "2023-04-30T02:12:00.000Z",
        "voteCount": 2,
        "content": "B. Increasing capacity using the update-file-system command is not applicable to FSx for Windows File Server. The command is for Amazon EFS, not FSx for Windows File Server."
      },
      {
        "date": "2023-05-21T06:46:00.000Z",
        "voteCount": 4,
        "content": "StorageCapacity\nUse this parameter to increase the storage capacity of an FSx for Windows File Server, FSx for Lustre, FSx for OpenZFS, or FSx for ONTAP file system. Specifies the storage capacity target value, in GiB, to increase the storage capacity for the file system that you're updating.\nhttps://docs.aws.amazon.com/fsx/latest/APIReference/API_UpdateFileSystem.html\nExample using the CLI\naws fsx update-file-system --file-system-id fs-0123456789abcdef0 --storage-capacity 10240"
      },
      {
        "date": "2023-04-30T00:55:00.000Z",
        "voteCount": 3,
        "content": "B\nAs you need additional storage, you can increase the storage capacity that is configured on your FSx for Windows File Server file system. You can do so using the Amazon FSx console, the Amazon FSx API, or the AWS Command Line Interface (AWS CLI)."
      },
      {
        "date": "2023-04-12T21:56:00.000Z",
        "voteCount": 2,
        "content": "https://chat.openai.com/chat"
      },
      {
        "date": "2023-03-27T00:39:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-01-29T18:25:00.000Z",
        "voteCount": 4,
        "content": "B seems to be the correct answer.\nthe unique possible solution is to add storage capacity using CLI"
      },
      {
        "date": "2023-01-25T03:34:00.000Z",
        "voteCount": 3,
        "content": "To increase the storage capacity for an FSx for Windows File Server file system, use the AWS CLI command update-file-system. https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html It's B."
      },
      {
        "date": "2023-01-16T08:03:00.000Z",
        "voteCount": 2,
        "content": "B is correct. It can prevent issue happen again with EventBridge and Lambda\nA: not make sense at all\nC: Cannot use Step Function to increase capacity\nD: not prevent happen again"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/amazon/view/95564-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient\u2019s signature or a photo of the package with the recipient. The driver\u2019s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.<br><br>As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated.<br><br>A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T11:10:00.000Z",
        "voteCount": 14,
        "content": "C is correct. Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 and using S3 event notifications through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance. \n\nOption A and B still use EC2 instance, which is the source of the problem. Option D requires modification to the handheld devices which is not possible."
      },
      {
        "date": "2023-09-02T17:27:00.000Z",
        "voteCount": 7,
        "content": "I agree that \"C\" is the ideal design.\nBut here the question states that :\n\tEc2 instance is running the SFTP server.\n\tFile is uploaded from handheld devices to a file system in the Ec2 instance.\n\tThe Ec2 instance then adds  metadata to the file.\n\tThe file is then placed in s3.\nThe condition states that:\n\tThe company cannot deploy a new application.\n\nBased on the condition, if I use lambda to add meta data, then its like deploying a new application.\n(We don't know if the application can be seamlessly rewritten in lambda. Will it finish under 15 mins ? etc.,)\nIf we strictly interpret this as not being able to introduce any new logic or components (like a Lambda function for metadata processing), then Option (B) is the answer.\nOption B essentially replaces the FTP server with AWS Transfer Family and uses Amazon EFS as the file storage, which can scale and handle more connections. The existing EC2 instance, which already has the logic for metadata addition, would simply point to this new file path on EFS. This minimizes changes to the existing application logic."
      },
      {
        "date": "2023-10-23T11:38:00.000Z",
        "voteCount": 1,
        "content": "From the app description, I am sure that it should work under 15min."
      },
      {
        "date": "2024-03-17T09:34:00.000Z",
        "voteCount": 2,
        "content": "the text is: \"The handheld devices cannot be modified, so the company cannot deploy a new application\". Following your comment, you can't use neither the AWS Transfer Family. This is also new :D"
      },
      {
        "date": "2024-03-24T04:16:00.000Z",
        "voteCount": 1,
        "content": "B is the best answer. The system is such that each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. This means that we need a file storage that the data are stored hierarchically in a top-down network of folders. And a file system that has adaptive throughtput to resolve the dropped connections and memory issues. EFS will be the suitable solution component. S3 however has all the data stored on the same flat plane requiring more comprehensive metadata (labels) to make it manageable."
      },
      {
        "date": "2024-03-21T23:27:00.000Z",
        "voteCount": 1,
        "content": "It says \"so the company cannot deploy a new application\".\n\nThis means that it's the handheld devices they can't deploy a new application into. While B works, It still relies on one EC2 instance, which is a part of the problem."
      },
      {
        "date": "2024-03-17T09:36:00.000Z",
        "voteCount": 1,
        "content": "C, transfer family + S3"
      },
      {
        "date": "2024-02-03T18:45:00.000Z",
        "voteCount": 2,
        "content": "C.\nA: No. FTP is not HTTP / HTTPS. FTP -&gt; NLB. HTTP / HTTPS -&gt; ALB.\nB: No. This needs extra steps (DataSync?) to move to S3, and the billing team would still complain about not always updated since it will be certain lag-behind time.\nC: Correct.\nD: No. S3 event notification can directly trigger Lambda."
      },
      {
        "date": "2023-12-31T17:42:00.000Z",
        "voteCount": 1,
        "content": "C. does not require handheld device to be changed. And it solves EC2 dropped Conection by uisng S3."
      },
      {
        "date": "2023-12-23T18:00:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-10-18T11:51:00.000Z",
        "voteCount": 3,
        "content": "The answer is A.\n\n Q: Can I use FTP with an internet-facing endpoint?\n\nA: No, when you enable FTP, you will only be able to use VPC hosted endpoint\u2018s internal access option. If traffic needs to traverse the public network, secure protocols such as SFTP or FTPS should be used.\n\nSource: https://aws.amazon.com/aws-transfer-family/faqs/"
      },
      {
        "date": "2024-03-02T01:17:00.000Z",
        "voteCount": 1,
        "content": "ALB is a load balancer that operates at Layer 7. Only HTTP and HTTPS can be used as ALB protocols.\nTherefore, it is not possible to set ALB at the front of the FTP server."
      },
      {
        "date": "2023-07-21T08:42:00.000Z",
        "voteCount": 1,
        "content": "This one of those tricky questions. I'm not sure if to go with A or C"
      },
      {
        "date": "2023-07-11T21:09:00.000Z",
        "voteCount": 1,
        "content": "IDK yall, it does say clearly \"cannot deploy a new application\" and the only instance of that is A.\n\nI Agree C is better but IDK the semantics here"
      },
      {
        "date": "2023-07-03T13:31:00.000Z",
        "voteCount": 1,
        "content": "its a c"
      },
      {
        "date": "2023-06-22T10:51:00.000Z",
        "voteCount": 2,
        "content": "Since AWS Transfer Family supports Amazon S3 Access Point then it's a standard scenario - FTP-&gt;S3-&gt;Event-&gt;Lambda. Scalable and serverless"
      },
      {
        "date": "2023-06-19T17:17:00.000Z",
        "voteCount": 1,
        "content": "olabiba.ai says C.\n\n\n1. Scalability: By using AWS Transfer Family to create an FTP server that places the files directly in Amazon S3, you can leverage the scalability and durability of S3. S3 is designed to handle high volumes of data and can scale seamlessly as your company expands.\n\n2. Reliability: With S3 as the destination for the files, you can ensure that the archive always receives the files. S3 provides high durability and availability, reducing the chances of data loss.\n\n3. System updates: By using an S3 event notification through Amazon SNS, you can trigger an AWS Lambda function whenever a new file is uploaded to S3. This Lambda function can then add the necessary metadata and update the delivery system, ensuring that the central system is always updated.\n\n4. No modification to handheld devices: Since the handheld devices cannot be modified, this solution allows the devices to continue uploading files through FTP. The only change is the destination, which is now the S3 bucket."
      },
      {
        "date": "2023-03-27T00:40:00.000Z",
        "voteCount": 3,
        "content": "C is the most efficient"
      },
      {
        "date": "2023-01-29T18:30:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-01-16T08:04:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/amazon/view/95565-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.<br><br>Which solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Aurora Replica in a different Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS DataSync for continuous replication of the data to a different Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T11:11:00.000Z",
        "voteCount": 8,
        "content": "A is correct. Provision an Aurora Replica in a different Region will meet the requirement of the application being able to recover to a separate AWS Region in the event of an application failure, and no data can be lost, with the least amount of operational overhead.\n\nB. AWS DataSync can replicate data, but it is not a fully managed service and requires more configuration and management.\n\nC. AWS DMS is a fully managed service for migrating data between databases, but it may require additional configuration and management to continuously replicate data in real-time.\n\nD. Amazon DLM can be used for scheduling snapshots, but it does not provide real-time replication and may not meet the requirement of no data loss in case of a failure."
      },
      {
        "date": "2023-12-23T18:04:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-07-03T14:24:00.000Z",
        "voteCount": 2,
        "content": "its an A"
      },
      {
        "date": "2023-05-20T10:21:00.000Z",
        "voteCount": 3,
        "content": "When you provision an Aurora Replica in a different AWS Region, the replica is kept in sync with the primary database using Aurora's replication capabilities. In the event of a failure in the primary Region, you can promote the Aurora Replica to become the new primary database, which allows you to continue operations with no data loss.\n\nHowever, provisioning and maintaining an Aurora Replica in a different AWS Region requires ongoing management and monitoring to ensure that it stays in sync with the primary database"
      },
      {
        "date": "2023-03-27T00:41:00.000Z",
        "voteCount": 4,
        "content": "Replica"
      },
      {
        "date": "2023-03-10T22:15:00.000Z",
        "voteCount": 4,
        "content": "B,C are on premises usecase solutions. D is wrong because 5 minute worth of data could be lost against the requirement. So A is correct. In fact replica works as standby if primary DB fails."
      },
      {
        "date": "2023-01-29T18:34:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2023-01-16T08:08:00.000Z",
        "voteCount": 2,
        "content": "A is correct\nB: cannot use DataSync for Aurora backup\nC: too complex\nD: DLM is for EBS backup. Here use managed Aurora server, no access to EBS"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/amazon/view/95566-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.<br><br>Which solutions will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 39,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T00:00:00.000Z",
        "voteCount": 21,
        "content": "Extract Data from S3 + mask + Send to another S3 + Transform/Process + Load into S3\nAll these are ETL, ELT tasks which should ring Glue\n\nEMR is more focused on big data processing frameworks such as Hadoop and Spark, \nwhile Glue is more focused on ETL, More over 5000 records every 15 minutes is not soo big data..So I choose C"
      },
      {
        "date": "2023-04-11T04:54:00.000Z",
        "voteCount": 2,
        "content": "EMR and Glue are the same; Glue is managed cluster by AWS , EMR customer manages the clutster"
      },
      {
        "date": "2023-01-16T11:15:00.000Z",
        "voteCount": 7,
        "content": "C is correct. It will process the data in batch mode using Glue ETL job which can handle large amount of data and can be scheduled to run periodically. This solution is also easily expandable for future feeds.\n\nA: It uses multiple Lambda functions, SQS queue and S3 temporary location which will increase operational overhead.\nB: Using Fargate may not be the most cost-effective solution and also it may not handle large amount of data.\nD: Athena and EMR both are powerful tools but they are more complex and can be more costly than Glue."
      },
      {
        "date": "2023-12-23T18:08:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-10-05T08:31:00.000Z",
        "voteCount": 2,
        "content": "Option C is the most suitable solution for the described scenario:\n\n1) AWS Glue Crawler and Custom Classifier: Use AWS Glue to create a crawler and custom classifier to understand and catalogue the data feed formats. This step ensures that AWS Glue can work with the incoming data effectively.\n\n2) AWS Glue ETL Job: Create an AWS Lambda function that triggers an AWS Glue ETL job when a new data file is delivered. This ETL job can perform the required transformation, including masking, field removal, and converting records to JSON format. AWS Glue is a suitable service for data preparation and transformation.\n\n3) Output to S3 Bucket.\n\nThis approach is scalable, easily expandable to handle additional feeds in the future, and leverages AWS Glue's capabilities for data transformation and processing. It also maintains a clear separation of tasks, making it a robust and efficient solution for the given requirements."
      },
      {
        "date": "2023-09-10T01:17:00.000Z",
        "voteCount": 1,
        "content": "C is the good option EMR(Big data, Spark, Hadoop) is for near real-time data processing and it isn't a good fit in this case"
      },
      {
        "date": "2023-07-03T14:29:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-06-18T13:52:00.000Z",
        "voteCount": 1,
        "content": "EMR is big data but not is need in this case \nthen AWS Glue + Lambdas + S3 is good option \nC"
      },
      {
        "date": "2023-03-22T06:56:00.000Z",
        "voteCount": 2,
        "content": "C makes the most sense."
      },
      {
        "date": "2023-02-06T12:05:00.000Z",
        "voteCount": 1,
        "content": "The question is at what point Athena and EMR are a better choice because it is a lot of data to store and process"
      },
      {
        "date": "2023-03-03T08:26:00.000Z",
        "voteCount": 1,
        "content": "That, I agree. Honestly, I will use it from day one, regardless."
      },
      {
        "date": "2023-01-29T18:40:00.000Z",
        "voteCount": 4,
        "content": "C is correct."
      },
      {
        "date": "2023-01-16T08:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/amazon/view/95568-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.<br><br>Which solution will achieve the company's goal with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 67,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T00:54:00.000Z",
        "voteCount": 26,
        "content": "Tricky one. This is not an on premise migration use case which prompts for answer C. Its a current situation of on premise application which the company wants to continue its state in the requirement of using AWS as DR solution.\nhttps://docs.aws.amazon.com/images/drs/latest/userguide/images/drs-failback-arc.png\nhttps://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html"
      },
      {
        "date": "2023-12-10T02:04:00.000Z",
        "voteCount": 3,
        "content": "I also agreed with the answer but then see this \"The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store\" just database and physical server has other applications which not mentioned. Also from DR the statement gets changed to Migrate"
      },
      {
        "date": "2023-03-11T00:58:00.000Z",
        "voteCount": 4,
        "content": "Moreover, B has least operational over head of just initiating DR solution with replicating agents. C has operational overhead with DMS , SCT ,CDC,migration etc"
      },
      {
        "date": "2023-01-27T21:47:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html\nhttps://docs.aws.amazon.com/drs/latest/userguide/recovery-workflow-gs.html\nOption C is wrong. That just mentions the migration method. I think this question asks us the DR architecture between on-premises and AWS cloud."
      },
      {
        "date": "2024-01-23T08:44:00.000Z",
        "voteCount": 3,
        "content": "A = to use AWS DRS you first need to set it up in each AWS Region in which you want to use it. installing AWS Replication agent is not enough\nB = correct (to me the sentence \"Frequently perform failover and fallback from the most recent point in time\" is ambiguous as this points to actual failover/failback and not to drills) \nC =  SCT is not needed wwith same engine db migration. also, install the rest of the software is not enough for app DR\nD = Volume Gateway can be used in a Back and Restore DR scenario, but the option D is very confused. Anyway, Storage Gateway for DR requires more overhead with respect to AWS DRS"
      },
      {
        "date": "2023-12-23T18:15:00.000Z",
        "voteCount": 1,
        "content": "Option B is right option. \nOption C only addresses DB instance replication and DR, it  does not meet  requirements of replicating other applications running on on-premise."
      },
      {
        "date": "2023-12-10T02:05:00.000Z",
        "voteCount": 1,
        "content": "Selected answer C changed from B\n\nThe application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store."
      },
      {
        "date": "2023-11-16T01:26:00.000Z",
        "voteCount": 1,
        "content": "Elastic Disaster Recovery does the job"
      },
      {
        "date": "2023-09-03T07:39:00.000Z",
        "voteCount": 1,
        "content": "C\nWe are looking for a Business Continuity Solution\nMeaning RTO should be low"
      },
      {
        "date": "2023-09-11T05:13:00.000Z",
        "voteCount": 1,
        "content": "but how is failover happening\nthe very own purpose of DR is its automatic failover which is supported by option B"
      },
      {
        "date": "2023-09-03T07:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nQuestions mentions \"least operational overhead\" (efforts in the future), and B mentions \"Frequently performing...\".\nHowever, that is the best-practice for AWS DR (as misleading as it sounds):\nhttps://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html"
      },
      {
        "date": "2023-08-28T23:01:00.000Z",
        "voteCount": 3,
        "content": "the question is a bit misleading, first part says \"company is planning for business continuity\" the later part of the sentence says \"applications are migrating\". \nnevertheless, we should focus on the word business continuity. Going by that \"no migration\" is required so choose B.\n\nthat is my analysis."
      },
      {
        "date": "2023-07-03T14:46:00.000Z",
        "voteCount": 1,
        "content": "B for BC"
      },
      {
        "date": "2023-06-18T13:53:00.000Z",
        "voteCount": 1,
        "content": "keyword = AWS Elastic Disaster Recovery \nB"
      },
      {
        "date": "2023-05-21T08:05:00.000Z",
        "voteCount": 2,
        "content": "The company is looking for a disaster recovery solution and not a full migration to cloud. In my view the answer should use Elastic Disaster Recovery and not DMS. \nReferences\nhttps://www.cloudthat.com/resources/blog/scalable-cost-effective-cloud-disaster-recovery-with-aws-drs-elastic-disaster-recovery\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/080af3a5-623d-4147-934d-c8d17daba346/en-US/introduction\nhttps://docs.aws.amazon.com/pt_br/mgn/latest/ug/Network-Settings-Video.html"
      },
      {
        "date": "2023-04-29T07:01:00.000Z",
        "voteCount": 3,
        "content": "it appears that option C has the least operational overhead since it involves creating AWS DMS replication servers and a target Amazon Aurora MySQL DB cluster to host the database, creating a DMS replication task to copy existing data to the target DB cluster, creating a local AWS SCT CDC task to keep data synchronized, and installing the rest of the software on EC2 instances by starting with a compatible base AMI. The other options involve additional steps such as setting up replication for all servers (option A), initializing AWS Elastic Disaster Recovery and frequently performing failover and fallbacks (option B), or deploying an AWS Storage Gateway Volume Gateway and mounting volumes on all on-premises servers (option D)."
      },
      {
        "date": "2023-04-09T11:48:00.000Z",
        "voteCount": 1,
        "content": "C seems correct to me (DMS with SCT and CDC)"
      },
      {
        "date": "2023-03-22T06:59:00.000Z",
        "voteCount": 3,
        "content": "B has less operational overhead."
      },
      {
        "date": "2023-03-19T04:49:00.000Z",
        "voteCount": 2,
        "content": "B, tricky"
      },
      {
        "date": "2023-02-26T11:43:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/disaster-recovery/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/amazon/view/95569-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-01T15:47:00.000Z",
        "voteCount": 7,
        "content": "Option B is the best solution. This solution creates an IAM role that trusts the auditors' AWS account and attaches the required IAM policies to the role. This ensures that the auditors have read-only access to the company's AWS account while ensuring that the company's AWS account is secure and complies with AWS security best practices. Additionally, the unique external ID assigned to the role's trust policy adds an extra layer of security."
      },
      {
        "date": "2024-02-22T20:26:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html"
      },
      {
        "date": "2024-02-22T20:23:00.000Z",
        "voteCount": 1,
        "content": "To create an IAM role that trusts the auditors' AWS account, you can do the following:\nSign in to the AWS Management Console and open the IAM console.\nIn the navigation pane, choose Roles, and then choose Create role.\nChoose the Custom trust policy role type.\nIn the Custom trust policy section, enter or paste the following trust policy:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::&lt;auditor-account-id&gt;:root\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}"
      },
      {
        "date": "2023-12-23T18:18:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-09-10T01:25:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-07-04T09:15:00.000Z",
        "voteCount": 1,
        "content": "its a b"
      },
      {
        "date": "2023-03-27T00:46:00.000Z",
        "voteCount": 3,
        "content": "In the company's AWS account, create an IAM role that trusts the auditors' AWS account."
      },
      {
        "date": "2023-01-29T18:45:00.000Z",
        "voteCount": 3,
        "content": "B seems to be the right answer"
      },
      {
        "date": "2023-01-16T12:53:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.\n\nThis solution meets the requirement of providing the external auditors with secure, read-only access to the company's AWS account while also complying with AWS security best practices. In this solution, an IAM role is created that trusts the auditors' AWS account and has an IAM policy with the required permissions attached to it. The role's trust policy should include a unique external ID for added security. This allows the external auditors to assume the role and access the resources with the permissions specified in the policy, without the need to share access keys or create individual IAM users for each auditor."
      },
      {
        "date": "2023-01-16T12:54:00.000Z",
        "voteCount": 2,
        "content": "Option A is incorrect because it grants access to all resources in the company's AWS account and does not provide a way to restrict the permissions that the external auditors have.\n\nOption C is incorrect because it creates an IAM user in the company's account and shares the API access keys with the external auditors, which is not secure and does not comply with AWS security best practices.\n\nOption D is incorrect because it creates an IAM user in the company's account for each auditor, which would be tedious and difficult to manage for the company. It would be more secure and efficient to use an IAM role that trusts the auditors' AWS account instead of creating individual users for each auditor."
      },
      {
        "date": "2023-01-16T08:20:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/amazon/view/95602-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.<br><br>Which solution will meet these requirements with the LEAST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-27T22:06:00.000Z",
        "voteCount": 19,
        "content": "3 nodes are required for a DAX cluster to be fault-tolerant.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.cluster.html"
      },
      {
        "date": "2023-08-24T03:31:00.000Z",
        "voteCount": 12,
        "content": "This is a poorly framed question with very little attention to how applications are architected in real life. Here's my reasoning:\nThis being a trading platform, you have a high volume of writes and reads, and stale data is essentially worse than useless. This automatically eliminates all but A, because of the way DAX performs. DAX caches data from the first query, and subsequent queries will continue to receive that cached data regardless of whether it has been updated in DynamoDB. This behavior continues till cache eviction. The only way around it is to read and write data using DAX.\nHere's the curveball - the solution must be HA, which eliminates A and D, leaving only B &amp; C. And between B &amp; C, you really want to use DAX for reading and DynamoDB for writing. So final answer is B - if you want to get certified.\nApplying this solution in real world however will cause you a lot of pain and grief!"
      },
      {
        "date": "2023-11-27T07:19:00.000Z",
        "voteCount": 2,
        "content": "Cahing in DAX is always write through. Correct answer is B."
      },
      {
        "date": "2023-09-14T10:43:00.000Z",
        "voteCount": 1,
        "content": "Totally agree. \n\nBut an additional issue with the question is the fact that it requires High Availability, not Fault Tolerance. These are quite different concepts and, at least up to this point, there would be no need for 3x DAX instances (in theory)."
      },
      {
        "date": "2024-10-02T18:47:00.000Z",
        "voteCount": 1,
        "content": "B is Correct.\n- To achieve high availability for your application, we recommend that you provision your DAX cluster with at least three nodes. Ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html#DAX.consistency.nodes\n- If the request specifies eventually consistent reads (the default behavior), it tries to read the item from DAX. \n- With these operations, data is first written to the DynamoDB table, and then to the DAX cluster. The operation is successful only if the data is successfully written to both the table and to DAX.\nRef: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html#DAX.concepts.request-processing"
      },
      {
        "date": "2024-02-11T19:23:00.000Z",
        "voteCount": 3,
        "content": "DAX is cache and can only be used to read so A and C are out.\nBetween B and D the question says Highly Available so we will select B (three node) instead of D (single node).\n\nSo correct answer B"
      },
      {
        "date": "2024-01-23T09:41:00.000Z",
        "voteCount": 1,
        "content": "A = 2 nodes DAX is not fault-tolerant\nB = correct (write-around strategy ensure lower latency)\nC = write-through strategy can have higher latency\nD = 1 node DAX is not fault-tolerant"
      },
      {
        "date": "2023-12-23T18:24:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best option. Though Option A is also possible solution."
      },
      {
        "date": "2023-12-11T10:20:00.000Z",
        "voteCount": 1,
        "content": "The breakpoint is latency.\n\nYou write throught DAX, but for latency sensitive apps, AWS instruct write directly on DynamoDB instead on DAX.\n\n\"For applications that are sensitive to latency, writing through DAX incurs an extra network hop. So a write to DAX is a little slower than a write directly to DynamoDB. If your application is sensitive to write latency, you can reduce the latency by writing directly to DynamoDB instead. For more information, see Write-around.\"\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html#DAX.consistency.strategies-for-writes"
      },
      {
        "date": "2023-12-01T13:07:00.000Z",
        "voteCount": 1,
        "content": "Option A would be the least latency solution for this use case. Using a two node DAX cluster with the application reading and writing via DAX provides:\n\nCaching of both reads and writes within the DAX cluster nodes. This eliminates the need to go directly to DynamoDB for reads and writes, reducing latency.\n\nRedundancy with two nodes to ensure high availability of the cache.\n\nThe other options would lead to some reads or writes still going directly to DynamoDB rather than being fully served from the lower latency cached data in DAX. This could increase latency compared to option A. A single node DAX cluster would work but lacks the redundancy needed for high availability.\n\nDAX is fully managed, in-memory cache for DynamoDB that delivers low-latency data access. By caching the entire dataset in-memory across nodes, it can serve requests much faster than going to the DynamoDB tables on every request. The AWS documentation provides more details on how to configure DAX and monitor latency metrics."
      },
      {
        "date": "2023-10-03T16:44:00.000Z",
        "voteCount": 1,
        "content": "Question only ask for High Availability, not Fault Tolerant. You need 3 nodes only for the latter. You must write through to keep data getting stale as mentioned by Ganshank. I would go with two-node cluster as strong consistency adds extra latency as number of clusters increase. So for this question best answer should be A."
      },
      {
        "date": "2023-09-10T01:34:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct: DAX is also used for caching so it improves the performance and for production 3 nodes are strongly recommended so i ll go with B."
      },
      {
        "date": "2023-09-06T19:39:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/amazon-dynamodb-accelerator-dax-a-read-throughwrite-through-cache-for-dynamodb/"
      },
      {
        "date": "2023-09-06T19:38:00.000Z",
        "voteCount": 1,
        "content": "sorry guys a is wrong ans: B is correct ans Important\nFor production usage, we strongly recommend using DAX with at least three nodes, where each node is placed in different Availability Zones. Three nodes are required for a DAX cluster to be fault-tolerant.\n\nA DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data."
      },
      {
        "date": "2023-09-06T19:30:00.000Z",
        "voteCount": 1,
        "content": "A is Ans : \nRead replicas serve two additional purposes:\n\nScalability. If you have a large number of application clients that need to access DAX concurrently, you can add more replicas for read-scaling. DAX spreads the load evenly across all the nodes in the cluster. (Another way to increase throughput is to use larger cache node types.)\n\nHigh availability. In the event of a primary node failure, DAX automatically fails over to a read replica and designates it as the new primary. If a replica node fails, other nodes in the DAX cluster can still serve requests until the failed node can be recovered. For maximum fault tolerance, you should deploy read replicas in separate Availability Zones. This configuration ensures that your DAX cluster can continue to function, even if an entire Availability Zone becomes unavailable."
      },
      {
        "date": "2023-09-03T07:55:00.000Z",
        "voteCount": 2,
        "content": "A\nOnce u enable DAX you cant directly write onto  or Read from Dynamo DB."
      },
      {
        "date": "2023-07-23T12:27:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-14T07:18:00.000Z",
        "voteCount": 1,
        "content": "AWS recommend 3 nodes for production workloads.\nSo it must B"
      },
      {
        "date": "2023-07-04T09:19:00.000Z",
        "voteCount": 1,
        "content": "B for DAX HA"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/amazon/view/95570-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application.<br><br>The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day.<br><br>A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the application frontend to a static website that is hosted on Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange all the backend EC2 instances to Spot Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-16T01:46:00.000Z",
        "voteCount": 10,
        "content": "Burstable instances let you save costs, you pay for some baseline - say 40 percent, if the instance is utilized less - credits get accumulated. So, it is good for workloads with changing CPU loads."
      },
      {
        "date": "2023-02-26T11:57:00.000Z",
        "voteCount": 5,
        "content": "Burstable EC2 instances, also known as T instances, provide a baseline level of CPU performance with the ability to burst CPU usage when additional cycles are available. They are designed for workloads that do not require sustained high CPU performance but occasionally need more CPU power. Burstable instances can be a cost-effective option for workloads that have moderate CPU requirements but still require flexibility to handle occasional spikes in demand."
      },
      {
        "date": "2024-05-19T11:04:00.000Z",
        "voteCount": 1,
        "content": "Uhm, S3 static website with a Python backend? Am I missing something? How can S3 interact with a backend?"
      },
      {
        "date": "2024-08-24T01:14:00.000Z",
        "voteCount": 1,
        "content": "just B,E"
      },
      {
        "date": "2023-12-23T22:11:00.000Z",
        "voteCount": 1,
        "content": "Option B and E"
      },
      {
        "date": "2023-07-04T09:21:00.000Z",
        "voteCount": 3,
        "content": "it's BE"
      },
      {
        "date": "2023-05-21T08:42:00.000Z",
        "voteCount": 4,
        "content": "You cannot move all backend to Spot Instances this will break the requirement for not affecting the application availability. \nYou can improve by moving the static site to S3, front end, and change the on demand instances to burst capacity."
      },
      {
        "date": "2023-04-08T01:34:00.000Z",
        "voteCount": 2,
        "content": "Amazon EC2 Spot Instances allow you to take advantage of unused EC2 capacity in the AWS Cloud at a steep discount compared to On-Demand Instance prices. Spot Instances are well-suited for workloads that can be interrupted, such as batch processing, data analysis, and image or video processing. They can also be used for fault-tolerant workloads that can withstand the loss of an instance, such as web services or stateless applications."
      },
      {
        "date": "2023-04-08T01:34:00.000Z",
        "voteCount": 1,
        "content": "Option C suggests deploying the application frontend using AWS Elastic Beanstalk and using the same instance type for the nodes. Elastic Beanstalk is a fully managed service that makes it easy to deploy, run, and scale applications. It automatically handles the deployment and management of the underlying infrastructure, including capacity provisioning, load balancing, and auto-scaling. However, using Elastic Beanstalk with the same instance type as the existing EC2 instances may not necessarily reduce costs."
      },
      {
        "date": "2023-04-08T01:34:00.000Z",
        "voteCount": 2,
        "content": "Option E suggests deploying the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances. Burstable instances provide a baseline level of CPU performance with the ability to burst above the baseline when needed. This can be a cost-effective option for workloads that have variable CPU usage and can benefit from the ability to burst during periods of high demand. However, if the workload consistently requires high CPU usage, using burstable instances may not provide significant cost savings compared to using larger general purpose instances."
      },
      {
        "date": "2023-03-27T01:11:00.000Z",
        "voteCount": 1,
        "content": "BE makes the most sense here"
      },
      {
        "date": "2023-03-11T15:32:00.000Z",
        "voteCount": 5,
        "content": "Burstable because peak performance is needed at lunch time and its cost effective based on this - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\nS3 static website hosting is cost effective"
      },
      {
        "date": "2023-02-01T16:01:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B, E.\nOption B of moving the frontend to a static website hosted on Amazon S3 will reduce the cost of running the frontend, as S3 is a lower cost storage option than EC2 instances.\nOption E of deploying the backend Python application to general purpose burstable EC2 instances will ensure that the backend EC2 instances have the capacity to handle spikes in usage, as burstable instances are designed to handle unpredictable workloads. This will help to optimize the cost of running the backend, as burstable instances are less expensive than On-Demand instances and more cost-effective than Spot instances."
      },
      {
        "date": "2023-01-27T22:37:00.000Z",
        "voteCount": 4,
        "content": "B and E.\nOption D is wrong. A spot instance is not appropriate for a production server.\nBy the way, I would like another option that mentions changing the backend Python API Gateway and Lambda because Option B mentions changing the frontend serverless. I think this question is a typical use case of the serverless architecture."
      },
      {
        "date": "2023-01-24T19:07:00.000Z",
        "voteCount": 2,
        "content": "Correct answers are\nB &amp; E\nOption B as S3 is a cost-effective storage solution for static websites.\nOption E as burstable general-purpose instances provides a cost-effective solution for this kind of workload."
      },
      {
        "date": "2023-01-16T13:00:00.000Z",
        "voteCount": 4,
        "content": "B. Move the application frontend to a static website that is hosted on Amazon S3.\nD. Change all the backend EC2 instances to Spot Instances.\n\nStep 1: Moving the application frontend to a static website that is hosted on Amazon S3 will reduce the cost and increase the scalability of the application. S3 is a highly scalable object storage service that can handle large amounts of data and traffic at a lower cost than running EC2 instances.\n\nStep 2: Changing the backend EC2 instances to Spot Instances can help reduce cost without negatively affecting the application availability. Spot Instances allow customers to bid on unused Amazon EC2 capacity, which can result in significant cost savings. You can also use AWS Auto Scaling to automatically increase or decrease the number of Spot Instances based on the application's traffic."
      },
      {
        "date": "2023-01-16T13:00:00.000Z",
        "voteCount": 1,
        "content": "Option A, C: Changing to compute optimized instances or using Elastic Beanstalk will not help reducing the cost, it will only change the instances type and not helping the cost optimization.\nOption E: Deploying the backend Python application to general purpose burstable EC2 instances will not help reducing the cost, as it still using On-Demand instances.\n\nIt is important to note that using spot instances comes with the risk of instances being terminated when the spot price goes up. To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available."
      },
      {
        "date": "2023-01-16T08:28:00.000Z",
        "voteCount": 2,
        "content": "BE are correct\nA: Compute optimized instance is expensive than burstable instance\nB: S3 hosted static web server is cheaper\nC: Not save money\nD: Spot instance affect availibility\nE: Burstable EC2 is cheaper"
      },
      {
        "date": "2023-01-17T10:33:00.000Z",
        "voteCount": 1,
        "content": "To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/amazon/view/95572-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.<br><br>The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.<br><br>Which solution will provide the MOST cost-effective setup for the platform?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-27T23:05:00.000Z",
        "voteCount": 19,
        "content": "Option A, C and D are wrong. They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate.\nhttps://aws.amazon.com/savingsplans/compute-pricing/"
      },
      {
        "date": "2023-01-16T08:38:00.000Z",
        "voteCount": 12,
        "content": "B is correct. Compute saving plan will also cover Fargate\nA: use spot instance is not reliable\nCD: manually scale up DB"
      },
      {
        "date": "2024-07-28T01:44:00.000Z",
        "voteCount": 1,
        "content": "D looks more correctable\nMainly diff. between B and D is \npredicable workload-- all upfront\nno specific for read-replication traffic\nOn-Demand Capacity Reservations ensure availability during peak times without long-term commitments. but no cost-effective"
      },
      {
        "date": "2024-08-24T01:31:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-05-06T00:57:00.000Z",
        "voteCount": 4,
        "content": "It's between b and d. D is more cost effective because of spot instances. And B is wrong because there is no reason to scale read replicas for RDS (the question doesn't say read only load)"
      },
      {
        "date": "2024-03-06T11:56:00.000Z",
        "voteCount": 4,
        "content": "I really don't understand why people are saying that Spot instances aren't suitable for production. There is a two-minute respite before they shut down, and since the application is not said to be stateful, this is plenty of time for a single request/response cycle.\n\nWith this in mind, the correct solution is D."
      },
      {
        "date": "2024-04-04T10:34:00.000Z",
        "voteCount": 2,
        "content": "slightly difference betwee B and D {other than spot instances ofcourse}. Since the platform experinces peaks, might be a better idea to go for savings plan with medium load"
      },
      {
        "date": "2024-02-11T19:33:00.000Z",
        "voteCount": 2,
        "content": "A and C: The company will have a mix of EKS on EC2 and EKS Fargate hence reserved instance is not possible as it will cover only EKS on EC2 hence A and C are out\n\nBetween B and C:\nC seems to save the most cost, but during peak load spot instances (both EC2 or Fargate) will not provide guaranteed availability. Hence we should go ahead with B.\n\nCorrect Answer: B"
      },
      {
        "date": "2024-02-10T11:21:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. Everything about B is wrong: Compute savings plan is more expensive than RI, on demand more expensive than spot for peaks and no upfront more exponsive than all upfront."
      },
      {
        "date": "2024-01-23T23:06:00.000Z",
        "voteCount": 4,
        "content": "The scenario ask for the most cost-effective setup. Thus:\nA = RI doesn't cover Fargate\nB = ODCR doesn't bring cost benefits, they just ensure you have capacity. Read replicas are for read only, I would expect workload peaks includes writes so this is not saving money nor fully helping with capacity needs\nC = EC2 Saving Plans do not cover Fargate\nD = correct (this is the most cost-effective setup, Compute Savings Plans apply to both EC2 and Fargate, Spot Instances applies to both EC2 and Fargate, All Upfront Reserved Instances is most cost effective option for RDS. Manually scaling RDS adds a lot of overhead, but this is not the point of the question)"
      },
      {
        "date": "2024-01-23T23:14:00.000Z",
        "voteCount": 2,
        "content": "Also, for a temporarily limited change it is easier to manually vertically scale your instance rather than adding Read replicas as adding replicas to a single instance requires to change your app to send read requests to the reader endpoint and not to the cluster (aka writer) endpoint"
      },
      {
        "date": "2023-12-27T08:51:00.000Z",
        "voteCount": 1,
        "content": "I might be leaning toward D as it does ask for the. most cost-effective solution"
      },
      {
        "date": "2023-12-23T22:30:00.000Z",
        "voteCount": 7,
        "content": "Compute saving plans are more cost effective so B or D  are right two options.\nBetween B and D - Spot instances offers better cost and Fargate supports spot instances\nhttps://aws.amazon.com/blogs/aws/aws-fargate-spot-now-generally-available/\nOption B says, scale RDS Read-Replica for based on events which may not work as workload description does not mentioned that peak load is only read traffic. So D is best and most cost effective solution."
      },
      {
        "date": "2023-07-20T06:00:00.000Z",
        "voteCount": 2,
        "content": "I am leaning towards C, since Instance savings provide the biggest discount.\nI also couldn't find a way to scale EKS based on dates, which B suggests: \"Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks\""
      },
      {
        "date": "2023-07-04T09:23:00.000Z",
        "voteCount": 2,
        "content": "its a b"
      },
      {
        "date": "2023-06-18T15:05:00.000Z",
        "voteCount": 4,
        "content": "Option A, C and D are wrong. They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate."
      },
      {
        "date": "2023-06-05T15:54:00.000Z",
        "voteCount": 1,
        "content": "I go with B post reading aws portal.\n\nhttps://aws.amazon.com/savingsplans/compute-pricing/\n\nCompute Savings Plans\n\nCompute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage. For example, with Compute Savings Plans, you can change from C4 to M5 instances, shift a workload from EU (Ireland) to EU (London), or move a workload from EC2 to Fargate or Lambda at any time and automatically continue to pay the Savings Plans price."
      },
      {
        "date": "2023-05-21T11:25:00.000Z",
        "voteCount": 4,
        "content": "Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance [...], and also apply to Fargate or Lambda usage. For example, with Compute Savings Plans, you can [...] move a workload from EC2 to Fargate.\n\nVertical scaling is the most straightforward approach to adding more capacity in your database. [...] You can vertically scale up [or down] your RDS instance with a click of a button.\n\nSuppose that you purchase a db.t2.medium reserved DB instance, [...] if you have one db.t2.large instance running in your account in the same AWS Region, the billing benefit is applied to 50 percent of the usage of the DB instance. \n\nhttps://aws.amazon.com/savingsplans/compute-pricing/\nhttps://aws.amazon.com/blogs/database/scaling-your-amazon-rds-instance-vertically-and-horizontally/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithReservedDBInstances.html"
      },
      {
        "date": "2023-04-19T01:16:00.000Z",
        "voteCount": 1,
        "content": "B\nCompute Savings Plans saving EC2 and Fargate.\nproduction don't using Spot Instances"
      },
      {
        "date": "2023-04-09T12:46:00.000Z",
        "voteCount": 2,
        "content": "A makes sense to me"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/amazon/view/95573-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application.<br><br>Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message.<br><br>A solutions architect creates an Amazon S3 bucket as the first step in the process.<br><br>Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload static informational content to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new CloudFront distribution. Set the S3 bucket as the origin.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuring the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuring the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuring the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket."
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T13:12:00.000Z",
        "voteCount": 16,
        "content": "A. Upload static informational content to the S3 bucket.\nC. Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).\nD. During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.\n\nStep 1: The solutions architect should upload static informational content to the S3 bucket, this content will be shown to the users when the application is down for maintenance.\n\nStep 2: The solutions architect should set the S3 bucket as a second origin in the original CloudFront distribution. To keep the S3 bucket secure, the solutions architect should configure the distribution and the S3 bucket to use an origin access identity (OAI). This will ensure that only CloudFront has access to the S3 bucket."
      },
      {
        "date": "2023-01-16T13:12:00.000Z",
        "voteCount": 5,
        "content": "Step 3: During the weekly maintenance, the solutions architect should edit the default cache behavior of the CloudFront distribution to use the S3 origin. This will redirect all incoming traffic to the S3 bucket and show the static informational content to the users. Once the maintenance is complete, the solutions architect should revert the change back to the original Elastic Beanstalk origin.\n\nOption B: Creating a new CloudFront distribution and setting the S3 bucket as the origin is unnecessary and could cause confusion for the users.\nOption E: During the weekly maintenance, creating a cache behavior for the S3 origin on the new distribution is unnecessary, it is more complex and prone to human error.\nOption F: Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not necessary because CloudFront is already being used as the web request server."
      },
      {
        "date": "2023-12-28T08:37:00.000Z",
        "voteCount": 1,
        "content": "From the given options ACD makes the most sense.\nIn real life the CloudFront feature to show custom error responses might make a lot more sense: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html#custom-error-pages-procedure\nThis would avoid the manual steps and by that is less prone to human errors."
      },
      {
        "date": "2023-12-23T22:41:00.000Z",
        "voteCount": 1,
        "content": "A, C and D is correct."
      },
      {
        "date": "2023-11-16T02:10:00.000Z",
        "voteCount": 1,
        "content": "CacheBehaviour defines path and origin"
      },
      {
        "date": "2023-07-04T09:27:00.000Z",
        "voteCount": 1,
        "content": "ACD morelikely"
      },
      {
        "date": "2023-06-18T15:07:00.000Z",
        "voteCount": 2,
        "content": "A C D \nE is good option but is more overhead and propone error human then C is more accesible"
      },
      {
        "date": "2023-06-14T07:41:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      },
      {
        "date": "2023-03-27T01:20:00.000Z",
        "voteCount": 2,
        "content": "ACD is the best fit"
      },
      {
        "date": "2023-02-07T12:00:00.000Z",
        "voteCount": 4,
        "content": "About E, the lowest possible value for the \"Origin Priority\" field in AWS CloudFront is 1"
      },
      {
        "date": "2023-01-29T19:17:00.000Z",
        "voteCount": 4,
        "content": "ACD is correct"
      },
      {
        "date": "2023-01-16T08:43:00.000Z",
        "voteCount": 1,
        "content": "ABD is correct"
      },
      {
        "date": "2023-01-17T11:56:00.000Z",
        "voteCount": 2,
        "content": "ACD is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/amazon/view/95574-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.<br><br>The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users.<br><br>A solutions architect needs to simplify this process to minimize disruption to users.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-01T16:20:00.000Z",
        "voteCount": 12,
        "content": "D is correct\nBy using a function alias, the custom application invokes the latest version of the Lambda function without the need to modify the application code every time the company updates the image processing parameters. This reduces the risk of causing interruptions for users."
      },
      {
        "date": "2023-01-16T13:15:00.000Z",
        "voteCount": 5,
        "content": "D. Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.\n\nCreating a Lambda function alias allows the solutions architect to change the version of the Lambda function that the alias points to without modifying the client application. This eliminates the need for frequent updates to the custom application and minimizes disruption to users. The solutions architect can test different parameters by using different versions of the function and reconfigure the alias to point to the new version after validating results. This allows the company to update the image processing parameters without affecting the users."
      },
      {
        "date": "2023-01-16T13:15:00.000Z",
        "voteCount": 2,
        "content": "Option A: Directly modifying the environment variables of the published Lambda function version would cause all clients to use the updated environment variables immediately and would not allow for testing.\nOption B: Using DynamoDB to store image processing parameters increases complexity and operational overhead, and it would not eliminate the need for updating the custom application.\nOption C: Directly coding the image processing parameters within the Lambda function and publishing new versions would not eliminate the need for updating the custom application."
      },
      {
        "date": "2023-12-23T22:47:00.000Z",
        "voteCount": 1,
        "content": "Option D has least operational overhead."
      },
      {
        "date": "2023-12-01T22:51:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html"
      },
      {
        "date": "2023-08-19T08:34:00.000Z",
        "voteCount": 1,
        "content": "Look for ALIAS"
      },
      {
        "date": "2023-07-04T09:29:00.000Z",
        "voteCount": 1,
        "content": "D\nB is ok, but more overhead"
      },
      {
        "date": "2023-06-18T15:08:00.000Z",
        "voteCount": 1,
        "content": "keyword = Lambda ALIAS\nthen D"
      },
      {
        "date": "2023-03-27T01:22:00.000Z",
        "voteCount": 1,
        "content": "Create a Lambda function alias."
      },
      {
        "date": "2023-01-16T08:47:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/amazon/view/95605-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.<br><br>Which solution will meet these requirements with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB\u2019s static IP address. Use a geolocation routing policy to route traffic based on user location.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator\u2019s static IP address to create a record in public DNS for the apex domain.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T19:29:00.000Z",
        "voteCount": 21,
        "content": "No, an apex domain cannot use CNAME records in AWS. This is because of the way DNS resolution works. A CNAME record specifies an alias for a domain name, which points to the canonical name of another domain. However, the DNS standard does not allow CNAME records for apex domains, as they should only have A or AAAA records.\n\nWhen you try to create a CNAME record for an apex domain in AWS Route 53, you will receive an error message indicating that the record set type is not valid for the apex domain. To work around this limitation, you can use an alias record instead."
      },
      {
        "date": "2023-01-16T10:44:00.000Z",
        "voteCount": 10,
        "content": "C is correct\nABD all have CNAME record that is not allowed for apex domain"
      },
      {
        "date": "2023-12-23T22:54:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2023-12-17T08:33:00.000Z",
        "voteCount": 2,
        "content": "C https://aws.amazon.com/blogs/networking-and-content-delivery/solving-dns-zone-apex-challenges-with-third-party-dns-providers-using-aws/"
      },
      {
        "date": "2023-10-05T19:17:00.000Z",
        "voteCount": 1,
        "content": "You can create alias record for apex domain in route 53 However the question is asking about least effort and the client is managing domain internally"
      },
      {
        "date": "2023-09-03T00:33:00.000Z",
        "voteCount": 1,
        "content": "The answer is C"
      },
      {
        "date": "2023-07-04T09:32:00.000Z",
        "voteCount": 1,
        "content": "C\nno CNAME for apex"
      },
      {
        "date": "2023-06-18T15:10:00.000Z",
        "voteCount": 1,
        "content": "A , B no seems because reference geolocation \nD no seems because apex domain with API Gateway ? \nthen C Global Accelerator is good option"
      },
      {
        "date": "2023-06-09T01:11:00.000Z",
        "voteCount": 4,
        "content": "fun fact: CNAME records does not support APEX domain\nwhich simply rules out the options with CNAME in it\nanswer is C"
      },
      {
        "date": "2023-03-27T01:23:00.000Z",
        "voteCount": 3,
        "content": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions."
      },
      {
        "date": "2023-01-16T13:19:00.000Z",
        "voteCount": 5,
        "content": "C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator\u2019s static IP address to create a record in public DNS for the apex domain.\n\nThis solution meets the requirements with the least effort because it uses AWS Global Accelerator, which automatically routes traffic to the optimal endpoint based on health and geography, eliminating the need for manual configuration or additional routing policies. It also eliminates the need to create a CNAME record for the apex domain to point to the ALB or NLB's IP address, which can be less efficient and less reliable."
      },
      {
        "date": "2023-01-16T13:21:00.000Z",
        "voteCount": 1,
        "content": "A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.\nWhile this solution uses Route 53 and geolocation routing, it requires manual configuration and maintenance of the routing policy and could introduce additional latency as traffic is routed through the ALB first.\n\nB. Place a Network Load Balancer (NLB) in front of the ALB. Migrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB\u2019s static IP address. Use a geolocation routing policy to route traffic based on user location.\nThis solution is similar to the first one, but it uses a Network Load Balancer (NLB) instead of an Application Load Balancer (ALB). It has the same downsides as the first solution."
      },
      {
        "date": "2023-01-16T13:21:00.000Z",
        "voteCount": 1,
        "content": "D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.\n\nThis solution uses Amazon API Gateway and AWS Lambda to route traffic, but the round-robin method is not the best way to ensure optimal performance and availability for a multi-region deployment. Additionally, routing traffic through a Lambda function can introduce additional latency.\n\nAWS Global Accelerator is a more efficient solution that automatically routes traffic to the optimal endpoint based on health and geography, eliminating the need for manual configuration or additional routing policies."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/amazon/view/95494-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes.<br><br>A solutions architect needs to simplify the deployment of the solution and optimize for code reuse.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 71,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-16T14:05:00.000Z",
        "voteCount": 42,
        "content": "Don't understand why so many people are choosing B. Read up. A container image cannot be used with Lambda layers. That means A B C are out instantly. Its literally one of the first things they mention about Lamba layers. Answer is D and ABC simply impossible to configure.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html"
      },
      {
        "date": "2024-05-06T07:00:00.000Z",
        "voteCount": 3,
        "content": "You can create a Lambda function from an ECR image, but you CANNOT create a Lambda function layer from an ECR image!"
      },
      {
        "date": "2023-08-08T18:40:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/\n\nPreviously, Lambda functions were packaged only as .zip archives. This includes functions created in the AWS Management Console. You can now also package and deploy Lambda functions as container images.\n\nYou can use familiar container tooling such as the Docker CLI with a Dockerfile to build, test, and tag images locally. Lambda functions built using container images can be up to 10 GB in size. You push images to an Amazon Elastic Container Registry (ECR) repository, a managed AWS container image registry service. You create your Lambda function, specifying the source code as the ECR image URL from the registry."
      },
      {
        "date": "2023-03-10T09:50:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/"
      },
      {
        "date": "2023-02-17T10:06:00.000Z",
        "voteCount": 5,
        "content": "B suggests deploying the shared libraries and custom classes to a Docker image, uploading it to Amazon Elastic Container Registry (Amazon ECR), creating a Lambda layer that uses the Docker image as the source, and deploying the API's Lambda functions as Zip packages. Configuring the packages to use the Lambda layer simplifies deployment, and the Docker image allows for code reuse. This option takes advantage of the built-in features provided by AWS API Gateway and Lambda, making it the optimal solution."
      },
      {
        "date": "2023-02-23T07:30:00.000Z",
        "voteCount": 4,
        "content": "The requirement is code reuse: \nhttps://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/\nLambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsible for packaging your preferred runtimes and dependencies as a part of the container image during the build process."
      },
      {
        "date": "2023-01-28T07:07:00.000Z",
        "voteCount": 8,
        "content": "Option A, B and C are wrong. An AWS Lambda Layer does not support a Docker image or a deployed container as the source.\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\nhttps://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/"
      },
      {
        "date": "2024-08-07T20:24:00.000Z",
        "voteCount": 1,
        "content": "If any of you ever really worked in lambda with docker image, you will instantly choose D without hesitation.\n\nzipped package can be deployed straightaway and it doesn't need a container. Don't get those two things(lambda zip deployment vs lambda container deployment) mixed up"
      },
      {
        "date": "2024-07-23T06:14:00.000Z",
        "voteCount": 1,
        "content": "Please read the requirement, \"simplify the deployment\" with D you need only to maintain the docker image, with B you need to maintain the docker image and the process to deploy the lambda  as ZIP Packages."
      },
      {
        "date": "2023-12-24T06:50:00.000Z",
        "voteCount": 2,
        "content": "Option B is the right one, see: https://docs.aws.amazon.com/lambda/latest/dg/images-create.html"
      },
      {
        "date": "2023-12-23T23:06:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-16T02:28:00.000Z",
        "voteCount": 1,
        "content": "check Iunt's answer"
      },
      {
        "date": "2023-10-14T00:05:00.000Z",
        "voteCount": 2,
        "content": "B. \n* A Lambda layer is a .zip file archive that contains supplementary code or data. Layers usually contain library dependencies, a custom runtime, or configuration files.\n* Lambda functions packaged as container images do not support adding Lambda layers to the function configuration.However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsible for packaging your preferred runtimes and dependencies as a part of the container image during the build process."
      },
      {
        "date": "2023-09-10T01:58:00.000Z",
        "voteCount": 1,
        "content": "Ans is D: https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/#:~:text=Lambda%20functions%20packaged%20as%20container,Lambda%20layers%20with%20container%20images."
      },
      {
        "date": "2023-08-08T18:40:00.000Z",
        "voteCount": 2,
        "content": "Answer B. \nPreviously, Lambda functions were packaged only as .zip archives. This includes functions created in the AWS Management Console. You can now also package and deploy Lambda functions as container images.\n\nYou can use familiar container tooling such as the Docker CLI with a Dockerfile to build, test, and tag images locally. Lambda functions built using container images can be up to 10 GB in size. You push images to an Amazon Elastic Container Registry (ECR) repository, a managed AWS container image registry service. You create your Lambda function, specifying the source code as the ECR image URL from the registry."
      },
      {
        "date": "2023-08-21T23:00:00.000Z",
        "voteCount": 1,
        "content": "https://www.youtube.com/watch?v=17R0vN8bt-0"
      },
      {
        "date": "2023-07-27T15:09:00.000Z",
        "voteCount": 2,
        "content": "Correct B."
      },
      {
        "date": "2023-07-04T09:37:00.000Z",
        "voteCount": 1,
        "content": "D\nlayers not supported w container-based lambdas"
      },
      {
        "date": "2023-06-25T10:28:00.000Z",
        "voteCount": 1,
        "content": "Docker images cannot be used in Lambda layers."
      },
      {
        "date": "2023-06-19T21:17:00.000Z",
        "voteCount": 1,
        "content": "From olabiba.ai: Overall, option B provides a streamlined approach to optimize code reuse by centralizing the shared code in a Docker image and using a Lambda layer to share it across multiple functions."
      },
      {
        "date": "2023-06-05T15:13:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      },
      {
        "date": "2023-05-21T16:26:00.000Z",
        "voteCount": 6,
        "content": "\"Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsible for packaging your preferred runtimes and dependencies as a part of the container image during the build process.\"\nhttps://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/"
      },
      {
        "date": "2023-05-07T00:14:00.000Z",
        "voteCount": 3,
        "content": "Although the following URL says that you can deploy Lambda layers as container but this can't be used when the Lambda function in zip. The function will be created as another layer in the container image and it should use Lambda runtime environment.\nhttps://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/amazon/view/97669-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.<br><br>The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory\u2019s internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers.<br><br>How should the company deploy the ML model to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T12:45:00.000Z",
        "voteCount": 18,
        "content": "Offline operation: AWS IoT Greengrass supports offline operation by enabling devices to continue processing data even when they are disconnected from the internet."
      },
      {
        "date": "2023-02-10T02:28:00.000Z",
        "voteCount": 5,
        "content": "https://aws.amazon.com/blogs/machine-learning/anomaly-detection-with-amazon-sagemaker-edge-manager-using-aws-iot-greengrass-v2/"
      },
      {
        "date": "2023-12-23T23:12:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-09-11T07:54:00.000Z",
        "voteCount": 1,
        "content": "Option B: Greengrass supports offline operation"
      },
      {
        "date": "2023-08-19T09:17:00.000Z",
        "voteCount": 2,
        "content": "Offline = IoT Greengrass"
      },
      {
        "date": "2023-08-19T09:25:00.000Z",
        "voteCount": 1,
        "content": "If you can't commission your sensors\nConsider the following questions.\n\nDoes the mobile phone running the Amazon Monitron App have a stable internet connection?\n\nhttps://docs.aws.amazon.com/Monitron/latest/user-guide/troubleshooting.html\n\nFor commissioning a sensor, the mobile phone running the Amazon Monitron App should have internet connectivity."
      },
      {
        "date": "2023-07-04T09:41:00.000Z",
        "voteCount": 1,
        "content": "B for offline"
      },
      {
        "date": "2023-06-18T15:12:00.000Z",
        "voteCount": 1,
        "content": "keyword = WS IoT Greengrass"
      },
      {
        "date": "2023-05-27T07:49:00.000Z",
        "voteCount": 3,
        "content": "Can't be D.\nAmazon Monitron requires Internet connection.Q: Can I use Amazon Monitron when it is not connected to the AWS Region or in a disconnected environment?\n\nA: Amazon Monitron Sensors and Gateways, and their use with the Amazon Monitron service, rely on connectivity over internet to the AWS Region. \nhttps://aws.amazon.com/monitron/faqs/\nAmazon Monitron Sensors and Gateways are not designed for disconnected operations or environments with no connectivity. We recommend that customers have highly available internet connectivity."
      },
      {
        "date": "2023-05-09T12:30:00.000Z",
        "voteCount": 1,
        "content": "AWS IoT Greengrass is software that extends cloud capabilities to local devices. This enables devices to collect and analyze data closer to the source of information, react autonomously to local events, and communicate securely with each other on local networks. Local devices can also communicate securely with AWS IoT Core and export IoT data to the AWS Cloud. AWS IoT Greengrass developers can use AWS Lambda functions and prebuilt connectors to create serverless applications that are deployed to devices for local execution."
      },
      {
        "date": "2023-03-27T01:40:00.000Z",
        "voteCount": 3,
        "content": "The ML model is run locally, so it can still provide feedback when the internet is down."
      },
      {
        "date": "2023-03-12T07:20:00.000Z",
        "voteCount": 1,
        "content": "Quote \"The company must be able to provide this feedback even if the factory\u2019s internet connectivity is down\"\nSo everything that needs internet can be ignored. Leaves D.\nWhile there is a lot of garbage text about how they process date with SargeMaker, the question only asks for a solution to detect failures in the equipment. Amazon Monitron does this plus it can work even when internet is down.\n\nAll other options provide solutions for things, the question didn't ask for and/or already in place and need internet."
      },
      {
        "date": "2023-02-09T22:20:00.000Z",
        "voteCount": 2,
        "content": "The point is how to offload ML workloads to the local."
      },
      {
        "date": "2023-02-08T11:16:00.000Z",
        "voteCount": 1,
        "content": "Monitron is something different"
      },
      {
        "date": "2023-02-02T09:34:00.000Z",
        "voteCount": 4,
        "content": "this is taking about detecting defects from an image that is taken from a camera. I would go for running a ML model on IoT greengras pc and transfer it to IoT core, then store it in s3 bucket, which can be called by api function via lambda to send it to users. \noption D would monitor only sensor data of machines."
      },
      {
        "date": "2023-02-02T08:57:00.000Z",
        "voteCount": 2,
        "content": "Amazon Monitron is a machine-learning based end-to-end condition monitoring system that detects potential failures within equipment. You can use it to implement a predictive maintenance program and reduce lost productivity from unplanned machine downtime. Amazon Monitron includes purpose-built sensors to capture vibration and temperature data, as well as gateways to automatically transfer data to the AWS Cloud. It also comes with an application in two versions. The mobile application handles system setup, analytics, and noti\ufb01cation when tracking equipment conditions. The web application provides all the same functions as the mobile app except setup. Reliability managers can quickly deploy Amazon Monitron to track the machine health of industrial equipment, such as such as bearings, motors, gearboxes, and pumps, without any development work or specialized training."
      },
      {
        "date": "2023-02-06T08:19:00.000Z",
        "voteCount": 2,
        "content": "B is wrong, D is correct."
      },
      {
        "date": "2023-02-03T14:52:00.000Z",
        "voteCount": 3,
        "content": "B is correct.\nAWS IoT Greengrass enables ML inference locally using models that are created, trained, and optimized in the cloud using Amazon SageMaker, AWS Deep Learning AMI, or AWS Deep Learning Containers, and deployed on the edge devices"
      },
      {
        "date": "2023-02-06T11:02:00.000Z",
        "voteCount": 1,
        "content": "when do you take the exam man i would like to see if everything is still valid after you test"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/amazon/view/97670-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Discovery Service to import the CMDB data to perform an analysis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-06T00:46:00.000Z",
        "voteCount": 17,
        "content": "B\nhttps://aws.amazon.com/blogs/architecture/accelerating-your-migration-to-aws/\nBuild a business case with AWS Migration Evaluator\nThe foundation for a successful migration starts with a defined business objective (for example, growth or new offerings). In order to enable the business drivers, the established business case must then be aligned to a technical capability (increased security and elasticity). AWS Migration Evaluator (formerly known as TSO Logic) can help you meet these objectives.\n\nTo get started, you can choose to upload exports from third-party tools such as Configuration Management Database (CMDB) or install a collector agent to monitor. You will receive an assessment after data collection, which includes a projected cost estimate and savings of running your on-premises workloads in the AWS Cloud. This estimate will provide a summary of the projected costs to re-host on AWS based on usage patterns. It will show the breakdown of costs by infrastructure and software licenses. With this information, you can make the business case and plan next steps."
      },
      {
        "date": "2023-03-12T14:11:00.000Z",
        "voteCount": 10,
        "content": "The AWS Migration Evaluator works by analyzing data about your current on-premises environment, including servers, storage, networking, and applications. It then provides a report that outlines the recommended AWS services and configurations that best match your existing infrastructure and applications. This report includes a detailed cost analysis that estimates the total cost of running your applications in the AWS cloud."
      },
      {
        "date": "2024-03-04T09:29:00.000Z",
        "voteCount": 2,
        "content": "This is again another example of completely stupid, nonsensical and useless exposition to ambiguity. Which one is correct because yeah, B seems to be well supported  by https://aws.amazon.com/blogs/architecture/accelerating-your-migration-to-aws/\nbut in the faqs for AWS Application Discovery Service https://aws.amazon.com/blogs/architecture/accelerating-your-migration-to-aws/ there is literally a question about Application Discovery service \n\nQ: Can I ingest data into Application Discovery Service from my existing configuration management database (CMDB)?\n\n\"Yes, you can import information about your on-premises servers and applications into the Migration Hub so you can track the status of application migrations. To import your data, you can download and populate the import CSV template and then upload it using the Migration Hub import console or by invoking the Application Discovery Service APIs\"\n\nSo which one is correct? And what real knowledge are we getting from this pile of shit?"
      },
      {
        "date": "2024-02-12T01:49:00.000Z",
        "voteCount": 2,
        "content": "A - It is a questionnaire tool used to assess your AWS architecture\nC - We will need to create Complex Application using SDK\nD- Application Discovery is free and does support CMDB import but it can only give you plan and not a business use case\nB - Correct answer: Free and helps you create bussiness use case."
      },
      {
        "date": "2023-12-23T23:14:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-11T22:56:00.000Z",
        "voteCount": 1,
        "content": "Yes B is correct. But can you imagine any real architect in the world would trust such a solution for migration? It's a joke."
      },
      {
        "date": "2023-11-01T12:35:00.000Z",
        "voteCount": 1,
        "content": "When you see business case for migration, you think of Migration Evaluator.\nAccording to ChatGPT,\nA: AWS Well-Architected Tool: no option to import CMDB data\nC: only provide insight about current data, doesnt consider the nuances of migration task\nD: Application Discovery Service is for discover, not for building business cases"
      },
      {
        "date": "2023-10-22T09:56:00.000Z",
        "voteCount": 1,
        "content": "Migration evaluation \nB"
      },
      {
        "date": "2023-09-17T18:10:00.000Z",
        "voteCount": 1,
        "content": "https://www.youtube.com/watch?v=2qautbhuJC8"
      },
      {
        "date": "2023-07-13T06:18:00.000Z",
        "voteCount": 2,
        "content": "D\n\nThis tools  for Analitycs data : https://aws.amazon.com/pt/migration-evaluator/\nMigration data or vm : https://aws.amazon.com/pt/application-discovery/faqs/"
      },
      {
        "date": "2023-07-04T09:43:00.000Z",
        "voteCount": 1,
        "content": "B - use case for ME"
      },
      {
        "date": "2023-06-18T15:13:00.000Z",
        "voteCount": 1,
        "content": "Question say : Migration\nthen Anwser is : Migration Evaluator and other responde in this comments"
      },
      {
        "date": "2023-03-27T01:44:00.000Z",
        "voteCount": 3,
        "content": "B is the best fit"
      },
      {
        "date": "2023-02-26T12:39:00.000Z",
        "voteCount": 2,
        "content": "Migration Evaluator is a complimentary service to create data-driven assessments and business cases for AWS cloud planning and migration."
      },
      {
        "date": "2023-02-25T15:01:00.000Z",
        "voteCount": 2,
        "content": "B is right answer"
      },
      {
        "date": "2023-02-15T01:27:00.000Z",
        "voteCount": 3,
        "content": "B \nFree service, focus on cost of migration"
      },
      {
        "date": "2023-02-14T04:38:00.000Z",
        "voteCount": 2,
        "content": "B - Evaluator"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/amazon/view/97671-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL.<br><br>The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL\u2019s deny list.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server\u2019s subnet route table for any IP addresses that activate the alarm.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T14:46:00.000Z",
        "voteCount": 12,
        "content": "AWS Shield Advanced is focused on protecting against DDoS attacks, while AWS WAF is focused on protecting against web exploits. However, both services can be used together to provide comprehensive protection for your applications."
      },
      {
        "date": "2024-08-16T08:43:00.000Z",
        "voteCount": 1,
        "content": "Nothing mentioned about DDoS in the question, plus A is simplier and less operational oevrhead"
      },
      {
        "date": "2024-08-24T01:45:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2023-12-23T23:22:00.000Z",
        "voteCount": 2,
        "content": "Option B sounds most logical answer in terms of least operational overhead. \nthough it does not provide details about how to identify and add those IP addresses to Shield Advanced for DDos protection."
      },
      {
        "date": "2023-11-25T01:13:00.000Z",
        "voteCount": 4,
        "content": "I think its option A. Option B is a paid service and it is for DDoS. Here that attack is not DDoS and it is excess traffic generated at application layer by certain IPs. Not in a distributed attack pattern. Advanced shield will give DDoS+WAF. But you already have WAF and using which you can block the IPs that is crossing set threshold. So option A is better choice. Option B is additional cost. Option C is wrong as you can not add deny rule in route table. Route table has only routes. Option D is operational overhead and then if you block the whole country , genuine traffic will also get blocked, which is not good."
      },
      {
        "date": "2023-08-19T09:33:00.000Z",
        "voteCount": 1,
        "content": "\"Least\" Operational Overhead - B"
      },
      {
        "date": "2023-07-04T09:44:00.000Z",
        "voteCount": 1,
        "content": "B 100%"
      },
      {
        "date": "2023-06-11T14:45:00.000Z",
        "voteCount": 1,
        "content": "Research more information and correct my answer \nLetter B with this information \nhttps://docs.aws.amazon.com/waf/latest/developerguide/ddos-app-layer-protections.html"
      },
      {
        "date": "2023-05-23T19:32:00.000Z",
        "voteCount": 4,
        "content": "For me it would be the letter A\nBecause AWS Shield Advanced is for DDOS attacks that happen at layer 3.\nHowever, in the question they say attacks in the application layer\n\"The website often encounters attacks in the application layer.\"\nFor this reason, I would consider that it cannot be B and A would be a more feasible solution.\nIf anyone has more data, welcome to improve the community\n\nAttached answer from Bard from Google\n\nHere are some additional details about each solution:"
      },
      {
        "date": "2023-05-23T19:32:00.000Z",
        "voteCount": 1,
        "content": "Solution C: This solution would require creating an AWS Lambda function, which is a paid service. AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. The Lambda function would be used to inspect access logs and identify IP addresses that are launching attacks. The function would then add those IP addresses to the application server's subnet route table, which would prevent traffic from those IP addresses from reaching the application server."
      },
      {
        "date": "2023-05-23T19:32:00.000Z",
        "voteCount": 1,
        "content": "Solution D: This solution would require inspecting access logs, which can be a time-consuming process. The access logs would be used to find a pattern of IP addresses that launched the attacks. The IP addresses could then be used to create a geolocation routing policy in Amazon Route 53. The geolocation routing policy would deny traffic from the countries that host those IP addresses.\nOverall, solution A is the most efficient solution because it uses existing AWS services and does not require any additional infrastructure."
      },
      {
        "date": "2023-05-23T19:32:00.000Z",
        "voteCount": 1,
        "content": "Solution A: This solution is the most efficient because it uses existing AWS services and does not require any additional infrastructure. The CloudWatch alarm will monitor server access and trigger an action when the threshold is reached. The action can be configured to add the IP address to the web ACL's deny list, which will prevent traffic from that IP address from reaching the application server.\n\nSolution B: This solution would require deploying AWS Shield Advanced, which is a paid service. AWS Shield Advanced provides additional protection against DDoS attacks, including application layer attacks. However, it is more expensive than AWS WAF."
      },
      {
        "date": "2024-10-10T16:33:00.000Z",
        "voteCount": 1,
        "content": "The attack is at the application layer. Solution A detects attack by IP which is at network layer, hence it is not valid."
      },
      {
        "date": "2023-04-10T02:17:00.000Z",
        "voteCount": 2,
        "content": "\u05f4with the LEAST operational overhead\u05f4 is AWS SHIELD Advanced without doubts\u2705"
      },
      {
        "date": "2023-04-01T03:24:00.000Z",
        "voteCount": 2,
        "content": "B 100% AWS SHIELD"
      },
      {
        "date": "2023-03-27T01:45:00.000Z",
        "voteCount": 2,
        "content": "Deploy AWS Shield Advanced in addition to AWS WAF."
      },
      {
        "date": "2023-02-22T14:51:00.000Z",
        "voteCount": 2,
        "content": "as long as i know or think to know, shield advanced, does nothing by default and needs to be configured.\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/enable-ddos-prem.html\nhttps://docs.aws.amazon.com/waf/latest/developerguide/getting-started-ddos.html\nNote\nShield Advanced doesn't automatically protect your resources after you subscribe. You must specify the resources you want Shield Advanced to protect configure the protections."
      },
      {
        "date": "2023-02-11T20:31:00.000Z",
        "voteCount": 3,
        "content": "According to ChatGPT, the ff are what you get with Advanced over Basic.\n\nAWS Shield Advanced is a paid version of the service that provides additional protection against large scale and sophisticated DDoS attacks. This version includes all the features of the Basic version, but with additional capabilities such as 24/7 availability, a dedicated DDoS response team, and advanced attack analytics and reporting. Additionally, AWS Shield Advanced provides access to advanced DDoS protection and mitigation capabilities, such as the ability to customize protections for specific application requirements, and to mitigate attacks more quickly and effectively."
      },
      {
        "date": "2023-02-08T14:49:00.000Z",
        "voteCount": 4,
        "content": "Reading more about option B, I pick B"
      },
      {
        "date": "2023-02-08T14:45:00.000Z",
        "voteCount": 1,
        "content": "Not sure. With WAF you get Shield, which hs DDoS. Not sure the the Shield dvnced gives you much more."
      },
      {
        "date": "2023-02-02T09:04:00.000Z",
        "voteCount": 4,
        "content": "AWS Shield is a managed distributed denial of service (DDoS) protection service that safeguards applications running on AWS. It provides dynamic detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield: Standard and Advanced."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/amazon/view/97684-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions.<br><br>Company policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd another Region to the Aurora MySQL DB cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd another Region to each table in the Aurora MySQL DB cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the existing DynamoDB table to a global table by adding another Region to its configuration\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-16T09:49:00.000Z",
        "voteCount": 7,
        "content": "Badly written question: \n\"The RTO and RPO must be no more than a few minutes each.\"\nWhat is few minutes mean? May be it is 2-3 min for me, may be it is 9-10 min for you."
      },
      {
        "date": "2023-03-17T23:25:00.000Z",
        "voteCount": 5,
        "content": "A. Add another Region to the Aurora MySQL DB cluster\nD. Convert the existing DynamoDB table to a global table by adding another Region to its configuration"
      },
      {
        "date": "2023-12-23T23:26:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2023-11-23T22:18:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2023-08-19T09:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html"
      },
      {
        "date": "2023-07-04T09:46:00.000Z",
        "voteCount": 1,
        "content": "its AD"
      },
      {
        "date": "2023-06-25T10:36:00.000Z",
        "voteCount": 2,
        "content": "For DynamoDB use global table, for Aurora use cross-region read-replicas."
      },
      {
        "date": "2023-06-20T10:07:00.000Z",
        "voteCount": 1,
        "content": "a-d-a-d-a-d-a-d-a-d"
      },
      {
        "date": "2023-06-05T09:38:00.000Z",
        "voteCount": 1,
        "content": "Answer : A, D\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-disaster-recovery/choosing-database.html"
      },
      {
        "date": "2023-03-12T19:41:00.000Z",
        "voteCount": 3,
        "content": "A solves multi region for DB layer. but question also asks for minimum RPO and RTO which means quick uptime of application in case of failure which is possible with backups.\nhttps://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide\n/CrossRegionAccountCopyAWS.html"
      },
      {
        "date": "2023-03-12T19:43:00.000Z",
        "voteCount": 4,
        "content": "Hint given is - Aurora MySQL engine version supports a global database which makes this possible - https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2021/03/08/Aurora-Global-database-2.jpg"
      },
      {
        "date": "2023-08-19T09:52:00.000Z",
        "voteCount": 1,
        "content": "Why use C and do replication with multiple steps when Global Tables support it \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html"
      },
      {
        "date": "2023-02-02T14:08:00.000Z",
        "voteCount": 4,
        "content": "A and D"
      },
      {
        "date": "2023-02-02T11:07:00.000Z",
        "voteCount": 4,
        "content": "you can create only db's not global tables, hence A and D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/amazon/view/97685-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.<br><br>The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-09T23:27:00.000Z",
        "voteCount": 20,
        "content": "The background is the below.\n- The company is using ALB features and must keep them.\n- The new on-premise firewall needs a static IP address of the ALB as the next hop.\n- However, ALB cannot have a static IP address.\nSo the point is how ALB can have a static IP address endpoint.\n\nSolution\nhttps://aws.amazon.com/premiumsupport/knowledge-center/alb-static-ip/"
      },
      {
        "date": "2023-03-04T13:51:00.000Z",
        "voteCount": 6,
        "content": "The question is confusing. If I understood this question correctly, I do this almost every day, and I don't use those terms. Basically, the solution is inserting an NLB in front of the existing ALB, so traffic is Client-&gt;FW-&gt;NLB-&gt;ALB-&gt;EC2. Another point is that fixing the public IP address makes a lot of sense, but not the private one, like in this case. Every time you create an ALB 2 or more ENI are created and you have the IP addresses there."
      },
      {
        "date": "2024-02-12T01:59:00.000Z",
        "voteCount": 4,
        "content": "A - Cannot assign static IP to ALB\nC - Cannot attach target group directly as path-based forwarding is not possible with NLB\nD - Gateway load balancer supports only Instance and IP as target\nB - This is correct since using NLB we can have a static IP assigned and also attach ALB as target to NLB"
      },
      {
        "date": "2024-01-22T17:10:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/solutions/implementations/git-to-s3-using-webhooks/"
      },
      {
        "date": "2023-12-24T14:42:00.000Z",
        "voteCount": 1,
        "content": "Option B is only feasible option is ALB is using path based routingg."
      },
      {
        "date": "2023-12-22T07:19:00.000Z",
        "voteCount": 1,
        "content": "bjexamprep \"Anyone help why A not correct?\"\nWhere is the On Prem element, the Direct Connect, the ALB covering Multi AZ ? \n\"The objective of this question is achieved\"\nYou don't even have the basic structure implemented\nto attempt to address the questions requirements in your scenario\nRegarding answer A :\nhttps://repost.aws/knowledge-center/alb-static-ip\nYou can't assign a static IP address to an Application Load Balancer."
      },
      {
        "date": "2023-12-12T20:53:00.000Z",
        "voteCount": 2,
        "content": "Anyone can help explain why A is not correct? I created a private network facing ALB and it has a private IP address automatically created. Which means by adding the private IP address to the firewall, the objective of this question is achieved."
      },
      {
        "date": "2024-02-12T01:55:00.000Z",
        "voteCount": 1,
        "content": "A is not correct because, though the IP attached to the ALB is the private IP, the control of which IP is assign in with AWS, any change in the ALB can result in change of IP or even over a period of time AWS can change the IP (though it will be something in the CIDR)"
      },
      {
        "date": "2023-11-23T22:58:00.000Z",
        "voteCount": 2,
        "content": "Option B as ALB can not have static IP address so Option A is not possible."
      },
      {
        "date": "2023-09-27T22:05:00.000Z",
        "voteCount": 1,
        "content": "D is also not the write answer \nTarget type\nWhen you create a target group, you specify its target type, which determines how you specify its targets. After you create a target group, you cannot change its target type.\n\nThe following are the possible target types:\n\ninstance\nThe targets are specified by instance ID.\n\nip\nThe targets are specified by IP address.\n\nWhen the target type is ip, you can specify IP addresses from one of the following CIDR blocks:\n\nThe subnets of the VPC for the target group\n\n10.0.0.0/8 (RFC 1918)\n\n100.64.0.0/10 (RFC 6598)\n\n172.16.0.0/12 (RFC 1918)\n\n192.168.0.0/16 (RFC 1918)"
      },
      {
        "date": "2023-09-27T21:58:00.000Z",
        "voteCount": 1,
        "content": "Elastic IP support\nNetwork Load Balancer also allows you the option to assign an Elastic IP per Availability Zone (subnet) thereby providing your own fixed IP.\nBoth B anc C state single IP for multiple zones"
      },
      {
        "date": "2023-08-08T19:47:00.000Z",
        "voteCount": 1,
        "content": "Option B says \"ALAdd\" what is AL add? I see this very often. Can someone help to explain? \nCreate an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB."
      },
      {
        "date": "2023-07-21T05:23:00.000Z",
        "voteCount": 1,
        "content": "A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity between virtual appliances in the service provider VPC, and application servers in the service consumer VPC. The Gateway Load Balancer is deployed in the same VPC as that of the virtual appliances. These appliances are registered as a target group of the Gateway Load Balancer.\nSince the firewall is deployed on-prem I dont think D is a viable option"
      },
      {
        "date": "2023-07-04T09:51:00.000Z",
        "voteCount": 1,
        "content": "B\nneed to keep ALB behind NLB for path routing"
      },
      {
        "date": "2023-06-23T11:17:00.000Z",
        "voteCount": 1,
        "content": "Since ALB does not support static IP addresses by design then we need to use NLB before the ALB or instead. However, since we are heavily utilizing the application layer of the OSI then we cannot use NLB directly. Hence B remains the only choice"
      },
      {
        "date": "2023-06-18T15:19:00.000Z",
        "voteCount": 1,
        "content": "ALB's cannot use static IP's. NLB's have static IP's , addicionally need  based on the URL path use ALB then \nB is more apropiate"
      },
      {
        "date": "2023-05-21T22:03:00.000Z",
        "voteCount": 3,
        "content": "I agree with B. since clients need access to the ALB using a private connection between on premises and AWS. The firewall which is inside company data center operates at network level but we cannot lose ALB due to many path based routing. So we need something like this:\nhttps://www.scalefactory.com/blog/2021/12/13/aws-network-load-balancers-new-features/\nhttps://www.scalefactory.com/blog/2021/12/13/aws-network-load-balancers-new-features/img/now-firewall-egress.png\nand this:\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/application-load-balancer-type-target-group-for-network-load-balancer/"
      },
      {
        "date": "2023-03-12T20:26:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/\nGateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand. This decreases potential points of failure in your network and increases availability."
      },
      {
        "date": "2023-03-12T20:43:00.000Z",
        "voteCount": 3,
        "content": "https://youtu.be/-j2smz_VCH4?t=1270\nALB (L7)- HTTP, HTTPS\nNLB (L4)- TCP, UDP, TLS traffic\nGWLB(L3)- IP traffic and 3rd party Appliances"
      },
      {
        "date": "2023-03-12T21:06:00.000Z",
        "voteCount": 1,
        "content": "AWS Gateway Load Balancer (GWLB) can terminate TLS traffic. GWLB supports SSL/TLS offloading, which means that it can terminate SSL/TLS connections from clients and then forward the decrypted traffic to backend servers over HTTP or HTTPS."
      },
      {
        "date": "2023-03-29T03:06:00.000Z",
        "voteCount": 1,
        "content": "I think main question is can it support static IP address which is needed by the firmware to waitlist it?"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/amazon/view/97934-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.<br><br>The company needs a solution that will prevent internet traffic from directly accessing the ALB.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the existing web ACL with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a security group rule to the ALB to allow only the various CloudFront IP address ranges."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-04T07:34:00.000Z",
        "voteCount": 11,
        "content": "https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html\nAWS managed prefix list is more recommended."
      },
      {
        "date": "2023-05-21T22:15:00.000Z",
        "voteCount": 5,
        "content": "https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html\nIf your origin is hosted on Amazon and protected by an Amazon VPC security group, you can use the CloudFront managed prefix list to allow inbound traffic to your origin only from CloudFront's origin-facing servers, preventing any non-CloudFront traffic from reaching your origin\n, imagine that your origin is an Amazon EC2 instance in the Europe (London) Region (eu-west-2). If the instance is in a VPC, you can create a security group rule that allows inbound HTTPS access from the CloudFront managed prefix list. This allows all of CloudFront's global origin-facing servers to reach the instance. If you remove all other inbound rules from the security group, you prevent any non-CloudFront traffic from reaching the instance"
      },
      {
        "date": "2023-12-24T14:44:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-11-23T23:06:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-04T09:52:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-03-27T02:05:00.000Z",
        "voteCount": 2,
        "content": "C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only."
      },
      {
        "date": "2023-02-04T03:47:00.000Z",
        "voteCount": 2,
        "content": "C https://aws.amazon.com/blogs/news/limit-access-to-your-origins-using-the-aws-managed-prefix-list-for-amazon-cloudfront/"
      },
      {
        "date": "2023-02-04T03:11:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/about-aws/whats-new/2022/02/amazon-cloudfront-managed-prefix-list/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/amazon/view/97689-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.<br><br>A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-05T16:52:00.000Z",
        "voteCount": 1,
        "content": "B=Better :-)"
      },
      {
        "date": "2024-05-28T18:55:00.000Z",
        "voteCount": 1,
        "content": "It seems to configure in-transit encryption in both new cluster and existing cluster, but updating is supported on Redis version 7 and later. So I will choose option A.\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html#in-transit-encryption-constraints"
      },
      {
        "date": "2024-08-25T23:05:00.000Z",
        "voteCount": 2,
        "content": "just B"
      },
      {
        "date": "2024-03-17T23:38:00.000Z",
        "voteCount": 3,
        "content": "A or B? I didn't read any comparison between these 2 options... For sure we need an auth token. Both, using SSM Parameter Store or Secrets Manager will work. Both, create a new cluster or update the current one will work. I will choose B because this approach avoids the need to set up a new cluster, potentially reducing effort and costs associated with migration or duplication of resources..."
      },
      {
        "date": "2023-12-24T14:50:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-11-23T23:25:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-07-04T09:56:00.000Z",
        "voteCount": 2,
        "content": "B, per redis docs. \nEC encr in transit is a config option"
      },
      {
        "date": "2023-06-20T10:35:00.000Z",
        "voteCount": 4,
        "content": "b-b-b-b-b-b-b\n\nCreating an AUTH token provides a form of authentication for accessing the ElastiCache cluster.\nStoring the AUTH token in AWS Secrets Manager ensures secure and centralized management of the token.\nConfiguring the existing ElastiCache cluster to use the AUTH token enables authentication for accessing the cache.\nEnabling encryption in transit ensures that data is encrypted when it is transferred between the client and the ElastiCache cluster.\nUpdating the application to retrieve the AUTH token from Secrets Manager and use it for authentication ensures that only authorized users can access the cache."
      },
      {
        "date": "2023-03-27T02:16:00.000Z",
        "voteCount": 1,
        "content": "Create an AUTH token. Store the token in AWS Secrets Manager."
      },
      {
        "date": "2023-03-13T22:29:00.000Z",
        "voteCount": 4,
        "content": "Redis CLI has AUTH command as a feature to SET/ROTATE strategies\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html"
      },
      {
        "date": "2023-03-08T03:51:00.000Z",
        "voteCount": 3,
        "content": "B seems right. \nTo enable authentication on an existing Redis server, call the ModifyReplicationGroup API operation. Call ModifyReplicationGroup with the --auth-token parameter as the new token and the --auth-token-update-strategy with the value ROTATE.\n\nAfter the modification is complete, the cluster supports the AUTH token specified in the auth-token parameter in addition to supporting connecting without authentication. Enabling authentication is only supported on Redis servers with encryption in transit (TLS) enabled.\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html"
      },
      {
        "date": "2023-02-14T04:51:00.000Z",
        "voteCount": 2,
        "content": "As per https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html"
      },
      {
        "date": "2023-02-05T12:38:00.000Z",
        "voteCount": 1,
        "content": "You have to create a new cluster, otherwise the the cluster supports the AUTH token specified and supports connecting without authentication."
      },
      {
        "date": "2023-02-04T03:23:00.000Z",
        "voteCount": 1,
        "content": "Previously, you needed to set up authentication for ElastiCache for Redis clusters using Redis user passwords or store the password in AWS Secrets Manager or on a third-party secrets management tool. However, in large organizations that host many applications, passwords can often become out of sync when it comes time to rotate the password. IAM authentication provides a streamlined security posture by allowing access management from a centralized service. With IAM authentication, ElastiCache users can use their IAM identities when connecting to their Redis clusters"
      },
      {
        "date": "2023-02-02T11:28:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/amazon/view/97695-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.<br><br>Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the launch template Auto Scaling group to increase the number of placement groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the launch template to use a larger instance type."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 39,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-02T12:24:00.000Z",
        "voteCount": 14,
        "content": "launch config is replaced by launch template hence is not advisible, option A rulled out. C is wrong because launch template cannot be updated. D is also wrong for the same reason"
      },
      {
        "date": "2023-08-22T01:20:00.000Z",
        "voteCount": 6,
        "content": "As an alternative to manually specifying the instance types, you can specify the attributes that an instance must have, and Amazon EC2 will identify all the instance types with those attributes. \nThis is known as attribute-based instance type selection. \nFor example, you can specify the minimum and maximum number of vCPUs required for your instances, and EC2 Fleet will launch the instances using any available instance types that meet those vCPU requirements."
      },
      {
        "date": "2024-10-05T16:55:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: B"
      },
      {
        "date": "2023-12-24T14:59:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-11-24T13:33:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-10-08T09:46:00.000Z",
        "voteCount": 5,
        "content": "When you use attribute-based instance type selection, you allow AWS to diversify the instances across different instance types within a specified instance family or similar characteristics. This helps in reducing the risk of Spot Instance termination due to capacity issues or price fluctuations."
      },
      {
        "date": "2023-07-26T16:00:00.000Z",
        "voteCount": 1,
        "content": "B\nAmazon EC2 Auto Scaling can select from a wide range of instance types for launching Spot Instances. This meets the Spot best practice of being flexible about instance types, which gives the Amazon EC2 Spot service a better chance of finding and allocating your required amount of compute capacity."
      },
      {
        "date": "2023-07-07T14:40:00.000Z",
        "voteCount": 2,
        "content": "key word \"spot instance launch failure\"-&gt; attribute based selection"
      },
      {
        "date": "2023-07-04T09:57:00.000Z",
        "voteCount": 1,
        "content": "its a b"
      },
      {
        "date": "2023-06-20T10:42:00.000Z",
        "voteCount": 5,
        "content": "b-b-b-b-bb-b-\n\nCreating a new launch template version allows for making changes to the template without disrupting the existing instances.\nUsing attribute-based instance type selection enables the Auto Scaling group to automatically select the most suitable instance type based on the defined attributes, such as availability zone, instance family, or instance size.\nBy leveraging attribute-based instance type selection, the Auto Scaling group can adapt to changing Spot Instance availability and launch instances in zones with higher availability, reducing launch failures.\nUpdating the launch template with this new version ensures that new instances launched by the Auto Scaling group utilize the improved instance selection process, thereby enhancing reliability."
      },
      {
        "date": "2023-03-27T02:17:00.000Z",
        "voteCount": 2,
        "content": "B. Create a new launch template version that uses attribute-based instance type selection."
      },
      {
        "date": "2023-05-28T12:59:00.000Z",
        "voteCount": 1,
        "content": "Agreed with B"
      },
      {
        "date": "2023-03-13T22:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-instance-type-requirements.html#use-attribute-based-instance-type-selection-prerequisites"
      },
      {
        "date": "2023-02-26T13:12:00.000Z",
        "voteCount": 1,
        "content": "Confused between B and D , will choose B"
      },
      {
        "date": "2023-02-25T15:23:00.000Z",
        "voteCount": 2,
        "content": "b is correct \n\nhttps://aws.amazon.com/blogs/aws/new-attribute-based-instance-type-selection-for-ec2-auto-scaling-and-ec2-fleet/"
      },
      {
        "date": "2023-02-09T23:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/amazon/view/97690-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.<br><br>During the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.<br><br>Which solution will meet these requirements with the LEAST amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 70,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-10T03:35:00.000Z",
        "voteCount": 24,
        "content": "B is correct imo\nC is incorrect, FSx for Luster doesn't support NFS protocol\nIt actually support only POSIX protocol:\nCustom (POSIX-compliant) protocol optimized for performance"
      },
      {
        "date": "2023-02-02T11:43:00.000Z",
        "voteCount": 23,
        "content": "C: \nAmazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Powered by Lustre, the world\u2019s most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API."
      },
      {
        "date": "2023-07-05T03:58:00.000Z",
        "voteCount": 5,
        "content": "I wouldnt choose Lustre.. would only pick it if its related to HPC (high performance computing), the amount of files generated here is nothing.."
      },
      {
        "date": "2023-05-22T06:56:00.000Z",
        "voteCount": 9,
        "content": "I disagree with option C. This is an example of how to mount a Lustre from an EC2 Linux system. it does not use NFS \nsudo mount -t lustre &lt;fsx-dns-name&gt;@tcp:/&lt;mount-point&gt;\nAmazon FSx for Lustre provides its own Lustre-specific mount command and protocol for mounting the file system on Linux instances.\nThe lustre file system type in the mount command indicates that it is specifically for mounting Lustre-based file systems, such as Amazon FSx for Lustre.\nI would still go for option B"
      },
      {
        "date": "2024-10-07T12:06:00.000Z",
        "voteCount": 1,
        "content": "Note that it keeps saying \"The Server\" implying 1 server and not a fleet or multiple. NFS is from 1 client to 1 server. \n\nSo C is incorrect"
      },
      {
        "date": "2024-10-05T17:01:00.000Z",
        "voteCount": 1,
        "content": "Option C is not possible: how will you mount the document store on the EC2 instance through the Lustre client using NFS? Lustre is not compatible with NFS!"
      },
      {
        "date": "2024-08-06T22:13:00.000Z",
        "voteCount": 2,
        "content": "The English in this question is very confusing, what is it trying do? what is the problem? where is the processing server?"
      },
      {
        "date": "2024-02-24T18:12:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/storage-gateway-automate-refreshcache"
      },
      {
        "date": "2024-01-26T09:16:00.000Z",
        "voteCount": 3,
        "content": "A = migrating to lambda requires a lot of work and doesn't solve the need to have fast access to files\nB = correct\nC = FSx for Lustre doesn't support NFS\nD = DataSynch can schedule transfer hourly, daily or weekly, cannot meet 30 minutes requirement"
      },
      {
        "date": "2023-12-24T15:08:00.000Z",
        "voteCount": 3,
        "content": "Option B as Fsx Luster though supports Linux, it does not support NFS."
      },
      {
        "date": "2023-11-24T14:45:00.000Z",
        "voteCount": 2,
        "content": "B is right. Though it is meant to be used to with on-premise in Hybrid environment, it is possible to use it on EC2."
      },
      {
        "date": "2023-11-16T23:41:00.000Z",
        "voteCount": 2,
        "content": "just because NFS mentioned with Lustre, but everything else is pointing to the Lustre: Linux, fast, read/writes to S3"
      },
      {
        "date": "2023-09-30T08:48:00.000Z",
        "voteCount": 1,
        "content": "B. Extra effort due to refreshCache API\nD. DataSync runs in task schedule, which can't run faster than once per hour.\nSo remaining answer is C"
      },
      {
        "date": "2023-09-27T22:53:00.000Z",
        "voteCount": 1,
        "content": "The core of the problem is make the file available in S3\n for When the server finishes processing, the files must be available to the public for download within 30 minutes.\nWhich solution will meet these requirements with the LEAST amount of effort?\nI think Option D (AWS DataSync) is a more straightforward and efficient choice."
      },
      {
        "date": "2023-09-30T08:45:00.000Z",
        "voteCount": 5,
        "content": "DataSync task cannot run faster than 1 hour. \"Even with a cron expression, you can't schedule a task to run at an interval faster than 1 hour.\" https://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html"
      },
      {
        "date": "2023-09-01T08:00:00.000Z",
        "voteCount": 4,
        "content": "The server is running Linux, How can we use Fsx?"
      },
      {
        "date": "2023-09-03T00:22:00.000Z",
        "voteCount": 3,
        "content": "FSX for Lustre is for Linux and does not support Windows"
      },
      {
        "date": "2023-08-30T03:15:00.000Z",
        "voteCount": 3,
        "content": "I believe that B is correct, given that Lustre does not support NFS (it supports POSIX)"
      },
      {
        "date": "2023-08-18T04:16:00.000Z",
        "voteCount": 5,
        "content": "B as file gateway seems simple working solution for this. Lustre does not support NFS and might be an overkill for this solution - its primary used for HPC clusters. DataSync is rather for batch daad migrations and periodic data migration jobs, isn't it?"
      },
      {
        "date": "2023-08-06T02:08:00.000Z",
        "voteCount": 3,
        "content": "don't understand the question and answer, include B&amp;C. how does it mount to EC2 by using NFS? I think the  processing server is running on Premise??"
      },
      {
        "date": "2023-07-25T17:52:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/amazon/view/98107-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.<br><br>The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 67,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-15T03:53:00.000Z",
        "voteCount": 17,
        "content": "C seems correct; SQS is one queue to one microservice, could not find anything on dynamodb event notifications."
      },
      {
        "date": "2023-02-10T01:19:00.000Z",
        "voteCount": 16,
        "content": "The trigger is that the central user service deletes a user in the DynamoDB table. The DynamoDB Streams meets the requirement.\nhttps://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/\nOption B is wrong. There is no feature named DynamoDB event notifications."
      },
      {
        "date": "2023-03-28T08:10:00.000Z",
        "voteCount": 1,
        "content": "Correct, the point they want to make is central user service is system of record. You should not be deleting from other services until you delete from DynamoDB."
      },
      {
        "date": "2023-08-29T11:02:00.000Z",
        "voteCount": 3,
        "content": "how can you use 1 sqs queue for all microservises?"
      },
      {
        "date": "2023-11-28T03:02:00.000Z",
        "voteCount": 5,
        "content": "You can have many consumers which means any of the consumers can receive and process the message."
      },
      {
        "date": "2024-10-09T08:32:00.000Z",
        "voteCount": 1,
        "content": "C, The problem with A the SQS solution ist that the \"other microservices which stores data chunks seperatly\". We do not know how many services are storing the userdata, and with SQS we would have one message on the queue which is processed by one of these microservices. how could the other microservices know that they have to delete the data when the message is allready consumed and processed?"
      },
      {
        "date": "2024-09-07T19:06:00.000Z",
        "voteCount": 3,
        "content": "SQS does not have a fan-out capability. You need SNS --&gt; SQS to achieve the microservices to be notified. Hence A is incorrect and C is correct."
      },
      {
        "date": "2024-03-07T04:12:00.000Z",
        "voteCount": 4,
        "content": "A is not viable since SQS is not used in a fan-out situation.\nB is not viable since there's no such thing as \"DynamoDB event notifications\".\nC is viable.\nD is not viable, again due to the fact that SQS is not used for fan-out."
      },
      {
        "date": "2023-12-24T15:33:00.000Z",
        "voteCount": 4,
        "content": "This is tricky question. C seems to be best and feasible. Rest options are not correct as they are using SQS where messages can be delivered only to one reader while in this scenario there are multiple microservices that needs to read the same message and delete the user information."
      },
      {
        "date": "2023-12-22T09:04:00.000Z",
        "voteCount": 3,
        "content": "Lets Ignore the insanity of \nSeveral other microservices store in   ----    different storage services.    ------\ncentral user service deletes a user, every other microservice must\nalso delete its copy of the data immediately. \nYET ALL the options attempt a delete in the OG DynamoDB\nYeah OK Whatever   Blue is green and Red is Orange these days.\nBTW ans. == C ,  A will work but why poll SQS when Evt Brdg can invoke Microservice.\nPersonally I'd invoke a lambda to delete related records from the disparate data sources per KeyId and not bother the services   but I'm not Architecting this mess maybe they want a clean log trail of the delete process as invoked by central user service  whatever"
      },
      {
        "date": "2024-02-12T13:27:00.000Z",
        "voteCount": 3,
        "content": "Agreed. If this is an actual exam question, I am concerned about the intellect of the exam writers."
      },
      {
        "date": "2023-12-14T10:46:00.000Z",
        "voteCount": 2,
        "content": "I vote for C because the question says: Delete the user IMMEDIATELY\nA and D use SQS and messages in SQS can stay a pretty long time"
      },
      {
        "date": "2023-11-28T02:47:00.000Z",
        "voteCount": 1,
        "content": "Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Powered by Lustre, the world's most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API."
      },
      {
        "date": "2023-11-28T02:59:00.000Z",
        "voteCount": 2,
        "content": "this is for Q165,"
      },
      {
        "date": "2023-11-24T22:32:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-09-16T09:04:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/vi/getting-started/hands-on/send-fanout-event-notifications/?nc1=f_ls"
      },
      {
        "date": "2023-08-26T01:05:00.000Z",
        "voteCount": 4,
        "content": "A real-world use case utterly destroyed with some of the worst possible options for solutions.\nSimplest solution is to have the interested parties consume events off the DynamoDB streams and delete the user information in their respective datastores. Too many red herrings in the options given, and the only relatively sane one of the lot is Option C.\nThe bar for coming up with questions with SA professional keeps getting lowered."
      },
      {
        "date": "2023-08-19T12:32:00.000Z",
        "voteCount": 2,
        "content": "Event trigger from DynamoDb -- Choose DynamoDb Streams"
      },
      {
        "date": "2023-08-18T04:36:00.000Z",
        "voteCount": 3,
        "content": "Where the hell is fan-out pattern? stupid answers ..."
      },
      {
        "date": "2023-08-17T00:06:00.000Z",
        "voteCount": 1,
        "content": "* The central user service stores sensitive data in an Amazon DynamoDB table. \n* Several of the other microservices store a copy of parts of the sensitive data in different storage services.\n\nApparently only the central user service stores user data in DynamoDB. The others use \"different storage services\". Yet, all of the answers focus on DynamoDB..."
      },
      {
        "date": "2023-07-04T10:19:00.000Z",
        "voteCount": 2,
        "content": "C\nA would be preferable with SNS instead of SQS"
      },
      {
        "date": "2023-08-16T23:55:00.000Z",
        "voteCount": 2,
        "content": "Can you seriously mean one should \"Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.\"???\n\nWhy not SQS?"
      },
      {
        "date": "2023-08-29T11:03:00.000Z",
        "voteCount": 1,
        "content": "it should be SQSs but all answers indicates only 1 queue"
      },
      {
        "date": "2023-08-17T00:03:00.000Z",
        "voteCount": 1,
        "content": "Instead of configuring multiple EventBridge rules, there could be multiple SQS streams :)"
      },
      {
        "date": "2023-06-23T12:20:00.000Z",
        "voteCount": 3,
        "content": "No matter how I would like to use the native DynamoDB services, option A and B have some major issues - A and D expects SQS to be used by several microservices, which is not really what the service is supposed to do. B seems like a nice scenario, however, there isn't something like \"DynamoDB event notifications\". So we leave with option C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/amazon/view/97664-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.<br><br>An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's public IP address. Provide the IP address to the customer."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-10T03:42:00.000Z",
        "voteCount": 19,
        "content": "https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html\nOption A is wrong. AWS WAF does not support associating with NLB.\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html\nOption B is wrong. An ALB does not support an Elastic IP address.\nhttps://aws.amazon.com/elasticloadbalancing/features/"
      },
      {
        "date": "2023-02-04T07:55:00.000Z",
        "voteCount": 15,
        "content": "static IP can made below method.\n\u30fbNLB (replace NLB from ALB)\n\u30fbNLB + ALB\n\u30fbglobal accelarator + ALB\n\u30fboriginal load balancer (ex. made by EC2 + nginx)"
      },
      {
        "date": "2024-10-08T10:50:00.000Z",
        "voteCount": 1,
        "content": "Global Accelerator provides two global static public IPs that act as a fixed entry point to your application endpoints, such as Application Load Balancers, Network Load Balancers, Amazon Elastic Compute Cloud (EC2) instances, and elastic IPs.\n\nhttps://aws.amazon.com/global-accelerator/"
      },
      {
        "date": "2024-01-26T09:27:00.000Z",
        "voteCount": 4,
        "content": "A = NLB doe not integrates with WAF\nB = ALB cannot have Elastic IP attached, ALB cannot have static IP at all\nC = corect\nD = CloudFront distributions replies from many IPs, AWS manages a prefix list for this. Not easy to configure on customers side"
      },
      {
        "date": "2023-12-24T15:34:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-12-22T09:35:00.000Z",
        "voteCount": 1,
        "content": "An Application Load Balancer cannot be assigned an Elastic IP address --\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-lambda-to-enable-static-ip-addresses-for-application-load-balancers/"
      },
      {
        "date": "2023-11-24T22:37:00.000Z",
        "voteCount": 2,
        "content": "Option C has least operational overhead. Option A is possible but changing ALB to NLB requires higher operational effort."
      },
      {
        "date": "2023-07-04T10:21:00.000Z",
        "voteCount": 1,
        "content": "C - basic use case for GA"
      },
      {
        "date": "2023-03-27T02:38:00.000Z",
        "voteCount": 1,
        "content": "C. Create an AWS Global Accelerator standard accelerator."
      },
      {
        "date": "2023-03-14T23:08:00.000Z",
        "voteCount": 1,
        "content": "An Application Load Balancer cannot be assigned an Elastic IP address (static IP address).\nhttps://stackoverflow.com/questions/55236806/how-to-assign-elastic-ip-to-application-load-balancer-in-aws"
      },
      {
        "date": "2023-03-14T23:14:00.000Z",
        "voteCount": 2,
        "content": "This feature allows you to migrate your applications to AWS without requiring your partners and customers to change their IP address whitelists. (which could be used in WAF)\nBYOIP - Bring your own IP https://aws.amazon.com/blogs/networking-and-content-delivery/using-bring-your-own-ip-addresses-byoip-with-global-accelerator/"
      },
      {
        "date": "2023-02-26T13:24:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/alb-static-ip/\n\nCan assisng Static IP to ALB"
      },
      {
        "date": "2023-02-04T13:04:00.000Z",
        "voteCount": 2,
        "content": "......"
      },
      {
        "date": "2023-02-06T06:43:00.000Z",
        "voteCount": 1,
        "content": "C\nWAF cannot be assoicated with NLB"
      },
      {
        "date": "2023-02-04T07:59:00.000Z",
        "voteCount": 1,
        "content": "NLB cannot be used when WAF is used"
      },
      {
        "date": "2023-02-04T04:14:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://aws.amazon.com/jp/premiumsupport/knowledge-center/alb-static-ip/"
      },
      {
        "date": "2023-02-04T04:18:00.000Z",
        "voteCount": 1,
        "content": "Sorry C\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html"
      },
      {
        "date": "2023-02-02T14:19:00.000Z",
        "voteCount": 1,
        "content": "This solution meets the requirement with the least operational overhead, as it only requires the allocation of an Elastic IP address, assignment to the ALB, and providing the address to the customer. The other options involve configuring additional services, which can increase operational overhead."
      },
      {
        "date": "2023-02-02T12:36:00.000Z",
        "voteCount": 4,
        "content": "this option has the least admin effort. A has more admin effort, B is not possible, D will not give static IP address"
      },
      {
        "date": "2023-02-02T07:47:00.000Z",
        "voteCount": 1,
        "content": "B will works"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/amazon/view/97939-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Control Tower landing zone in the company\u2019s management account. Add production and development accounts to production and development OUs. respectively.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a guardrail from the management account to detect EBS encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a guardrail for the production OU to detect EBS encryption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDF",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "ACF",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "ABD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-15T19:26:00.000Z",
        "voteCount": 12,
        "content": "When you enable controls on an organizational unit (OU) that is registered with AWS Control Tower, preventive controls apply to all member accounts under the OU, enrolled and unenrolled. Detective controls apply to enrolled accounts only.\nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html"
      },
      {
        "date": "2023-02-10T04:11:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/controls.html\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption\nAWS is now transitioning the previous term 'guardrail' new term 'control'."
      },
      {
        "date": "2023-12-24T15:39:00.000Z",
        "voteCount": 1,
        "content": "C, D, F are the right choices."
      },
      {
        "date": "2023-11-24T22:53:00.000Z",
        "voteCount": 1,
        "content": "C, D, F"
      },
      {
        "date": "2023-09-13T03:21:00.000Z",
        "voteCount": 1,
        "content": "Basically order is DCF of the setup"
      },
      {
        "date": "2023-07-04T10:25:00.000Z",
        "voteCount": 1,
        "content": "CDF for sure"
      },
      {
        "date": "2023-06-27T18:11:00.000Z",
        "voteCount": 3,
        "content": "CEF\nA ) AWS Config not enforce rule \nB) Why developer account ? is incorrect is management account\nC ) Sounds good\nD) SCP for enforce sounds good\nE ) EBS encryption in managament account ? not only required in production \nF ) encryption in production OU sounds great"
      },
      {
        "date": "2023-06-27T18:12:00.000Z",
        "voteCount": 1,
        "content": "CDF is correct"
      },
      {
        "date": "2023-06-27T18:11:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/97939-exam-aws-certified-solutions-architect-professional-sap-c02/"
      },
      {
        "date": "2023-06-27T18:11:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/97939-exam-aws-certified-solutions-architect-professional-sap-c02/"
      },
      {
        "date": "2023-06-06T01:27:00.000Z",
        "voteCount": 3,
        "content": "C because we want to use Control Tower\n\nA and C because we're going to use Controls and Config\n\nNot D because Control Tower is a parallel product to Organisations and it doesn't use SCPs although it can import existing OUs."
      },
      {
        "date": "2023-06-06T01:35:00.000Z",
        "voteCount": 1,
        "content": "I meant to say A and F because we're going to use Controls and Config"
      },
      {
        "date": "2023-05-28T11:59:00.000Z",
        "voteCount": 1,
        "content": "Answer : C,D,F"
      },
      {
        "date": "2023-04-26T10:25:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is ACF. \nI don't think you need D once you have C. Also, control tower uses config rules to set up guardrails. See the link below:\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#:~:text=isn%27t%20enabled%20on%20any%20OUs.-,The%20artifact%20for%20this%20control%20is%20the%20following%20AWS%20Config%20rule.,-AWSTemplateFormatVersion%3A%202010%2D09%2D09"
      },
      {
        "date": "2023-05-11T04:38:00.000Z",
        "voteCount": 2,
        "content": "You still need to invite accounts before you can organize them in OUs.  All steps are needed.  I don't like the way they scatter between answers though."
      },
      {
        "date": "2023-03-27T05:17:00.000Z",
        "voteCount": 1,
        "content": "CDF seems the best choice"
      },
      {
        "date": "2023-02-21T10:26:00.000Z",
        "voteCount": 2,
        "content": "B. Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.\nD. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.\nF. Create a control for the production OU to detect EBS encryption.\n\nBy creating a new AWS Control Tower landing zone, the company can create OUs for accounts and add them to the appropriate production and development OUs. This will enable centralized governance and enforce consistent policies and best practices. The company can then invite existing accounts to join the organization in AWS Organizations and create SCPs to ensure compliance. Finally, the company can create a control for the production OU to detect EBS encryption, ensuring that encryption at rest is enforced in production accounts."
      },
      {
        "date": "2023-02-12T12:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is CDF\nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption"
      },
      {
        "date": "2023-02-17T13:53:00.000Z",
        "voteCount": 1,
        "content": "The artifact for this control is AWS Config rule and AWS Config rules cannot be deployed using AWS CloudFormation StackSets."
      },
      {
        "date": "2023-02-17T13:59:00.000Z",
        "voteCount": 1,
        "content": "moderator, delete above as the statement is incorrect that I posted, don't approve post."
      },
      {
        "date": "2023-02-12T00:38:00.000Z",
        "voteCount": 1,
        "content": "In F, guardrails are proposed to detect. Guardrails don't detect but prevent."
      },
      {
        "date": "2023-02-20T07:26:00.000Z",
        "voteCount": 1,
        "content": "I found this, and after further reading I vote for CDF: https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption"
      },
      {
        "date": "2023-02-10T16:59:00.000Z",
        "voteCount": 1,
        "content": "CloudTower and guard rails are custom built for this kind of situation"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/amazon/view/97663-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application.<br><br>The solution must meet the following objectives:<br><br>\u2022\tApplication tier: RPO of 2 minutes. RTO of 30 minutes<br>\u2022\tDatabase tier: RPO of 5 minutes. RTO of 30 minutes<br><br>The company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-15T20:56:00.000Z",
        "voteCount": 21,
        "content": "DRS includes EC2 instances as well not just data related as offered by DLM or Backup\n\nQ: What operating systems and applications are supported by AWS DRS?\nA: You can use AWS DRS to recover all of your applications and databases that run on supported Windows and Linux operating system versions. This includes critical databases such as Oracle, MySQL, and SQL Server, and enterprise applications such as SAP.\n\nAWS Elastic Disaster Recovery (DRS) vs AWS DLM vs AWS Backup \n\nYou should use DLM when you want to automate the creation, retention, and deletion of EBS snapshots. You should use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place."
      },
      {
        "date": "2023-02-02T12:44:00.000Z",
        "voteCount": 10,
        "content": "its understood that others cannot meet the RTO and RPO requirements, because restore from back can take time based on the size of the data"
      },
      {
        "date": "2024-05-25T11:27:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2024-08-25T23:17:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2024-04-23T00:21:00.000Z",
        "voteCount": 1,
        "content": "DRS Maintains state of EC2 machines while snapshot doesnt"
      },
      {
        "date": "2023-12-24T15:44:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-24T22:56:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-10-10T15:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct\nD is not correct because snapshot is one region and  must to be copied and keep in sync  to DR region which cannot meet the RTO...for sure D is wrong"
      },
      {
        "date": "2023-09-16T02:47:00.000Z",
        "voteCount": 1,
        "content": "DRS is faster to recover than Backups  &gt; https://youtu.be/07EHsPuKXc0?si=w_dZQKOAynE2T4JY"
      },
      {
        "date": "2023-07-04T10:28:00.000Z",
        "voteCount": 1,
        "content": "A for low RPO"
      },
      {
        "date": "2023-05-12T17:29:00.000Z",
        "voteCount": 1,
        "content": "I don't understand the sentence \"Update DNS records to point to the Global Accelerator endpoint\" in A and B. It doesn't make sense. I think it should \"update DNS records to point to the GA two static IP addresses or GA's DNS name"
      },
      {
        "date": "2023-04-10T06:33:00.000Z",
        "voteCount": 5,
        "content": "RDS Cross-region replication has the best RPO and RTO:\nhttps://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-disaster-recovery/choosing-database.html\n\nAWS Elastic Disaster Recovery also provide the best RTO/RPO (with Warm standby and active-active)\nhttps://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_planning_for_recovery_disaster_recovery.html"
      },
      {
        "date": "2023-04-06T20:32:00.000Z",
        "voteCount": 2,
        "content": "You are correct that AWS Elastic Disaster Recovery (DRS) can be used to recover both data and EC2 instances. However, in the scenario described in the question, the specified RPO and RTO objectives for the application tier can be met using Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes attached to the EC2 instances.\n\nWhile restoring from a backup can take time depending on the size of the data, using Amazon DLM to take snapshots of the EBS volumes provides a way to recover data within the specified RPO of 2 minutes and RTO of 30 minutes for the application tier.\n\nIn addition, creating a cross-Region read replica for the RDS DB instance provides a way to recover data within the specified RPO of 5 minutes and RTO of 30 minutes for the database tier."
      },
      {
        "date": "2024-05-20T07:09:00.000Z",
        "voteCount": 1,
        "content": "Option D doesn't mention DNS, so it's not correct"
      },
      {
        "date": "2023-04-06T20:33:00.000Z",
        "voteCount": 1,
        "content": "Overall, while AWS Elastic Disaster Recovery (DRS) can be a useful service in certain scenarios, it is not necessary in this case because the specified RPO and RTO objectives can be met using other AWS services such as Amazon Data Lifecycle Manager (Amazon DLM) and cross-Region read replicas for the RDS DB instance."
      },
      {
        "date": "2023-07-07T00:05:00.000Z",
        "voteCount": 1,
        "content": "The process of starting up new instances and mount the EBS volumes to them will absolutely take more than 30 minutes."
      },
      {
        "date": "2023-04-06T20:33:00.000Z",
        "voteCount": 1,
        "content": "Overall, while AWS Elastic Disaster Recovery (DRS) can be a useful service in certain scenarios, it is not necessary in this case because the specified RPO and RTO objectives can be met using other AWS services such as Amazon Data Lifecycle Manager (Amazon DLM) and cross-Region read replicas for the RDS DB instance."
      },
      {
        "date": "2023-04-06T20:34:00.000Z",
        "voteCount": 1,
        "content": "Option A is not the best solution because it involves using AWS Elastic Disaster Recovery, which is not necessary to meet the specified RPO and RTO objectives for the application and database tiers.\n\nAWS Elastic Disaster Recovery is a service that helps customers prepare for and recover from disasters by providing a cost-effective, fully managed, and scalable solution for disaster recovery. While it can be useful in certain scenarios, it is not necessary in this case because the specified RPO and RTO objectives can be met using other AWS services such as Amazon Data Lifecycle Manager (Amazon DLM) and cross-Region read replicas for the RDS DB instance.\n\nTherefore, Option D is a better solution because it meets the specified requirements without introducing unnecessary complexity or cost."
      },
      {
        "date": "2023-02-12T00:50:00.000Z",
        "voteCount": 2,
        "content": "I agree it's A"
      },
      {
        "date": "2023-02-02T07:44:00.000Z",
        "voteCount": 3,
        "content": "DRS should fulfill the requirements"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/amazon/view/99148-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase AWS Business Support or AWS Enterprise Support for the account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Trusted Advisor and review any \u201cLow Utilization Amazon EC2 Instances\u201d recommendations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-15T21:02:00.000Z",
        "voteCount": 10,
        "content": "Not B because, Trusted Advisor is available for Enterprise support only which is not cheap and the SA needs to cost optimize here. CPU, memory, and network relate to Compute so D for sure. C will enable to know how much actual memory/CPU is needed for instances and SA can provision based on cw logs"
      },
      {
        "date": "2024-10-05T17:24:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: C and D\n\"Memory utilization metrics are analyzed for the following resources: EC2 instances with the CloudWatch agent that's installed on them.\"\n"
      },
      {
        "date": "2024-03-13T08:33:00.000Z",
        "voteCount": 2,
        "content": "NOT Option B -   To have Compute Optimizer analyze the memory utilization metric of your instances, install the CloudWatch agent on your instances. Enabling Compute Optimizer to analyze memory utilization data for your instances provides an additional measurement of data that further improves Compute Optimizer's recommendations. \nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html#ec2-metrics-analyzed"
      },
      {
        "date": "2023-12-24T15:49:00.000Z",
        "voteCount": 2,
        "content": "Option C and D"
      },
      {
        "date": "2023-12-22T13:52:00.000Z",
        "voteCount": 1,
        "content": "The solutions architect should take the following two steps to meet the requirements:\n\nConfigure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations. Compute Optimizer uses machine learning to analyze historical utilization metrics and provides recommendations to reduce costs and increase workload performance by recommending the optimal instance types.\n\nTurn on AWS Trusted Advisor and review any \"Low Utilization Amazon EC2 Instances\" recommendations. Trusted Advisor checks for underutilized instances and provides recommendations to right-size them, helping optimize costs."
      },
      {
        "date": "2023-11-24T23:04:00.000Z",
        "voteCount": 2,
        "content": "C and D"
      },
      {
        "date": "2023-10-01T15:37:00.000Z",
        "voteCount": 2,
        "content": "AWS Trusted Advisor and AWS Compute Optimizer can both provide recommendations for right-sizing EC2 instances without requiring the installation of the CloudWatch agent or the collection of memory metrics.\n\nThe CloudWatch agent is primarily used for monitoring EC2 instances and collecting data for performance analysis. While it can be helpful to collect memory metrics for EC2 instances, it is not required for cost-optimizing and appropriately sizing them."
      },
      {
        "date": "2023-08-26T00:06:00.000Z",
        "voteCount": 1,
        "content": "AWS Compute Optimizer recommends optimal AWS resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics."
      },
      {
        "date": "2023-08-19T13:03:00.000Z",
        "voteCount": 1,
        "content": "Cloud Watch Agent for memory metric &amp; Compute Optimizer for recommendations"
      },
      {
        "date": "2023-07-04T10:30:00.000Z",
        "voteCount": 1,
        "content": "cd for sure"
      },
      {
        "date": "2023-04-26T16:05:00.000Z",
        "voteCount": 2,
        "content": "A &amp; B will incur more cost. CD are correct"
      },
      {
        "date": "2023-05-28T11:21:00.000Z",
        "voteCount": 1,
        "content": "Agreed. Answers are C,D\nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html"
      },
      {
        "date": "2023-03-27T05:23:00.000Z",
        "voteCount": 1,
        "content": "CD is right"
      },
      {
        "date": "2023-02-25T15:57:00.000Z",
        "voteCount": 1,
        "content": "trusted advisor does not take memory in consideration hence CD is right answer.\n\nhttps://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html"
      },
      {
        "date": "2023-02-16T10:48:00.000Z",
        "voteCount": 1,
        "content": "D,OK..  but, why not B trusted advisor rather than C cloudwatch ?"
      },
      {
        "date": "2023-03-12T02:15:00.000Z",
        "voteCount": 1,
        "content": "Memory taken by the os is almost always 100% - but most of it caches, buffers. To get you need the actually used memory by applications. This is number is os specific(need to ask the os how the memory is used: only caches or actual use?) and as such can't be gathered from the virtualizer. So you need an agent for that."
      },
      {
        "date": "2023-02-23T10:16:00.000Z",
        "voteCount": 3,
        "content": "seems like you need cloud watch agent installed in order to check memory parameter\nNote:\nTo have Compute Optimizer analyze the memory utilization of your instances, install the CloudWatch agent on your instances. Enabling Compute Optimizer to analyze memory utilization data for your instances provides an additional measurement of data that further improves Compute Optimizer's recommendations\nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html"
      },
      {
        "date": "2023-02-15T15:03:00.000Z",
        "voteCount": 2,
        "content": "CD according to https://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html"
      },
      {
        "date": "2023-02-14T05:13:00.000Z",
        "voteCount": 3,
        "content": "For Memory - CLoudwatch and \nCompute Optimizer"
      },
      {
        "date": "2023-02-15T07:52:00.000Z",
        "voteCount": 1,
        "content": "What about the other metrics?\nCPU and network metrics."
      },
      {
        "date": "2023-02-17T14:15:00.000Z",
        "voteCount": 2,
        "content": "CD is correct, cloudwatch agents supports the metrics mentioned. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/amazon/view/99150-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-13T15:10:00.000Z",
        "voteCount": 10,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html\nHard to believe a product from AWS can be designed in such an amateur way."
      },
      {
        "date": "2024-10-08T11:01:00.000Z",
        "voteCount": 1,
        "content": "AWS Backup does not support AWS CodeCommit directly."
      },
      {
        "date": "2024-10-08T11:01:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-24T23:07:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2023-11-17T00:26:00.000Z",
        "voteCount": 3,
        "content": "yes, AWS Backup cannot do this for you, so you should use Code Build to clone repo and upload zip to s3"
      },
      {
        "date": "2023-07-04T10:36:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-06-20T11:12:00.000Z",
        "voteCount": 1,
        "content": "b-b-b-b-b-b-b-b"
      },
      {
        "date": "2023-06-20T11:13:00.000Z",
        "voteCount": 3,
        "content": "b in incorrect as AWS Backup does not backup code commit as a source."
      },
      {
        "date": "2023-06-20T11:13:00.000Z",
        "voteCount": 2,
        "content": "c-c-c-c-cc-c-c-c-c-c-c"
      },
      {
        "date": "2023-05-28T11:09:00.000Z",
        "voteCount": 3,
        "content": "Answer : C\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html"
      },
      {
        "date": "2023-03-27T05:25:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2023-03-15T21:26:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html"
      },
      {
        "date": "2023-02-26T13:38:00.000Z",
        "voteCount": 3,
        "content": "https://www.automat-it.com/post/backup-aws-codecommit"
      },
      {
        "date": "2023-02-17T14:57:00.000Z",
        "voteCount": 2,
        "content": "C is correct, AWS Backup does not backup code commit as a source."
      },
      {
        "date": "2023-02-17T10:53:00.000Z",
        "voteCount": 2,
        "content": "B is wrong &gt; AWS Backup does not support CodeCommit as source.\nA is out.\nC is right."
      },
      {
        "date": "2023-02-15T15:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html"
      },
      {
        "date": "2023-02-15T09:55:00.000Z",
        "voteCount": 1,
        "content": "It says backup so I think B is the answer:\n\nB. Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region."
      },
      {
        "date": "2023-02-17T14:40:00.000Z",
        "voteCount": 2,
        "content": "Changing to C, thanks."
      },
      {
        "date": "2023-02-14T05:19:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-code-in-multiple-aws-regions-using-aws-codepipeline-aws-codecommit-and-aws-codebuild.html\n\nhttps://medium.com/geekculture/replicate-aws-codecommit-repositories-between-regions-using-codebuild-and-codepipeline-39f6b8fcefd2"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/amazon/view/99151-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company\u2019s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. Perform NAT where necessary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-14T05:20:00.000Z",
        "voteCount": 13,
        "content": "Private link is the solution for IP Overlapping and Securely access the app between accounts"
      },
      {
        "date": "2023-02-15T09:54:00.000Z",
        "voteCount": 10,
        "content": "With AWS PrivateLink, the marketing team can create an endpoint service to share their internal application with other accounts securely using private IP addresses. They can grant permission to specific AWS accounts to connect to the service and create interface VPC endpoints in the other accounts to access the application by using private IP addresses. This option does not require any changes to the network of the other business units, and it does not require peering or NATing. This solution is both scalable and secure."
      },
      {
        "date": "2023-12-20T10:36:00.000Z",
        "voteCount": 1,
        "content": "\"LEAST OPERATIONAL OVERHEAD\" - is key word in a question. Its not so easy to migrate any on-premise infra to any AWS. Looking at the answers here I see no one eve done that before and just answering as from AWS docs.\nThe easiest way to migrate any on-premise infra - ec2"
      },
      {
        "date": "2024-08-25T23:35:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-03-17T23:44:00.000Z",
        "voteCount": 1,
        "content": "who mentioned migration?!"
      },
      {
        "date": "2023-12-20T10:35:00.000Z",
        "voteCount": 1,
        "content": "\"LEAST OPERATIONAL OVERHEAD\" - is key word in a question. Its not so easy to migrate any on-premise infra to any AWS. Looking at the answers here I see no one eve done that before and just answering as from AWS docs.\nThe easiest way to migrate any on-premise infra - ec2"
      },
      {
        "date": "2024-08-25T23:35:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2023-11-24T23:10:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-04T10:37:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-22T15:57:00.000Z",
        "voteCount": 2,
        "content": "The solution that will meet the requirements with the least operational overhead is:\n\nC. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application using private IP addresses.\n\nAWS PrivateLink provides secure and scalable private connectivity between VPCs, AWS services, and on-premises applications, without using public IP addresses. In this case, you can create an AWS PrivateLink endpoint service for the marketing application, which allows other business units to access the application using private IP addresses.\n\nBy granting permission to specific AWS accounts to connect to the PrivateLink endpoint service, you can control access to the marketing application. Then, in each business unit's VPC, you can create interface VPC endpoints to connect to the PrivateLink service, allowing them to access the marketing application privately."
      },
      {
        "date": "2023-03-27T05:26:00.000Z",
        "voteCount": 1,
        "content": "Private link"
      },
      {
        "date": "2023-03-15T22:24:00.000Z",
        "voteCount": 5,
        "content": "Networking &amp; Content Delivery blog -\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/amazon/view/99152-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to audit the security posture of a newly acquired AWS account. The company\u2019s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type \u201cAccess Analyzer Finding\u201d with a filter for \u201cisPublic: true.\u201d Select the SNS topic as the EventBridge rule target.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule for the event type \u201cBucket-Level API Call via CloudTrail\u201d with a filter for \u201cPutBucketPolicy.\u201d Select the SNS topic as the EventBridge rule target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type \u201cConfig Rules Re-evaluation Status\u201d with a filter for \u201cNON_COMPLIANT.\u201d Select the SNS topic as the EventBridge rule target."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-03T14:14:00.000Z",
        "voteCount": 12,
        "content": "A. No, because Amazon S3 can NOT currently publish notifications for isPublic events.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\n\nB. Yes, because IAM Access Analyzer for S3 alerts you to S3 buckets that are configured to allow access to anyone on the internet or other AWS accounts\nhttps://aws.amazon.com/blogs/security/how-to-prioritize-iam-access-analyzer-findings/\n\nC. No, because PutBucketPolicy notifies us of an Amazon S3 bucket policy event to an Amazon S3 bucket, and we are looking for a SPECIFIC event to the bucket permissions, not ALL events.\n\nD. No, because cloudtrail-s3-dataevents-enabled checks if at least one AWS CloudTrail trail is logging Amazon Simple Storage Service (Amazon S3) data events for all S3 buckets.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-s3-dataevents-enabled.html"
      },
      {
        "date": "2023-03-16T19:36:00.000Z",
        "voteCount": 11,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html"
      },
      {
        "date": "2023-03-16T19:39:00.000Z",
        "voteCount": 8,
        "content": "Click on the \"Create rule\" button.\n\nEnter a name for the rule and a brief description, if desired.\n\nUnder \"Define pattern\", select \"Event pattern\".\n\nSelect \"Custom pattern\".\n\nIn the \"Event pattern\" field, enter the following code:\n\n{\n\"source\": [\"aws.securityhub\"],\n\"detail-type\": [\"Access Analyzer Finding\"],\n\"detail\": {\n\"findings\": [\n{\n\"isPublic\": [\ntrue\n]\n}\n]\n}\n}\n\nThis code will match all Access Analyzer Finding events where the \"isPublic\" field is set to \"true\"."
      },
      {
        "date": "2024-02-01T13:25:00.000Z",
        "voteCount": 1,
        "content": "This question.. is seriously ! a googling one"
      },
      {
        "date": "2023-09-12T06:28:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-04T14:00:00.000Z",
        "voteCount": 2,
        "content": "it's B"
      },
      {
        "date": "2023-06-23T22:27:00.000Z",
        "voteCount": 1,
        "content": "Ideally, I would use config rule, but here, of course, they suggest the wrong rule. The other option remains the access analyzer"
      },
      {
        "date": "2023-06-18T15:41:00.000Z",
        "voteCount": 2,
        "content": "keyword = AWS Identity and Access Management Access Analyzer\nthen B"
      },
      {
        "date": "2023-05-24T23:14:00.000Z",
        "voteCount": 1,
        "content": "The code by God_is_love did not worked for me. I guess something has been changed.\nThe following code worked in my environment.\n{\n\"source\":[\"aws.access-analyzer\"],\n\"detail-type\":[\"Access Analyzer Finding\"],\n\"detail\":\n{\n\"isPublic\":[true]\n}\n}"
      },
      {
        "date": "2023-05-24T14:44:00.000Z",
        "voteCount": 1,
        "content": "Aws is letter B \n\nPrevious writing is a error"
      },
      {
        "date": "2023-05-24T14:43:00.000Z",
        "voteCount": 1,
        "content": "Letter C"
      },
      {
        "date": "2023-05-24T14:43:00.000Z",
        "voteCount": 2,
        "content": "Solution D will not meet the requirements because it will notify the data security team whenever an S3 bucket is not compliant with the cloudtrail-s3-dataevents-enabled rule, even if the bucket is not publicly exposed. The cloudtrail-s3-dataevents-enabled rule checks if at least one AWS CloudTrail trail is logging Amazon Simple Storage Service (Amazon S3) data events for all S3 buckets. If a bucket is not compliant with this rule, it does not mean that the bucket is publicly exposed. The bucket may simply not be logging S3 data events."
      },
      {
        "date": "2023-05-24T14:43:00.000Z",
        "voteCount": 1,
        "content": "Here are some reasons why an S3 bucket may not be logging S3 data events:\n\nThe bucket may not have a CloudTrail trail associated with it.\nThe CloudTrail trail for the bucket may not be enabled.\nThe CloudTrail trail for the bucket may not be configured to log S3 data events.\nIf the data security team is only interested in being notified when an S3 bucket becomes publicly exposed, then solution D is not the best solution. Solution B is a better solution because it will only notify the data security team when an S3 bucket becomes publicly exposed."
      },
      {
        "date": "2023-05-21T18:12:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-eventbridge.html"
      },
      {
        "date": "2023-03-27T05:28:00.000Z",
        "voteCount": 2,
        "content": "B eventbirdge and access analyser"
      },
      {
        "date": "2023-02-19T19:07:00.000Z",
        "voteCount": 7,
        "content": "B is the correct solution because it uses AWS Identity and Access Management Access Analyzer to continuously monitor access control configurations and detect whether any S3 buckets have been configured to be publicly accessible. When a publicly accessible bucket is detected, an Amazon EventBridge rule is triggered, and the SNS topic is notified with the finding."
      },
      {
        "date": "2023-02-16T01:03:00.000Z",
        "voteCount": 2,
        "content": "Access Analyzer is to assess the access policy.\nhttps://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/access-control-block-public-access.html"
      },
      {
        "date": "2023-02-15T11:03:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/security/how-to-use-aws-iam-access-analyzer-api-to-automate-detection-of-public-access-to-aws-kms-keys/"
      },
      {
        "date": "2023-02-15T08:25:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html"
      },
      {
        "date": "2023-02-14T05:26:00.000Z",
        "voteCount": 2,
        "content": "D by elimination rule"
      },
      {
        "date": "2023-05-06T03:57:00.000Z",
        "voteCount": 1,
        "content": "I thought D, as well, but it seems everyone else things Access Analyzer."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/amazon/view/99153-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to assess a newly acquired company\u2019s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.<br><br>The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-14T05:47:00.000Z",
        "voteCount": 16,
        "content": "First need to evaluate"
      },
      {
        "date": "2023-02-15T09:49:00.000Z",
        "voteCount": 7,
        "content": "C. Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies."
      },
      {
        "date": "2023-11-24T23:17:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-04T14:02:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-05-28T09:53:00.000Z",
        "voteCount": 2,
        "content": "Answer : C\nhttps://aws.amazon.com/migration-evaluator/"
      },
      {
        "date": "2023-05-27T17:52:00.000Z",
        "voteCount": 1,
        "content": "The emphasis is on applications. \"Some applications are batch processes that run at the end of each month\" \nI do not understand why C is better than B"
      },
      {
        "date": "2024-08-25T23:38:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2023-03-27T05:32:00.000Z",
        "voteCount": 3,
        "content": "Use migration evaluator"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/amazon/view/99154-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.<br><br>Which solution will meet these requirements while providing the FASTEST storage performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-15T09:48:00.000Z",
        "voteCount": 11,
        "content": "Explanation: Amazon EFS provides shared file storage that is highly available and durable. It is an ideal solution to share files between containers running on multiple instances in a cluster. Mounting an Amazon EFS file system on each subnet provides a shared file system for multiple instances running in different Availability Zones. Additionally, AWS Backup provides automated backup and recovery of Amazon EFS file systems."
      },
      {
        "date": "2023-02-14T05:54:00.000Z",
        "voteCount": 7,
        "content": "EFS = Fastest storage performance compare to S3/EBS"
      },
      {
        "date": "2023-02-16T01:19:00.000Z",
        "voteCount": 1,
        "content": "I vote B.\nI think EBS is faster than S3/EBS.\nhttps://www.msp360.com/resources/blog/amazon-s3-vs-ebs-vs-efs/"
      },
      {
        "date": "2024-08-25T23:40:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2023-02-16T01:20:00.000Z",
        "voteCount": 2,
        "content": "typo.\nEBS faster than S3/EFS."
      },
      {
        "date": "2023-02-16T09:59:00.000Z",
        "voteCount": 9,
        "content": "I just read the question refers to multiple AZs, so B is not an option."
      },
      {
        "date": "2024-10-08T11:09:00.000Z",
        "voteCount": 1,
        "content": "I missed this too \ud83d\udc4f good spot"
      },
      {
        "date": "2023-11-24T23:29:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-01T09:43:00.000Z",
        "voteCount": 4,
        "content": "A: sounds valid\nB: EBS multi attach can only do same AZ -&gt; out \nC: S3 is for durability, not for perfomance\nD: can drop when seeing third party tool."
      },
      {
        "date": "2023-07-04T14:04:00.000Z",
        "voteCount": 2,
        "content": "A - EFS for multi-AZ"
      },
      {
        "date": "2023-07-03T13:25:00.000Z",
        "voteCount": 4,
        "content": "A. Yes, because Amazon EFS offers you the choice of creating file systems using Standard or One Zone storage classes. Standard storage classes store data with and across multiple AZs.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/run-stateful-workloads-with-persistent-data-storage-by-using-amazon-efs-on-amazon-eks-with-aws-fargate.html\n\nB. No, because Amazon EBS Multi-Attach enabled volumes can be attached to up to 16 Linux instances built on the Nitro System that are in the same Availability Zone. We need to solve for \"nodes in multiple Availability Zones\"\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\n\nC. No, because  if you\u2019re looking to run file-based applications that need to collaborate or coordinate on shared data across instances or users, AWS recommends fully managed file services, such as Amazon FSx or Amazon Elastic File System (EFS).\n\nD. No, because the company needs to back up the files, not backup the EKS Cluster."
      },
      {
        "date": "2023-03-27T06:46:00.000Z",
        "voteCount": 2,
        "content": "A for sure"
      },
      {
        "date": "2023-03-26T10:52:00.000Z",
        "voteCount": 3,
        "content": "Keyword here is multiple small files and shared between multiple clusters"
      },
      {
        "date": "2023-03-16T20:47:00.000Z",
        "voteCount": 3,
        "content": "In the past, EBS can be attached only to one ec2 instance but not anymore but there are limitations like - it works only on io1/io2 instance types and many others as described here. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\nEFS has shareable storage \n\nIn terms of performance, Amazon EFS is optimized for workloads that require high levels of aggregate throughput and IOPS, whereas EBS is optimized for low-latency, random access I/O operations. Amazon EFS is designed to scale throughput and capacity automatically as your storage needs grow, while EBS volumes can be resized on demand."
      },
      {
        "date": "2023-03-09T03:23:00.000Z",
        "voteCount": 1,
        "content": "I support A since their is a multi-AZ requirement.\n\nhttps://repost.aws/questions/QUK2RANw1QTKCwpDUwCCI72A/efs-vs-ebs-mult-attach\n\nEFS is also designed for high availability and high durability. To achieve these levels of availability and durability, EFS automatically replicates data within and across 3 Availability Zones, with no single points of failure. EBS multi-attach volumes can be used for clients within a single Availability Zone."
      },
      {
        "date": "2023-03-06T05:45:00.000Z",
        "voteCount": 3,
        "content": "When you have an EKS cluster and use the EBS that is local to the node, only Pods running on that node have access to the storage. If the node starts on any other Pod, it will potentially break. There are ways to fix this, but they are beyond this question. I believe we need shared fast storage here, so it should be S3 vs EFS the decision."
      },
      {
        "date": "2023-02-16T09:58:00.000Z",
        "voteCount": 2,
        "content": "I've been reding here and there, and B does not seem that feasible, although if supported it would be faster than A."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/amazon/view/99155-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.<br><br>Which solution will meet these requirements with the LEAST ongoing operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-16T20:57:00.000Z",
        "voteCount": 12,
        "content": "Amazon Connect is a cloud-based contact center service that allows you to set up a virtual call center for your business. It provides an easy-to-use interface for managing customer interactions through voice and chat. Amazon Connect integrates with other AWS services, such as Amazon S3 and Amazon Kinesis, to help you collect, store, and analyze customer data for insights into customer behavior and trends.\n\nOn the other hand, Amazon Pinpoint is a marketing automation and analytics service that allows you to engage with your customers across different channels, such as email, SMS, push notifications, and voice. It helps you create personalized campaigns based on user behavior and enables you to track user engagement and retention.\n\nWhile both services allow you to communicate with your customers, they serve different purposes. Amazon Connect is focused on customer support and service, while Amazon Pinpoint is focused on marketing and engagement."
      },
      {
        "date": "2023-12-20T10:37:00.000Z",
        "voteCount": 1,
        "content": "\"LEAST OPERATIONAL OVERHEAD\" - is key word in a question. Its not so easy to migrate any on-premise infra to any AWS. Looking at the answers here I see no one eve done that before and just answering as from AWS docs.\nThe easiest way to migrate any on-premise infra - ec2"
      },
      {
        "date": "2023-11-24T23:35:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-07-12T07:28:00.000Z",
        "voteCount": 1,
        "content": "Why not b though? SNS is easy as heck to use."
      },
      {
        "date": "2023-07-12T07:28:00.000Z",
        "voteCount": 3,
        "content": "nvm text message surveys are probably a pinpoint thing. I was thinking like a link to a survey."
      },
      {
        "date": "2024-02-26T17:59:00.000Z",
        "voteCount": 4,
        "content": "\"managed, interactive, two-way experience\" means a personalised and customised message, so it should be Pinpoint here."
      },
      {
        "date": "2023-07-04T14:05:00.000Z",
        "voteCount": 1,
        "content": "A - basic AWS connect use case"
      },
      {
        "date": "2023-06-23T23:21:00.000Z",
        "voteCount": 1,
        "content": "Amazon connect + Pinpoint are the best choice here"
      },
      {
        "date": "2023-05-28T09:31:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2023-03-27T06:47:00.000Z",
        "voteCount": 1,
        "content": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers."
      },
      {
        "date": "2023-02-15T09:48:00.000Z",
        "voteCount": 3,
        "content": "The solution that will meet the company's requirements with the LEAST ongoing operational overhead and send two-way experience survey is to use Amazon Connect to replace the old call center hardware and use Amazon Pinpoint to send text message surveys to customers. Amazon Connect is a fully managed, cloud-based contact center service that is easy to set up and configure, while Amazon Pinpoint can be used to send text message surveys and gather responses. By using these services, the company can offload the operational overhead of running and maintaining the call center hardware and survey system to AWS."
      },
      {
        "date": "2023-02-14T06:02:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/amazon/view/99304-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building a call center by using Amazon Connect. The company\u2019s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.<br><br>Which solution will provide DR with the LOWEST RTO?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-16T21:42:00.000Z",
        "voteCount": 10,
        "content": "The solution that will provide DR with the LOWEST RTO (Recovery Time Objective) is option D.\n\nOption D provisions a new Amazon Connect instance with all existing users and contact flows in a second Region. It also sets up an Amazon Route 53 health check for the URL of the Amazon Connect instance, an Amazon CloudWatch alarm for failed health checks, and an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. This option allows for the fastest recovery time because all the necessary components are already provisioned and ready to go in the second Region. In the event of a disaster, the failed health check will trigger the AWS Lambda function to deploy the CloudFormation template to provision the claimed phone numbers, which is the only missing component."
      },
      {
        "date": "2023-02-19T13:32:00.000Z",
        "voteCount": 9,
        "content": "D looks most appropriate"
      },
      {
        "date": "2024-01-08T12:40:00.000Z",
        "voteCount": 3,
        "content": "Amazon Connect is not on the list of services required for this exam. At least as of 08.01.24 https://d1.awsstatic.com/training-and-certification/docs-sa-pro/AWS-Certified-Solutions-Architect-Professional_Exam-Guide.pdf"
      },
      {
        "date": "2023-11-24T23:38:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-17T03:45:00.000Z",
        "voteCount": 1,
        "content": "Amazon Connect gives you a URL, for which you can add a record in route 53 and hence have a health check."
      },
      {
        "date": "2023-08-20T09:49:00.000Z",
        "voteCount": 3,
        "content": "D seems to fit all requirements, however C &amp; D seem to be very similar. Only difference is whether to upload users or phone numbers through Cloud Formation. It seems users, routing profiles, queues, and flows get created with ReplicateInstance API\nhttps://docs.aws.amazon.com/connect/latest/adminguide/create-replica-connect-instance.html"
      },
      {
        "date": "2023-07-29T21:45:00.000Z",
        "voteCount": 1,
        "content": "Apparently Route 53 can't manage Amazon Connect DNS names or health checks.\nhttps://docs.aws.amazon.com/connect/latest/adminguide/update-your-connect-domain.html#new-domain-custom"
      },
      {
        "date": "2023-07-04T14:13:00.000Z",
        "voteCount": 1,
        "content": "D i guess"
      },
      {
        "date": "2023-06-23T23:45:00.000Z",
        "voteCount": 1,
        "content": "I vote for B since I was not able to find a way to make Route53 serve the Amazon connect URL and therefore it cannot perform healthcheck. If someone has more information on this - please share"
      },
      {
        "date": "2023-05-24T15:23:00.000Z",
        "voteCount": 3,
        "content": "why not letter C \n\"CloudFormation template that provisions all users\" insted of  \"CloudFormation template that provisions claimed phone numbers\" of letter D"
      },
      {
        "date": "2023-04-10T08:52:00.000Z",
        "voteCount": 1,
        "content": "I'm voting B because i don't think it's possible to use Amazon Route 53 health check to verify the availability  of Amazon Connect"
      },
      {
        "date": "2023-03-27T16:09:00.000Z",
        "voteCount": 1,
        "content": "why not C?"
      },
      {
        "date": "2024-01-28T07:27:00.000Z",
        "voteCount": 2,
        "content": "I think, but I was not able to very it, that if your instance is active and you have phone numbers configured it is receiving actual phone traffic that is a and Active/Active scenario, however you do not have users (aka Agents) configured to handle calls. This is just me guessing"
      },
      {
        "date": "2023-03-27T06:49:00.000Z",
        "voteCount": 3,
        "content": "D. Provision a new Amazon Connect instance with all existing users and contact flows in a second Region."
      },
      {
        "date": "2023-02-17T15:23:00.000Z",
        "voteCount": 3,
        "content": "D is the better solution."
      },
      {
        "date": "2023-02-15T09:41:00.000Z",
        "voteCount": 2,
        "content": "B. Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region will provide disaster recovery with the LOWEST Recovery Time Objective."
      },
      {
        "date": "2023-02-17T15:22:00.000Z",
        "voteCount": 2,
        "content": "Thanks for pointing that out,  D is the better solution."
      },
      {
        "date": "2023-02-16T11:14:00.000Z",
        "voteCount": 5,
        "content": "With D you can have a quicker reaction if you use high-resolution CloudWatch alarms that alert as soon as 10-second or 30-second periods. Additionally, contact flows are alredy there so you don't need to deploy when the error occurs."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/amazon/view/107088-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.<br><br>The company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-25T20:43:00.000Z",
        "voteCount": 10,
        "content": "The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data. With B, customer can get data from Redshift directly  with no time lag and additional  operations."
      },
      {
        "date": "2023-04-25T03:58:00.000Z",
        "voteCount": 9,
        "content": "I think it's B.\nAccording to https://aws.amazon.com/data-exchange/why-aws-data-exchange/redshift-data-tables/\n\nCustomers can find and subscribe to third-party data in AWS Data Exchange and directly query the data in minutes in Amazon Redshift without extracting, transforming, or loading it. \n\nIn B, customers can query Redshift directly. No need to use S3 periodically. Minimizes operational overhead."
      },
      {
        "date": "2023-07-05T11:34:00.000Z",
        "voteCount": 1,
        "content": "it's a B"
      },
      {
        "date": "2023-07-03T08:02:00.000Z",
        "voteCount": 5,
        "content": "Keyword is datashare\nhttps://docs.aws.amazon.com/redshift/latest/dg/adx-getting-started.html"
      },
      {
        "date": "2023-06-20T11:24:00.000Z",
        "voteCount": 3,
        "content": "b-b-b-b-bb-b-b-b-b-b\nLEAST operational overhead...\nOption (A) uses AWS Data Exchange for APIs, which requires you to create an Amazon API Gateway Data API service integration with Amazon Redshift. This is a more complex solution than using a datashare.\n\nOption (C) uses AWS Data Exchange for S3, which requires you to download the data from Amazon Redshift to Amazon S3 periodically. This is also a more complex solution than using a datashare.\n\nOption (D) publishes the data to an Open Data on AWS Data Exchange, which does not allow you to configure subscription verification. This means that anyone can access the data, which is not ideal for a company that wants to protect its proprietary algorithms."
      },
      {
        "date": "2023-06-18T02:49:00.000Z",
        "voteCount": 5,
        "content": "AWS Data Exchange for APIs enables customers to discover and utilize third-party APIs in the cloud, with authentication using AWS IAM credentials and SDKs. It simplifies access permissions and governance. Users can access data APIs from numerous providers. On the other hand, AWS Data Exchange Datashare focuses on licensing access to Amazon Redshift data. It utilizes AWS-native authentication and automatically adds customers as data consumers. With read-only access, customers can retrieve objects from datashares. While both services integrate with AWS, Data Exchange for APIs is geared towards API usage, while Data Exchange Datashare is centered around licensing access to Amazon Redshift data."
      },
      {
        "date": "2023-05-28T09:04:00.000Z",
        "voteCount": 3,
        "content": "Answer : B\nhttps://www.youtube.com/watch?v=BeIoTSql4IM   \n(AWS Data Exchange for Amazon Redshift demo | Amazon Web Services)"
      },
      {
        "date": "2023-05-16T13:42:00.000Z",
        "voteCount": 2,
        "content": "B is the closest one but is not correct either.\nhttps://docs.amazonaws.cn/en_us/redshift/latest/dg/adx-getting-started-producer.html, like every thing else in AWS you need policy to grant access and that is missing in B."
      },
      {
        "date": "2023-04-25T00:28:00.000Z",
        "voteCount": 1,
        "content": "\u30c7\u30fc\u30bf\u306e\u9867\u5ba2\u6570\u306f\u5927\u5e45\u306b\u5897\u52a0\u3057\u305f\u5bfe\u7b56\u306bS3"
      },
      {
        "date": "2023-06-20T11:19:00.000Z",
        "voteCount": 5,
        "content": "yup! was about to say the same."
      },
      {
        "date": "2023-11-05T01:40:00.000Z",
        "voteCount": 1,
        "content": "hahahaaha"
      },
      {
        "date": "2023-04-23T22:33:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C. Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.\n\nExporting the data to an Amazon S3 bucket periodically ensures that customers have access to the most recent data when the company publishes it.\nAWS Data Exchange for S3 allows you to share data with customers easily and manage their subscriptions.\nSubscription verification helps confirm the identity of customers before sharing data with them.\nThis solution minimizes operational overhead as it leverages AWS Data Exchange and Amazon S3, which are managed services.\nThe unique keywords combination in this option that makes it easier to remember is Amazon S3, AWS Data Exchange, and subscription verification."
      },
      {
        "date": "2023-04-22T23:08:00.000Z",
        "voteCount": 2,
        "content": "Answer is B. https://aws.amazon.com/data-exchange/?adx-cards2.sort-by=item.additionalFields.eventDate&amp;adx-cards2.sort-order=desc"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/amazon/view/107090-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the solution receives. If a processing error occurs, the event must move into a separate queue for review.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 90,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 56,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-25T11:11:00.000Z",
        "voteCount": 24,
        "content": "I would go with B just because of the wording. I believe A should work just fine, but the question asks for \"scale in and out based on the number of events.\" In my opinion, that is what SNS-&gt;Lambda-&gt;SQS(DLQ) would do, too; I think the SNS-&gt;Lambda scale in/out behavior is more implicit. So I will go with B here because it is more explicit."
      },
      {
        "date": "2023-11-17T05:06:00.000Z",
        "voteCount": 10,
        "content": "Configuring scaling based on the age of the oldest message is nowhere near as good as scaling based on size of the Queue for this use case.\n\nage of the oldest message will grow linearly based on time. If there is a dramatic spike in the Queue size due to increased traffic, like 100X increase in size. Then the queue will have grown a lot but the oldest message will only increase in age linearly, so the scaling will not be able to realize how much the workload has increased."
      },
      {
        "date": "2024-08-25T23:48:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-08-05T23:51:00.000Z",
        "voteCount": 1,
        "content": "there will just be a lag in scaling, but eventually this metric will scale as needed"
      },
      {
        "date": "2023-11-26T06:09:00.000Z",
        "voteCount": 1,
        "content": "makes sense"
      },
      {
        "date": "2023-11-30T03:33:00.000Z",
        "voteCount": 1,
        "content": "very good explanation. Moreover, go serverless as much as possible. EC2 vs Lambda - Lamda is always preferred."
      },
      {
        "date": "2024-10-12T02:17:00.000Z",
        "voteCount": 1,
        "content": "Only B and D mention about reviewing error in a separate queue by dead letter Q, with D never use SQS where this is supported."
      },
      {
        "date": "2024-08-15T12:00:00.000Z",
        "voteCount": 1,
        "content": "People who are choosing A. Have you done associate level certs as this does not make any sense. You shouldnt be attempting this exam if that's how lost you are."
      },
      {
        "date": "2024-08-06T21:16:00.000Z",
        "voteCount": 1,
        "content": "The Auto Scaling group of EC2 instances can automatically adjust the number of instances based on the ApproximateAgeOfOldestMessage metric. This ensures that the solution scales dynamically with the volume of events, maintaining efficient processing.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html"
      },
      {
        "date": "2024-08-15T00:23:00.000Z",
        "voteCount": 2,
        "content": "Dude, check you link again, which says \"ApproximateNumberOfMessages\" not \" ApproximateAgeOfOldestMessage\", so answer will be option A."
      },
      {
        "date": "2024-07-24T22:54:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2024-07-09T09:07:00.000Z",
        "voteCount": 1,
        "content": "I do think the B is correct way with ApproximateNumberOfMessages instead of ApproximateAgeOfOldestMessage. there is no such metric ...AgeOfOldestMessage"
      },
      {
        "date": "2024-05-04T09:00:00.000Z",
        "voteCount": 1,
        "content": "publishing events to an SQS queue, creating an EC2 Auto Scaling group that scales based on the queue's ApproximateAgeOfOldestMessage metric, and configuring the application to write failed messages to a dead-letter queue provides a scalable, fault-tolerant, and cost-effective solution for event processing with the ability to handle processing errors separately."
      },
      {
        "date": "2024-05-02T10:07:00.000Z",
        "voteCount": 3,
        "content": "Vote for A"
      },
      {
        "date": "2024-05-02T10:07:00.000Z",
        "voteCount": 1,
        "content": "A seems better. Dont really know."
      },
      {
        "date": "2024-03-14T06:15:00.000Z",
        "voteCount": 1,
        "content": "Option B.  Question states, \" must move to into a separate queue for review\"   Dead-Letter queues give you this capability for debugging or troubleshooting the issue.  https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "date": "2024-03-07T05:10:00.000Z",
        "voteCount": 6,
        "content": "A fulfils all objectives.\nIn B, ApproximateAgeOfOldestMessage doesn't translate to a reliable scaling pattern, and EC2s are implied.\nC does not implement a dead-letter queue\nD is overengineered."
      },
      {
        "date": "2024-02-20T12:43:00.000Z",
        "voteCount": 1,
        "content": "SQS with DLQ"
      },
      {
        "date": "2024-02-11T18:53:00.000Z",
        "voteCount": 5,
        "content": "ApproximateAgeOfOldestMessage metric may not be as responsive as needed, and it doesn't directly address the requirement for handling processing errors by moving events to a separate queue for review."
      },
      {
        "date": "2024-02-01T13:48:00.000Z",
        "voteCount": 2,
        "content": "The question typical AWS thrown words and leaving gaps.. but still going for B"
      },
      {
        "date": "2023-12-20T04:14:00.000Z",
        "voteCount": 4,
        "content": "I would go with A. \nScaling in/out based on message age does not align with what question asks i.e. it should be based on the number of events. So, B is not right here."
      },
      {
        "date": "2024-01-25T17:48:00.000Z",
        "voteCount": 1,
        "content": "Scaling based on how long someone is waiting is another way of basing it on the number of events, but I see what you mean. Lambda will scale based on the number 1:1 and B will scale in whatever configuration you want based on time, not number of events specifically."
      },
      {
        "date": "2023-12-13T07:57:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nI would go with A, except a Dead Letter Q is not an SQS queue.  There are only two types of SQS Queues, namely, Standard and FIFO.   \n\nA DLQ is a special message queue (not SQS).  See here for confirmation: https://aws.amazon.com/what-is/dead-letter-queue/#:~:text=A%20dead%2Dletter%20queue%20(DLQ)%20is%20a%20special%20type,communication%20in%20a%20distributed%20system."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/amazon/view/107094-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.<br><br>The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-13T15:44:00.000Z",
        "voteCount": 18,
        "content": "Option A is incorrect because Application Load Balancer (ALB) can't directly target an Amazon SQS queue.\n\nOption C is incorrect because while Amazon API Gateway and EC2 Auto Scaling can handle high loads, they don't provide a built-in mechanism to ensure that all messages are processed without loss.\n\nOption D is incorrect because Amazon CloudFront is a content delivery network (CDN), and it is not typically used to handle incoming API requests. It is primarily used to cache and deliver content to users."
      },
      {
        "date": "2023-12-13T18:20:00.000Z",
        "voteCount": 6,
        "content": "In real life, I wouldn't trust SQS to handle such large amount of data."
      },
      {
        "date": "2024-07-09T08:57:00.000Z",
        "voteCount": 1,
        "content": "Restful API is not REST API, so HTTP API-GW + SQS"
      },
      {
        "date": "2024-02-28T01:42:00.000Z",
        "voteCount": 2,
        "content": "but normally API gateway can not handle high burst request. it will make 429 too many requests error."
      },
      {
        "date": "2023-11-25T15:09:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-11-23T17:31:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-11-17T04:36:00.000Z",
        "voteCount": 2,
        "content": "yes you can integrate API Gateway HTTP Api with SQS"
      },
      {
        "date": "2023-08-20T10:28:00.000Z",
        "voteCount": 1,
        "content": "KDS need to implement Sharding for unpredictable bursts"
      },
      {
        "date": "2023-07-23T11:43:00.000Z",
        "voteCount": 1,
        "content": "Similar to #179"
      },
      {
        "date": "2023-07-05T11:49:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-05-27T22:05:00.000Z",
        "voteCount": 1,
        "content": "Answer : B"
      },
      {
        "date": "2023-05-22T14:48:00.000Z",
        "voteCount": 2,
        "content": "I agree with B \nhttps://aws.amazon.com/blogs/architecture/things-to-consider-when-you-build-rest-apis-with-amazon-api-gateway/\nThis pattern can decouple the data ingestion from the data processing.\n\"you should look for opportunities to design an asynchronous, loosely coupled architecture. A decoupled architecture separates the data ingestion from the data processing and allows you to scale each system separately\""
      },
      {
        "date": "2023-05-07T04:04:00.000Z",
        "voteCount": 2,
        "content": "Kinesis DataStreams can't be the origin for the CloudFront"
      },
      {
        "date": "2023-04-26T02:36:00.000Z",
        "voteCount": 1,
        "content": "Kinesis retention"
      },
      {
        "date": "2023-04-25T12:44:00.000Z",
        "voteCount": 1,
        "content": "Kinesis retention"
      },
      {
        "date": "2023-04-26T02:37:00.000Z",
        "voteCount": 1,
        "content": "Answer D, sorry typo"
      },
      {
        "date": "2023-04-25T11:35:00.000Z",
        "voteCount": 1,
        "content": "B is the best option."
      },
      {
        "date": "2023-04-24T00:58:00.000Z",
        "voteCount": 3,
        "content": "B is correct, you can integrate SQS with API Gateway HTTP. I have checked it in AWS API Gateway Console\nhttps://repost.aws/knowledge-center/api-gateway-rest-api-sqs-errors\n\nA is incorrect because you can not set SQS queue as the target of ALB \nC is incorrect because a fleet of EC2 instances and ASG can lead instances to terminated unexpectedly \u2192 data loss\nD is incorrect because Kinesis Data Streams is a provisioned service, It can not handle unpredictable bursts"
      },
      {
        "date": "2023-04-25T21:39:00.000Z",
        "voteCount": 1,
        "content": "KDS has on-demand mode.\nhttps://docs.aws.amazon.com/streams/latest/dev/how-do-i-size-a-stream.html"
      },
      {
        "date": "2023-04-28T19:59:00.000Z",
        "voteCount": 1,
        "content": "Yes, KDS has on-demand mode, my wrong. But according to the above link, KDS on-demand can only accommodate up to double the peak write throughput observed in the previous 30 days. While SQS standard Queue has Unlimited Throughput\nhttps://aws.amazon.com/sqs/features/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/amazon/view/107022-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The company has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the same AWS Region.<br><br>The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive traffic between the VPCs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-20T10:34:00.000Z",
        "voteCount": 12,
        "content": "Fits the use case \nhttps://aws.amazon.com/transit-gateway/"
      },
      {
        "date": "2023-08-20T10:38:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-isolated.html"
      },
      {
        "date": "2023-10-14T02:55:00.000Z",
        "voteCount": 5,
        "content": "C.\nTransit gateway and RAM is a regional service.  \nAWS RAM is a Regional service, and a resource share is Regional. Therefore, a resource share can contain resources from the same AWS Region as the resource share, and any supported global resources. \nhttps://docs.aws.amazon.com/ram/latest/userguide/working-with-regional-vs-global.html\nhttps://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-orgs"
      },
      {
        "date": "2024-04-29T20:31:00.000Z",
        "voteCount": 1,
        "content": "Typical transit gateway use case"
      },
      {
        "date": "2024-03-15T22:12:00.000Z",
        "voteCount": 3,
        "content": "The question is asking \u201ca solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs\u201d. \nA: it works. But it may create 2500+ VPC peering in each OU\nB: It works. But it may create 2500+ VPC peering in each OU\nC: This is wrong, cause it is sharing the transit gateway to all the account in the organization instead of sharing to all the account in that OU.\nD: That means 2500+ VPN connections in each OU and cost a lot of internet bandwidth.\nI guess the C was worded with mistake. It should be sharing the transit gateway to the accounts in each OU and create VPC attachment for each VPC in that OU."
      },
      {
        "date": "2024-02-26T19:44:00.000Z",
        "voteCount": 1,
        "content": "The requirement said, \"VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs\". There is no reason to share the TGW across the organisation with RAM because it will enable cross OUs communication."
      },
      {
        "date": "2024-01-28T08:24:00.000Z",
        "voteCount": 5,
        "content": "Option C is very poorly worded: \"Provision a transit gateway in an account in each OU\" to me this results in having 3 Transit Gateways, but then it go ahead just referring to a single Transit Gateway \"Share the transit gateway across the organization ...\""
      },
      {
        "date": "2024-01-02T04:21:00.000Z",
        "voteCount": 2,
        "content": "\"Least operational overhead\"\nA is correct.\nC creating Transit Gateway in each account.. and there are more than 100 accounts in each OU. Which is time consuming and requires lot of efforts."
      },
      {
        "date": "2024-01-15T02:32:00.000Z",
        "voteCount": 4,
        "content": "\"A\" would mean having 1:1 peering attachments with EACH ACCOUNT which is too much operational overhead. A transit gateway is more viable so it's \"C\"."
      },
      {
        "date": "2023-11-30T06:18:00.000Z",
        "voteCount": 1,
        "content": "typical use case of intra region peering with transit gateway."
      },
      {
        "date": "2023-11-30T06:18:00.000Z",
        "voteCount": 1,
        "content": "oops right answer is 'C'."
      },
      {
        "date": "2023-11-23T17:34:00.000Z",
        "voteCount": 3,
        "content": "Option C"
      },
      {
        "date": "2023-07-29T22:28:00.000Z",
        "voteCount": 1,
        "content": "A for two reasons:\n1. Sharing the TGW with the entire organization (C) will make every VPC in every account propagate its subnet in the default TGW route table which will enable organization-wide communication which is categorically prohibited by the question.\n2. The question only says more than 100 accounts and 1 VPC per account. It does not mention anything about 125+ VPCs. Plus the peerings are being created by stack sets so there's automation involved. So I believe A is the only solution here."
      },
      {
        "date": "2023-07-29T22:33:00.000Z",
        "voteCount": 1,
        "content": "Disabling default route table association/propagation could be a solution for TGW, but creating 100s of VPC attachments manually is too much operational overhead."
      },
      {
        "date": "2023-07-05T11:53:00.000Z",
        "voteCount": 3,
        "content": "I thik C"
      },
      {
        "date": "2023-07-02T17:09:00.000Z",
        "voteCount": 4,
        "content": "C. Yes, because, Transit Gateway is a managed service from AWS that acts as a hub interconnecting VPCs and VPN connections within a single region. It allows you to build more complex networks without the need for VPC peering.\nSimilar to: https://aws.amazon.com/blogs/networking-and-content-delivery/automating-aws-transit-gateway-attachments-to-a-transit-gateway-in-a-central-account/\n \nA,B. No, because a VPC peering connection has a limit of 125 Active VPC peering connections per VPC. In this case, each OU contains MORE THAN 100 AWS accounts -- this could mean 101 accounts or 10001 accounts.\n\nD. No, because this is not the answer choice with the LEAST operational overhead. Third-party routing software is not required to route transitive traffic between the VPCs."
      },
      {
        "date": "2023-08-11T15:04:00.000Z",
        "voteCount": 1,
        "content": "I believe in this context the organization is the OU, not the entire company. The company is referred to as \"the company\". \nTherefore it's C."
      },
      {
        "date": "2023-06-27T06:11:00.000Z",
        "voteCount": 2,
        "content": "A separate transit GW for each OU."
      },
      {
        "date": "2023-06-24T00:32:00.000Z",
        "voteCount": 2,
        "content": "The answer should be C. Since VPC peering is not transitive then for 100+ accounts in OU then we'll breach the limit of 125. As for VPN - I wouldn't use VPN to connect AWS resources - I don't know even if that's possible"
      },
      {
        "date": "2023-06-21T06:34:00.000Z",
        "voteCount": 2,
        "content": "Olabiba.ai says C."
      },
      {
        "date": "2023-06-24T00:34:00.000Z",
        "voteCount": 2,
        "content": "I have an exam on 27th june, what question set should I prepare? I have only done from Question#1 to Question#181 yet. Please help"
      },
      {
        "date": "2023-05-27T22:00:00.000Z",
        "voteCount": 1,
        "content": "Answer : C\n\nReference : https://catalog.workshops.aws/networking/en-US/intermediate/6-vpc-peering/10-vpc-peering-overview"
      },
      {
        "date": "2023-05-25T06:23:00.000Z",
        "voteCount": 2,
        "content": "D wrong, shared network with transit gateway"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/amazon/view/110410-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:<br><br>1. The data must be highly durable and available<br>2. The data must always be encrypted at rest and in transit<br>3. The encryption key must be managed by the company and rotated periodically<br><br>Which of the following solutions should the solutions architect recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-18T12:13:00.000Z",
        "voteCount": 27,
        "content": "if you have come far it means that you are persistent, good luck in your exam"
      },
      {
        "date": "2024-08-08T03:33:00.000Z",
        "voteCount": 1,
        "content": "Man, what can I say"
      },
      {
        "date": "2023-06-20T17:07:00.000Z",
        "voteCount": 9,
        "content": "My man. Respect, we are all cloud brothers here."
      },
      {
        "date": "2023-11-01T07:44:00.000Z",
        "voteCount": 6,
        "content": "I went backward, does it count?"
      },
      {
        "date": "2024-07-02T10:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-11-23T17:36:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-08-20T10:40:00.000Z",
        "voteCount": 2,
        "content": "Easy breezy"
      },
      {
        "date": "2023-07-05T11:56:00.000Z",
        "voteCount": 1,
        "content": "its a b"
      },
      {
        "date": "2023-06-24T00:34:00.000Z",
        "voteCount": 3,
        "content": "At least an easy one - the provided configuration for S3 in B satisfies the requirements for encryption, durability and availability"
      },
      {
        "date": "2023-06-19T04:41:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-19T04:11:00.000Z",
        "voteCount": 2,
        "content": "Not C because _large_ documents and\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html#limits-items"
      },
      {
        "date": "2023-06-09T18:50:00.000Z",
        "voteCount": 2,
        "content": "Definitely B"
      },
      {
        "date": "2023-06-08T17:38:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2023-05-28T11:04:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2023-05-27T21:46:00.000Z",
        "voteCount": 2,
        "content": "Answer : B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/amazon/view/110388-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company\u2019s public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.<br><br>Recently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had occurred against the API and that the API service had scaled to its maximum amount.<br><br>A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate traffic through and must maximize operational efficiency.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T16:29:00.000Z",
        "voteCount": 10,
        "content": "C. Yes, because The SQL database rule group contains rules to block request patterns associated with exploitation of SQL databases, like SQL injection attacks. This can help prevent remote injection of unauthorized queries. Evaluate this rule group for use if your application interfaces with an SQL database. \nhttps://docs.aws.amazon.com/waf/latest/developerguide/aws-managed-rule-groups-use-case.html\n\nA. No, because this does not prevent SQL injection attacks from reaching the ECS API service\n\nB. No, because with Bot Control, you can easily monitor, block, or rate limit bots such as scrapers, scanners, crawlers, status monitors, and search engines.\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-bot-control.html\n\nD. No, because because this is a reactive response after a SQL injection attack has occurred for new IP addresses"
      },
      {
        "date": "2023-11-23T17:39:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-05T11:58:00.000Z",
        "voteCount": 1,
        "content": "C 100%"
      },
      {
        "date": "2023-06-27T06:15:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-19T04:44:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-01T13:20:00.000Z",
        "voteCount": 3,
        "content": "C; the wording is bad. rule is block, and then set the acl to allow everything else that is not matching the block rule? \n\nB: if attacker knows what to attach, coming from a legitment IP, B will not be able to block it, but C can.\nD is crazy"
      },
      {
        "date": "2023-05-29T16:59:00.000Z",
        "voteCount": 3,
        "content": "Adding new rule for blocking requests which matches SQL database rule group is more 'operationally efficient' than manually scraping API logs and IP based blocking."
      },
      {
        "date": "2023-05-30T04:05:00.000Z",
        "voteCount": 1,
        "content": "why not B?"
      },
      {
        "date": "2023-05-28T11:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-05-27T21:42:00.000Z",
        "voteCount": 4,
        "content": "Answer : C\nhttps://docs.aws.amazon.com/waf/latest/developerguide/aws-managed-rule-groups-use-case.html"
      },
      {
        "date": "2023-05-27T11:25:00.000Z",
        "voteCount": 1,
        "content": "B- is correct---&gt; AWS WAF Bot Control"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/amazon/view/110406-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data readings. The company stores the data in Amazon DynamoDB.<br><br>For business continuity, the company must have the ability to ingest and store data in two AWS Regions.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to Amazon Aurora global tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-29T08:57:00.000Z",
        "voteCount": 9,
        "content": "https://aws.amazon.com/solutions/implementations/disaster-recovery-for-aws-iot/\nA, B Wrong. No need to replace DynamoDB with any other DB. DynamoDB Global Table is enough\nD- Wrong, Not a use-case for Change Data Capture through Streams"
      },
      {
        "date": "2024-07-16T00:52:00.000Z",
        "voteCount": 1,
        "content": "The above URL is not available now. You can refer to this URL: https://aws.amazon.com/blogs/iot/how-to-implement-a-disaster-recovery-solution-for-iot-platforms-on-aws/"
      },
      {
        "date": "2024-01-29T17:39:00.000Z",
        "voteCount": 2,
        "content": "For C, how failover routing policy have the ability to ingest and store data in two AWS Regions, there is only one active record"
      },
      {
        "date": "2023-11-23T17:44:00.000Z",
        "voteCount": 4,
        "content": "Option C Business continuity = Failover -&gt; DynamoDB Global DB"
      },
      {
        "date": "2023-07-05T12:30:00.000Z",
        "voteCount": 3,
        "content": "its a C"
      },
      {
        "date": "2023-06-24T00:48:00.000Z",
        "voteCount": 2,
        "content": "The only answer which configures DynamoDB properly for multi-region is C"
      },
      {
        "date": "2023-05-30T19:57:00.000Z",
        "voteCount": 2,
        "content": "Removed B because is replacing Dynamo, unnecessary"
      },
      {
        "date": "2023-05-29T05:21:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2023-05-27T19:10:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/amazon/view/110405-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.<br><br>The DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The finance team and the marketing team have separate AWS accounts.<br><br>What should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team's account. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team's account, create an IAM role that has permissions to access the DynamoDB table in the finance team's account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-29T05:21:00.000Z",
        "voteCount": 7,
        "content": "Answer is B"
      },
      {
        "date": "2024-09-05T18:41:00.000Z",
        "voteCount": 1,
        "content": "I prefer B over C. Attach the policy (specific DynamoDB attributes) to the DynamoDB table. This will result in the finance team's account not being able to fully access DynamoDB."
      },
      {
        "date": "2024-05-28T13:00:00.000Z",
        "voteCount": 2,
        "content": "I choose C over B.\nBoth solutions work and are standard approaches for allowing cross-account access. But as compared to S3, option C allows the marketing account to use their usual IAM identities without compromising their permissions. When you assume the role in a different account (option B), you can no longer access resources in your own account.\n\nThe resource-based policy for the DynamoDB table supports conditions as well: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/rbac-examples.html#rbac-examples-cross-account"
      },
      {
        "date": "2024-08-28T22:54:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-08-31T23:32:00.000Z",
        "voteCount": 2,
        "content": "Dude stop generating garbage info for everyone. I've seen you replying a lot of `just X`. If you have a reason for some choice, then write it down. `just x` sounds so dumb and premature."
      },
      {
        "date": "2024-05-20T05:53:00.000Z",
        "voteCount": 2,
        "content": "Starting march 24', DynamoDB supports resource based policies :\nhttps://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-resource-based-policies/\nSo another way to achieve this would be to create an index for the marketing team, and have the policy restrict their role to that particular index.\nOn the one hand the new index would incur more costs, on the other hand, having only certain attributes fetched would mean less read units consumed..."
      },
      {
        "date": "2023-12-17T10:24:00.000Z",
        "voteCount": 3,
        "content": "B https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_dynamodb_attributes.html"
      },
      {
        "date": "2023-11-23T17:55:00.000Z",
        "voteCount": 2,
        "content": "Option B as DynamoDB does not support Resource based policies."
      },
      {
        "date": "2023-12-13T21:49:00.000Z",
        "voteCount": 1,
        "content": "Service-linked roles for DynamoDB is not supported\nService roles for DynamoDB  is  supported\nIdentity-based policies for DynamoDB  is  supported\nResource-based policies within DynamoDB  is not supported"
      },
      {
        "date": "2023-08-28T06:08:00.000Z",
        "voteCount": 1,
        "content": "For Cross Account permission we attach Resource Policy with Principal identified as incoming Request Account ARN\n+ IAM permissions to query the Finance Account.\nC seems more of a resonable answer."
      },
      {
        "date": "2023-09-03T06:49:00.000Z",
        "voteCount": 1,
        "content": "i dont think C can address the requirement of \"he marketing team can have access to only specific attributes of data in the DynamoDB table\"\nhence, B"
      },
      {
        "date": "2023-07-24T13:56:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-08-20T13:06:00.000Z",
        "voteCount": 1,
        "content": "While resource-based policies can provide granular access control, they are typically used for controlling access within the same AWS account. Cross-account access control is typically achieved using IAM roles with trust relationships. It is B."
      },
      {
        "date": "2023-08-28T06:04:00.000Z",
        "voteCount": 1,
        "content": "No, Resource based policies can specify which Principals to give access to Cross Account."
      },
      {
        "date": "2023-07-05T13:00:00.000Z",
        "voteCount": 1,
        "content": "B. DynamoDB fine-grained access using IAM"
      },
      {
        "date": "2023-07-02T15:40:00.000Z",
        "voteCount": 2,
        "content": "B for sure.\nKey word: trust"
      },
      {
        "date": "2023-06-24T01:02:00.000Z",
        "voteCount": 3,
        "content": "D would be the perfect choice, since the boundaries are the \"new fancy thing\" but it's lacking the trust to the marketing account which is a requirement to assume role from one account to another. So it should be B"
      },
      {
        "date": "2023-12-26T15:58:00.000Z",
        "voteCount": 1,
        "content": "This would not be a good use case for permissions boundaries by itself. Even with permissions boundaries you would still need to implement a solution like B to provide the required permissions."
      },
      {
        "date": "2023-06-19T04:53:00.000Z",
        "voteCount": 1,
        "content": "B for sure. \nKey word: trust"
      },
      {
        "date": "2023-06-08T17:47:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\n\nDynamoDB doesn't support resource based policy\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html"
      },
      {
        "date": "2023-07-24T13:57:00.000Z",
        "voteCount": 1,
        "content": "That is not correct. DynamoDB does support resource-based policies for tables and indexes. You can attach a resource-based policy to a DynamoDB table or index to specify who can access that resource and under what conditions. You can also use resource-based policies to grant cross-account access or fine-grained access control for specific DynamoDB attributes. For more information, please refer to this documentation: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html"
      },
      {
        "date": "2023-06-06T09:26:00.000Z",
        "voteCount": 1,
        "content": "Resource-based IAM policy"
      },
      {
        "date": "2023-05-27T19:04:00.000Z",
        "voteCount": 2,
        "content": "Answer : B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/amazon/view/110403-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in two AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to store objects in each S3 bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Versioning for each S3 bucket\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket"
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 36,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-03T07:37:00.000Z",
        "voteCount": 14,
        "content": "A - Multi Region Access points are like a proxy. It can dynamically request traffic to the nearest S3 bucket (latency based). [1]\n\nB - Two way replication must be enabled to have data in sync. [1]\nE - Versioning must be enabled for Replication. [3]\n\n\n[1] https://aws.amazon.com/s3/features/multi-region-access-points/\n[2] https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-replication-adds-support-two-way-replication/\n[3] https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html#two-way-replication-scenario:~:text=Both%20source%20and%20destination%20buckets%20must%20have%20versioning%20enabled.%20For%20more%20information%20about%20versioning%2C%20see%20Using%20versioning%20in%20S3%20buckets."
      },
      {
        "date": "2023-07-02T15:42:00.000Z",
        "voteCount": 7,
        "content": "Cross Region Replication(CRR) requires versioning to be activated due to the way that data is replicated between S3 buckets.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPointRequestRouting.html\n\nhttps://stackoverflow.com/questions/60947157/aws-s3-replication-without-versioning#:~:text=The%20automated%20Same%20Region%20Replication,is%20replicated%20between%20S3%20buckets."
      },
      {
        "date": "2023-11-23T17:59:00.000Z",
        "voteCount": 1,
        "content": "A, B, E"
      },
      {
        "date": "2023-08-20T11:36:00.000Z",
        "voteCount": 1,
        "content": "Reason as explained by everyone"
      },
      {
        "date": "2023-07-05T13:03:00.000Z",
        "voteCount": 1,
        "content": "ABE for sure"
      },
      {
        "date": "2023-05-30T19:48:00.000Z",
        "voteCount": 3,
        "content": "I only chosen E because the other options were not making much sense. I guess we need versioning in order to use two-way replication."
      },
      {
        "date": "2023-06-03T08:01:00.000Z",
        "voteCount": 2,
        "content": "yes, Cross Region Replication can be implemented only when the versioning of both the buckets is enabled."
      },
      {
        "date": "2023-05-29T18:32:00.000Z",
        "voteCount": 5,
        "content": "A. Create an S3 Multi-Region Access Point. - this gives you Single Endpoint for accessing S3 into multiple regions \nB. Configure CRR between the two S3 - For automatic replication to diffrent region\nE. Enable S3 Versioning on both S3 - Will give you an ability to track and recover from previous versions if needed\n\nC, D and F doesnt meet the criteria from LEAST operation overhead perspective."
      },
      {
        "date": "2023-05-29T09:55:00.000Z",
        "voteCount": 3,
        "content": "If the reason for E is not obvious then read this: \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\nBoth source and destination buckets must have versioning enabled."
      },
      {
        "date": "2023-05-29T00:43:00.000Z",
        "voteCount": 1,
        "content": "Cross Region Replication(CRR) requires versioning to be activated due to the way that data is replicated between S3 buckets. \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPointRequestRouting.html\n\nhttps://stackoverflow.com/questions/60947157/aws-s3-replication-without-versioning#:~:text=The%20automated%20Same%20Region%20Replication,is%20replicated%20between%20S3%20buckets."
      },
      {
        "date": "2023-05-28T11:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is A B E"
      },
      {
        "date": "2023-05-27T18:27:00.000Z",
        "voteCount": 2,
        "content": "Answer : A,B,E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/amazon/view/110389-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using the MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device metadata in a MongoDB cluster.<br><br>An application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The application creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take 120-600 seconds to run. However, the web application is always running.<br><br>The company is moving the platform to AWS and must reduce the operational overhead of the stack.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda functions to connect to the IoT devices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the IoT devices to publish to AWS IoT Core\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the metadata to a self-managed MongoDB database on an Amazon EC2 instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the metadata to Amazon DocumentDB (with MongoDB compatibility)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports"
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T19:44:00.000Z",
        "voteCount": 5,
        "content": "Not A - lambda to connect to IoT is no good\nNot C - ec2 instance to run MongoDB\nE or F - the job should be short 600 seconds top and serve the reports using Cloud Front - E"
      },
      {
        "date": "2023-11-23T18:10:00.000Z",
        "voteCount": 2,
        "content": "B, D, E"
      },
      {
        "date": "2023-08-20T11:41:00.000Z",
        "voteCount": 3,
        "content": "F is EKS on EC2 and question is Least Operational overhead"
      },
      {
        "date": "2023-08-06T07:04:00.000Z",
        "voteCount": 1,
        "content": "E=&gt; how does step function run periodic jobs?"
      },
      {
        "date": "2023-07-24T13:43:00.000Z",
        "voteCount": 1,
        "content": "Correct BDE."
      },
      {
        "date": "2023-07-05T13:08:00.000Z",
        "voteCount": 2,
        "content": "BDE for sure"
      },
      {
        "date": "2023-05-29T05:23:00.000Z",
        "voteCount": 1,
        "content": "Answer is B D E"
      },
      {
        "date": "2023-05-28T11:21:00.000Z",
        "voteCount": 3,
        "content": "Support B D E"
      },
      {
        "date": "2023-05-27T18:13:00.000Z",
        "voteCount": 4,
        "content": "Answer : B,D,E\nhttps://aws.amazon.com/step-functions/use-cases/"
      },
      {
        "date": "2023-05-27T11:54:00.000Z",
        "voteCount": 1,
        "content": "Correct is ABD"
      },
      {
        "date": "2023-05-28T01:48:00.000Z",
        "voteCount": 1,
        "content": "why E is wrong?"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/amazon/view/110345-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications that need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory sites, where limited network infrastructure exists.<br><br>The company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or in a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.<br><br>Which solution will provide a consistent hybrid experience to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T23:03:00.000Z",
        "voteCount": 12,
        "content": "Key comment: \"specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds.\"\nA - No - Region doesn't assure you have in country presence for data soverignty\nB - No - Snowball part is correct. However, Wavelength access is only via mobile networks, and not in every country, so this is not possible unless all developers are connecting over the mobile network that will have speed variations\nD - No - Local Zones can be fast with a DX connection, but this option like Wavelenght is not in every country\nCorrect answer is C. 100% of the time you are on premise providing single-digit milliseconds latency as Outposts (rack or server) and Snowball will be in the country for the requirements"
      },
      {
        "date": "2023-11-23T18:22:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-08-20T11:43:00.000Z",
        "voteCount": 1,
        "content": "Wavelength doesn't makes sense here"
      },
      {
        "date": "2023-07-05T13:12:00.000Z",
        "voteCount": 1,
        "content": "C works"
      },
      {
        "date": "2023-06-27T06:50:00.000Z",
        "voteCount": 2,
        "content": "Wasn't sure abut Snowball Edge compute optimized to run workloads, but it appears to be quite capable option.\nRef: https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html#edge-related"
      },
      {
        "date": "2023-05-30T19:38:00.000Z",
        "voteCount": 1,
        "content": "short decision based on brief search\nNot B nor D - https://aws.amazon.com/wavelength/ \nA will not meet the millisecond requirement"
      },
      {
        "date": "2023-05-28T02:57:00.000Z",
        "voteCount": 3,
        "content": "Answer C\n\nInstalling AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds will provide a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises1. AWS Outposts allows customers to run some AWS services locally and connect to a broad range of services available in the local AWS Region1. Using AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites will provide local compute and storage resources for locations with limited network infrastructure2. AWS Snowball Edge devices can run Amazon EC2 instances and AWS Lambda functions locally and sync data with AWS when network connectivity is available2."
      },
      {
        "date": "2023-05-27T17:47:00.000Z",
        "voteCount": 2,
        "content": "Answer : C\n\nReference : https://aws.amazon.com/blogs/compute/aws-local-zones-and-aws-outposts-choosing-the-right-technology-for-your-edge-workload/#:~:text=Unlike%20Outposts%2C%20which%20you%20deploy,using%20for%20an%20AWS%20Region\n\nLocal Zones and Outposts can both help you achieve low latency for their latency sensitive workloads. With Direct Connect available in Local Zones, you can achieve low single-digit millisecond latencies, require for applications in online gaming, Media and Entertainment, some SaaS services, AR and VR content delivery etc.\n\nBecause Outposts are installed on premises of customers or their data centers, you can achieve under 1 millisecond latencies for workloads that require it."
      },
      {
        "date": "2023-05-26T21:08:00.000Z",
        "voteCount": 4,
        "content": "Local Zone reduce the latency issue"
      },
      {
        "date": "2023-05-28T01:59:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/wavelength/latest/developerguide/what-is-wavelength.html"
      },
      {
        "date": "2023-06-04T11:05:00.000Z",
        "voteCount": 1,
        "content": "Answer : C\n\nhttps://aws.amazon.com/blogs/compute/aws-local-zones-and-aws-outposts-choosing-the-right-technology-for-your-edge-workload/#:~:text=Unlike%20Outposts%2C%20which%20you%20deploy,using%20for%20an%20AWS%20Region.\n\nWhat is Outposts?\nOutposts is a family of fully managed solutions delivering AWS infrastructure and services to virtually any on-premises or edge location for a truly consistent hybrid experience."
      },
      {
        "date": "2023-05-28T14:45:00.000Z",
        "voteCount": 1,
        "content": "Wavelength is not present is every country with a datacenter, so  B and D options are automatically wrong"
      },
      {
        "date": "2023-05-27T17:48:00.000Z",
        "voteCount": 1,
        "content": "@Masonyeoh, can you review this aws information page on local zones and outposts, confirm your answer again.  \n\nhttps://aws.amazon.com/blogs/compute/aws-local-zones-and-aws-outposts-choosing-the-right-technology-for-your-edge-workload/#:~:text=Unlike%20Outposts%2C%20which%20you%20deploy,using%20for%20an%20AWS%20Region."
      },
      {
        "date": "2023-05-28T14:46:00.000Z",
        "voteCount": 1,
        "content": "Yes, a local zone reduces latency, but local zone are not in every country. The closest thing to an every country option is Snowball and Outpost"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/amazon/view/110401-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has increased recently.<br><br>The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon DynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company must prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.<br><br>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon ElastiCache to reduce overhead on DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-21T07:26:00.000Z",
        "voteCount": 5,
        "content": "From Olabiba.ai:\n\nOption A: By adding a custom header and random value on the CloudFront domain and configuring the ALB to conditionally forward traffic if the header and value match, you can implement a form of request validation. This helps to filter out potentially malicious requests and prevent attacks from reaching the application.\n- Option E: Deploying an AWS WAF web ACL that includes an appropriate rule group and associating it with the Amazon CloudFront distribution adds an additional layer of protection. The web ACL can include rules to block common attack patterns and provide protection against various types of attacks, such as SQL injection and cross-site scripting (XSS)."
      },
      {
        "date": "2024-04-29T22:20:00.000Z",
        "voteCount": 1,
        "content": "simple BCD are not at all related to question"
      },
      {
        "date": "2023-12-16T08:39:00.000Z",
        "voteCount": 1,
        "content": "none of the previous responses really make use of Business continuity as indicated in the scenario. my picks are options B and E. The combination of these two options (E and B) provides both security (via AWS WAF) and high availability (via multi-region deployment) for your application. It helps in preventing attacks and ensuring business continuity with minimal service interruptions during ongoing attacks, making it a cost-effective choice."
      },
      {
        "date": "2024-01-08T21:57:00.000Z",
        "voteCount": 5,
        "content": "Can't use E without A.  E depends on A for the CloudFront distribution."
      },
      {
        "date": "2023-11-23T18:33:00.000Z",
        "voteCount": 1,
        "content": "A and E"
      },
      {
        "date": "2023-07-13T19:11:00.000Z",
        "voteCount": 2,
        "content": "its AE"
      },
      {
        "date": "2023-06-24T22:23:00.000Z",
        "voteCount": 1,
        "content": "The only options that helps to protect are A E"
      },
      {
        "date": "2023-05-30T19:29:00.000Z",
        "voteCount": 3,
        "content": "its a combination of steps, only two of them mention cloud front A and E. it would also be the cheapest option to protect against attacks without having to increase unnecessary performance to the infrastructure which would only cost more money (setup additional region - B , configure auto scaling for ECS and add a DAX - C, configure caching , D)."
      },
      {
        "date": "2023-05-29T05:29:00.000Z",
        "voteCount": 2,
        "content": "The only options that helps to protect are A E"
      },
      {
        "date": "2023-05-27T17:16:00.000Z",
        "voteCount": 1,
        "content": "Answer : A  E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/amazon/view/110393-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.<br><br>Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-10T12:25:00.000Z",
        "voteCount": 1,
        "content": "C because of custom error pages \n https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/creating-custom-error-pages.html"
      },
      {
        "date": "2024-03-07T09:17:00.000Z",
        "voteCount": 4,
        "content": "A and B are plainly wrong and can be eliminated straight away. The choice therefore is between C and D. The question asks for an immediate display of a custom error page - NOT about permanent failover. Therefore, the correct answer is D."
      },
      {
        "date": "2024-05-28T13:26:00.000Z",
        "voteCount": 1,
        "content": "According to https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\nCloudFront always tries to serve the content from the primary origin first.\n&gt; CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin. CloudFront only sends requests to the secondary origin after a request to the primary origin fails.\n\nTherefore option C is still valid as it does not leave CloudFront in \"permanent failover\"."
      },
      {
        "date": "2024-02-02T01:13:00.000Z",
        "voteCount": 1,
        "content": "I go for D: it contains all steps to setup the requested solution, and CloudFront function suits here\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html\n\"URL redirects or rewrites \u2013 You can redirect viewers to other pages based on information in the request, or rewrite all requests from one path to another\"."
      },
      {
        "date": "2024-02-01T15:00:00.000Z",
        "voteCount": 3,
        "content": "'The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.' The purpose of the question obviously is to return that error page not really a FAILOVER mechanism --&gt; Leaves D as an asnwer"
      },
      {
        "date": "2024-01-01T06:39:00.000Z",
        "voteCount": 3,
        "content": "For people are asking why C is better than A:\nThe approach of A is more suited for scenarios where there is a complete failure of the primary endpoint rather than intermittent errors. The health checks may not register a failure if the 502 errors are sporadic and the system is generally operational, thus the failover might not be triggered.\nWith the approach of C CloudFront will always automatically switch to the secondary origin when the primary origin returns specific HTTP status code failure responses."
      },
      {
        "date": "2023-12-22T13:37:00.000Z",
        "voteCount": 2,
        "content": "Least Operational Overhead is C"
      },
      {
        "date": "2023-11-23T18:40:00.000Z",
        "voteCount": 2,
        "content": "Least Operational Overhead is C"
      },
      {
        "date": "2023-10-20T09:38:00.000Z",
        "voteCount": 1,
        "content": "I know C is good, but why not A, seems to me A is much easier."
      },
      {
        "date": "2023-11-17T06:29:00.000Z",
        "voteCount": 2,
        "content": "Route 53 failover will not be as immediate as C. Cloudfront will immediately seerve up the error page if the request to the primary origin fails, so there is no delay between the primary origin health being degraded and the failover page being served."
      },
      {
        "date": "2023-09-14T00:32:00.000Z",
        "voteCount": 1,
        "content": "Repeat question?"
      },
      {
        "date": "2023-09-02T00:34:00.000Z",
        "voteCount": 3,
        "content": "why not A?"
      },
      {
        "date": "2024-10-06T20:11:00.000Z",
        "voteCount": 1,
        "content": "Route 53 fail over to S3? How can Route 53 display the image?"
      },
      {
        "date": "2023-07-05T13:16:00.000Z",
        "voteCount": 2,
        "content": "it's a C"
      },
      {
        "date": "2023-06-27T06:57:00.000Z",
        "voteCount": 4,
        "content": "Origin Groups in CloudFront is what we need here."
      },
      {
        "date": "2023-06-21T07:36:00.000Z",
        "voteCount": 4,
        "content": "From olabiba.ai: \n\nBy using a CloudFront origin group with two origins, you can configure failover between the ALB endpoint and the S3 bucket hosting the static website. This ensures that if the ALB returns HTTP 503 Service Unavailable errors, CloudFront will automatically failover to the S3 bucket and serve the custom error page.\n\nSetting up origin failover for the CloudFront distribution allows for immediate failover to the secondary origin when the primary origin is unavailable. This minimizes the impact of the ALB errors and provides a seamless experience for users by displaying the custom error page.\n\nUpdating the S3 static website to incorporate the custom error page ensures that the error page is readily available and can be served to users without any additional processing or delays."
      },
      {
        "date": "2023-05-30T19:13:00.000Z",
        "voteCount": 2,
        "content": "Almost went for D but this would take too much operational overhead."
      },
      {
        "date": "2023-05-30T19:14:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-05-29T05:34:00.000Z",
        "voteCount": 3,
        "content": "Answer is C, you can use origin groups and configure error response pages in Cloud Front based on different request response codes (503, 404, 403 etc)"
      },
      {
        "date": "2023-05-27T12:40:00.000Z",
        "voteCount": 3,
        "content": "Answer : C\nhttps://repost.aws/knowledge-center/cloudfront-distribution-serve-content"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/amazon/view/110392-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.<br><br>A solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the underlying infrastructure.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T11:49:00.000Z",
        "voteCount": 1,
        "content": "It's very easy... you read docker =&gt; ECS. NFS =&gt; EFS, no underlaying infrastructure =&gt; Fargate"
      },
      {
        "date": "2024-02-13T17:30:00.000Z",
        "voteCount": 3,
        "content": "C and D: Both these options have hassles of EC2 management\nBetween A and B:  Mounting FSx for Lustre on an AWS Fargate launch type isn't supported.\n\nHence the correct option is A"
      },
      {
        "date": "2023-12-22T13:39:00.000Z",
        "voteCount": 1,
        "content": "ECS, EFS - answer A"
      },
      {
        "date": "2023-11-23T18:49:00.000Z",
        "voteCount": 3,
        "content": "Option A - \nEFS = NFS 4\nFargate = No mgmt or provisioning overheads for servers"
      },
      {
        "date": "2023-07-07T15:50:00.000Z",
        "voteCount": 4,
        "content": "Amazon EFS is a managed NAS filer for EC2 instances based on Network File System (NFS) version 4."
      },
      {
        "date": "2023-07-05T13:18:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-07-02T16:02:00.000Z",
        "voteCount": 2,
        "content": "A is correct\nB Fsx For Lustre is POSIX Compilance not is correct in this question\nC and D usage EC2 more overhead administrative is incorrect"
      },
      {
        "date": "2023-07-10T21:04:00.000Z",
        "voteCount": 2,
        "content": "EFS is POSIX Compliant too. A is correct, because  EFS file systems can be accessed by Amazon EC2 Linux instances, Amazon ECS, Amazon EKS, AWS Fargate, and AWS Lambda functions via a file system interface such as NFS protocol."
      },
      {
        "date": "2023-06-24T01:41:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/fsx/when-to-choose-fsx/"
      },
      {
        "date": "2023-05-30T19:10:00.000Z",
        "voteCount": 1,
        "content": "Must be fargate due to the \"not require provisioning or management of the underlying infra\"\nA or B , tie breaker using EFS and not FSx\nHence option A."
      },
      {
        "date": "2023-05-29T05:37:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A, fargate(no infra management) &amp;  efs for NFSv4"
      },
      {
        "date": "2023-05-27T12:42:00.000Z",
        "voteCount": 3,
        "content": "A is correct due to -- NFS version 4."
      },
      {
        "date": "2023-05-27T12:33:00.000Z",
        "voteCount": 1,
        "content": "Answer : A\nhttps://aws.amazon.com/about-aws/whats-new/2017/03/amazon-elastic-file-system-amazon-efs-now-supports-nfsv4-lock-upgrading-and-downgrading/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/amazon/view/110391-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.<br><br>The company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10% of customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing window.<br><br>How should the company deploy the updates to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T19:03:00.000Z",
        "voteCount": 4,
        "content": "B is better option considering the fact that a customer should get same business logic during testing window. This means we need session stickiness that only option B can provide."
      },
      {
        "date": "2023-11-20T07:15:00.000Z",
        "voteCount": 3,
        "content": "This is canary deployment not blue/green"
      },
      {
        "date": "2023-11-01T02:04:00.000Z",
        "voteCount": 1,
        "content": "I was struggled between A and B because I overlooked this line \"A customer must use the same version of the business logic during the testing window.\"\nSo we need session stickiness in place, then B is the obvious choice."
      },
      {
        "date": "2023-08-18T04:16:00.000Z",
        "voteCount": 1,
        "content": "The problem I have with B is that is does not mention stickiness. The problem I have with A is that the stickiness will work only as long as the DNS entry does not time out..."
      },
      {
        "date": "2023-08-18T04:22:00.000Z",
        "voteCount": 1,
        "content": "Oops. It does mention stickiness..."
      },
      {
        "date": "2023-07-24T13:25:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-05T13:20:00.000Z",
        "voteCount": 1,
        "content": "B better"
      },
      {
        "date": "2023-06-25T08:24:00.000Z",
        "voteCount": 2,
        "content": "B ) Classic usage of Blue/Green deployment \nA is good option but not have a stickness with Route 53 more apropiate is ALB with stickness"
      },
      {
        "date": "2023-06-24T02:02:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/load-balancer-stickiness/target-group-stickiness.html"
      },
      {
        "date": "2023-05-30T19:04:00.000Z",
        "voteCount": 4,
        "content": "Agree with B\nblue green deployment, using target group"
      },
      {
        "date": "2023-05-30T19:05:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/"
      },
      {
        "date": "2023-05-29T14:05:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/"
      },
      {
        "date": "2023-05-27T12:18:00.000Z",
        "voteCount": 2,
        "content": "Answer : B\nhttps://medium.com/capital-one-tech/deploying-with-confidence-strategies-for-canary-deployments-on-aws-7cab3798823e"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/amazon/view/110340-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.<br><br>An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.<br><br>What should the solutions architect do to meet these requirements with the LEAST administrative effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 43,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T05:46:00.000Z",
        "voteCount": 17,
        "content": "B is wrong : https://aws.amazon.com/fsx/windows/faqs/#:~:text=A%3A%20While%20you%20cannot%20change,with%20a%20different%20storage%20type.\nI can modify the capacity, but not the type."
      },
      {
        "date": "2023-10-04T14:17:00.000Z",
        "voteCount": 7,
        "content": "Storage type can be modified \nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html"
      },
      {
        "date": "2023-10-18T01:26:00.000Z",
        "voteCount": 2,
        "content": "You can change your file system storage type from HDD to SSD using the Amazon FSx console or Amazon FSx API. You can't change your file system storage type from SSD to HDD.  So A is correct as we can do this during the downtime"
      },
      {
        "date": "2023-10-18T01:27:00.000Z",
        "voteCount": 2,
        "content": "So B is correct. my apologies"
      },
      {
        "date": "2023-11-08T10:58:00.000Z",
        "voteCount": 7,
        "content": "Storage type can be modified\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html"
      },
      {
        "date": "2024-10-07T11:31:00.000Z",
        "voteCount": 1,
        "content": "\"You CAN CHANGE your file system storage type from HDD to SSD using the AWS Management Console and AWS CLI.\"\n\"You CANNOT CHANGE your file system storage type from SSD to HDD.\"\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-configuration.html#managing-storage-type"
      },
      {
        "date": "2024-08-20T09:36:00.000Z",
        "voteCount": 1,
        "content": "Option B would be incorrect because it mentions updating the throughput and storage type directly in the FSx console, which is not supported for an existing FSx for Windows File Server."
      },
      {
        "date": "2024-08-28T23:22:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-06-21T08:12:00.000Z",
        "voteCount": 2,
        "content": "B is right answer."
      },
      {
        "date": "2024-06-20T18:12:00.000Z",
        "voteCount": 1,
        "content": "Since hdd to ssd type is doable. B is better answer."
      },
      {
        "date": "2024-05-04T13:23:00.000Z",
        "voteCount": 2,
        "content": "AWS Backup to create a point-in-time backup of the existing file system, restoring the backup to a new FSx for Windows File Server file system with SSD storage and higher throughput capacity, adjusting the DNS alias, and deleting the original file system provides the most efficient and least administratively intensive solution to improve the performance of the file system during a defined maintenance window"
      },
      {
        "date": "2024-05-02T10:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-01T13:00:00.000Z",
        "voteCount": 2,
        "content": "It's possible to change storage type from HDD to SSD:\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html\n\n\"B\" is correct because it needs less administrative effort."
      },
      {
        "date": "2024-03-20T00:44:00.000Z",
        "voteCount": 3,
        "content": "change your file system storage type from HDD to SSD using the Amazon FSx console or Amazon FSx API\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html"
      },
      {
        "date": "2024-03-14T08:12:00.000Z",
        "voteCount": 3,
        "content": "Option B. Two important points.  1. Changing the storage type.  2. Must improve the performance of the file system during a defined maintenance window.   Solution: 1. You can change your file system storage type from HDD to SSD using the Amazon FSx console or Amazon FSx API. You can't change your file system storage type from SSD to HDD.  2. AWS recommend updating your storage type when there is minimal traffic on your file system.\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html"
      },
      {
        "date": "2024-03-14T08:18:00.000Z",
        "voteCount": 1,
        "content": "Plus these give you the \" LEAST administrative effort\"   Not A , because the question doesn't ask or states a need to backup data"
      },
      {
        "date": "2024-02-27T06:49:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/fsx/windows/faqs/ Can I change the storage type (SSD/HDD) of my file system?\nWhile you cannot change the storage type on your existing file system, you can take a backup and restore that backup to a new file system with a different storage type."
      },
      {
        "date": "2024-03-19T18:23:00.000Z",
        "voteCount": 2,
        "content": "correct myself it is possible to upgrade from HDD to ssd https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html"
      },
      {
        "date": "2024-02-26T21:54:00.000Z",
        "voteCount": 4,
        "content": "B is possible."
      },
      {
        "date": "2024-02-22T11:53:00.000Z",
        "voteCount": 4,
        "content": "I am confused:\nhttps://aws.amazon.com/fsx/windows/faqs/#:~:text=Q%3A%20Can%20I%20change%20the%20storage%20type%20(SSD/HDD)%20of%20my%20file%20system%3F \"While you cannot change the storage type on your existing file system, you can take a backup and restore that backup to a new file system with a different storage type.\"\n\nbut\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html\n\nYou can change your file system storage type from HDD to SSD using the Amazon FSx console or Amazon FSx API."
      },
      {
        "date": "2024-01-28T09:12:00.000Z",
        "voteCount": 3,
        "content": "A is correct.\nYou cannot change  storage type - https://aws.amazon.com/fsx/windows/faqs/#:~:text=Q%3A%20Can%20I%20change%20the%20storage%20type%20(SSD/HDD)%20of%20my%20file%20system%3F\nWhile you can increase/decrease troughput at any time  - https://aws.amazon.com/fsx/windows/faqs/#:~:text=Q%3A%20Can%20I%20change%20my%20file%20system%E2%80%99s%20storage%20capacity%20and%20throughput%20capacity%3F"
      },
      {
        "date": "2024-01-03T08:26:00.000Z",
        "voteCount": 3,
        "content": "While you cannot change the storage type on your existing file system, you can take a backup and restore that backup to a new file system with a different storage type."
      },
      {
        "date": "2023-12-24T19:29:00.000Z",
        "voteCount": 1,
        "content": "[ https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html#updating-storage-type ]  You can update a file system's storage type using the Amazon FSx console, the AWS CLI, or the Amazon FSx API.     \n [ https://aws.amazon.com/fsx/windows/faqs/ ]\nQ: Can I change the storage type (SSD/HDD) of my file system?\nA: While you cannot change the storage type on your existing file system, you can take a backup and restore that backup to a new file system with a different storage type.\nRelated Entertainment  [ https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html ] \nYou can't increase storage capacity for file systems created before June 23, 2019 or file systems restored from a backup belonging to a file system that was created before June 23, 2019."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/amazon/view/110338-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-03T12:59:00.000Z",
        "voteCount": 1,
        "content": "C is missing \"bidirectional S3 Cross-Region Replication\""
      },
      {
        "date": "2023-11-23T19:27:00.000Z",
        "voteCount": 2,
        "content": "B is always a better option. C is possible but less preferred. \nIrrespective of B or C application will need modification to deploy in 2nd region as Bucket URL has to be change in application."
      },
      {
        "date": "2023-10-07T10:37:00.000Z",
        "voteCount": 3,
        "content": "An S3 Multi-Region Access Point is a global endpoint that provides access to data in one or more S3 buckets. To create an S3 Multi-Region Access Point, you must specify a set of S3 buckets that you want to include in the Multi-Region Access Point. You must also configure routing rules to determine which requests are routed to which S3 buckets.\n\nOnce you have created an S3 Multi-Region Access Point, you must modify your application to use the Multi-Region Access Point endpoint instead of the S3 bucket endpoints. This requires changes to your application code and configuration.\n\nOption C does not require the creation of an S3 Multi-Region Access Point. Instead, you can simply deploy the application in two Regions and configure the application to use the S3 bucket endpoints in each Region. This is a simpler and more straightforward approach, which reduces operational overhead."
      },
      {
        "date": "2024-08-28T23:23:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2023-12-29T03:39:00.000Z",
        "voteCount": 3,
        "content": "Option C includes \"Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket\". By that the application in the new region will have access to the files from the \"old\" and the new region, and the application running in the \"old\" region only has access to the data of the \"old\" region, as no bidirectional CRR is being set up. That doesn't make a lot of sense. Option B contains bidirectional CRR which keeps both buckets in sync."
      },
      {
        "date": "2023-08-10T11:47:00.000Z",
        "voteCount": 3,
        "content": "Option B creates a new S3 bucket in a second Region and sets up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. S3 CRR is a feature that enables automatic, asynchronous copying of objects across S3 buckets in different AWS Regions. You can use S3 CRR to keep your data synchronized across Regions for lower latency, compliance, security, disaster recovery, and regional efficiency."
      },
      {
        "date": "2023-07-29T21:39:00.000Z",
        "voteCount": 1,
        "content": "The answer is B"
      },
      {
        "date": "2023-07-09T01:54:00.000Z",
        "voteCount": 1,
        "content": "it's a B"
      },
      {
        "date": "2023-07-05T13:26:00.000Z",
        "voteCount": 2,
        "content": "its a B"
      },
      {
        "date": "2023-07-13T19:23:00.000Z",
        "voteCount": 2,
        "content": "the \"stored in a single Amazon S3 bucket\" comment is confusing though. have to assume new versionn will have buckets in each region"
      },
      {
        "date": "2023-07-05T08:23:00.000Z",
        "voteCount": 3,
        "content": "S3 CRR prefer S3 Multi-Region Access Point"
      },
      {
        "date": "2023-07-05T07:58:00.000Z",
        "voteCount": 1,
        "content": "B sounds right for deploying in 2 different regions though."
      },
      {
        "date": "2023-07-05T07:56:00.000Z",
        "voteCount": 1,
        "content": "this question seems incomplete?"
      },
      {
        "date": "2023-05-26T21:05:00.000Z",
        "voteCount": 3,
        "content": "B, enable the S3 sync"
      },
      {
        "date": "2023-05-26T19:06:00.000Z",
        "voteCount": 2,
        "content": "Answer : B\nhttps://aws.amazon.com/s3/features/multi-region-access-points/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/amazon/view/110339-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing (HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js application for game display. Game state is tracked in an on-premises Redis instance.<br><br>The company needs a migration strategy that optimizes application performance.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache for Redis cluster to maintain the leaderboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-26T19:08:00.000Z",
        "voteCount": 9,
        "content": "Answer : C\n\nhttps://aws.amazon.com/blogs/database/building-a-real-time-gaming-leaderboard-with-amazon-elasticache-for-redis/"
      },
      {
        "date": "2023-05-30T18:47:00.000Z",
        "voteCount": 5,
        "content": "Elastic Cache for Redis, C or D.\nBoth are on demand, we cant use spot\nTie breaker is the instance type c5."
      },
      {
        "date": "2024-06-05T03:46:00.000Z",
        "voteCount": 1,
        "content": "D is the write answer"
      },
      {
        "date": "2024-01-15T08:05:00.000Z",
        "voteCount": 1,
        "content": "Answer: C\nB/c: not use spot instance"
      },
      {
        "date": "2023-11-23T21:18:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-09-13T01:33:00.000Z",
        "voteCount": 1,
        "content": "Agree with option C"
      },
      {
        "date": "2023-08-20T12:32:00.000Z",
        "voteCount": 1,
        "content": "Agree with C."
      },
      {
        "date": "2023-07-24T13:07:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-05T13:27:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-07-05T08:00:00.000Z",
        "voteCount": 1,
        "content": "C is the way"
      },
      {
        "date": "2023-06-22T17:02:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-05-30T06:28:00.000Z",
        "voteCount": 2,
        "content": "A, B : Wrong. Spot instances. B: OpeSearch instead of Redis\nD: Wrong, DynamoDB instead of Redis"
      },
      {
        "date": "2023-05-29T22:31:00.000Z",
        "voteCount": 2,
        "content": "The answer is C as compute optimized instance is required c5, and ElastiCache is the for Redis."
      },
      {
        "date": "2023-05-26T21:06:00.000Z",
        "voteCount": 2,
        "content": "Agree with C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/amazon/view/110346-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.<br><br>Which combination of steps meets these requirements while minimizing operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service<br>Auto Scaling to add capacity before the high volume of submissions on Fridays.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span><span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T05:36:00.000Z",
        "voteCount": 15,
        "content": "i'm going with BE.\nA not correct with EC2 instances to mantain.\nC is not correct because we cannot host webapplication on S3 (only static contents)\nD too much effort for Redshift"
      },
      {
        "date": "2023-08-20T13:33:00.000Z",
        "voteCount": 1,
        "content": "It looks like BE are the best options. While deploying the frontend to S3 and using API Gateway with Lambda for the backend is a good architectural approach, it might not directly address the requirement for load scaling and scheduling."
      },
      {
        "date": "2023-07-05T08:09:00.000Z",
        "voteCount": 9,
        "content": "A. EC2 on-demand instances don't make sense to accept timesheet entries\nB. ECS can be done but they want to minimise operational overhead where option C sounds better/simple\nC. Sounds simple enough to use s3. I choose this.\nD. I already chose s3 so this doesn't apply + redshift seems overkill\nE.This goes with Option C\nSo answer C and E"
      },
      {
        "date": "2024-10-09T10:51:00.000Z",
        "voteCount": 1,
        "content": "It's B &amp; E, the question says that timeshares will need ro be submitted. Therefore making it dynamic. \n\nSelecting option C to use S3 to host webapp can't work because S3 can host static sites."
      },
      {
        "date": "2024-08-20T10:05:00.000Z",
        "voteCount": 1,
        "content": "C is Incorrect"
      },
      {
        "date": "2024-06-12T10:09:00.000Z",
        "voteCount": 2,
        "content": "Voting for BE"
      },
      {
        "date": "2024-06-10T13:47:00.000Z",
        "voteCount": 2,
        "content": "C is not correct. We need Lambda, not Lambda Proxy. BTW, APIGW + Lambda for the unknown load, in this case we knew that high load on Friday"
      },
      {
        "date": "2024-05-21T00:27:00.000Z",
        "voteCount": 4,
        "content": "Answer is B and E.\nWe already know which on friday a lot of people will submit timesheet, so scheduled autoscaling is perfect.\nFinally S3 + Athena are enough. No need of Redshift database to run report."
      },
      {
        "date": "2024-05-19T00:27:00.000Z",
        "voteCount": 1,
        "content": "Between A or B I don't see any difference regarding minimizing the operational overhead. The right answer can be either. That leads me to think that the right answer is C (indeed, it's full serverless!)\n\nC, E"
      },
      {
        "date": "2024-04-29T22:54:00.000Z",
        "voteCount": 2,
        "content": "c is simple but not scalable one\ni feel C and B are almost same operation overhead. \ni will go for BE"
      },
      {
        "date": "2024-03-20T10:13:00.000Z",
        "voteCount": 2,
        "content": "Option C is wrong. The requirement states that the data must be stored in a format that allows payroll administrators to run monthly reports.\nAmazon S3 and Amazon API Gateway do not inherently provide the necessary data storage and querying capabilities for generating reports."
      },
      {
        "date": "2024-03-03T13:03:00.000Z",
        "voteCount": 1,
        "content": "Minimal admin effort"
      },
      {
        "date": "2023-11-23T21:34:00.000Z",
        "voteCount": 2,
        "content": "C &amp; E is most operationally efficient. Redshift cluster needs more operational effort to manage."
      },
      {
        "date": "2023-11-01T01:07:00.000Z",
        "voteCount": 2,
        "content": "Why do I feel C somehow tricky because it says deploy backend using APIGW with Lamda proxy integration, and doesnt mention a Lambda function to process data? \"Lamda proxy integration\" only means an option to tick in configuration of APIGW, no?"
      },
      {
        "date": "2024-01-09T20:45:00.000Z",
        "voteCount": 2,
        "content": "Agreed.  C is a misdirect.  You don't need Lambda Proxy.  APIGW can integrate with S3 API directly.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-s3.html"
      },
      {
        "date": "2023-09-02T22:48:00.000Z",
        "voteCount": 1,
        "content": "C &amp; E. \"minimizing operational overhead\" is the deciding factor between C and B. operating and managing ECS and ALB would be more cumbersome versus a more serverless approach like APIGW, Lambda, and S3."
      },
      {
        "date": "2023-08-20T12:39:00.000Z",
        "voteCount": 2,
        "content": "Least Operational overhead"
      },
      {
        "date": "2023-07-28T14:13:00.000Z",
        "voteCount": 2,
        "content": "b-e-b-e-b-e-b-e"
      },
      {
        "date": "2023-07-24T12:46:00.000Z",
        "voteCount": 1,
        "content": "Correct  CE."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/amazon/view/110347-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3 bucket.<br><br>Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CloudTrail to log S3 data events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure S3 server access logging for the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new S3 bucket to store the logs with an S3 Lifecycle policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 70,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 46,
        "isMostVoted": false
      },
      {
        "answer": "ADE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BCF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T13:51:00.000Z",
        "voteCount": 9,
        "content": "ADF\nA or B work, but docs recomment cloud trail:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"
      },
      {
        "date": "2023-12-09T17:26:00.000Z",
        "voteCount": 7,
        "content": "ADF are correct choices."
      },
      {
        "date": "2024-09-10T16:42:00.000Z",
        "voteCount": 2,
        "content": "\"We recommend that you use CloudTrail for logging bucket-level and object-level actions for your Amazon S3 resources.\""
      },
      {
        "date": "2024-06-21T08:11:00.000Z",
        "voteCount": 1,
        "content": "ADFis right answer."
      },
      {
        "date": "2024-06-20T18:36:00.000Z",
        "voteCount": 4,
        "content": "Both A and B can log s3 activities. Difference is A real time log but cost more. B log has delay but cheaper. The requirement is the most cost-effective so choose B to meet this requirement."
      },
      {
        "date": "2024-05-02T10:22:00.000Z",
        "voteCount": 2,
        "content": "ADF logs everything, BDF doesnt."
      },
      {
        "date": "2024-04-01T13:50:00.000Z",
        "voteCount": 3,
        "content": "BDF meet the requirements."
      },
      {
        "date": "2024-03-07T10:24:00.000Z",
        "voteCount": 4,
        "content": "Probably B is cheaper but A is safer and more accurate and remember the \"The company must log ALL activities for objects\"\n\nAccording to this https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html#LogDeliveryBestEffort\n\n\"The log record for a particular request might be delivered long after the request was actually processed, or it might not be delivered at all. \"\n\nso for me is A not B"
      },
      {
        "date": "2024-02-15T16:08:00.000Z",
        "voteCount": 4,
        "content": "Given the requirement to log all activities for objects in an S3 bucket and keep logs for 5 years, combined with a focus on cost-effectiveness, S3 server access logging (Option B) would indeed be a cheaper solution for capturing basic access logs. However, for advanced auditing and compliance requirements where detailed API call tracking is needed, CloudTrail's data event logging provides valuable insights that S3 access logs do not."
      },
      {
        "date": "2024-01-28T09:21:00.000Z",
        "voteCount": 1,
        "content": "B is cheaper than A\nAWS CloudTrail (A) - Management events (first delivery) are free; data events incur a fee, in addition to storage of logs\nS3 Server Logs (B) - No other cost in addition to storage of logs\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html#:~:text=S3%20Server%20Logs-,Price,-Management%20events%20(first"
      },
      {
        "date": "2024-01-26T14:18:00.000Z",
        "voteCount": 4,
        "content": "For capturing object-level events, such as object deletions, you would typically use Amazon S3 Event Notifications or enable AWS CloudTrail data events for S3."
      },
      {
        "date": "2024-01-12T21:11:00.000Z",
        "voteCount": 3,
        "content": "S3 server access logging does not capture object-level events like object deletions. so I will go ADF."
      },
      {
        "date": "2024-01-13T06:57:00.000Z",
        "voteCount": 5,
        "content": "wrong. check \"operation\" in https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html\nBDF"
      },
      {
        "date": "2023-12-31T06:55:00.000Z",
        "voteCount": 1,
        "content": "BDF\nBecause it asked for cost-effective."
      },
      {
        "date": "2023-12-24T06:39:00.000Z",
        "voteCount": 2,
        "content": "B is better than A because S3 server logs  -- &gt; Cost efficient and get more log information (Lifecycle,  Authentication info)\nLink:  https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"
      },
      {
        "date": "2023-12-19T11:36:00.000Z",
        "voteCount": 1,
        "content": "My Choice"
      },
      {
        "date": "2023-11-24T19:31:00.000Z",
        "voteCount": 3,
        "content": "ADF are correct choices.\n\nUsing server access logging provides basic access logs for requests made to the S3 bucket, but it is not as comprehensive for auditing purposes as CloudTrail and can result in a large volume of data, increasing costs."
      },
      {
        "date": "2024-01-28T09:23:00.000Z",
        "voteCount": 1,
        "content": "S3 Server Access log is cheaper as you only pay for the storage of logs, while CloudTrail Data Event incur into additional cost + storage of logs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\n\nS3 Server Access log - You can use server access logs for the following purposes:\nPerforming security and access audits\nLearning about your customer base\nUnderstanding your Amazon S3 bill\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html"
      },
      {
        "date": "2023-11-23T21:39:00.000Z",
        "voteCount": 3,
        "content": "B D F are the right options"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/amazon/view/110379-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to the data center from the Region that is closest to the data center.<br><br>The company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.<br><br>Which combination of steps will meet these requirements with the LEAST cost? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up additional Direct Connect connections from the on-premises data center to the other two Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T08:50:00.000Z",
        "voteCount": 19,
        "content": "There is no correct answer. NONE.\nA.Direct Connect gateway are global. You dont create them in a \"region\"\nB. Not needed, since you have DX-GW.\nC. Cant establish site-to-site VPN over private VIF. You do it over public or transit (recommended).\nD. Yes, should use private VIF, but for access to AWS public resources, not the other VPCs.\nE. VPC peering wont allow Onprem to access other VPCs via peering.\n\nBest Answer is DX-Gateway AND Public VIF (A and D). However they're both wrong.\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html"
      },
      {
        "date": "2023-05-27T09:24:00.000Z",
        "voteCount": 12,
        "content": "Answer : A, D\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html"
      },
      {
        "date": "2024-07-13T00:23:00.000Z",
        "voteCount": 1,
        "content": "A, D for sure.\nMust have access to AWS public services."
      },
      {
        "date": "2023-11-23T21:47:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2023-07-05T13:41:00.000Z",
        "voteCount": 1,
        "content": "its AD"
      },
      {
        "date": "2023-07-02T16:21:00.000Z",
        "voteCount": 1,
        "content": "Answer : A, D\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html"
      },
      {
        "date": "2023-06-27T08:02:00.000Z",
        "voteCount": 2,
        "content": "got to use Public VIN in order to connect to AWS Services via Direct Connect."
      },
      {
        "date": "2023-06-21T12:57:00.000Z",
        "voteCount": 1,
        "content": "a-d-a-d-a-d-a-d"
      },
      {
        "date": "2023-06-03T11:01:00.000Z",
        "voteCount": 5,
        "content": "Agree Roontha.\nFor E, \"Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs\" is wrong. private VIF can only connect to the vpc which is in the same region with direct connection,  you can't extend private VIF to the  VPCs in other 2 regions."
      },
      {
        "date": "2023-05-30T18:33:00.000Z",
        "voteCount": 3,
        "content": "agree with A and D tks to Roontha"
      },
      {
        "date": "2023-05-29T23:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is A,D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/amazon/view/110372-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.<br><br>Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Config in all accounts\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty in all accounts",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable all features for the organization\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CDF",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ABD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ADE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BDF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T07:39:00.000Z",
        "voteCount": 14,
        "content": "My Answer A,C,D\n\nhttps://aws.amazon.com/blogs/security/using-aws-firewall-manager-and-waf-to-protect-your-web-applications-with-master-rules-and-application-specific-rules/\n\ncan someone post the link if you feel my answer is incorrect"
      },
      {
        "date": "2023-05-28T05:09:00.000Z",
        "voteCount": 1,
        "content": "why you pickup C? why we need enable all the features?"
      },
      {
        "date": "2023-06-04T06:55:00.000Z",
        "voteCount": 17,
        "content": "@ShinLi,\n\nC is must requirement in order leverage AWS Firewall Manager according to aws.\n\nPrerequisites\nAWS Firewall Manager has the following prerequisites:\n\nAWS Organizations: Your organization must be using AWS Organizations to manage your accounts, and All Features must be enabled. For more information, see Creating an Organization and Enabling All Features in Your Organization.\nA firewall administrator AWS Account: You must designate one of the AWS accounts in your organization as the administrator for AWS Firewall Manager. This gives the account permission to deploy AWS WAF rules across the organization.\nAWS Config: You must enable AWS Config for all of the accounts in your organization so that AWS Firewall Manager can detect newly created resources. To enable AWS Config for all of the accounts in your organization, you can use the Enable AWS Config template on the StackSets Sample Templates page. For more information, see Getting Started with AWS Config."
      },
      {
        "date": "2024-09-07T22:39:00.000Z",
        "voteCount": 2,
        "content": "AWS Firewall Manager has the following prerequisites:\nAWS Organizations: Your organization must be using AWS Organizations to manage your accounts, and All Features must be enabled.\nA firewall administrator AWS Account: You must designate one of the AWS accounts in your organization as the administrator for AWS Firewall Manager. \nAWS Config: You must enable AWS Config for all of the accounts in your organization so that AWS Firewall Manager can detect newly created resources.\nReference: https://aws.amazon.com/blogs/security/using-aws-firewall-manager-and-waf-to-protect-your-web-applications-with-master-rules-and-application-specific-rules/"
      },
      {
        "date": "2024-04-07T07:55:00.000Z",
        "voteCount": 1,
        "content": "ACD is the correct combination to establish a base line security when deploying within the organization in AWS Organization."
      },
      {
        "date": "2023-12-05T06:21:00.000Z",
        "voteCount": 2,
        "content": "Answer - ACD \nPrerequisites - AWS Config and All Features should be enabled in the organization."
      },
      {
        "date": "2023-11-23T21:56:00.000Z",
        "voteCount": 1,
        "content": "A, C, D"
      },
      {
        "date": "2023-11-17T23:43:00.000Z",
        "voteCount": 2,
        "content": "AWS config must be enabled in all accounts to identify new resources so AWS Firewall manager works properly"
      },
      {
        "date": "2023-07-28T14:27:00.000Z",
        "voteCount": 2,
        "content": "a-c-d----a-c-d----a-c-d\nGuardDuty, Shield Advanced, and Security Hub provide other security capabilities but are not directly related to deploying WAF rules across all accounts and distributions."
      },
      {
        "date": "2023-07-05T13:43:00.000Z",
        "voteCount": 1,
        "content": "its ACD"
      },
      {
        "date": "2023-06-28T11:40:00.000Z",
        "voteCount": 1,
        "content": "D is clear. A and C are needed for D to work\n\nhttps://aws.amazon.com/es/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/#:~:text=Firewall%20Manager%20prerequisites"
      },
      {
        "date": "2023-06-25T08:34:00.000Z",
        "voteCount": 3,
        "content": "ACD\nLink reference : https://aws.amazon.com/es/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/#:~:text=Firewall%20Manager%20prerequisites"
      },
      {
        "date": "2023-06-21T13:20:00.000Z",
        "voteCount": 1,
        "content": "baseline for OWASP = b-d-f"
      },
      {
        "date": "2023-06-20T05:57:00.000Z",
        "voteCount": 4,
        "content": "baseline protection vconfiguration.\nA to evaluate the configurations of AWS resources\nC enabling all features required by Firewall manager\nD to enable the waf rules"
      },
      {
        "date": "2023-06-13T04:52:00.000Z",
        "voteCount": 1,
        "content": "Enable AWS Config in all accounts: AWS Config provides a detailed view of the configuration of AWS resources within an organization. By enabling AWS Config, the solutions architect can track and monitor the configuration of CloudFront distributions and ensure that they adhere to the desired baseline configuration, including AWS WAF settings.\n\nEnable Amazon GuardDuty in all accounts: Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior within AWS accounts. Enabling GuardDuty in all accounts allows for real-time threat detection and alerts related to potential web application vulnerabilities."
      },
      {
        "date": "2023-06-06T10:24:00.000Z",
        "voteCount": 2,
        "content": "Prerequisites for using AWS Firewall Manager\nYour account must be a member of AWS Organizations\nYour AWS account must be a member of an organization in the AWS Organizations service, and the organization must have all features enabled.\n\nYour account must be the AWS Firewall Manager administrator\nTo configure Firewall Manager policies, your account must be set as the AWS Firewall Manager administrator account, in the Settings pane.\n\nYou must have AWS Config enabled for your accounts and Regions\nYou must enable AWS Config for each of your AWS Organizations member accounts and for each AWS Region that contains resources that you want to protect using AWS Firewall Manager."
      },
      {
        "date": "2023-06-03T11:16:00.000Z",
        "voteCount": 3,
        "content": "A,C,D is right answer.\nInfact My initial choice is B,C,D.\nAfter I rewatch neal Davis' video, GuardDuty is intelligent thread detection service based ML,\nit does continuous monitoring for : 1) CloudTrail Management events; 2) CloudTrail S3 Data Events;3)VPC Flow Logs 4) DNS logs. so guardduty is not right in this scenario."
      },
      {
        "date": "2023-06-03T09:53:00.000Z",
        "voteCount": 1,
        "content": "The tutorial is here.\n\nhttps://aws.amazon.com/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/#:~:text=Firewall%20Manager%20prerequisites"
      },
      {
        "date": "2023-08-20T13:43:00.000Z",
        "voteCount": 1,
        "content": "I assume if you want to secure AWS you need Guard duty enabled, it also interact with AWS WAF: https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/"
      },
      {
        "date": "2023-06-02T19:54:00.000Z",
        "voteCount": 2,
        "content": "Updating My Vote to BDE \nEnabling Amazon GuardDuty will help monitor and detect malicious activity.\nDeploying WAF rules via Firewall Manager or Shield Advanced will filter incoming traffic and block common attack patterns.   These steps can help protect against many of the most common web application security risks identified by OWASP.\nA (Enable AWS Config) is not directly related to providing baseline protection for web applications against OWASP's top 10 vulnerabilities. \nC (Enable All Features) is too broad and does not specifically address web application security. \nF (Use Security Hub) does not have a native capability to deploy WAF rules at scale."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/amazon/view/110334-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.<br><br>Which items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM user's permissions policy has allowed the use of SAML federation for that user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.<br>B. Test users are not in the AWSFederatedUsers group in the company's IdP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-28T14:23:00.000Z",
        "voteCount": 22,
        "content": "Kindly correct the Answers' sequence. A to F"
      },
      {
        "date": "2023-05-28T14:28:00.000Z",
        "voteCount": 3,
        "content": "Ref: BDF    https://www.examtopics.com/discussions/amazon/view/36355-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-05-30T01:14:00.000Z",
        "voteCount": 20,
        "content": "B) The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.\nD) The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.\nF)The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions."
      },
      {
        "date": "2024-05-10T19:40:00.000Z",
        "voteCount": 4,
        "content": "B1,C,E"
      },
      {
        "date": "2023-11-29T15:24:00.000Z",
        "voteCount": 3,
        "content": "For sure - BCE"
      },
      {
        "date": "2023-11-17T23:56:00.000Z",
        "voteCount": 3,
        "content": "B1, C, E"
      },
      {
        "date": "2023-09-13T03:02:00.000Z",
        "voteCount": 1,
        "content": "BDF is correct"
      },
      {
        "date": "2023-09-03T04:51:00.000Z",
        "voteCount": 2,
        "content": "B,C, &amp; E was my first choice"
      },
      {
        "date": "2023-08-29T01:13:00.000Z",
        "voteCount": 3,
        "content": "C- STS AssumerolewithSAML\nB1- Define trust policy for IAM assumed by the principal\nE - SAML Assertion"
      },
      {
        "date": "2023-08-20T13:16:00.000Z",
        "voteCount": 1,
        "content": "BDF is correct"
      },
      {
        "date": "2023-08-08T21:30:00.000Z",
        "voteCount": 2,
        "content": "Should be BEF, right? \nD. The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP. This is already being done by the federated identity web portal.\n\nSo E)  The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs. The on-premises IdP's DNS hostname must be reachable from the AWS environment VPCs. This is because the AWS STS AssumeRoleWithSAML API will need to be able to resolve the DNS hostname of the IdP in order to retrieve the SAML assertion."
      },
      {
        "date": "2023-07-27T15:22:00.000Z",
        "voteCount": 2,
        "content": "BDF is the right answers"
      },
      {
        "date": "2023-07-24T04:26:00.000Z",
        "voteCount": 1,
        "content": "Correct BCE."
      },
      {
        "date": "2023-07-19T22:01:00.000Z",
        "voteCount": 1,
        "content": "Admin The Order from the Question is not right.. Answer is BDF!"
      },
      {
        "date": "2023-07-05T13:49:00.000Z",
        "voteCount": 2,
        "content": "B (the 1st B, as there are two in this version of question) CE"
      },
      {
        "date": "2023-06-21T17:36:00.000Z",
        "voteCount": 2,
        "content": "it's B-D-F Jeff."
      },
      {
        "date": "2023-05-26T17:40:00.000Z",
        "voteCount": 2,
        "content": "Answer : B, C, E"
      },
      {
        "date": "2023-05-26T17:44:00.000Z",
        "voteCount": 4,
        "content": "Sorry...it is BDF\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/amazon/view/110333-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of the application\u2019s operations insert records into the database. The application currently stores credentials in a text-based configuration file.<br><br>The solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure and must provide the ability to rotate the credentials automatically on a regular basis.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-26T21:36:00.000Z",
        "voteCount": 6,
        "content": "Using RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might cause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool. This approach avoids the memory and CPU overhead of opening a new database connection each time. To protect the database against oversubscription, you can control the number of database connections that are created."
      },
      {
        "date": "2023-12-29T06:22:00.000Z",
        "voteCount": 2,
        "content": "Use replicas to scale read, this use-case is about writing so C &amp; D are out.\nSecret manager offers rotation, parameter store doesn't.\nSo its A."
      },
      {
        "date": "2023-12-11T08:58:00.000Z",
        "voteCount": 1,
        "content": "D. Aurora Replica with Parameter Store:\n\nPros:\nImproves database capacity and reduces load on the primary instance.\nParameter Store provides centralized configuration management.\nCons:\nManually rotating credentials in Parameter Store poses security risks."
      },
      {
        "date": "2024-08-21T01:37:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2023-11-22T23:14:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-10-31T23:09:00.000Z",
        "voteCount": 1,
        "content": "straight A. love these questions \ud83d\ude02"
      },
      {
        "date": "2023-07-05T13:51:00.000Z",
        "voteCount": 1,
        "content": "easy A"
      },
      {
        "date": "2023-06-27T08:41:00.000Z",
        "voteCount": 1,
        "content": "Agree with other explanations here."
      },
      {
        "date": "2023-05-30T17:04:00.000Z",
        "voteCount": 3,
        "content": "Agree with A \nRotate the keys using Secrets Manager, Param store does not cover it.\nRDS Proxy is exactly to solve the issues with overloaded connection because is a connection pool component."
      },
      {
        "date": "2023-05-26T17:37:00.000Z",
        "voteCount": 4,
        "content": "Answer : A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/amazon/view/110331-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones.<br><br>In the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T21:06:00.000Z",
        "voteCount": 5,
        "content": "Bad question design. EC2 is in ASG, which means the application part is stateless, so no need to backup or replicate. Only database need replication."
      },
      {
        "date": "2023-06-12T04:23:00.000Z",
        "voteCount": 5,
        "content": "Explanation:\nOption B leverages infrastructure as code (IaC) to provision the necessary infrastructure in the DR Region, which allows for automated and repeatable deployments.\nCreating a cross-Region read replica for the Amazon RDS DB instance ensures that the database is replicated and available in the DR Region.\nAWS Elastic Disaster Recovery can be used to continuously replicate the EC2 instances from the primary Region to the DR Region, ensuring up-to-date copies of the application.\nRunning the EC2 instances at the minimum capacity in the DR Region helps reduce costs, as resources are only utilized when failover occurs.\nUsing an Amazon Route 53 failover routing policy allows for automatic failover to the DR Region in the event of a disaster, minimizing downtime.\nIncreasing the desired capacity of the Auto Scaling group ensures that sufficient resources are available in the DR Region to handle the workload during failover."
      },
      {
        "date": "2023-11-22T23:18:00.000Z",
        "voteCount": 3,
        "content": "Option B most cost effective for RTO=10 min and RPO=30 min."
      },
      {
        "date": "2023-11-22T23:19:00.000Z",
        "voteCount": 2,
        "content": "RPO=30 sec"
      },
      {
        "date": "2023-11-21T06:51:00.000Z",
        "voteCount": 3,
        "content": "RPO of 30 seconds can be achieved with Elastic disaster recovery for continuous EC2 instance replication, while DB read replica can be promoted to primary within 30 seconds"
      },
      {
        "date": "2023-08-20T13:27:00.000Z",
        "voteCount": 3,
        "content": "Close between B &amp; D but Max out ASG is tie-breaker"
      },
      {
        "date": "2023-08-07T18:29:00.000Z",
        "voteCount": 2,
        "content": "I think (D) only aurora global database can meet RPO 30 seconds? although B is cost-effective"
      },
      {
        "date": "2023-07-05T16:29:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-27T19:30:00.000Z",
        "voteCount": 3,
        "content": "A) Not seems for my , posible backup\nB) Active Pasive\nC) Backup \nD ) Active Active\nThen B is correct in this case"
      },
      {
        "date": "2023-06-15T22:38:00.000Z",
        "voteCount": 1,
        "content": "olabiba.ai said B."
      },
      {
        "date": "2023-06-10T12:16:00.000Z",
        "voteCount": 1,
        "content": "Do the math, option A is 5.55 days."
      },
      {
        "date": "2023-05-30T17:44:00.000Z",
        "voteCount": 5,
        "content": "A Wrong - I have stopped reading after 'create cron' , Same goes with C.\nD Wrong - Running ASG at full capacity in the DR is not cost efficient"
      },
      {
        "date": "2023-05-30T16:57:00.000Z",
        "voteCount": 2,
        "content": "i think i agree with option B, initially chosen D\nthe problem is that we need a cost effective solution and based on the following the global database might be more expensive and the fact the RDS cross region replication may cover the RTO of 10 minutes.\nquick compare on global database and cross region replication\nRDS Cross Region Replication - You will accrue charges for data transfer between Amazon EC2 and Amazon RDS across Regions, charged on both sides of the transfer ($0.02/GB out)\nAurora Global Database - you pay for replicated write I/O operations between the primary Region and each secondary Region. The number of replicated write I/O operations to each secondary Region is the same as the number of in-Region write I/O operations performed by the primary Region Replicated Write I/Os\t$0.20 per million replicated write I/Os"
      },
      {
        "date": "2023-05-30T01:26:00.000Z",
        "voteCount": 1,
        "content": "I would go with B as 10minutes RTO allows for scale up the ASG size. Also read replica is cheaper and can be promoted to primary. Also aurora replication to read replica is usually much less than 100 milliseconds after the primary writes operation which will be enough fot the RPO of 30 seconds."
      },
      {
        "date": "2023-05-29T11:39:00.000Z",
        "voteCount": 2,
        "content": "Cost efective = B"
      },
      {
        "date": "2023-05-28T13:43:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2023-05-26T21:40:00.000Z",
        "voteCount": 1,
        "content": "save the running EC2 cost. Only bring up when needed"
      },
      {
        "date": "2023-05-27T06:33:00.000Z",
        "voteCount": 3,
        "content": "but the question is saying \"web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes\"\nHow RPO/RTO can be achieved with bare minimum EC2 is up and running  in DR site. \n\nCan you paste the link/reading to justify your answer.\nThanks"
      },
      {
        "date": "2023-05-26T17:28:00.000Z",
        "voteCount": 3,
        "content": "I agree with Answer B"
      },
      {
        "date": "2023-05-27T06:40:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/disaster-recovery/"
      },
      {
        "date": "2023-05-28T16:05:00.000Z",
        "voteCount": 1,
        "content": "me too. B looks better."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/amazon/view/110304-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly.<br><br>Which solution will migrate the database in the LEAST amount of time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T12:18:00.000Z",
        "voteCount": 24,
        "content": "Why Not D:\n1- C=SnowBall Edge, D=SnowBall Device. \nThe basic difference between Snowball and Snowball Edge is the capacity they provide. Snowball provides a total of 50 TB or 80 TB, out of which 42 TB or 72 TB is available, while Amazon Snowball Edge provides 100 TB, out of which 83 TB is available.\n\n2- C=AWS Database Migration . D=Application Migration Service,  \nApplication Migration Service simplifies, expedites, and reduces the cost of migrating and modernizing applications. Not for Database"
      },
      {
        "date": "2024-04-09T09:21:00.000Z",
        "voteCount": 2,
        "content": "Option C :  How To \n\nhttps://aws.amazon.com/blogs/storage/enable-large-scale-database-migrations-with-aws-dms-and-aws-snowball/"
      },
      {
        "date": "2024-01-10T06:57:00.000Z",
        "voteCount": 1,
        "content": "AWS Snowball and Snowball Edge refers the same thing. From the Snowball FAQ \"AWS Snowball is a service that provides secure, rugged devices, so you can bring AWS computing and storage capabilities to your edge environments, and transfer data into and out of AWS. Those rugged devices are commonly referred to as AWS Snowball or AWS Snowball Edge devices. \". Between C and D, it's C using Snowball edge with AWS DMS."
      },
      {
        "date": "2023-11-22T23:24:00.000Z",
        "voteCount": 1,
        "content": "Option C - Direct connection would take 1 month"
      },
      {
        "date": "2023-07-05T16:31:00.000Z",
        "voteCount": 1,
        "content": "Basic Snowball edge / DMS use case"
      },
      {
        "date": "2023-06-10T12:18:00.000Z",
        "voteCount": 2,
        "content": "Do the math, option A is 5.55 days. It's A"
      },
      {
        "date": "2023-10-03T04:30:00.000Z",
        "voteCount": 2,
        "content": "Keyword is one-time migration. In addition to time it takes to deliver, it will be huge waste for one-time task."
      },
      {
        "date": "2023-06-15T22:44:00.000Z",
        "voteCount": 2,
        "content": "it takes ages to order a 1G circuit."
      },
      {
        "date": "2023-07-27T15:29:00.000Z",
        "voteCount": 2,
        "content": "It can take months to provision a DX connection, its not A."
      },
      {
        "date": "2023-05-30T16:38:00.000Z",
        "voteCount": 1,
        "content": "I agree with option C.\nOption D does not seem ideal because mentions Application Migration Service, also the snowball is more required for petabyte scale data migration while edge seems to be a better fit."
      },
      {
        "date": "2023-05-30T01:32:00.000Z",
        "voteCount": 3,
        "content": "First of all a snowball solution is required for one time migration will focus in C &amp; D.\nNow since we are looking o migrate a database, DMS is needed also Snowball edge can accommodate the 60TB of data as the capacity limit it 80TB. \nD is wrong by mentioning Application Migration service to migrate a database. \n\nSo correct answer is C). Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL."
      },
      {
        "date": "2023-05-29T11:47:00.000Z",
        "voteCount": 1,
        "content": "D better cost than C and it does the same for S3. Need adpter too"
      },
      {
        "date": "2023-05-26T06:42:00.000Z",
        "voteCount": 2,
        "content": "Answer : C ( Key words : Limited bandwidth + DB migration should be done quickly)\n\nif there no DB migration, we can go with B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/amazon/view/110302-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.<br><br>The company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T01:37:00.000Z",
        "voteCount": 17,
        "content": "Correct is C. For those voting with B, you missed the Instance configuration part. DLM will only backup the EBS volume not the instance settings also. AWS backup will backup ebs &amp; instance settings. \n\nOption C, using AWS Backup, provides a centralized and cost-effective solution for managing backups across multiple services, including EC2 instances. By creating a scheduled daily backup plan for the EC2 instances, AWS Backup ensures regular backups are taken. The backups can be configured to be stored in a vault in the secondary Region, fulfilling the requirement of maintaining backups in a separate Region.\nThe EC2 instance volumes and configurations can then be restored from the backup vault using AWS Backup's restore capabilities. This allows for the recovery of EC2 instances and their configurations within the required timeframe of 1 business day, with a maximum data loss of 1 day's worth."
      },
      {
        "date": "2023-06-04T09:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\n\nhttps://aws.amazon.com/ebs/data-lifecycle-manager/\n\nIt has aws sponsored video which stated clearly can take EBS backed AMIs with AWS DLM"
      },
      {
        "date": "2024-08-21T20:18:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2023-07-20T02:09:00.000Z",
        "voteCount": 1,
        "content": "B is Wrong!\nWhy? They must!! So that means Compliance is important. AWS Backup is a service for Compliance and Goverment Targets. C Match"
      },
      {
        "date": "2024-10-12T11:57:00.000Z",
        "voteCount": 1,
        "content": "C because of this one. \"The company has limited staff and needs a backup solution that optimizes operational efficiency and cost.\" AWS Backup really optimizes your backup solution. We backup everthing now with AWS Backup. B works too but it more complicated. The restore from AWS Backup is nearly a no brainer"
      },
      {
        "date": "2024-07-13T00:59:00.000Z",
        "voteCount": 1,
        "content": "C, for sure.\nUse AWS Backup.\nDLM itself does not directly support restore operations."
      },
      {
        "date": "2024-02-04T04:30:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C.\nWhy not B, DLM can only take backup on restore. The options says using DLM restore the volumes."
      },
      {
        "date": "2024-02-04T04:31:00.000Z",
        "voteCount": 1,
        "content": "I meant DLM cannot restore so the option B is wrong."
      },
      {
        "date": "2023-11-22T23:31:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2023-11-18T00:26:00.000Z",
        "voteCount": 1,
        "content": "Because AWS Back ups supports restore and DLM doesn't"
      },
      {
        "date": "2023-08-20T13:48:00.000Z",
        "voteCount": 1,
        "content": "B\nThe explanation here fits the use-case\nhttps://aws.amazon.com/blogs/storage/automating-amazon-ebs-snapshot-and-ami-management-using-amazon-dlm/"
      },
      {
        "date": "2023-07-05T16:47:00.000Z",
        "voteCount": 2,
        "content": "C\nB would be ok, if DLM supported restore. it doesn't"
      },
      {
        "date": "2023-06-29T07:57:00.000Z",
        "voteCount": 1,
        "content": "I think correct is C. AWS Backup is easier and perfectly fits the scenario"
      },
      {
        "date": "2023-06-24T05:06:00.000Z",
        "voteCount": 2,
        "content": "B says \"Use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region\" - just tested it and could not find any option for DLM to restore volumes, think the snapshots are managed the usual way."
      },
      {
        "date": "2023-06-21T13:40:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-c-c-c-c"
      },
      {
        "date": "2023-06-12T04:04:00.000Z",
        "voteCount": 1,
        "content": "Its B!!!!!!!!!!!!!!!!"
      },
      {
        "date": "2023-06-10T07:21:00.000Z",
        "voteCount": 3,
        "content": "Why not A?"
      },
      {
        "date": "2023-06-03T11:57:00.000Z",
        "voteCount": 2,
        "content": "I prefer B to C as this sentence \"The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes\", in this question, there is no database mentioned, I assume all persistent data is in EBS, so no need to backup ec2 instances, you can directly startup ec2 instance by cloudformation and load backuped ebs."
      },
      {
        "date": "2023-05-30T16:26:00.000Z",
        "voteCount": 2,
        "content": "AWS Backup is more cost effective so I would chose C as well. The DLM option B, does not contemplate the back up in another region as far as I could see."
      },
      {
        "date": "2023-06-03T11:53:00.000Z",
        "voteCount": 1,
        "content": "DLM can copy snapshots to another region, see https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-data-lifecycle-manager-enables-automation-snapshot-copy-via-policies/"
      },
      {
        "date": "2023-05-30T13:06:00.000Z",
        "voteCount": 1,
        "content": "AWS Backup is a latter service which tries to simplify the challenge of administering a backup in each service individually.\n\nHowever AWS Lifecycle Manager originally only made EBS snapshots but has been expanded to create AMIs. I don't believe AWS Backup can trigger AMI creation."
      },
      {
        "date": "2023-05-31T01:40:00.000Z",
        "voteCount": 1,
        "content": "But B mentions only EBS snapshots (Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes)! Does not say anything about AMI's.\nSo IMO the answer is C"
      },
      {
        "date": "2023-05-27T21:23:00.000Z",
        "voteCount": 2,
        "content": "The answer is B"
      },
      {
        "date": "2023-05-27T21:25:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/storage/automating-amazon-ebs-snapshots-management-using-data-lifecycle-manager/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/amazon/view/110300-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.<br><br>Which combination of steps will meet the encryption requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on S3 server-side encryption for the S3 bucket that the web application uses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure redirection of HTTP requests to HTTPS requests in CloudFront.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ADE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-03T08:54:00.000Z",
        "voteCount": 16,
        "content": "Answer : ACE\nA) SSE S3 sounds good encript in rest data\nB) sounds good until say in ACLs is incorrect \nC) Bucket Policy avoid upload unencrypted is correct sounds good\nD) CloudFront with KMS ? why ? not seems\nE) HTTP redirect to HTTPS sounds good is clasic this case \nF) why ? not seems in this case"
      },
      {
        "date": "2023-07-20T02:12:00.000Z",
        "voteCount": 6,
        "content": "ACE.\nBut A is deprecated :)\nbecause since the 05.01.2023 S3 use automatical atRest encryption for new objekts."
      },
      {
        "date": "2024-02-13T16:51:00.000Z",
        "voteCount": 2,
        "content": "Right I would go with CEF for 2024 onwards"
      },
      {
        "date": "2024-03-22T09:21:00.000Z",
        "voteCount": 1,
        "content": "BCE\nYou need B to enforce encryption in transit with S3. Other options cannot do that."
      },
      {
        "date": "2024-08-21T20:19:00.000Z",
        "voteCount": 1,
        "content": "just ACE"
      },
      {
        "date": "2024-03-13T01:15:00.000Z",
        "voteCount": 1,
        "content": "This question was obviously formulated before S3 buckets were encrypted by default."
      },
      {
        "date": "2023-12-09T01:23:00.000Z",
        "voteCount": 2,
        "content": "A. Turn on S3 server-side encryption for the S3 bucket that the web application uses.\nD. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).\nE. Configure redirection of HTTP requests to HTTPS requests in CloudFront.\n\nHere's why these steps are necessary:\n\nA. S3 server-side encryption: This encrypts data in the S3 bucket at rest, ensuring data confidentiality even if someone gains unauthorized access to the bucket.\nD. CloudFront SSE-KMS: This encrypts data in transit between CloudFront and the client, ensuring data confidentiality when users upload and download files.\nE. HTTP to HTTPS redirect: This ensures all communication between the client and CloudFront occurs over HTTPS, encrypting data in transit and preventing eavesdropping."
      },
      {
        "date": "2023-11-22T23:36:00.000Z",
        "voteCount": 1,
        "content": "Options A, C , E"
      },
      {
        "date": "2023-09-28T21:14:00.000Z",
        "voteCount": 1,
        "content": "A. Turn on S3 server-side encryption for the S3 bucket that the web application uses.\nD. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).\nE. Configure redirection of HTTP requests to HTTPS requests in CloudFront.\nData at rest encrypted for Both S3 and Cloudfront\nE for data in transit"
      },
      {
        "date": "2023-08-22T02:01:00.000Z",
        "voteCount": 2,
        "content": "How to Prevent Uploads of Unencrypted Objects to Amazon S3\nhttps://aws.amazon.com/tw/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/"
      },
      {
        "date": "2023-08-17T04:38:00.000Z",
        "voteCount": 1,
        "content": "ACE but why not F?"
      },
      {
        "date": "2023-09-03T10:11:00.000Z",
        "voteCount": 3,
        "content": "question nowhere mentions the use of pre-signed URLs\nif it was used in this scenario then it could potentially be one of the right answers"
      },
      {
        "date": "2024-08-08T18:36:00.000Z",
        "voteCount": 1,
        "content": "When you have pre-signed urls, you don't even necessarily need cloudFront"
      },
      {
        "date": "2023-07-07T16:36:00.000Z",
        "voteCount": 1,
        "content": "we don't have a \"encrytion at rest\" for cloudfront in the console"
      },
      {
        "date": "2023-07-05T17:00:00.000Z",
        "voteCount": 1,
        "content": "A and C are a bit redundant. I'd pick D instead of C, but for ACL reference"
      },
      {
        "date": "2023-06-21T13:43:00.000Z",
        "voteCount": 2,
        "content": "a-d-e    a-d-e   a-d-e"
      },
      {
        "date": "2023-06-03T11:25:00.000Z",
        "voteCount": 1,
        "content": "Source: https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule\n\nB is wrong as \"aws:SecureTransport\": \"true\" does not deny 'http' traffic"
      },
      {
        "date": "2023-05-31T20:48:00.000Z",
        "voteCount": 2,
        "content": "Why not B?"
      },
      {
        "date": "2023-06-03T12:08:00.000Z",
        "voteCount": 6,
        "content": "you should add  \"aws:SecureTransport\": \"true\" in the S3 bucket policy not S3 ACL.\nsee https://stackoverflow.com/questions/47815526/s3-bucket-policy-vs-access-control-list\n\nand \" We recommend allowing only encrypted connections over HTTPS (TLS) by using the aws:SecureTransport condition in your Amazon S3 bucket policies\" from https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html"
      },
      {
        "date": "2023-06-04T07:19:00.000Z",
        "voteCount": 1,
        "content": "Because C does just that"
      },
      {
        "date": "2023-06-03T11:24:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule\nit is not enough"
      },
      {
        "date": "2023-05-30T01:49:00.000Z",
        "voteCount": 2,
        "content": "I will go with ACE"
      },
      {
        "date": "2023-05-26T06:04:00.000Z",
        "voteCount": 4,
        "content": "Answer : ACE"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/amazon/view/110299-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments for development and production, including a clone of the database system.<br><br>The company's developers are allowed to access the credentials for the development database. However, the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a regular basis.<br><br>What should a solutions architect do in the production environment to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T18:26:00.000Z",
        "voteCount": 10,
        "content": "Answer : D\nRotation = Secret Manager (and Not Parameter store)"
      },
      {
        "date": "2024-08-29T03:28:00.000Z",
        "voteCount": 2,
        "content": "Answer should be A , As we are talking of encryption Key rotation by customer IT key responisble person and not the database credential rotation"
      },
      {
        "date": "2024-08-20T06:23:00.000Z",
        "voteCount": 1,
        "content": "To use parameters from Parameter Store in AWS Lambda functions without using an SDK, you can use the AWS Parameters and Secrets Lambda Extension.\nTo use parameters in a Lambda function without the Lambda extension, you must configure your Lambda function to receive configuration updates by integrating with the GetParameter API action for Parameter Store."
      },
      {
        "date": "2023-11-22T23:39:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-07-05T17:02:00.000Z",
        "voteCount": 1,
        "content": "its a D"
      },
      {
        "date": "2023-06-29T08:04:00.000Z",
        "voteCount": 2,
        "content": "Keys is DB credentials rotation"
      },
      {
        "date": "2023-06-21T13:46:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-dd-d-dd-d-d-d"
      },
      {
        "date": "2023-06-16T17:00:00.000Z",
        "voteCount": 1,
        "content": "From olabiba.ai \n\"Based on the requirements of resolving scaling issues and minimizing licensing costs, the most cost-effective solution would be option A: Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.\""
      },
      {
        "date": "2023-07-20T02:40:00.000Z",
        "voteCount": 2,
        "content": "Nice description, but A is Wrong. Parameter Store is not the best practice for Secrets based on AWS Well Architecting Framework"
      },
      {
        "date": "2023-06-16T17:02:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. This is for the next question."
      },
      {
        "date": "2023-05-30T15:58:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is A the requirement is to rotate the KEY and not the password, looks like this question was created to make us chose option D. \nOption A stores the password in the Param Store encrypting it with KMS which is the requirement \u201cthe credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access.\u201d\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/ps-integration-lambda-extensions.html \nCheck the Authentication section."
      },
      {
        "date": "2023-05-31T06:56:00.000Z",
        "voteCount": 3,
        "content": "A does not satisfy the requirement \"This key must be rotated on a regular basis.\""
      },
      {
        "date": "2024-01-11T21:23:00.000Z",
        "voteCount": 1,
        "content": "Agreed.  Requirement is to rotate the Key.  KMS CMKs can be rotated:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html"
      },
      {
        "date": "2023-05-30T01:52:00.000Z",
        "voteCount": 1,
        "content": "Answering D"
      },
      {
        "date": "2023-05-26T21:46:00.000Z",
        "voteCount": 1,
        "content": "D, Secret Manager is the accurate solution"
      },
      {
        "date": "2023-05-26T05:53:00.000Z",
        "voteCount": 1,
        "content": "Answer : D  \nKeys is DB credentials rotation"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/amazon/view/110298-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web servers, load-balanced application servers, and a Microsoft SQL Server database.<br><br>The company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to implement a solution to resolve scaling issues and minimize licensing costs as the application scales.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon DynamoDB as the database tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server to host the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query the data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T22:55:00.000Z",
        "voteCount": 10,
        "content": "\"does not want to rewrite the application. \" leaves the possible answer between A and C, cause B and D will force the application team to rewrite the data access part of the application.\nC is using EKS, which makes AutoScalingGroup is not required. ASG scales instances. ASG doesn't scale PODs in EKS. \nBabelfish is the key point in this question. \"Babelfish for Aurora PostgreSQL is a new capability for Amazon Aurora PostgreSQL-Compatible Edition that enables Aurora to understand commands from applications written for Microsoft SQL Server.\""
      },
      {
        "date": "2023-05-31T07:21:00.000Z",
        "voteCount": 6,
        "content": "There is no good solution here. A is just forcing that company to use AWS services as \"MOST cost-effectively\" alternative. Practically Bablefish has bad reviews, companies prefer to migrate SQL-Server as-is."
      },
      {
        "date": "2024-07-31T13:53:00.000Z",
        "voteCount": 1,
        "content": "All answers are wrong. A is not using managed services where possible (EKS would be better than EC2 and can run windows) and on C you can't have Auto Scaling group for EKS. Realistically C is the better option if scaled with Karpenter, etc."
      },
      {
        "date": "2024-08-21T20:23:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2024-05-02T18:42:00.000Z",
        "voteCount": 1,
        "content": "Key here is AWS Managed = EKS\nA. Says both Web tier and Application tier is behind ALB, which is not secure. \nA good design should have web tier behind ALB, and application tier behind NLB"
      },
      {
        "date": "2024-03-15T05:56:00.000Z",
        "voteCount": 1,
        "content": "Option A: Babelfish for Aurora PostgreSQL is a capability for Amazon Aurora PostgreSQL-Compatible Edition developed using the PostgreSQL extension framework that enables Aurora to understand commands from applications written for Microsoft SQL Server. Babelfish for Aurora PostgreSQL understands T-SQL, Microsoft SQL Server\u2019s SQL dialect, and supports\n\n https://aws.amazon.com/blogs/database/run-sql-server-reporting-services-reports-against-babelfish-for-aurora-postgresql/"
      },
      {
        "date": "2023-11-22T23:48:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-21T07:34:00.000Z",
        "voteCount": 2,
        "content": "As much as I would like to choose A but the question request for lift and shift approach rather than a replatform"
      },
      {
        "date": "2023-11-17T09:10:00.000Z",
        "voteCount": 1,
        "content": "I vote C.  Babelfish - another layer to keep an eye on.  Is it really going to translate all SQL app calls perfectly, or will they need tuning?"
      },
      {
        "date": "2023-09-03T00:09:00.000Z",
        "voteCount": 1,
        "content": "why not C"
      },
      {
        "date": "2023-11-07T15:03:00.000Z",
        "voteCount": 3,
        "content": "C would be probably the most realistic way a team work to engage such case regarding to the choices we have. However Babelfish is a tool made to execute Microsoft SQL on a postgreSQL server. In practice Babelfish is a toy and should not be used for a real strong usage since the database engine is the last thing you want to play with. Still, people who answered A have followed the theory, and it's probably the expected answer here."
      },
      {
        "date": "2023-08-24T08:03:00.000Z",
        "voteCount": 3,
        "content": "A : the best of the worst"
      },
      {
        "date": "2023-07-23T15:41:00.000Z",
        "voteCount": 1,
        "content": "Correct A."
      },
      {
        "date": "2023-07-05T19:56:00.000Z",
        "voteCount": 5,
        "content": "A. The other options sound fishy."
      },
      {
        "date": "2023-08-03T08:23:00.000Z",
        "voteCount": 1,
        "content": "golden."
      },
      {
        "date": "2023-07-05T17:06:00.000Z",
        "voteCount": 2,
        "content": "A by elimination"
      },
      {
        "date": "2023-06-21T13:53:00.000Z",
        "voteCount": 1,
        "content": "a-a-a-a-a-a-a\n\nafter much consideration it's the babelfish to the rescue - \nzaphod beeblebrox ftw"
      },
      {
        "date": "2023-05-30T15:34:00.000Z",
        "voteCount": 3,
        "content": "Agree with A the NLB with EKS might be a interesting choice if chose too fast. \nThe correct option should be A, using an ALB and rehost to from M SQL Server to Aurora using Belfish.\nhttps://aws.amazon.com/rds/aurora/babelfish/\n\"With Babelfish, Aurora PostgreSQL now understands T-SQL, Microsoft SQL Server's proprietary SQL dialect, and supports the same communications protocol, so your apps that were originally written for SQL Server can now work with Aurora with fewer code changes\""
      },
      {
        "date": "2023-05-30T01:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. B and C are wrong as putting web apps behind NLB is not the correct approach. Also D is wrong as having SQL DB on S3 is impossible to do it straight forward, will require to refactor everything in the backend side and data layer side."
      },
      {
        "date": "2023-05-26T05:49:00.000Z",
        "voteCount": 4,
        "content": "Answer : A\nIt includes a network end-point added to PostgreSQL to enable your PostgreSQL database to understand the SQL Server wire protocol and commonly used SQL Server commands. With Babelfish, applications that were originally built for SQL Server can work directly with PostgreSQL, with little to no code changes, and without changing database drivers."
      },
      {
        "date": "2023-05-28T16:45:00.000Z",
        "voteCount": 1,
        "content": "agree A https://aws.amazon.com/rds/aurora/babelfish/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/amazon/view/110297-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST methods: LINK, UNLINK, LOCK, and UNLOCK.<br><br>Users outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this problem with a solution that minimizes operational overhead.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon CloudFront distribution. Configure the ALB as the origin.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an accelerator in AWS Global Accelerator. Configure the ALB as the origin.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T02:00:00.000Z",
        "voteCount": 7,
        "content": "AWS Global Accelerator is a service that improves the availability and performance of applications for global users. By adding an accelerator in AWS Global Accelerator and configuring the ALB as the origin, the traffic from users outside the United States will be routed through the Global Accelerator network, which uses the AWS global network infrastructure to optimize the delivery of the application traffic."
      },
      {
        "date": "2024-08-28T07:39:00.000Z",
        "voteCount": 1,
        "content": "as the ALB and EKS are running in one region only which is us-east-1, how does the global accelerator help when traffic comes from other region e.g. EU ?\nAlso you don't configure origin in global accelerator, you configure endpoint group."
      },
      {
        "date": "2023-06-04T06:22:00.000Z",
        "voteCount": 2,
        "content": "Yes you can, see - https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html\n\n--&gt;  For standard accelerators, the endpoints are Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses."
      },
      {
        "date": "2024-10-03T16:58:00.000Z",
        "voteCount": 1,
        "content": "C"
      },
      {
        "date": "2024-06-21T08:10:00.000Z",
        "voteCount": 1,
        "content": "Not B. Because Gateway Edge-Optimized API Endpoint improve the performance by caching API responses. But (un)link the call is not supported by API Gateway so the rest will be passed to ALB anyway. So unlikely API gateway will cache and no benefit for performance improvement."
      },
      {
        "date": "2024-05-07T15:44:00.000Z",
        "voteCount": 1,
        "content": "Answer: C\n\nAWS Global Accelerator is a service in which you create accelerators to improve the performance of your applications for local and global users.\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\n\nWhen you create an ALB or NLB, you can optionally add an accelerator at the same time. Elastic Load Balancing and Global Accelerator work together to transparently add the accelerator for you. The accelerator is created in your account, with the load balancer as an endpoint. Using an accelerator provides static IP addresses and improves the availability and performance of your applications.\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html"
      },
      {
        "date": "2024-05-07T15:53:00.000Z",
        "voteCount": 3,
        "content": "Ans \u201cB\u201d is wrong, because API Gateway does NOT support non-standard REST methods. The supported methods are DELETE, GET, HEAD, OPTIONS, PATCH, POST, PUT, and ANY (which can substitute any of the other 7).\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-settings-method-request.html#setup-method-add-http-method ."
      },
      {
        "date": "2024-02-13T17:08:00.000Z",
        "voteCount": 2,
        "content": "A: No\nB: No, API Gateway doesn\u2019t support LINK, UNLINK, LOCK, UNLOCK.\nC: No, GA doesn\u2019t have the concept of \u201corigin\u201d - this is a CloudFront concept.\nD: Yes, because this addresses the main concern which is latency."
      },
      {
        "date": "2023-12-14T03:28:00.000Z",
        "voteCount": 4,
        "content": "imho answer is C.  Here is my thinking: There are two issues that we need to consider:\n1-  Non US Users are reporting long and inconsistent response times for these APIs\n2- The APIs are running in EKS and are exposed by the ALB (i.e., not the other way round)\n\nSo the issue is about latency not API design."
      },
      {
        "date": "2023-12-10T09:03:00.000Z",
        "voteCount": 1,
        "content": "b IS ANS\nMinimal operational overhead: API Gateway edge-optimized endpoints offer several advantages:\n\nReduced latency: They leverage AWS's global network of edge locations, significantly reducing latency for users outside the United States.\nScalability: They automatically scale to handle traffic spikes, eliminating the need for manual intervention.\nSecurity: They offer built-in security features, including access control and throttling, minimizing the need for additional configuration.\nNon-standard methods compatibility: API Gateway supports a wide range of HTTP methods, including custom methods like LINK, UNLINK, LOCK, and UNLOCK, ensuring compatibility with the existing APIs.\n\nEase of configuration: Configuring API Gateway with ALB as the target is straightforward and requires minimal changes to the existing infrastructure."
      },
      {
        "date": "2023-12-05T08:27:00.000Z",
        "voteCount": 3,
        "content": "Amazon CloudFront primarily supports standard HTTP/HTTPS request methods like GET, POST, PUT, DELETE, HEAD, OPTIONS, and PATCH. It does not natively support non-standard methods such as LINK and UNLINK, LOCK...etc\nHOWEVER&gt;&gt;&gt;&gt;&gt;\nIf you need to use these non-standard methods, you have a couple of options:\nCustom Handling with Lambda@Edge\nAPI Gateway Integration: If you require more complex routing and method handling, integrating AWS API Gateway with CloudFront might be a more suitable solution. API Gateway provides robust support for various HTTP methods and can be set up to handle non-standard methods.\n\nClearly its B"
      },
      {
        "date": "2023-11-22T23:56:00.000Z",
        "voteCount": 1,
        "content": "Option C, GA is safest option."
      },
      {
        "date": "2023-11-18T01:16:00.000Z",
        "voteCount": 2,
        "content": "there is no proper use case for API gateway here"
      },
      {
        "date": "2023-10-31T15:53:00.000Z",
        "voteCount": 2,
        "content": "A is invalid because cloudFront only support standard Rest Methods\nB C D all technically feasible but let's consider \"minimized operational overhead\" requirement, it's must be C."
      },
      {
        "date": "2023-08-19T13:09:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\nI don't understand why people are choosing GA. I would rather go with option D.\n\nFrom AWS documentation: \n\nEdge-optimized API endpoint\nThe default hostname of an API Gateway API that is deployed to the specified Region while using a CloudFront distribution to facilitate client access typically from across AWS Regions. API requests are routed to the nearest CloudFront Point of Presence (POP), which typically improves connection time for geographically diverse clients.\n\nI couldn't find any document mentioning that Edge-optimized API endpoints won't support non-standard REST methods."
      },
      {
        "date": "2023-08-19T13:14:00.000Z",
        "voteCount": 1,
        "content": "I know we can't trust AI assistants, but take a look at my little chat with:\n\n=== Labiba\nYes, Amazon API Gateway Edge-optimized APIs can handle non-standard REST methods. Edge-optimized APIs are designed to provide low-latency access to your API by using the AWS CloudFront global network. You can set up API methods to handle any HTTP method, including non-standard ones, and configure them to work with your specific requirements and use cases.\n\n=== Bard\nYes, Amazon API Gateway edge-optimized APIs can handle non-standard REST methods. However, there are some limitations.\n\nThe non-standard REST method must be supported by the integration that you use for the API method. For example, if you are using a Lambda integration, the Lambda function must be able to handle the non-standard REST method."
      },
      {
        "date": "2023-08-19T13:19:00.000Z",
        "voteCount": 1,
        "content": "Now, why would I use GA? \n\nI don't know you, but I would use in a situation where I have an application that connects to a database and I need to reduce the latency of my application for users by launching EC2 instances around the world. Note that I can't do that (not that easy, at least) with my RDS DB, so what I do? I use Global Accelerator to speed up communication between my instances in different countries to the database server in a single location, for example."
      },
      {
        "date": "2023-08-24T04:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html#api-gateway-api-endpoint-types-edge-optimized:~:text=traffic%20originates%20from.-,Edge%2Doptimized%20API%20endpoints,-An%20edge%2Doptimized\nI think can help you, C is answer"
      },
      {
        "date": "2023-08-02T05:18:00.000Z",
        "voteCount": 4,
        "content": "Cloudfront cannot handle non standard REST methods. There are Clod front involved behind API Gateway edge-optimized. So only C make sense here"
      },
      {
        "date": "2023-07-20T03:08:00.000Z",
        "voteCount": 3,
        "content": "It only can B... \nHere is a AWS entry. https://repost.aws/knowledge-center/api-gateway-cloudfront-distribution"
      },
      {
        "date": "2023-07-05T17:25:00.000Z",
        "voteCount": 3,
        "content": "B would be nice if edge-optimized was supported for HTTP APIs"
      },
      {
        "date": "2023-06-25T03:50:00.000Z",
        "voteCount": 3,
        "content": "By adding an accelerator in AWS Global Accelerator and configuring the ALB as the origin, the traffic to the ALB will be routed through the global network, reducing latency and improving response times for users outside the United States.\nThis solution minimizes operational overhead as AWS Global Accelerator handles the routing and optimization automatically, without requiring additional infrastructure deployment or configuration changes."
      },
      {
        "date": "2023-06-24T07:08:00.000Z",
        "voteCount": 4,
        "content": "I was also supporting answer B, however just tested API Gateway and it seems that it only supports GET, POST, PUT, PATCH, DELETE, HEAD, and OPTIONS methods. I personally couldn't find a way to create a custom method which is part of the requirement. Please share if you find a way"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/amazon/view/112693-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The sensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2 instance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS service. The company stores the data in Amazon DynamoDB.<br><br>On several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the reliability of the solution.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Point the alias record to the ALB. Use the MQTT broker to store the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route 53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT broker to store the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint. Configure an AWS IoT rule to invoke an AWS Lambda function to store the data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-21T13:57:00.000Z",
        "voteCount": 5,
        "content": "b-b-b-b-bb-\n\nGreengrass is typically used for edge computing scenarios and may not be the most suitable solution for addressing MQTT broker reliability and scalability."
      },
      {
        "date": "2024-03-31T17:58:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2023-11-23T00:02:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-09-15T14:52:00.000Z",
        "voteCount": 1,
        "content": "I think this is repeat question."
      },
      {
        "date": "2023-08-20T14:47:00.000Z",
        "voteCount": 3,
        "content": "AWS service is the answer."
      },
      {
        "date": "2023-08-17T17:37:00.000Z",
        "voteCount": 2,
        "content": "IOT core for anything IOT"
      },
      {
        "date": "2023-07-05T17:26:00.000Z",
        "voteCount": 4,
        "content": "IOT core for anything IOT"
      },
      {
        "date": "2023-06-27T09:11:00.000Z",
        "voteCount": 1,
        "content": "Option C doesn't mention required auto-scaling group, hence eliminated."
      },
      {
        "date": "2023-06-25T09:19:00.000Z",
        "voteCount": 3,
        "content": "voting for B. IoT Core"
      },
      {
        "date": "2023-06-24T07:25:00.000Z",
        "voteCount": 2,
        "content": "Both C and B should work. I suggest AWS wants us to use as many native services as we can, therefore B should be the preferred answer."
      },
      {
        "date": "2023-06-21T01:14:00.000Z",
        "voteCount": 2,
        "content": "voting for B. IoT Core"
      },
      {
        "date": "2023-06-20T10:35:00.000Z",
        "voteCount": 1,
        "content": "IoT core, B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/amazon/view/112870-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine requires a unique EC2 key pair.<br><br>The company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a securely encrypted place. The company will accept less than 1 minute of downtime during key rotation.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Parameter Store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to invoke an AWS Lambda function to initiate the key rotation in AWS KMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to issue a Systems Manager Run Command document to generate new key pairs and to rotate public keys to all the instances in Fleet Manager."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-13T17:17:00.000Z",
        "voteCount": 2,
        "content": "Not sure why you would need to \u201cinvoke an AWS Lambda function to generate new key pairs\u201d when Secrets Manager natively supports automatic key rotation? Anyways, A seems to be the least worst answer."
      },
      {
        "date": "2024-02-24T13:55:00.000Z",
        "voteCount": 4,
        "content": "Lambda is part of the key creation and rotation see the  link\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/"
      },
      {
        "date": "2023-12-29T23:55:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/"
      },
      {
        "date": "2023-12-27T08:18:00.000Z",
        "voteCount": 1,
        "content": "@duriselvan ==&gt; How did you arrive at \"Automatic key rotation\" from \"key rotation policy that will, upon request\nB. Parameter Store: While Parameter Store can store keys, it's not designed for automated key rotation. It would require manual configuration and orchestration.\nC. AWS KMS: KMS is designed for managing encryption keys, not SSH key pairs.\n\tIt doesn't support the rotation of SSH key pairs on EC2 instances.\nD. Fleet Manager: Fleet Manager, while facilitating management tasks on EC2 instances,\n\tdoesn't intrinsically handle key rotation.\n\tIt would require integration with other services and custom scripts."
      },
      {
        "date": "2023-12-09T02:08:00.000Z",
        "voteCount": 3,
        "content": "C ans\nAutomatic key rotation: AWS KMS automatically rotates keys according to the configured schedule, eliminating the need for manual intervention and ensuring timely key updates.\n\nLess than 1 minute downtime: AWS KMS allows for seamless key rotation with minimal downtime. The old key remains active until the new key is generated and propagated, ensuring uninterrupted access to instances.\n\nSecure storage: AWS KMS provides a highly secure and encrypted environment for storing cryptographic keys, exceeding the security offered by Parameter Store.\n\nLambda function integration: The EventBridge rule can trigger a Lambda function to perform additional tasks during key rotation, such as updating user access controls or notifying administrators."
      },
      {
        "date": "2023-11-23T04:45:00.000Z",
        "voteCount": 1,
        "content": "Torn between A and D. I don't like the do-it-yourself nature (Lambda) of A, but I understand what everyone is saying about the unique key requirement, which would seem to imply that D is wrong. Don't know tbh."
      },
      {
        "date": "2023-11-23T00:10:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-18T01:42:00.000Z",
        "voteCount": 2,
        "content": "A will work, don't overthink, you can request secret rotation in the Secrets manager, and secrets will be stored in a safe place"
      },
      {
        "date": "2023-10-31T08:33:00.000Z",
        "voteCount": 1,
        "content": "D is best option if we need to rotate for all Ec2 with same key pair. Since each EC2 to have a different Key pair, will be better to store in Secrets Manager and have that rotated using lambda."
      },
      {
        "date": "2023-08-22T17:07:00.000Z",
        "voteCount": 3,
        "content": "I think the Systems Manager maintenance window is to perform some potentially disruptive actions, which means the duration of the window is equal to system downtime.  and I check the white paper, I seems the duration of system maintenance window should be  longer than 1 hour."
      },
      {
        "date": "2023-08-19T03:04:00.000Z",
        "voteCount": 1,
        "content": "Seriously, all. While it can be done in A, it's better to do that with D. Here is why:\n\nQuestion says:\n\"A company has Linux-based Amazon EC2 instances.\" and \"Each machine requires a unique EC2 key pair.\"\n\nWe might be talking about thousands of EC2 instances. But let's continue. Option A says:\n\"Store all the keys in AWS Secrets Manager.\" which is OK, you can store up to 500,000 apparently but, seriously, think about. Instances are generated and deleted all the time. This would be cumbersome, even if you do that programmatically. Not convinced? Let me continue."
      },
      {
        "date": "2023-08-19T03:04:00.000Z",
        "voteCount": 1,
        "content": "Same option A, says the following: \"Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances.\"\n\nNow, this is A lot, but how are we going to replace the public keys on EC2 instances? Answer doesn't say.\n\nFinally, for those who are supporting their answer on an AWS blog showing how to use SM to rotate SSH key to manage servers, pay attention to this part: \"A secret is created in AWS Secrets Manager. The secret holds the SSH keypair that the master node will use to connect to the other nodes in the cluster.\"\n\nTheir design is \"one to many\", that is not part of what question says, and I would like to remind you \"Each machine requires a unique EC2 key pair.\""
      },
      {
        "date": "2023-08-22T17:00:00.000Z",
        "voteCount": 1,
        "content": "I am curious about how we can define a 1-minute Systems Manager maintenance window."
      },
      {
        "date": "2023-08-24T05:02:00.000Z",
        "voteCount": 1,
        "content": "With D how to \"keep the keys in a securely encrypted place\" ? Should be A"
      },
      {
        "date": "2023-07-31T12:48:00.000Z",
        "voteCount": 1,
        "content": "a-a-a-a-a-a-a-a"
      },
      {
        "date": "2023-07-20T22:14:00.000Z",
        "voteCount": 1,
        "content": "A: Based on the Well Architecting Framework for best Practices and that tutorial :) https://aws.amazon.com/de/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/"
      },
      {
        "date": "2023-07-08T03:05:00.000Z",
        "voteCount": 2,
        "content": "Why A? Select D"
      },
      {
        "date": "2023-07-20T22:15:00.000Z",
        "voteCount": 1,
        "content": "D is wrong, Parameter Store is a good practice to store Parameters but not the Secrets. I know you can use KMS to encrypt the Parameters, but you need a secure store f\u00fcr Secrets and here we have for exmaple the secret manager with FIPS 140-2 Standard."
      },
      {
        "date": "2023-07-05T20:21:00.000Z",
        "voteCount": 1,
        "content": "going with A"
      },
      {
        "date": "2023-07-05T17:34:00.000Z",
        "voteCount": 1,
        "content": "as someone pointed out D breaks the requirement for unique keys"
      },
      {
        "date": "2023-06-29T08:18:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/"
      },
      {
        "date": "2023-06-24T07:37:00.000Z",
        "voteCount": 4,
        "content": "According to the link below A is a better answer since the process does not require manual generation of the keys\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/amazon/view/112758-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio.<br><br>A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T20:53:00.000Z",
        "voteCount": 1,
        "content": "C vs D\n\nMigration Evaluator is suited for initial inventory collection, and it is Agentless so low overhead\n\nIn D, the Application Migration Service needs to install agent on thousands of VMs, so it is not suitable for initial inventory collection and is high Operational overhead"
      },
      {
        "date": "2024-02-08T00:42:00.000Z",
        "voteCount": 2,
        "content": "why to remove highly utilized servers from the list, these answers can be rejected immediately."
      },
      {
        "date": "2023-11-23T00:35:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-10-12T16:28:00.000Z",
        "voteCount": 4,
        "content": "migration evaluator. I think C is correct"
      },
      {
        "date": "2023-07-05T17:37:00.000Z",
        "voteCount": 1,
        "content": "C no doubt"
      },
      {
        "date": "2023-07-02T16:47:00.000Z",
        "voteCount": 1,
        "content": "C\nThis solution can meet the requirements with the least operational overhead. and also, keyword for planning only"
      },
      {
        "date": "2023-06-29T08:27:00.000Z",
        "voteCount": 1,
        "content": "I was first thinking about D because is is stated that the company has little knowledge aboutVMWare. But option D introduces operational overhead"
      },
      {
        "date": "2023-06-27T09:36:00.000Z",
        "voteCount": 2,
        "content": "C seems like a good choice:\nhttps://aws.amazon.com/migration-evaluator/features/"
      },
      {
        "date": "2023-06-21T14:04:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-\nmigration evaluator ftw"
      },
      {
        "date": "2023-06-21T14:11:00.000Z",
        "voteCount": 1,
        "content": "Question 210 is a-a-a-a-a-a-a-a"
      },
      {
        "date": "2023-06-20T19:35:00.000Z",
        "voteCount": 3,
        "content": "C\nThis solution can meet the requirements with the least operational overhead. and also, keyword for planning only"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/amazon/view/112681-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a limited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes application downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The company wants to protect the database from crashes.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-20T23:27:00.000Z",
        "voteCount": 8,
        "content": "A tricky question :)\nThe RDS proxy sounds sexy, but it cannot be used because the database is on premise.\n\nThe creative solution here is SQS. \nSuch questions are partly about your understanding of the services and some solutions are good, even if they sound a bit strange at first :)"
      },
      {
        "date": "2023-10-31T14:21:00.000Z",
        "voteCount": 6,
        "content": "\"The company wants to protect the database from crashes\" means keep the existing one and do something that can prevent crashes, not to migrate it to another in anywhere. -&gt; B, C out\n\nChoice between SQS and SNS is easy."
      },
      {
        "date": "2024-06-24T17:37:00.000Z",
        "voteCount": 1,
        "content": "C,\n\nKeyword: \"supports a limited number of concurrent connections\"\n\nCreating an Amazon RDS Proxy DB instance and attaching it to the Amazon RDS DB instance can help manage the database connections efficiently and prevent the database from being overwhelmed by too many connections. The RDS Proxy can pool and share connections to the database, which can reduce the number of connections that each Lambda function invocation needs to establish. This can help to prevent the database from crashing when the number of Lambda function invocations is high.\n\nReconfiguring the Lambda function to write to the RDS Proxy DB instance instead of directly to the database can further help to protect the database from crashes. This is because the RDS Proxy can handle the connections to the database, reducing the load on the database and helping to ensure its stability."
      },
      {
        "date": "2024-08-21T22:10:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2024-04-28T20:57:00.000Z",
        "voteCount": 1,
        "content": "You can use SQS to write data, however the phrase \"reserved concurrency\" is incorrect, Lambda has \"provisioned concurrency\""
      },
      {
        "date": "2024-04-10T05:10:00.000Z",
        "voteCount": 1,
        "content": "Option A:  AWS Tutorial on How To \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-lambda-tutorial.html"
      },
      {
        "date": "2023-11-23T00:32:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-07-23T14:03:00.000Z",
        "voteCount": 1,
        "content": "Correct A."
      },
      {
        "date": "2023-07-05T17:39:00.000Z",
        "voteCount": 1,
        "content": "Its an A"
      },
      {
        "date": "2023-06-29T08:30:00.000Z",
        "voteCount": 2,
        "content": "correct is A as database is on-premises"
      },
      {
        "date": "2023-06-26T09:51:00.000Z",
        "voteCount": 1,
        "content": "MODERATOR Please delete my previous comment. I commented about RDS proxy which is totally WRONG.\nAnswer is A"
      },
      {
        "date": "2023-06-25T22:55:00.000Z",
        "voteCount": 1,
        "content": "Will go with C , don't think the question says they need to keep the on-prem db"
      },
      {
        "date": "2023-06-24T07:43:00.000Z",
        "voteCount": 3,
        "content": "apparently, we need to make the lambda \"not to rush that much\" and keep the connection within the limit of the on-pre DB. So if we want not to lose data while waiting we implement SQS before the lambda so it keeps the requests in the queue."
      },
      {
        "date": "2023-06-22T17:06:00.000Z",
        "voteCount": 1,
        "content": "C should be logical answer, that's what RDS proxy does. But, they want to keep the existing SQL on-prem and not migrate to RDS. So C and B are out. We need to throttle the connections. SNS is not designed for this. So, it's SQS (A)."
      },
      {
        "date": "2023-06-21T14:43:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-06-21T14:13:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c\n\nBy creating an Amazon RDS Proxy DB instance and attaching it to the existing Amazon RDS DB instance, you can protect the database from crashes caused by a high number of Lambda function invocations. The RDS Proxy acts as an intermediary between the Lambda function and the database, managing the connections and pooling them efficiently"
      },
      {
        "date": "2023-07-31T12:54:00.000Z",
        "voteCount": 3,
        "content": "a-a-a-a-a-a-a-a-a"
      },
      {
        "date": "2023-06-21T13:57:00.000Z",
        "voteCount": 1,
        "content": "A is the answer. RDS proxy is meant to help with connection pooling.  Amazon RDS Proxy instance maintains a pool of established connections to your RDS database instances, reducing the stress on database compute and memory resources that typically occurs when new connections are established. RDS Proxy also shares infrequently used database connections, so that fewer connections access the RDS database. This connection pooling enables your database to efficiently support a large number and frequency of application connections so that your application can scale without compromising performance."
      },
      {
        "date": "2023-06-29T22:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. But IGNORE my above comment on RDS.  The current situation is database is on-premises. So RDS proxy has nothing to do with onprem DB. so Answer is A"
      },
      {
        "date": "2023-06-20T07:24:00.000Z",
        "voteCount": 1,
        "content": "SNS is used for notification purpose not for data matter. we don't know how big can be the data to write.\ni use SQS to decuple"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/amazon/view/112862-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic dashboards where possible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group's minimum, desired, and maximum number of instances to one. Create an Application Load Balancer that serves at least two Availability Zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-21T14:15:00.000Z",
        "voteCount": 6,
        "content": "By creating an Amazon Managed Grafana workspace, you can offload the operational overhead of managing and maintaining the Grafana infrastructure. Amazon Managed Grafana is a fully managed service that takes care of the underlying infrastructure, including scalability, availability, and updates."
      },
      {
        "date": "2024-09-06T03:29:00.000Z",
        "voteCount": 1,
        "content": "The meaning of option B is to create a new Grafana workspace, configure the current CloudWatch data source to it, and then import the historical Grafana instances into the new Grafana workspace."
      },
      {
        "date": "2024-05-23T08:32:00.000Z",
        "voteCount": 1,
        "content": "The company has invested time and effort to create dashboards that the company wants to preserve.\n\nB is good but it won't preserve their dashboard"
      },
      {
        "date": "2024-05-23T08:33:00.000Z",
        "voteCount": 3,
        "content": "I mean B, sorry. Moderator please change."
      },
      {
        "date": "2023-12-04T20:58:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct, however read this, https://docs.aws.amazon.com/grafana/latest/userguide/AMG-workspace-content-migration.html"
      },
      {
        "date": "2023-11-23T00:30:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-05T17:42:00.000Z",
        "voteCount": 1,
        "content": "gotta be a B"
      },
      {
        "date": "2023-06-22T17:07:00.000Z",
        "voteCount": 1,
        "content": "Def B."
      },
      {
        "date": "2023-06-21T14:45:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-06-21T14:14:00.000Z",
        "voteCount": 3,
        "content": "B is the answer https://aws.amazon.com/grafana/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/amazon/view/112716-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that runs on a Linux server. According to a new security requirement, the company must rotate the database password each year.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passtard rotation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by using an AWS Lambda function on a yearly schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-22T07:21:00.000Z",
        "voteCount": 1,
        "content": "C is wrong as system manager parm store does not support auto rotate password"
      },
      {
        "date": "2024-01-15T11:29:00.000Z",
        "voteCount": 2,
        "content": "Answer B\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/"
      },
      {
        "date": "2023-11-23T00:26:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-10-31T13:54:00.000Z",
        "voteCount": 4,
        "content": "Wish all questions are clear like this. \nA: Drop immediately at the first sentence\nB: sounds good\nC: host database in ec2 instance will never a choice. Plus SSM parameter store + lambda for password rotation is not as good as secret manager\nD: Again, don't migrate one type of database to another"
      },
      {
        "date": "2023-09-13T07:57:00.000Z",
        "voteCount": 1,
        "content": "Doubt in question it mention yearly rotation, if you can see in Secret Manager the dropdown options are hourly, days, week, and months it doesn\u2019t have the yearly option, however, you can mention 12 if that is the case then option B is correct else option C"
      },
      {
        "date": "2023-08-28T00:53:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-other.html#rotate-secrets_turn-on-for-other_step1"
      },
      {
        "date": "2023-07-20T23:33:00.000Z",
        "voteCount": 1,
        "content": "It is sad that so many questions here are marked as correct with a wrong result.\n\nWell Architeting Framework!!!"
      },
      {
        "date": "2023-07-08T03:15:00.000Z",
        "voteCount": 1,
        "content": "ofc it's B"
      },
      {
        "date": "2023-07-05T17:43:00.000Z",
        "voteCount": 2,
        "content": "B for sure"
      },
      {
        "date": "2023-07-05T04:44:00.000Z",
        "voteCount": 1,
        "content": "Secrets manager has built-in rotation feature"
      },
      {
        "date": "2023-06-25T09:29:00.000Z",
        "voteCount": 1,
        "content": "keyword =  Secrets Manager.\nThen B"
      },
      {
        "date": "2023-06-21T14:46:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-06-21T14:42:00.000Z",
        "voteCount": 1,
        "content": "b-b-b-b-b-b-b-b"
      },
      {
        "date": "2023-06-21T14:24:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-06-21T04:30:00.000Z",
        "voteCount": 1,
        "content": "I'd vote for B. A keyword that leads me to B is \"rotate the database password each year.\" This is referring to Secrets Manager."
      },
      {
        "date": "2023-06-21T00:13:00.000Z",
        "voteCount": 1,
        "content": "least operation... rds + secret manager"
      },
      {
        "date": "2023-06-20T13:43:00.000Z",
        "voteCount": 2,
        "content": "the LEAST operational overhead. So B is the easest"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/amazon/view/112767-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same AWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premises network.<br><br>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Site-to-Site VPN for connectivity to the on-premises network.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Direct Connect for connectivity to the on-premises network."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-05T17:45:00.000Z",
        "voteCount": 8,
        "content": "BD\nthey need a (one) VPC, no need for TGW.\nUse case for subnet sharing via RAM"
      },
      {
        "date": "2024-07-31T14:12:00.000Z",
        "voteCount": 2,
        "content": "They are designing an account structure. This means multiple accounts, implicitly multiple VPCs. So A will take care of account provisioning. (B is incorrect, subnets cannot be shared). To connect to on-prem, site-to-site VPN is sufficient and most cost-effective, and we also need to give access to it from all accounts, so we need a Transit Gateway. Therefore C is the other correct answer. (D is incorrect because it only works for one VPC, one account, and E is incorrect because is more expensive than VPN and not necessary)"
      },
      {
        "date": "2024-08-21T22:14:00.000Z",
        "voteCount": 1,
        "content": "just BD"
      },
      {
        "date": "2024-07-31T14:20:00.000Z",
        "voteCount": 2,
        "content": "Correction. VPC subnets can be shared, so BC would work, but the resulting architecture is a networking nightmare. I would not do that."
      },
      {
        "date": "2024-08-21T22:15:00.000Z",
        "voteCount": 1,
        "content": "Transit gateways are not cost-effective"
      },
      {
        "date": "2024-07-10T11:26:00.000Z",
        "voteCount": 1,
        "content": "B, D for sure.\nNo need for a TGW"
      },
      {
        "date": "2024-05-23T08:15:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: BE\n\nWhy is nobody considering Direct Connect, it is cheaper than Site to Site VPN."
      },
      {
        "date": "2024-05-23T08:17:00.000Z",
        "voteCount": 1,
        "content": "the ask here is for most cost effectively choice."
      },
      {
        "date": "2024-05-04T14:46:00.000Z",
        "voteCount": 1,
        "content": "If you have one VPC why you need to share the subnets ?"
      },
      {
        "date": "2024-03-15T06:45:00.000Z",
        "voteCount": 2,
        "content": "Option BC &amp; NOT C - The MOST cost effective option:   AWS Site-to-Site VPN connection pricing still applies in addition to AWS Transit Gateway VPN attachment pricing. So you will be additional cost with both option\n\nhttps://aws.amazon.com/transit-gateway/pricing/"
      },
      {
        "date": "2024-01-22T04:06:00.000Z",
        "voteCount": 1,
        "content": "The problem did not say how many VPC. @@@"
      },
      {
        "date": "2023-12-14T04:52:00.000Z",
        "voteCount": 3,
        "content": "B+C in my humble opinion.  Reason for C is that this is a design for a company with \"multiple teams\" so it is only logical that these teams will want to have at some stage independent accounts from one another and different accounts within the same teams.   Thinking about a single VPC would be a bit short sighted."
      },
      {
        "date": "2023-11-23T11:22:00.000Z",
        "voteCount": 2,
        "content": "B and D is right choice."
      },
      {
        "date": "2023-11-09T19:04:00.000Z",
        "voteCount": 1,
        "content": "Most Cost Effective..."
      },
      {
        "date": "2023-11-08T05:18:00.000Z",
        "voteCount": 1,
        "content": "You need to create a singe VPC and a single Account."
      },
      {
        "date": "2023-08-20T17:04:00.000Z",
        "voteCount": 3,
        "content": "Direct Connect may be an overkill with 1GBPs"
      },
      {
        "date": "2023-08-20T12:34:00.000Z",
        "voteCount": 1,
        "content": "Other problem with VPN is 1.25 Gb limitation."
      },
      {
        "date": "2023-07-23T13:40:00.000Z",
        "voteCount": 3,
        "content": "Correct AD.\nI think A is correct because you can connect the VPN to each VPC by using a VPN connection resource in each AWS account. You do not need a shared network account for that. You can refer to this documentation for more details: https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\nB is not correct because it will create a single VPC for all the AWS accounts, which will reduce the isolation and security for the different teams. It will also require sharing the subnets by using AWS Resource Access Manager, which will add complexity and overhead."
      },
      {
        "date": "2023-07-05T04:47:00.000Z",
        "voteCount": 1,
        "content": "Tgw is for VPCs communication."
      },
      {
        "date": "2023-07-03T11:17:00.000Z",
        "voteCount": 3,
        "content": "BC. There are multiple teams and accounts."
      },
      {
        "date": "2023-07-01T19:51:00.000Z",
        "voteCount": 1,
        "content": "BD? dont think we need tgw here."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/amazon/view/112833-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect at a large company needs to set up network security for outbound traffic to the internet from all AWS accounts within an organization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound traffic to the internet. The company deploys resources only into a single AWS Region.<br><br>The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbps in each Availability Zone.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy's Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy's Auto Scaling group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-17T13:54:00.000Z",
        "voteCount": 5,
        "content": "Centrally managed egress, so C/D are out.\nBoth A and B are wrong, because \n1. There isn\u2019t internet gateway. \n2. \u201cModify all default routes to point to the \u2026\u201d. A firewall or \u201cproxy's Auto Scaling group\u201d don\u2019t have public IP, the default route must be pointing to the NAT gateway. And NAT gateway has a peer public IP configured on the IGW. The route should be: internet prefix of all the internal subnet-&gt; NAT gateway -&gt; firewall -&gt; internet gateway, and reverse routing rules are also required.\n.\nWell, considering the persistent low quality of AWS Exam Questions, I vote B"
      },
      {
        "date": "2024-03-06T22:43:00.000Z",
        "voteCount": 2,
        "content": "c,d in each AWS account. wrong  \na: use third party solution, not as good as b (use aws service)"
      },
      {
        "date": "2023-11-23T12:45:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-10-15T00:17:00.000Z",
        "voteCount": 4,
        "content": "B.\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/using-nat-gateway-with-firewall.html"
      },
      {
        "date": "2023-09-21T08:34:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/security/hands-on-walkthrough-of-the-aws-network-firewall-flexible-rules-engine/"
      },
      {
        "date": "2023-08-20T14:20:00.000Z",
        "voteCount": 1,
        "content": "Given the available options and the requirements:\nB. Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource policy to API Gateway to only allow access from the VPC endpoint. Change the API Gateway endpoint type to private. is the correct answer."
      },
      {
        "date": "2023-08-24T09:33:00.000Z",
        "voteCount": 2,
        "content": "bro what?"
      },
      {
        "date": "2023-07-05T17:48:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-07-05T04:51:00.000Z",
        "voteCount": 4,
        "content": "centrally managed outbound traffic: tgw-&gt; centralized VPC with network firewall with rules-&gt; internet"
      },
      {
        "date": "2023-06-22T18:43:00.000Z",
        "voteCount": 3,
        "content": "vote for B. The keyword is \"centrally managed rule-based filtering on outbound traffic to the internet for all AWS accounts...\". Network Firewall can centrally manage network security policies."
      },
      {
        "date": "2023-06-22T17:28:00.000Z",
        "voteCount": 2,
        "content": "B. Answer A is similar, but you have to deal with EC2 instances and dealing with 3rd party FW, not good - management overhead. C is impossible. D is waay to much hard to manage."
      },
      {
        "date": "2023-06-21T15:15:00.000Z",
        "voteCount": 4,
        "content": "b-b-b-b-b-b\n\nCreate a new VPC specifically dedicated to outbound traffic to the internet. This helps isolate and manage the outbound traffic separately from other resources.\nConnect the existing transit gateway to the new VPC. This ensures that the VPC is connected to the centralized transit gateway that routes traffic between AWS accounts.\nConfigure a new NAT gateway within the new VPC. This NAT gateway provides the necessary outbound connectivity to the internet for resources within the VPC.\nUse AWS Network Firewall, a managed firewall service, for rule-based filtering on the outbound traffic. Network Firewall allows you to define and enforce custom rules for traffic leaving the VPC.\nCreate Network Firewall endpoints in each Availability Zone. These endpoints serve as the traffic inspection points where Network Firewall applies the filtering rules.\nModify all default routes in the VPCs to point to the Network Firewall endpoints. This ensures that all outbound traffic from the VPCs flows through the Network Firewall for rule-based filtering."
      },
      {
        "date": "2023-06-21T14:55:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-06-21T07:54:00.000Z",
        "voteCount": 2,
        "content": "vote for B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/amazon/view/112768-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses a load balancer to distribute traffic to Amazon EC2 instances in a single Availability Zone. The company is concerned about security and wants a solutions architect to re-architect the solution to meet the following requirements:<br><br>\u2022\tInbound requests must be filtered for common vulnerability attacks.<br>\u2022\tRejected requests must be sent to a third-party auditing application.<br>\u2022\tAll resources should be highly available.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspector to monitor traffic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to the third-party auditing application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with Amazon CloudWatch Logs. Use an AWS Lambda function to frequently push the logs to the third-party auditing application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Create an Amazon Kinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T08:50:00.000Z",
        "voteCount": 14,
        "content": "Only A and D cover the requirement for high availability. A uses Inspector, which is a vulnerability scanner and does not monitor traffic. So - even that I don't like the complexity of D - this remains the only option"
      },
      {
        "date": "2023-08-20T17:19:00.000Z",
        "voteCount": 6,
        "content": "I was confused between A and D, but seems WAF can deliver logs to Firehose\nhttps://docs.aws.amazon.com/waf/latest/developerguide/logging-kinesis.html"
      },
      {
        "date": "2024-09-06T17:38:00.000Z",
        "voteCount": 1,
        "content": "Compared to A, prioritize AWS Kinesis over third-party auditing applications"
      },
      {
        "date": "2023-11-23T13:54:00.000Z",
        "voteCount": 1,
        "content": "D is good option but as the question does not mention about 3rd party auditing app it may not be possible to directly integrate it with Firehose. One may have to use http api to push the logs - as this is not mentioned I will go with Option B."
      },
      {
        "date": "2023-11-23T13:56:00.000Z",
        "voteCount": 2,
        "content": "Oh Mistake, I want to change it to D as B does not support High Availability."
      },
      {
        "date": "2023-08-20T14:19:00.000Z",
        "voteCount": 2,
        "content": "It's D, makes most sense,"
      },
      {
        "date": "2023-08-19T11:56:00.000Z",
        "voteCount": 2,
        "content": "This is such a mal formed question...\nYou see, nowhere in the question we are told about customer's application. However we are told they want ALL their resources highly available. B would be sooo much better if there wasn't that \"All resources should be highly available.\" because, seriously, D is not the best in my opinion. We don't know much what applications they use, what third party  auditing application and so on...\n\nAnyway, it might be D after all, but oh my..."
      },
      {
        "date": "2023-07-23T13:28:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-05T19:46:00.000Z",
        "voteCount": 1,
        "content": "its a D"
      },
      {
        "date": "2023-06-29T08:59:00.000Z",
        "voteCount": 1,
        "content": "ASG in Multiple AZ. WAF and WAF logs with kinesis"
      },
      {
        "date": "2023-06-26T06:17:00.000Z",
        "voteCount": 1,
        "content": "\"enable logging by selecting the Kinesis Data Firehose as the destination\"--- how can ALB write logs directly to Kinesis???\nit should be CW logs group \nany links for help??"
      },
      {
        "date": "2023-06-24T05:46:00.000Z",
        "voteCount": 3,
        "content": "Amazon inspector does NOT inspect traffic coming to an Application Load Balancer (ALB)"
      },
      {
        "date": "2023-06-23T10:39:00.000Z",
        "voteCount": 4,
        "content": "D is correct answer\nInbound requests must be filtered for common vulnerability attacks -&gt; WAF\nRejected requests must be sent to a third-party auditing application-&gt; Enable access log and use kinesis stream to send logs to third party\nAll resources should be highly available -&gt; Muti AZ auto scaling group."
      },
      {
        "date": "2023-06-23T10:33:00.000Z",
        "voteCount": 2,
        "content": "Inspector does not filter inbound traffic for attack signatures, this is what WAF is for"
      },
      {
        "date": "2023-06-22T17:35:00.000Z",
        "voteCount": 2,
        "content": "B and C do not provide HA. D is similar to A but lacks Inspector -&gt; \"Amazon Inspector automatically discovers workloads, such as Amazon EC2 instances, containers, and Lambda functions, and scans them for software vulnerabilities and unintended network exposure.\""
      },
      {
        "date": "2023-06-29T08:58:00.000Z",
        "voteCount": 1,
        "content": "but you need logs of the reject request on WAF. So I think correct answer is D"
      },
      {
        "date": "2023-07-06T14:18:00.000Z",
        "voteCount": 1,
        "content": "It\u2019s probably B. C and D are not correct, ALB can\u2019t send logs to Kinesis Fire Hose."
      },
      {
        "date": "2023-06-21T15:20:00.000Z",
        "voteCount": 1,
        "content": "a-a-a-a-a-a-a-a\nmulti-az for HA"
      },
      {
        "date": "2023-07-31T13:05:00.000Z",
        "voteCount": 1,
        "content": "it's d-d-d-d-d-d--d-d"
      },
      {
        "date": "2023-06-21T15:15:00.000Z",
        "voteCount": 3,
        "content": "I got with D. The reason to go with D is because other options ABC are wrong. \n1. It says use Amazon Inspector to inspect traffic to ALB. This is wrong. Amazon inspector does NOT inspect traffic coming to an Application Load Balancer (ALB). Amazon Inspector is a security assessment service that helps you analyze the security and compliance of your EC2 instances and applications running on them. To inspect traffic coming to an ALB, you can consider using other services such as AWS WAF (Web Application Firewall) or AWS Shield. AWS WAF allows you to define rules to filter and block malicious traffic targeting your ALB. \nB - Does NOT talk about HA as it is asked in ques\nC - Does NOT talk about HA as it is asked in ques"
      },
      {
        "date": "2023-06-21T15:07:00.000Z",
        "voteCount": 1,
        "content": "Option B and C does NOT talk about HA. Its between A and D .."
      },
      {
        "date": "2023-06-29T22:37:00.000Z",
        "voteCount": 1,
        "content": "D is answer \nA is wrong as Amazon inspector does NOT inspect traffic coming to an Application Load Balancer (ALB)"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/amazon/view/112848-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application in the AWS Cloud. The application consists of microservices that run on a fleet of Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon API Gateway. Some of the older microservices that run on EC2 instances need to call this new API.<br><br>The company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each microservice. Configure the API methods to require the key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource policy to API Gateway to only allow access from the VPC endpoint. Change the API Gateway endpoint type to private.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow access to the API Gateway. Move the API Gateway into a new VPDeploy a transit gateway and connect the VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets with a route to the created Global Accelerator endpoint IP address. Add an API key for each service to use for authentication."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T17:02:00.000Z",
        "voteCount": 10,
        "content": "Tip: Anytime you see \"don't want to traverse Internet traffic\" always look for endpoint in the answers. Most likely, that's the answer."
      },
      {
        "date": "2023-07-21T07:15:00.000Z",
        "voteCount": 6,
        "content": "The quality control here is unfortunately not as expected when you buy access.\nC is due nonsense.\nB is correct.\nVPC Endpoint to API Gateway and a policy on both sides!\n\nTrust me, i\u00b4m a Ninja"
      },
      {
        "date": "2023-07-27T14:43:00.000Z",
        "voteCount": 2,
        "content": "thanks Ninja"
      },
      {
        "date": "2023-11-30T07:46:00.000Z",
        "voteCount": 3,
        "content": "Answer B - VPC Interface endpoint to privately access services without data over internet."
      },
      {
        "date": "2023-11-23T14:00:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-05T19:48:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-22T18:38:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-22T17:38:00.000Z",
        "voteCount": 3,
        "content": "Tip: Anytime you see \"don't want to traverse Internet traffic\" always look for endpoint in the answers. Most likely, that's the answer."
      },
      {
        "date": "2023-06-21T15:24:00.000Z",
        "voteCount": 3,
        "content": "b-b-b-b-b-b-b\n\nBy implementing this solution, the company can ensure that the new API in API Gateway is not accessible from the public internet. The interface VPC endpoint provides private connectivity, allowing secure communication between the microservices running on EC2 instances and the API Gateway. This ensures the proprietary data does not traverse the public internet, enhancing security and data protection."
      },
      {
        "date": "2023-06-21T15:04:00.000Z",
        "voteCount": 1,
        "content": "I vote B"
      },
      {
        "date": "2023-06-21T11:28:00.000Z",
        "voteCount": 1,
        "content": "VPC endpoint usualy is the prefect answer to avoid internet traffic"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/amazon/view/112850-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.<br><br>A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances.<br><br>What is the FASTEST way for the solutions architect to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-30T19:53:00.000Z",
        "voteCount": 8,
        "content": "Both B and D work, except B has no notification set.\nhttps://aws.amazon.com/blogs/security/how-to-monitor-aws-account-configuration-changes-and-api-calls-to-amazon-ec2-security-groups/"
      },
      {
        "date": "2023-06-21T15:05:00.000Z",
        "voteCount": 6,
        "content": "I vote D. aws config changes can be sent to SNS topic https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html"
      },
      {
        "date": "2024-09-10T20:37:00.000Z",
        "voteCount": 1,
        "content": "B is faster"
      },
      {
        "date": "2024-08-08T23:54:00.000Z",
        "voteCount": 1,
        "content": "Both B and D works. But the question is asking for FASTEST.\n\nFor cloudTrail, you need: CloudTrail \u2192 CloudWatch Logs \u2192 CloudWatch Metric Filter \u2192 CloudWatch Alarm \u2192 SNS Notification\n\nFor aws Config, it natively support integration with SNS.\nHence we should choose D"
      },
      {
        "date": "2024-08-03T20:05:00.000Z",
        "voteCount": 1,
        "content": "I'm leaning towards D, but looks what it says in this blog:\nhttps://aws.amazon.com/blogs/security/how-to-monitor-aws-account-configuration-changes-and-api-calls-to-amazon-ec2-security-groups/\n\nFor the Config option, it says:\n\"The use of AWS Config in Method 1 allows for the configuration of a security group to be tracked along with other AWS resources. Changes to the security group\u2019s configuration are reported during the next Config compliance evaluation, typically within 10 minutes\"\n\nand for the CloudTrail option it says:\n\"The use of CloudTrail and CloudWatch Events in Method 2 allows for the near real-time detection of API calls that could change the configuration of a VPC security group\"\n\nSo it seems clear cut to me that the answer is B, although if I hadn't seen this blog I would've picked D probably"
      },
      {
        "date": "2024-05-03T15:49:00.000Z",
        "voteCount": 2,
        "content": "For me the answer is B.\nHere we are talking about \"tracking al changes\" and \"notify for non-compliant\".\nIt's certainly a very ambiguous question that the folks at AWS could have spared us, but for me (and for chat-gpt) B is the answer :)"
      },
      {
        "date": "2024-03-06T00:08:00.000Z",
        "voteCount": 1,
        "content": "D: AWS Config provides rules to detect non-complaint config\nB: Can track all event however doesn't provide native support for rules to detect non-complaint changes"
      },
      {
        "date": "2024-02-13T18:40:00.000Z",
        "voteCount": 1,
        "content": "In my opinion, the question asks for (1) a \u201csystem that tracks CHANGES\u201d and (2) asks to \u201csend alerts when the engineers make NONCOMPLIANT CHANGES,\u201d I would choose B since B satisfies the first condition and D does not. \n\nB: implies that CloudTrail tracks all changes.\nD: states that Config will only track noncompliant changes, but question is asking for all changes.\n\nBut overall this is just another poorly constructed and ambiguous question and answer, which seems to be the norm with these lol"
      },
      {
        "date": "2024-08-21T22:25:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-05-29T23:52:00.000Z",
        "voteCount": 1,
        "content": "Actually, AWS Config cannot track *only* non-compliant changes, it always tracks all changes against monitored resources - that's by design. You set rules in AWS Config that indicate whether the change is compliant, but all the changes must be recorded.\nhttps://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html#resource-tracking"
      },
      {
        "date": "2023-12-16T10:30:00.000Z",
        "voteCount": 2,
        "content": "B is ans\nhttps://aws.amazon.com/blogs/security/how-to-monitor-aws-account-configuration-changes-and-api-calls-to-amazon-ec2-security-groups/\nSpeed: Implementing CloudTrail and CloudWatch is faster than setting up AWS Organizations or using SCPs. You can do it in minutes without modifying the entire account structure or deploying additional resources.\nGranularity: CloudTrail and CloudWatch offer fine-grained control over monitoring and alerting, allowing you to define specific rules for noncompliant security settings.\nFlexibility: You can easily adapt the CloudWatch rules to different types of noncompliance and adjust the alerts to suit your notification needs.\nExisting infrastructure: If the company already uses CloudTrail for logging, setting up CloudWatch rules is a natural extension without requiring significant changes."
      },
      {
        "date": "2023-11-30T07:48:00.000Z",
        "voteCount": 4,
        "content": "Answer D. AWS Config is perfect to track config changes. SNS for notification."
      },
      {
        "date": "2023-11-23T14:08:00.000Z",
        "voteCount": 2,
        "content": "B is better option than D. D only sends an SNS alert when there are non-compliant changes. It does not allow you to actually track each and every changes engineers make."
      },
      {
        "date": "2023-11-24T05:53:00.000Z",
        "voteCount": 2,
        "content": "I thought so too, initially, but as others have said, B does not actually send the alert."
      },
      {
        "date": "2023-08-16T17:07:00.000Z",
        "voteCount": 2,
        "content": "It's D \nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html"
      },
      {
        "date": "2023-07-23T12:32:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-05T19:51:00.000Z",
        "voteCount": 4,
        "content": "D works and faster\nB would work with adding a CW alert, but D still better"
      },
      {
        "date": "2023-06-29T09:05:00.000Z",
        "voteCount": 2,
        "content": "correct is D"
      },
      {
        "date": "2023-06-25T11:42:00.000Z",
        "voteCount": 5,
        "content": "D\nreference link\nhttps://aws.amazon.com/es/blogs/industries/how-to-monitor-alert-and-remediate-non-compliant-hipaa-findings-on-aws/"
      },
      {
        "date": "2023-06-22T17:46:00.000Z",
        "voteCount": 4,
        "content": "It's D. Check this link, something similar: https://aws.amazon.com/blogs/industries/how-to-monitor-alert-and-remediate-non-compliant-hipaa-findings-on-aws/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/amazon/view/112851-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has IoT sensors that monitor traffic patterns throughout a large city. The company wants to read and collect data from the sensors and perform aggregations on the data.<br><br>A solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading from the stream. However, several consumers are experiencing throttling and are periodically encountering a ReadProvisionedThroughputExceeded error.<br><br>Which actions should the solutions architect take to resolve this issue? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReshard the stream to increase the number of shards in the stream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kinesis Producer Library (KPL). Adjust the polling frequency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse consumers with the enhanced fan-out feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReshard the stream to reduce the number of shards in the stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an error retry and exponential backoff mechanism in the consumer logic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the stream to use dynamic partitioning."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T11:47:00.000Z",
        "voteCount": 15,
        "content": "To resolve the issue of throttling and ReadProvisionedThroughputExceeded errors in the Amazon Kinesis Data Streams scenario, the solutions architect should take the following actions:\n\n1. A. Reshard the stream to increase the number of shards in the stream: By increasing the number of shards, you can increase the overall throughput capacity of the stream, allowing for more concurrent consumers to read from the stream without being throttled.\n\n2. C. Use consumers with the enhanced fan-out feature: Enhanced fan-out allows for multiple consumers to read from the same shard concurrently, without being limited by the read capacity of the shard. This helps distribute the load and reduces the chances of throttling.\n\n3. E. Use an error retry and exponential backoff mechanism in the consumer logic: Implementing an error retry mechanism with exponential backoff in the consumer logic will help handle throttling errors gracefully. When a ReadProvisionedThroughputExceeded error occurs, the consumer can retry the read operation after a certain delay, gradually increasing the delay between retries to avoid overwhelming the system."
      },
      {
        "date": "2023-10-21T06:56:00.000Z",
        "voteCount": 9,
        "content": "this link will explain it all. looks like this question was taken from here. \nhttps://repost.aws/knowledge-center/kinesis-readprovisionedthroughputexceeded"
      },
      {
        "date": "2023-11-30T07:50:00.000Z",
        "voteCount": 1,
        "content": "Answer ACE"
      },
      {
        "date": "2023-11-23T14:31:00.000Z",
        "voteCount": 1,
        "content": "A, C, E Options"
      },
      {
        "date": "2023-10-11T02:30:00.000Z",
        "voteCount": 4,
        "content": "Option (D) \"Reshard the stream to reduce the number of shards\" is generally not a recommended solution because it reduces the capacity of the stream, which might lead to more throttling issues. Reducing shards should only be considered if you're overprovisioned, and reducing capacity will not negatively impact your consumers.\n\nOption (B) \"Use the Kinesis Producer Library (KPL) and adjust the polling frequency\" may not be directly related to solving the throttling issue. The KPL is primarily used for producing data into the Kinesis stream, not consuming it.\n\nOption (F) \"Configure the stream to use dynamic partitioning\" can be beneficial for even distribution of data but is not directly related to resolving throttling issues. Dynamic partitioning is more about balancing the data across shards and does not increase overall read capacity.\n\nSo, the most relevant actions to address the throttling issue are (A), (C), and (E)."
      },
      {
        "date": "2023-12-20T19:30:00.000Z",
        "voteCount": 1,
        "content": "Nice way to explain the reasons other way round :-)"
      },
      {
        "date": "2023-07-23T12:23:00.000Z",
        "voteCount": 1,
        "content": "Correct ACE."
      },
      {
        "date": "2023-07-05T19:53:00.000Z",
        "voteCount": 1,
        "content": "ACE it"
      },
      {
        "date": "2023-07-03T08:58:00.000Z",
        "voteCount": 1,
        "content": "ACE is correct"
      },
      {
        "date": "2023-06-22T17:54:00.000Z",
        "voteCount": 3,
        "content": "Eliminate B, KPL is for writing. \"The Kinesis Producer Library (KPL) simplifies producer application development, allowing developers to achieve high write throughput to a Kinesis data stream. \" The error was reading.\n\nF, dynamic partitioning is used for different use cases.https://docs.aws.amazon.com/firehose/latest/dev/dynamic-partitioning.html"
      },
      {
        "date": "2023-06-21T15:01:00.000Z",
        "voteCount": 1,
        "content": "ACE is correct"
      },
      {
        "date": "2023-06-21T11:52:00.000Z",
        "voteCount": 1,
        "content": "not sure about E, but I would go with AC"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/amazon/view/112853-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have underutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances.<br><br>Which solution will meet these requirements with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization\u2019s management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in each account of the organization. Use the recommendations to downsize underutilized instances in all accounts of the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findings as files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T09:08:00.000Z",
        "voteCount": 8,
        "content": "Actually, the right answer is to use Compute Optimizer, I don't understand why it was not part of the choices here\nhttps://aws.amazon.com/compute-optimizer/"
      },
      {
        "date": "2023-10-11T02:39:00.000Z",
        "voteCount": 5,
        "content": "AWS Cost Explorer provides resource optimization recommendations, including rightsizing EC2 instances based on historical usage data. These recommendations are generated for each account in the organization's management account, so you can obtain insights for all accounts centrally.\n\nOption A introduces complexity by requiring the company to install a third-party tool on all EC2 instances, and then manually develop and maintain a custom script for identifying underutilized instances.\n\nOption C would require you to retrieve recommendations separately for each account within the organization, increasing the administrative overhead compared to a centralized management approach.\n\nOption D, while using native AWS services for data collection, involves creating and maintaining additional AWS services, which is more complex than the straightforward combination of CloudWatch and AWS Cost Explorer."
      },
      {
        "date": "2023-12-11T09:07:00.000Z",
        "voteCount": 1,
        "content": "Let's analyze each option based on effort:\n\nA. Marketplace tool:\n\nEffort: High\nRequires manual installation of a third-party tool on all instances.\nNeeds custom script development to identify underutilized instances.\nManual effort needed to reference pricing information for downsizing.\nB. Cost Explorer in Org Management Account:\n\nEffort: Low\nLeverages existing tools (CloudWatch agent &amp; Cost Explorer) already available.\nRecommendations readily available in the management account.\nDownsizing options directly available within Cost Explorer."
      },
      {
        "date": "2023-11-23T14:33:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-08-20T18:21:00.000Z",
        "voteCount": 1,
        "content": "IMO it could be done with either B or D. But the differentiator is \"Least Effort\" that makes it B"
      },
      {
        "date": "2023-07-05T19:54:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-06-29T21:23:00.000Z",
        "voteCount": 2,
        "content": "Though I vote B. No better choice. This is worst ques. How can cost explorer provide recommendations?. Its should be cost optimizer"
      },
      {
        "date": "2023-06-25T09:40:00.000Z",
        "voteCount": 1,
        "content": "Classic usage de Cloudwatch metrics and AWS Organization in master account .\nC not because more overhead each account for example 100 accounts.\nNote : Compute Optimizer is more apropiate in this case but no exist option"
      },
      {
        "date": "2023-06-23T11:51:00.000Z",
        "voteCount": 2,
        "content": "B. Install the Amazon CloudWatch agent on all the EC2 instances using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization's management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.\n\nThis solution leverages the capabilities of AWS CloudWatch and AWS Cost Explorer to monitor and analyze the CPU and memory usage of EC2 instances. By installing the CloudWatch agent, you can collect the necessary metrics for monitoring. AWS Cost Explorer provides resource optimization recommendations, which can be accessed from the organization's management account. These recommendations can then be used to identify underutilized instances and make informed decisions about downsizing.\n\nThis solution requires minimal effort as it utilizes existing AWS services and tools, eliminating the need for additional installations or custom scripts. It also provides a centralized approach by retrieving recommendations from the organization's management account, allowing for efficient management of all accounts within the organization."
      },
      {
        "date": "2023-06-22T17:57:00.000Z",
        "voteCount": 3,
        "content": "B. That's why you have the management account so you don't have to go to 1000+ accounts and get metrics."
      },
      {
        "date": "2023-06-21T15:22:00.000Z",
        "voteCount": 1,
        "content": "B - Management account is the key word"
      },
      {
        "date": "2023-06-21T11:56:00.000Z",
        "voteCount": 1,
        "content": "B. the standard way AWS recommended"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/amazon/view/112972-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2 instances.<br><br>Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement occurs.<br><br>Which combination of steps will resolve this issue? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 27,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-05T20:00:00.000Z",
        "voteCount": 9,
        "content": "CW agent-&gt;CW metric-&gt;CW alarm-&gt;Lambda action"
      },
      {
        "date": "2024-02-04T11:47:00.000Z",
        "voteCount": 5,
        "content": "This is a bad question design.\nThe question is looking for a solution for \u201cThe network routes are not updated when the instance replacement occurs.\u201d, which means the ASG already has the capability to detect the failure node. With this assumption, there is NO need to install a CloudWatch agent on the EC2 instance, cause the CloudWatch agent in B is doing the same thing. \nThe correct solution is to use the ASG Lifecycle Hook to invoke the Lambda to update the route. \nA better solution is to create a loadbalancer targeting the ASG, and update the route to point to the loadbalancer. With this solution, there is no need to update the route anymore."
      },
      {
        "date": "2024-10-13T10:09:00.000Z",
        "voteCount": 1,
        "content": "BDE.. but a professional should use ASG Lifecycle hooks https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html"
      },
      {
        "date": "2024-09-13T03:53:00.000Z",
        "voteCount": 1,
        "content": "you cannot update templates you can version them. \nADE"
      },
      {
        "date": "2024-10-13T10:07:00.000Z",
        "voteCount": 1,
        "content": "why can't you update CloudFormation templates?"
      },
      {
        "date": "2023-11-30T07:57:00.000Z",
        "voteCount": 1,
        "content": "Answer - BDE\nInstall CW agent on all instances using CF template\nConfigure CW to send out metrics to SNS\nConfigure Lambda as SNS target to terminate instance and update n/w routes on the new instances"
      },
      {
        "date": "2023-11-23T14:38:00.000Z",
        "voteCount": 2,
        "content": "B, D, E"
      },
      {
        "date": "2023-07-03T02:10:00.000Z",
        "voteCount": 2,
        "content": "A and F must be wrong."
      },
      {
        "date": "2023-06-23T23:26:00.000Z",
        "voteCount": 3,
        "content": "B, D and E"
      },
      {
        "date": "2023-06-23T12:02:00.000Z",
        "voteCount": 2,
        "content": "b-d-e seems reasonable."
      },
      {
        "date": "2023-06-22T18:07:00.000Z",
        "voteCount": 4,
        "content": "A is redundant because \"Whenever the analysis software stops working, the Auto Scaling group replaces an instance.\"\nC is not correct. AWS System Manager Agebt is not used \"to send process metrics for the application.\"\n\nSo, B, D and E because they make a flow."
      },
      {
        "date": "2023-06-22T11:47:00.000Z",
        "voteCount": 1,
        "content": "b----d----e"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/amazon/view/112864-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch and will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.<br><br>A solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute traffic to each ECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T18:13:00.000Z",
        "voteCount": 15,
        "content": "A and B are out, it says the app uses HTTPS. \nC is out because we have Fargate and there is no Cluster Auto Scaling there. \nSo, it's D because we have Service Auto Scaling. -&gt; https://repost.aws/knowledge-center/ecs-fargate-service-auto-scaling"
      },
      {
        "date": "2023-06-27T07:35:00.000Z",
        "voteCount": 1,
        "content": "NLB supports HTTPS so why excluding A?"
      },
      {
        "date": "2023-07-03T12:24:00.000Z",
        "voteCount": 9,
        "content": "Unlike a Classic Load Balancer or an Application Load Balancer, a Network Load Balancer can't have application layer (layer 7) HTTP or HTTPS listeners. It only supports transport layer (layer 4) TCP listeners. HTTP and HTTPS traffic can be routed to your environment over TCP.\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-nlb.html#"
      },
      {
        "date": "2023-11-23T14:49:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-07-23T12:20:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-21T15:50:00.000Z",
        "voteCount": 4,
        "content": "Answer is D. For those voting C, remember that it's on Fargate, so there is no such cluster autoscaling."
      },
      {
        "date": "2023-07-08T03:52:00.000Z",
        "voteCount": 2,
        "content": "select D.  for Fargate there is no Cluster Auto Scaling there."
      },
      {
        "date": "2023-07-05T20:04:00.000Z",
        "voteCount": 2,
        "content": "D\nno NLB for ECS, no Cluster for Fargate"
      },
      {
        "date": "2023-08-16T08:05:00.000Z",
        "voteCount": 1,
        "content": "D is correct but you can use NLB for ECS. Key word is Service Auto Scaling\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/create-network-load-balancer.html"
      },
      {
        "date": "2023-06-26T10:43:00.000Z",
        "voteCount": 2,
        "content": "@MODERATOR, PLEASE remove my previous comment as I mentioned C. \nAs per comment from SmileyCloud ,  C is not correct because there is no Cluster Auto Scaling. D is the answer.\nThank you @SmileyCloud for clarifying\n\nD is the answer"
      },
      {
        "date": "2023-06-25T09:44:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/ecs-fargate-service-auto-scaling"
      },
      {
        "date": "2023-06-23T12:06:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-d-d-d"
      },
      {
        "date": "2023-06-22T11:52:00.000Z",
        "voteCount": 2,
        "content": "\"Amazon ECS cluster auto scaling is only supported with Auto Scaling group capacity providers. For Amazon ECS workloads that are hosted on AWS Fargate, see AWS Fargate capacity providers.\""
      },
      {
        "date": "2023-06-21T15:33:00.000Z",
        "voteCount": 3,
        "content": "AB are eliminated because of NLB\nC has Auto Scaling Group with Cluster Autoscaler: As per ChatGPT - By implementing an Auto Scaling group for each ECS service using the Cluster Autoscaler, you can automatically adjust the number of tasks (containers) based on the demand. The Cluster Autoscaler scales the ECS tasks in response to CloudWatch alarms, allowing you to scale the infrastructure up or down to handle the increasing number of users."
      },
      {
        "date": "2023-06-29T22:32:00.000Z",
        "voteCount": 2,
        "content": "changing my vote to D as SmileyCloud pointed. for Fargate there is no Cluster Auto Scaling there."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/amazon/view/112772-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.<br><br>The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image version receives a unique tag.<br><br>The company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-31T07:34:00.000Z",
        "voteCount": 9,
        "content": "You want to look for \"scan on push\" solution, as scanning periodically is not enough, damage might have been done -&gt; C, D is out, only A, B\nA sounds complex, but B even worse, how can you put result in SQS? wording is so bad if they means sending message to SQS. Notifying by SES is a straight red flag that AWS exams like to use. \nOnly A makes sense."
      },
      {
        "date": "2024-03-24T04:20:00.000Z",
        "voteCount": 3,
        "content": "Problem with this approach is, if you scan only what's pushed, and it has a zero-day vulnerability, you won't see it. Since you are scanning only when you are pushing, you won't detect the vulnerability ever. IMO, scanning periodically gives a better shot. Ideally it should be scanning both on push and periodically."
      },
      {
        "date": "2024-03-24T04:25:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html\n\nIn a nutshell, 2 types of scans.\n\nBasic: Scanned against CVE DB, \"ON PUSH\" or a manual scan. Don't see any way of notifying anywhere.\nEnhanced: Ongoing scanning with Amazon Inspector, findings delivered via EventBridge notifications.\n\nClosest answer would be A."
      },
      {
        "date": "2023-11-30T08:16:00.000Z",
        "voteCount": 1,
        "content": "Answer A."
      },
      {
        "date": "2023-11-23T14:54:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-07-05T20:14:00.000Z",
        "voteCount": 3,
        "content": "A, but I think step function need to call Lambda to delete tag. there is not direct ecr integration"
      },
      {
        "date": "2023-06-25T11:07:00.000Z",
        "voteCount": 2,
        "content": "Use the building feature if you can, so scan on push.\nI go with A because other options are not good B - you cannot use SES."
      },
      {
        "date": "2023-06-24T10:58:00.000Z",
        "voteCount": 1,
        "content": "I vote A since I tested it and confirm it's achievable. As for B - I couldn't find any option to publish the result of the scan to SQS so I stopped there"
      },
      {
        "date": "2023-06-22T19:22:00.000Z",
        "voteCount": 2,
        "content": "A meet the requirements.\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr-eventbridge.html"
      },
      {
        "date": "2023-06-22T18:23:00.000Z",
        "voteCount": 2,
        "content": "C and D are out because they are not automatic but rather scheduled. \nB is out because you don't need SQS for this and def don't need SES. \nA makes sense because it's much leaner solution."
      },
      {
        "date": "2023-06-22T10:10:00.000Z",
        "voteCount": 1,
        "content": "Use the building feature if you can, so scan on push. And A make more sense"
      },
      {
        "date": "2023-06-21T15:41:00.000Z",
        "voteCount": 2,
        "content": "I go with A because other options are not good\nB - you cannot use SES. SES is generally used to send Bulk/marketing emails.\nC- schedule Lambda to scan every hour is not a good approach\nD - like B you cannot use SES for this use case.\nSo A sounds reasonable"
      },
      {
        "date": "2023-06-21T00:51:00.000Z",
        "voteCount": 1,
        "content": "why not A ?"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/amazon/view/112866-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2. AWS Fargate. and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in other months.<br><br>The company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts across the organization to calculate usage.<br><br>Which solution will provide the MOST cost savings for all the organization's compute usage?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T18:44:00.000Z",
        "voteCount": 13,
        "content": "A. Incorrect: RI's Supports only EC2 instances. \nB. Correct: Compute savings plan supports EC2, Fargate and Lambda. Applied in Organization's management account.\nC. Incorrect: RI's Supports only EC2 instances and Changes to be applied at Organizations management account.\nD. Incorrect: Instance Saving plan supports only EC2."
      },
      {
        "date": "2024-04-04T09:15:00.000Z",
        "voteCount": 1,
        "content": "B - \"Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage.\n\nhttps://aws.amazon.com/savingsplans/compute-pricing/"
      },
      {
        "date": "2023-11-30T08:17:00.000Z",
        "voteCount": 3,
        "content": "Answer B. Compute Savings plan covers EC2, Fargate &amp; Lambda. Instance Savings plan only for EC2 instances."
      },
      {
        "date": "2023-11-23T14:56:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-05T20:15:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-06-25T11:13:00.000Z",
        "voteCount": 2,
        "content": "A. Incorrect: RI's Supports only EC2 instances.\nB. Correct: Compute savings plan supports EC2, Fargate and Lambda. Applied in Organization's management account.\nC. Incorrect: RI's Supports only EC2 instances and Changes to be applied at Organizations management account.\nD. Incorrect: Instance Saving plan supports only EC2."
      },
      {
        "date": "2023-06-22T18:26:00.000Z",
        "voteCount": 1,
        "content": "B, magic keywords - Management account and Compute savings Plan."
      },
      {
        "date": "2023-06-22T10:13:00.000Z",
        "voteCount": 1,
        "content": "Compute Savings plan is made for this usage type"
      },
      {
        "date": "2023-06-21T15:46:00.000Z",
        "voteCount": 1,
        "content": "B- compute savings plans covers all ec2, fargate, lambda."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/amazon/view/112867-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company has turned on all features.<br><br>A finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization\u2019s management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization\u2019s costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T19:10:00.000Z",
        "voteCount": 9,
        "content": "A. Makes sense.\nB. Trusted advisor not required.\nC. Control Tower not required.\nD. Budgets can be managed in Org's Mgmt account itself."
      },
      {
        "date": "2024-08-09T04:26:00.000Z",
        "voteCount": 1,
        "content": "A: AWS Budgets + SNS = Easy budget (daily) tracking and alerts\n\nB: Trusted Advisor is for recommendations, not daily budgets.\n\nC: Control Tower is for governance, not budget alerts\n\nD: Complex setup with no added value over AWS Budgets"
      },
      {
        "date": "2023-11-23T14:57:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-07-08T03:59:00.000Z",
        "voteCount": 3,
        "content": "ofc it's Ahttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-professional-sap-c02/view/#"
      },
      {
        "date": "2023-07-05T20:16:00.000Z",
        "voteCount": 2,
        "content": "straight A"
      },
      {
        "date": "2023-07-02T17:18:00.000Z",
        "voteCount": 2,
        "content": "A. Makes sense.\nB. Trusted advisor not required.\nC. Control Tower not required.\nD. Budgets can be managed in Org's Mgmt account itself."
      },
      {
        "date": "2023-07-29T05:54:00.000Z",
        "voteCount": 6,
        "content": "you copy and paste other people answers"
      },
      {
        "date": "2023-06-23T12:13:00.000Z",
        "voteCount": 1,
        "content": "a-a-a-a-a-a-a"
      },
      {
        "date": "2023-06-23T05:31:00.000Z",
        "voteCount": 1,
        "content": "This one is simple. A"
      },
      {
        "date": "2023-06-22T10:18:00.000Z",
        "voteCount": 1,
        "content": "A, simple one"
      },
      {
        "date": "2023-06-22T08:12:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2023-06-21T15:47:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/amazon/view/112868-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a centralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.<br><br>How can a solutions architect improve the performance of the image upload process?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedeploy the application to use S3 multipart uploads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution and point to the application as a custom origin.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the buckets to use S3 Transfer Acceleration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group for the EC2 instances and create a scaling policy."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-18T09:01:00.000Z",
        "voteCount": 25,
        "content": "Main point of the question: \"The users in Europe are reporting slow performance for their image uploads.\"\nHow do we improve performance? If we look on the latency side, sure, S3 Transfer Acceleration (option C), but the question puts another variable to our scenario: \"Artists upload photos of their work as large-size. high-resolution image files from their mobile phones...\"\nIf you just look at that above, you would switch to A as we can improve upload with multipart.\n\nHere comes the plot twist \"The users in Europe are reporting slow performance for their image uploads.\" - Meaning, in \"Europe\", not in the \"NA\". Of course! The bucket in the US... So yeah, question really bad, not objective (in my pov) and with lots of interpretations, but C would help them with the perception of performance in this context."
      },
      {
        "date": "2023-12-27T14:41:00.000Z",
        "voteCount": 2,
        "content": "Kudos to you for such a great explanation!"
      },
      {
        "date": "2024-06-10T10:12:00.000Z",
        "voteCount": 1,
        "content": "Between A and C, I would choose C - Transfer Acceleration,  as this issue is focusing on improving the upload performance across the region"
      },
      {
        "date": "2024-06-09T21:15:00.000Z",
        "voteCount": 1,
        "content": "Why not B ?"
      },
      {
        "date": "2023-11-23T15:07:00.000Z",
        "voteCount": 2,
        "content": "Option C. As the users in Europe only are facing this issue. A would improve upload performance overall for both US and Europe."
      },
      {
        "date": "2023-11-23T02:42:00.000Z",
        "voteCount": 2,
        "content": "I believe this question should rightfully be a multi-choice question where A and C are the answer together to solve this problem statement\n\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/"
      },
      {
        "date": "2023-08-22T07:47:00.000Z",
        "voteCount": 1,
        "content": "I would choose A. Why does C say \"Configure the buckets [more than one] to use S3 Transfer Acceleration?\nSometimes you have to hate how these questions and answers are worded."
      },
      {
        "date": "2023-08-22T07:52:00.000Z",
        "voteCount": 1,
        "content": "C would be the answer if the 's' was removed. Will to go with C."
      },
      {
        "date": "2023-07-30T15:41:00.000Z",
        "voteCount": 1,
        "content": "I have some doubts about this question, it makes more sense to use multipart upload to split the file and gain upload speed. AWS Transfer Accelerator seems to be applied to reduce delay.http\ns://aws.amazon.com/pt/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/"
      },
      {
        "date": "2023-07-23T12:13:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-06T09:22:00.000Z",
        "voteCount": 1,
        "content": "C\nwould be good in combination with A, but better as a standalone choice"
      },
      {
        "date": "2023-07-05T05:25:00.000Z",
        "voteCount": 1,
        "content": "upload performance-&gt; transfer acceleration"
      },
      {
        "date": "2023-06-29T09:31:00.000Z",
        "voteCount": 1,
        "content": "correct is C"
      },
      {
        "date": "2023-06-27T11:29:00.000Z",
        "voteCount": 1,
        "content": "Transfer Acceleration doesn't guarantee a significant increase in upload speed.\nA multi-part upload on other hand does, because it uploads multiple smaller chunks of the files in parallel.\n\nIdeally multi-part upload and Transfer Accelerator should be deployed together. If we had to pick only one of the two, multi-part upload would result in better performance.\n\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/"
      },
      {
        "date": "2024-01-23T17:48:00.000Z",
        "voteCount": 1,
        "content": "Reading carefully into the blog looks like the author did some maths wrong. \nMultipart upload took 43s which is 40% faster than base of 72s\nTransfer acceleration took 45s which is 38% faster than base of 72s. \nSo based on this multipart gives better performance"
      },
      {
        "date": "2023-07-05T22:30:00.000Z",
        "voteCount": 3,
        "content": "Using your link, the tests mentioned show C is faster\nSingle upload with transfer acceleration 40% faster\nMultipart upload without transfer acceleration 38% faster"
      },
      {
        "date": "2023-06-25T11:19:00.000Z",
        "voteCount": 1,
        "content": "C. https://aws.amazon.com/s3/transfer-acceleration/"
      },
      {
        "date": "2023-06-23T05:43:00.000Z",
        "voteCount": 2,
        "content": "C. https://aws.amazon.com/s3/transfer-acceleration/"
      },
      {
        "date": "2023-06-22T08:14:00.000Z",
        "voteCount": 1,
        "content": "C of course"
      },
      {
        "date": "2023-06-21T15:52:00.000Z",
        "voteCount": 1,
        "content": "C - Transfer acceleration.  S3 Transfer Acceleration utilizes the Amazon CloudFront global network of edge locations to accelerate the transfer of data to and from S3 buckets. By enabling S3 Transfer Acceleration on the centralized S3 bucket, the users in Europe will experience faster uploads as their data will be routed through the closest CloudFront edge location."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/amazon/view/112869-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic.<br><br>Which solution will meet these requirements with the LEAST ongoing operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-27T11:35:00.000Z",
        "voteCount": 22,
        "content": "A looked good until \"store session data in SQS\"."
      },
      {
        "date": "2023-06-25T11:23:00.000Z",
        "voteCount": 15,
        "content": "what a worst ques\nA - Why do you need SQS to store web sever session data. SQS is for decoupling services\nB - EBS multi attach is for SAME availibility zone. The ques says multipel availibility zones\nC - Why do you need EFS to store web sever session data. Its damn expensive\nD - Better answer- But again why need for EKS.\nIf I were to choose one option, its D as its better compared to ABC"
      },
      {
        "date": "2024-10-07T14:31:00.000Z",
        "voteCount": 1,
        "content": "By exclusion of the other options, the least worst answer is D."
      },
      {
        "date": "2024-09-06T22:05:00.000Z",
        "voteCount": 1,
        "content": "B,D can do it all.\nMultiple EC2 accesses a single storage using EFS and EBS, with priority given to EFS file storage.\nDynamoDB can also be used for session storage.\nhttps://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html"
      },
      {
        "date": "2024-04-30T03:11:00.000Z",
        "voteCount": 3,
        "content": "one of the poor Question... so answer we give poor... its D. because i cant choose ABC"
      },
      {
        "date": "2023-12-06T01:13:00.000Z",
        "voteCount": 3,
        "content": "I think that the issue of multi-attach EBS in one AZ is dealt with by the manner in which it is explained.  It is the EC2 that are distributed in Multi-AZ not the EBS.   Just my pov."
      },
      {
        "date": "2023-11-23T15:12:00.000Z",
        "voteCount": 2,
        "content": "Option D, Though C is also possible but Multi-attach EBS has higher operational overhead."
      },
      {
        "date": "2023-09-30T06:03:00.000Z",
        "voteCount": 1,
        "content": "Due to operational efficiency D is better choice compared to B."
      },
      {
        "date": "2023-09-29T02:15:00.000Z",
        "voteCount": 1,
        "content": "deployments carry ReplicaSets \nDynamoDB table  for session data"
      },
      {
        "date": "2023-09-13T17:13:00.000Z",
        "voteCount": 1,
        "content": "There is a requirement for fault tolerance. I feel 'C\" satisfies that as it has replicasets. Option D does not talk about it"
      },
      {
        "date": "2023-08-24T08:31:00.000Z",
        "voteCount": 2,
        "content": "Now i'll have to go with B. Check out what alabiba says to question, \"Can aws sqs be used to store web server session data?\"\nalabiba \"No, AWS SQS (Simple Queue Service) is not typically used for storing web server session data. SQS is a message queuing service that is designed for reliable and scalable message communication between distributed systems. For storing session data, it is more common to use dedicated session storage solutions such as databases (e.g., Amazon DynamoDB) or in-memory caches (e.g., Redis).\""
      },
      {
        "date": "2023-09-12T22:09:00.000Z",
        "voteCount": 2,
        "content": "problem with option B is \" Multi-Attach on EC2 instances that are distributed across multiple Availability Zones\"; please note that multi-attach can only span since AZ\noption D is correct"
      },
      {
        "date": "2023-07-06T09:27:00.000Z",
        "voteCount": 7,
        "content": "D - best of the worst"
      },
      {
        "date": "2023-07-05T22:42:00.000Z",
        "voteCount": 2,
        "content": "A looked good until \"store session data in SQS\"."
      },
      {
        "date": "2023-07-01T01:30:00.000Z",
        "voteCount": 3,
        "content": "A looked good until \"store session data in SQS\"."
      },
      {
        "date": "2023-06-29T09:35:00.000Z",
        "voteCount": 2,
        "content": "A looked good until \"store session data in SQS\"."
      },
      {
        "date": "2023-06-24T11:47:00.000Z",
        "voteCount": 1,
        "content": "Fargate is the service, the only question remains the storage. Amazon EBS Multi-Attach is single-az service, so remains A. Even though I am not very confident with SQS caching web service sessions."
      },
      {
        "date": "2023-06-23T23:42:00.000Z",
        "voteCount": 3,
        "content": "Agree, this is a worst question\nD is best choice for this question, but I would prefer to change EKS to ECS Fargate for compute and Elasticache for Redis for session."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/amazon/view/112974-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T07:39:00.000Z",
        "voteCount": 15,
        "content": "C. The proper way is to use AWS DMS, but the answer here uses S3 (???) which will take forever. So the answer is C."
      },
      {
        "date": "2023-11-02T10:38:00.000Z",
        "voteCount": 3,
        "content": "the following link maybe helpful for some;\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Limitations"
      },
      {
        "date": "2023-08-27T03:26:00.000Z",
        "voteCount": 11,
        "content": "C\nhttps://aws.amazon.com/blogs/database/part-3-migrating-to-amazon-rds-for-sql-server-using-transactional-replication-with-native-backup-and-restore/"
      },
      {
        "date": "2024-08-14T14:51:00.000Z",
        "voteCount": 1,
        "content": "B look correct to me"
      },
      {
        "date": "2024-08-23T18:23:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-08-01T02:13:00.000Z",
        "voteCount": 1,
        "content": "B is the AWS way of doing a DB migration. C might work and could be better in some cases, but because the DBs are legacy you might run into compatibility issues or limitations with the tooling. If the databases are very large you really want to use B because you need to ship the bulk of the data with Snowball."
      },
      {
        "date": "2024-07-24T01:19:00.000Z",
        "voteCount": 1,
        "content": "C. Correct, Near-Zero Downtime.\nA. In-Place Upgrade and Migration to Aurora: This involves multiple steps and the potential for increased downtime during the cutover process. \nSchema Conversion: Depending on the complexity of the legacy system, converting schemas and ensuring compatibility with Amazon Aurora can be challenging and time-consuming.\nB. Intermediate Storage in Amazon S3: Adds complexity\nTwo-Step Process: First replicating to Amazon S3 and then loading into Amazon RDS adds additional steps and potential points of failure."
      },
      {
        "date": "2024-07-06T07:13:00.000Z",
        "voteCount": 1,
        "content": "In option C - \"Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance\",  should be target, not source"
      },
      {
        "date": "2024-06-10T18:44:00.000Z",
        "voteCount": 1,
        "content": "Use AWS Database Migration Service (AWS DMS) to \"rehost\" the database???? \nHow you can \"rehost\" database with DMS"
      },
      {
        "date": "2024-05-25T07:04:00.000Z",
        "voteCount": 2,
        "content": "DMS should be the better service for this use case"
      },
      {
        "date": "2024-05-09T01:27:00.000Z",
        "voteCount": 1,
        "content": "Answer: C."
      },
      {
        "date": "2024-04-30T19:26:00.000Z",
        "voteCount": 2,
        "content": "AWS Application Migration Service (MGN) is a highly automated lift-and-shift (rehost) solution that simplifies the migration of applications to AWS. It supports near-zero downtime migrations by continuously replicating the source servers to AWS.\n\nRepointing the applications to Amazon Aurora Serverless satisfies the migration to the modern data architecture."
      },
      {
        "date": "2024-03-31T09:32:00.000Z",
        "voteCount": 1,
        "content": "Why not A as the question the it should rearchitected from legacy"
      },
      {
        "date": "2024-03-15T02:26:00.000Z",
        "voteCount": 4,
        "content": "Native database high availability (HA) tools include the Always On or distributed availability group clusters in Microsoft SQL Server and Oracle\u2019s Data Guard replications. This approach requires a major effort to set up across extended, cross-site HA clusters, and might cause some performance degradation because of the longer latency to achieve fully synchronous active/active deployments. However, this method provides the closest to near-zero downtime during the cutover."
      },
      {
        "date": "2024-01-22T06:25:00.000Z",
        "voteCount": 1,
        "content": "What is \"native database high availability tools\"????"
      },
      {
        "date": "2024-01-17T02:23:00.000Z",
        "voteCount": 1,
        "content": "B. Use AWS Database Migration Service (AWS DMS) to rehost the database.\n\nThis action is not 'rehost'"
      },
      {
        "date": "2024-01-01T17:00:00.000Z",
        "voteCount": 1,
        "content": "C:\nUse distributed AG, it will work.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-sql-server-to-aws-using-distributed-availability-groups.html"
      },
      {
        "date": "2023-12-13T02:49:00.000Z",
        "voteCount": 4,
        "content": "A. Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.\n\nHere's why this approach offers the best advantages:\n\nMinimal downtime: In-place upgrade and cutover minimize downtime compared to traditional database migrations.\nModernization: AWS Schema Conversion Tool helps modernize legacy schema structures during migration.\nServerless architecture: Amazon Aurora Serverless simplifies management and scales effortlessly.\nApplication compatibility: Repointing applications directly to Aurora minimizes disruption."
      },
      {
        "date": "2023-12-06T01:16:00.000Z",
        "voteCount": 3,
        "content": "Agree with C; B was goo until it talked about S3 :("
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/amazon/view/113131-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization.<br><br>Data transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment.<br><br>Which guidelines meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the service provider applications and the service consumer applications in AWS accounts in the same organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span><span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T17:29:00.000Z",
        "voteCount": 13,
        "content": "A By sharing the subnets that host the service provider applications using AWS Resource Access Manager (RAM), the service consumer applications can be deployed in the same organization's accounts. This allows the traffic between the service consumer and service provider applications to stay within the organization's network, reducing data transfer charges.\nD By using the Availability Zone-specific endpoint service's local DNS name, the service consumer compute resources can directly access the service provider applications within the same Availability Zone. This eliminates the need for cross-Availability Zone data transfer, thus reducing data transfer charges."
      },
      {
        "date": "2024-08-23T18:26:00.000Z",
        "voteCount": 1,
        "content": "just CD"
      },
      {
        "date": "2023-09-07T08:07:00.000Z",
        "voteCount": 6,
        "content": "- **C. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.**\n- **D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.**"
      },
      {
        "date": "2024-10-07T14:47:00.000Z",
        "voteCount": 1,
        "content": "B is not an option: While placing resources in the same organization might simplify management, it does not inherently reduce data transfer charges. Data transfer costs between AWS Organizations accounts are typically not impacted by being in the SAME OR DIFFERENT organizations, especially when using PrivateLink."
      },
      {
        "date": "2024-07-10T10:52:00.000Z",
        "voteCount": 1,
        "content": "C D is correct one\nFor C, Cross-zone load balancing can distribute traffic across multiple AZs, which increases data transfer costs between AZs. Disabling cross-zone load balancing ensures that traffic remains within the same AZ, reducing the associated data transfer charges. This is particularly important for applications using AWS PrivateLink, as it will help keep data transfers within the same AZ as much as possible."
      },
      {
        "date": "2024-05-25T07:12:00.000Z",
        "voteCount": 3,
        "content": "B is useless because if you place the resource in the same org but in different AZs you will pay the same as different org in different AZs. So B is uncorrect (like A and E).\nRemains C and D as a solution that should reduce costs."
      },
      {
        "date": "2024-05-03T01:43:00.000Z",
        "voteCount": 2,
        "content": "BD for me"
      },
      {
        "date": "2024-04-10T21:42:00.000Z",
        "voteCount": 1,
        "content": "B - allows data transfer between linked accounts to be free of charge.\nD - ensures traffic stays within the same AZ as much as possible, minimizing inter-AZ data transfer costs.\n\nCD - Save money."
      },
      {
        "date": "2024-03-31T09:00:00.000Z",
        "voteCount": 4,
        "content": "\"The company manages two organisations in AWS Organizations,\" which means they have one organisation for service providers and one more for consumers. \n\nA. Since applications are created in the provider organisation, sharing the subnet with other accounts within the same organisation has no effect. \nB. Combining provider and consumer into one organisation is the first move for Option D.\nC. Cross-zone load balancing does not change the amount of data traffic passing through the NLB, it affects how that traffic is distributed across the targets. \nD. AZ-specific endpoint helps to reduce data transfer charges because it keeps the traffic in a single AZ and is designed for intra-regional communication within the same account or organization. \nE. WTF"
      },
      {
        "date": "2024-03-21T11:01:00.000Z",
        "voteCount": 5,
        "content": "It's B and D.\n\nA. Sharing subnets does not directly reduce data transfer charges.\nC. Turning off cross-zone load balancing does not impact data transfer costs between VPC endpoints and service consumers.\nE. A Savings Plan reduces costs for compute usage, not specifically for data transfer charges."
      },
      {
        "date": "2024-03-27T15:53:00.000Z",
        "voteCount": 3,
        "content": "Turning off cross-zone load balancing can reduce inter-AZ data transfer costs. With cross-zone load balancing disabled, a Network Load Balancer (NLB) only routes requests to targets in the same Availability Zone as the load balancer node that received the request. This setup reduces the data transferred across Availability Zones, thereby reducing costs."
      },
      {
        "date": "2024-03-12T09:53:00.000Z",
        "voteCount": 3,
        "content": "Answer: C, D"
      },
      {
        "date": "2024-02-13T11:45:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/ram/latest/userguide/shareable.html \"Can share with only AWS accounts in its own organization.\" ec2:Subnet"
      },
      {
        "date": "2024-02-10T02:36:00.000Z",
        "voteCount": 5,
        "content": "Answer is CD\nD) Obvious option, This approach minimizes data transfer costs by ensuring that traffic between service consumers and service providers stays within the same Availability Zone\nC) Only after setting up your NLB, you can create a VPC Endpoint Service (VPC-E) that is powered by AWS PrivateLink. Cross-zone lb feature is optional for NLB since 2018 so,  turning off cross-zone load balancing can help ensure that data does not unnecessarily cross Availability Zones, thereby once again reducing data transfer costs\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\n\n\n\nB) Incorrect: putting the workloads into 1 org - would not make any effect on billing neither, unless you change the topology profoundly and move away the VPCE solution - but we are not talking about Re-architecting, we are looking to provide guidelines\nA) Incorrect: RAM can be used only within 1 organization\nE) Incorrect: there is no a such flavor of Saving plans, AWS provides 3 Compute, EC Instance and SageMaker Saving plans"
      },
      {
        "date": "2024-03-15T02:33:00.000Z",
        "voteCount": 1,
        "content": "You can also share with specific AWS accounts by account ID, regardless of whether the account is part of an organization."
      },
      {
        "date": "2024-01-28T06:55:00.000Z",
        "voteCount": 5,
        "content": "Holy bageezus, never seen a discussion thread so divided.\n\n@NikkyDicky is spot on - cross zone traffic is indeed where the money is going. I think we all know that.\n\nA - appears incorrect, we cannot share subnets between accounts in different AWS Orgs. Even if you could, or even if you chose A+B, it would be impractical to assume all other workloads could be deployed in service provider subnets. Would probably run out of IPs. And even if the subnets were huge and we didn't run out of IPs, there is no mechanism in A to guide developers deploying their workloads to reduce or prevent cross-AZ traffic. You could share the subnets and deploy all provider/consumer workloads in the same set of subnets and still end up with the same huge bill :-)"
      },
      {
        "date": "2024-01-28T06:56:00.000Z",
        "voteCount": 1,
        "content": "B - appears correct. @Just_Ninja's explanation nails it. If you use Organizations and you create accounts, then in each member account, the logical identifiers for each availability zone (e.g. \"eu-central-1a\") are guaranteed to map to the same AZ Physical ID (e.g. \"euc1-az3\") for all accounts within the Organization. In other words, it's likely that AZ \"eu-central-1a\" for accounts in OrgABC is not the same as AZ \"eu-central-1a\" for accounts in OrgXYZ. That's a problem if you're trying to eliminate unnecessary cross-zone traffic. Without this, you could instruct developers to use AZ-specific DNS names and still end up with the same huge bill :-)"
      },
      {
        "date": "2024-01-28T06:56:00.000Z",
        "voteCount": 2,
        "content": "C - appears incorrect, but the reason has nothing to do with \"compromising high availability\". As pointed out by @elmoh, cross-zone load balancing isn't enabled by default in NLBs anyway. See  https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html#cross-zone-load-balancing. Even if cross-zone load balancing was enabled by default in NLBs, this option doesn't cover the Gateway Load Balancer VPC endpoint service use case."
      },
      {
        "date": "2024-01-14T01:51:00.000Z",
        "voteCount": 3,
        "content": "I go with C &amp; D.\nData transfer cost base on physical distance.(cross AZ, cross region, internal)\nA &amp; B - shared VPC doesn't distribute traffic to inter-az"
      },
      {
        "date": "2023-12-27T14:49:00.000Z",
        "voteCount": 3,
        "content": "This question is poorly framed. I go with A &amp; D, not because they are great, but because the others are terrible. You should not have to move into the same org (that can't be the answer). Also, we won't compromise HA, so that can't be the answer either."
      },
      {
        "date": "2023-12-21T23:28:00.000Z",
        "voteCount": 2,
        "content": "The question is badly framed. \nFirst, we need define the \"Data transfer\". Does it mean cross AZ data transfer or cross account data transfer? \nI assume there isn't private network connectivity between the two parties, because they are not even in the same organization, and there is not statement saying they are connected to each other with peering or transit gateway or VPN. So I assume the \"Data transfer\" is cross organization data transfer, which highly possible is internet data transfer cost. So, A and B will be the best answer.\nIf the question designer meant the cross AZ data transfer and forgot to mention there is already private network connectivity created between the two VPC, C and D might be the best answer. But we can't assume something without any evidence, right?"
      },
      {
        "date": "2024-01-14T01:54:00.000Z",
        "voteCount": 1,
        "content": "AWS PrivateLink is private network and support cross account VPC"
      },
      {
        "date": "2023-12-14T08:05:00.000Z",
        "voteCount": 1,
        "content": "Read B + A\nReduce the multi-organisation setup into a single one and then use Resource Sharing.  Simple"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/amazon/view/112879-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T17:33:00.000Z",
        "voteCount": 28,
        "content": "File Gateway == SMB , NFS\n Volumes Gateway == iSCSI \n Tape Gateway = VTL"
      },
      {
        "date": "2023-12-10T08:28:00.000Z",
        "voteCount": 1,
        "content": "Ans D \nhe most cost-effective solution for moving the backups to S3 is D. Deploy an AWS Storage Gateway volume gateway, create an SMB file share, and automate data copies to S3.\n\nHere's why:\n\nCost-effectiveness: Volume gateways use Amazon EBS volumes for local storage, which is typically more cost-effective than Amazon FSx for Windows File Server for storing large amounts of data. Additionally, this approach avoids the need for additional backups within Amazon FSx, further reducing costs.\n\nDirect Connect utilization: Leveraging the existing Direct Connect connection optimizes network bandwidth for transferring data to S3, minimizing latency and potential data transfer charges.\n\nAutomated backups: Automating copies of the nightly exports to S3 ensures reliable backups and minimizes manual intervention."
      },
      {
        "date": "2023-11-23T15:40:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-02T10:57:00.000Z",
        "voteCount": 1,
        "content": "if you read the end of the following link's paragraph, its right there in documentation;\nhttps://aws.amazon.com/storagegateway/features/#Gateway_Types\nunder \" Amazon S3 File Gateway \"\nCustomers can use Amazon S3 File Gateway to back up on-premises file data as objects in Amazon S3 (including Microsoft SQL Server and Oracle databases and logs), and for hybrid cloud workflows using data generated by on-premises applications for processing by AWS services such as machine learning or big data analytics."
      },
      {
        "date": "2023-11-03T06:22:00.000Z",
        "voteCount": 2,
        "content": "But the answer A never mentions S3 file gateway?"
      },
      {
        "date": "2023-08-20T20:18:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/storagegateway/features/"
      },
      {
        "date": "2023-08-04T12:52:00.000Z",
        "voteCount": 1,
        "content": "file gateway = most cheap"
      },
      {
        "date": "2023-07-06T15:33:00.000Z",
        "voteCount": 2,
        "content": "A - SMB mount = file gwy"
      },
      {
        "date": "2023-06-26T17:47:00.000Z",
        "voteCount": 2,
        "content": "file gateway -&gt; used to store file inside s3\n volume gateway  -&gt; used to store file in on-premises using iSCSI connectivity"
      },
      {
        "date": "2023-06-25T09:05:00.000Z",
        "voteCount": 2,
        "content": "Using Olabiba.ai to learn not to find an answer:\n\nJack: Labiba, what is the Microsoft SQL Server database export is it block or file?\n\noLabiba: The Microsoft SQL Server database export is typically a block-level backup. It captures the data at the database level, including the schema, tables, and records, and stores it in a binary format. This allows for efficient backup and restoration of the database.\n\nIn summary, if you primarily need file-level access to your backups, File Gateway is a better choice. If you require block-level storage and want to optimize for low-latency access, Volume Gateway is a better fit.\n\nLet me know if you know the answer now."
      },
      {
        "date": "2023-06-24T12:10:00.000Z",
        "voteCount": 3,
        "content": "File Gateway could be mapped as SMB file share and used by the database or other automation to transfer database backups. Volume Gateway is more used to perform volume snapshots on the on-premise system so I don't believe it's a sustainable approach here."
      },
      {
        "date": "2023-06-24T06:16:00.000Z",
        "voteCount": 2,
        "content": "It's A (file gateway). Volume gateway is iSCSI."
      },
      {
        "date": "2023-06-23T15:17:00.000Z",
        "voteCount": 2,
        "content": "olabiba.ai says D\nOption D: Using an AWS Storage Gateway volume gateway allows you to write the nightly database exports to an SMB file share on the volume gateway, which can be stored locally and automatically backed up to an S3 bucket. This solution is cost-effective as it utilizes the existing Direct Connect connection and requires minimal additional infrastructure."
      },
      {
        "date": "2023-06-23T12:57:00.000Z",
        "voteCount": 2,
        "content": "d-d-d-d-d-d\n\nBy deploying an AWS Storage Gateway volume gateway within the VPC connected to the Direct Connect connection, the company can leverage the high-speed, low-latency connection to transfer the nightly database exports to the SMB file share on the volume gateway. This allows for efficient and reliable data transfer.\n\nAutomating copies of this data from the SMB file share to an S3 bucket provides a cost-effective solution for storing the backups in more robust cloud storage on Amazon S3. The company can take advantage of the durability, scalability, and cost-effectiveness of S3 for long-term storage."
      },
      {
        "date": "2023-06-22T11:58:00.000Z",
        "voteCount": 2,
        "content": "Between A and D:\n\nwrite to local drive can also be a network drive mapped to the windows server. therefore SME file share is enough (A), D is Block level, for sure will cost more.\n\nthe File Gateway is designed for file-level access and presents Amazon S3 storage as a file share, while the Volume Gateway provides block-level access and appears as local block storage volumes. The choice between the two depends on the specific needs and requirements of your applications and data access patterns."
      },
      {
        "date": "2023-12-22T14:27:00.000Z",
        "voteCount": 1,
        "content": "The backend of storage gateway is actually S3 storage, which means both volume gateway and file gateway share the same cost for storage. \nAnd the gateway cost is the same according to aws pricing: https://aws.amazon.com/storagegateway/pricing/. \nso where did you get the \"D is Block level, for sure will cost more\"?"
      },
      {
        "date": "2023-06-21T19:13:00.000Z",
        "voteCount": 1,
        "content": "I am between A and D. ChatGpt says A. But The reason why I think D is because, the question says backups are written to local drive(which means its a volume on onpremises machine). So I thought a volume can be attached to volume gateway. But ChatGPT says In terms of cost-effectiveness and simplicity, option A is a better choice. It involves using an AWS Storage Gateway file gateway, which directly stores the data as objects in Amazon S3 without the need for on-premises storage. This eliminates the complexity and costs associated with maintaining an on-premises volume gateway."
      },
      {
        "date": "2023-06-23T14:41:00.000Z",
        "voteCount": 1,
        "content": "Volume will iSCSI so hat is out. Therefor A is correct"
      },
      {
        "date": "2023-06-23T15:21:00.000Z",
        "voteCount": 1,
        "content": "Use olabiba.ai. It is better."
      },
      {
        "date": "2023-06-24T00:00:00.000Z",
        "voteCount": 1,
        "content": "Q: are you using openAI as your AI engine?\nolabiba.com: Yes, I am powered by OpenAI's advanced AI technology. It allows me to understand and respond to your messages in a conversational manner. OpenAI provides the foundation for my capabilities, but the Olabiba team has also customized and trained me to better suit your needs. So, feel free to ask me anything or share your thoughts!"
      },
      {
        "date": "2023-06-29T21:47:00.000Z",
        "voteCount": 1,
        "content": "I might be wrong with my theory. Going with A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/amazon/view/112880-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T04:53:00.000Z",
        "voteCount": 8,
        "content": "In fact site to site VPN would be more affordable than deploying a Direct Connect leased line. However, AWS also wants to market their product by stating that there is a need to increase throughput (site to site only can achieve max of 1.25Gbps) and consistent user experience (AWS Direct Connect &gt; Site-to-Site VPN) so B would be a better choice."
      },
      {
        "date": "2024-07-11T03:10:00.000Z",
        "voteCount": 1,
        "content": "B, for sure.\nFor a consistent network experience"
      },
      {
        "date": "2024-03-15T10:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html"
      },
      {
        "date": "2023-11-23T15:43:00.000Z",
        "voteCount": 3,
        "content": "Option B may not be most cost-effective  best option in terms of performance."
      },
      {
        "date": "2023-10-31T05:14:00.000Z",
        "voteCount": 1,
        "content": "Anyone can explain that why Site to Site VPN not valid?"
      },
      {
        "date": "2024-05-30T02:27:00.000Z",
        "voteCount": 2,
        "content": "The company wants to increase bandwidth throughput, which is gained by establishing Direct Connect."
      },
      {
        "date": "2023-08-19T04:09:00.000Z",
        "voteCount": 1,
        "content": "what if the situation is 1 AWS account, different VPC's across different regions? Can we still use a TGW?"
      },
      {
        "date": "2023-07-07T03:48:00.000Z",
        "voteCount": 1,
        "content": "B.\nCant be D because TGW doesnt support transitive connections, so if users connect to a VPN it invalidate this options.\nA and C are skippable on the first phrase."
      },
      {
        "date": "2023-07-06T15:35:00.000Z",
        "voteCount": 1,
        "content": "B no doubt"
      },
      {
        "date": "2023-06-25T12:09:00.000Z",
        "voteCount": 3,
        "content": "direct connect + vpc = direct connect gw + TGW. so B"
      },
      {
        "date": "2023-07-29T09:07:00.000Z",
        "voteCount": 3,
        "content": "Mr.  copy and paste"
      },
      {
        "date": "2023-06-24T12:20:00.000Z",
        "voteCount": 1,
        "content": "Transit gateway is a regional service but you can peer different TGs in different regions\nhttps://aws.amazon.com/about-aws/whats-new/2019/12/aws-transit-gateway-supports-inter-region-peering/"
      },
      {
        "date": "2023-06-24T06:34:00.000Z",
        "voteCount": 1,
        "content": "B. No need for D and S2S VPN."
      },
      {
        "date": "2023-06-22T18:20:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBBBBBB?"
      },
      {
        "date": "2023-06-22T12:29:00.000Z",
        "voteCount": 3,
        "content": "direct connect + vpc = direct connect gw + TGW. so B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/amazon/view/112882-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member account for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member accounts.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-12T23:32:00.000Z",
        "voteCount": 1,
        "content": "A is ans\nA. Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.\n\nHere's why:\n\nCross-account roles: Provide a secure and managed way for users or services in one AWS account to access resources in another account.\nLeast privilege access: Configure the cross-account role with the minimum permissions needed to stop or terminate resources in the member accounts, minimizing potential security risks.\nCentralized control: Maintaining user credentials and access in the management account simplifies centralized management and auditing."
      },
      {
        "date": "2024-08-23T18:40:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2023-11-23T15:46:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2023-08-22T10:29:00.000Z",
        "voteCount": 1,
        "content": "Hmm, seems like alot of work. What if the question was,  In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in 100 organization or member accounts? Asked AI \"Using AWS Organizations, can you create both IAM user and permission sets in the management account for accessing managed organization resources?\" The answer was Yes."
      },
      {
        "date": "2023-07-06T15:37:00.000Z",
        "voteCount": 2,
        "content": "its a D"
      },
      {
        "date": "2023-06-25T08:37:00.000Z",
        "voteCount": 4,
        "content": "One user is sufficient and you need cross-account role."
      },
      {
        "date": "2023-06-22T10:58:00.000Z",
        "voteCount": 1,
        "content": "D - Cross account role should be created in destination(member) account. The role has trust entity to master account."
      },
      {
        "date": "2023-06-21T19:21:00.000Z",
        "voteCount": 4,
        "content": "D - Cross account role should be created in destination(member) account. The role has trust entity to master account."
      },
      {
        "date": "2023-06-21T19:18:00.000Z",
        "voteCount": 2,
        "content": "D - Cross account role should be created in destination account(which is member account) and trust policy should be there"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/amazon/view/112883-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run the application. All the servers mount a common share.<br><br>The company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-15T10:58:00.000Z",
        "voteCount": 1,
        "content": "The steps to on How To -  \n\nhttps://aws.amazon.com/blogs/storage/recovering-network-file-shares-with-aws-elastic-disaster-recovery-and-aws-datasync/"
      },
      {
        "date": "2023-11-30T09:05:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-11-23T15:47:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-08-20T20:29:00.000Z",
        "voteCount": 4,
        "content": "FSX for Windows and Elastic Disaster Recovery"
      },
      {
        "date": "2023-07-06T15:39:00.000Z",
        "voteCount": 1,
        "content": "its a D"
      },
      {
        "date": "2023-06-25T08:39:00.000Z",
        "voteCount": 2,
        "content": "You need FSx, not EFS and def not S3."
      },
      {
        "date": "2023-06-24T00:16:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2023-06-23T16:44:00.000Z",
        "voteCount": 1,
        "content": "D for sure\nB is wrong because you cannot use EFS for Windows EC2 Servers"
      },
      {
        "date": "2023-06-22T11:01:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2023-06-21T19:25:00.000Z",
        "voteCount": 4,
        "content": "Considering RTO and RPO, D is correct answer\nA is incorrect because, thought backups are in s3, its not possible to recover ec2 within  15-minute RTO and a 5-minute RPO"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/amazon/view/112884-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.<br><br>Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the HPC cluster is launched within a single Availability Zone.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the EC2 instances and attach elastic network interfaces in multiples of four.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the cluster is launched across multiple Availability Zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace Amazon EFS with multiple Amazon EBS volumes in a RAID array.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace Amazon EFS with Amazon FSx for Lustre.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "CDF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-24T08:17:00.000Z",
        "voteCount": 9,
        "content": "A. Ensure the HPC cluster is launched within a single Availability Zone: This choice ensures that the EC2 instances in the cluster have low network latency and high bandwidth, as they are located within the same data center.\nC. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled: EFA is a network interface that provides low-latency, high-bandwidth communication between EC2 instances. By selecting instance types with EFA enabled, the cluster can benefit from improved inter-instance communication.\nF. Replace Amazon EFS with Amazon FSx for Lustre: Amazon FSx for Lustre is a high-performance file system optimized for HPC workloads. By using FSx for Lustre instead of Amazon EFS, the cluster can achieve better performance for the large number of shared files generated by the workload.\n\nAnd what about a cluster placement group?"
      },
      {
        "date": "2023-12-08T21:54:00.000Z",
        "voteCount": 2,
        "content": "C. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.\nD. Ensure the cluster is launched across multiple Availability Zones.\nF. Replace Amazon EFS with Amazon FSx for Lustre."
      },
      {
        "date": "2023-12-07T11:28:00.000Z",
        "voteCount": 1,
        "content": "Going to ACF ,looks logical"
      },
      {
        "date": "2023-11-23T15:50:00.000Z",
        "voteCount": 2,
        "content": "A, C, F"
      },
      {
        "date": "2023-07-06T15:45:00.000Z",
        "voteCount": 1,
        "content": "ACF for performance"
      },
      {
        "date": "2023-06-29T22:03:00.000Z",
        "voteCount": 1,
        "content": "@MODERATOR - Please remove my previous comment. I agree with ACF. Thank you MoussaNoussa for clarifying"
      },
      {
        "date": "2023-06-29T10:22:00.000Z",
        "voteCount": 1,
        "content": "ACF is the correct answer"
      },
      {
        "date": "2023-06-25T12:13:00.000Z",
        "voteCount": 1,
        "content": "A, C and F"
      },
      {
        "date": "2023-06-25T12:15:00.000Z",
        "voteCount": 1,
        "content": "B ) Not is correct because ENI not more performance in this case with HPC Cluster\nD ) sonds good but not is good option because performance is required in same AZ is the cluster placement group strategy more adecuate \nE ) replace EFS by EBS not is apropiate for performance"
      },
      {
        "date": "2023-06-25T08:51:00.000Z",
        "voteCount": 2,
        "content": "A - Single AZ is better than multi AZ for performance\nC - Use EFA. https://aws.amazon.com/hpc/efa/ - It tells you that's HPC is a use case.\nF - Use FSx for Lustre - https://aws.amazon.com/fsx/lustre/. HPC is a use case."
      },
      {
        "date": "2023-06-24T11:02:00.000Z",
        "voteCount": 1,
        "content": "A, C and F"
      },
      {
        "date": "2023-06-23T22:39:00.000Z",
        "voteCount": 1,
        "content": "ACF is the correct answer"
      },
      {
        "date": "2023-06-23T13:11:00.000Z",
        "voteCount": 2,
        "content": "a-c-f...a-c-f...a-c-f\n\nTo achieve maximum performance from the HPC cluster, the following design choices should be made:\n\nA. Ensure the HPC cluster is launched within a single Availability Zone: This choice ensures that the EC2 instances in the cluster have low network latency and high bandwidth, as they are located within the same data center.\n\nC. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled: EFA is a network interface that provides low-latency, high-bandwidth communication between EC2 instances. By selecting instance types with EFA enabled, the cluster can benefit from improved inter-instance communication.\n\nF. Replace Amazon EFS with Amazon FSx for Lustre: Amazon FSx for Lustre is a high-performance file system optimized for HPC workloads. By using FSx for Lustre instead of Amazon EFS, the cluster can achieve better performance for the large number of shared files generated by the workload."
      },
      {
        "date": "2023-06-22T13:14:00.000Z",
        "voteCount": 1,
        "content": "B: more interface does not mean faster. so B is not a good choice.\nE: RAID? is often not recommended on Cloud Platform, aws has already raid the drive for you underlay. \nA: HPC recommended to use multiregion. \n\nso CDF"
      },
      {
        "date": "2023-06-22T11:03:00.000Z",
        "voteCount": 2,
        "content": "ACF is the right answer"
      },
      {
        "date": "2023-06-21T19:28:00.000Z",
        "voteCount": 1,
        "content": "CDF are correct\n\nC - EFA  provides low-latency and high-bandwidth communication between EC2 instances. It can optimize the network performance of the HPC cluster.\nD - Launching the HPC cluster across multiple Availability Zones allows you to distribute the workload and resources, reducing the chances of a single point of failure and increasing overall performance.\nF - FSx for Lustre is a high-performance file system optimized for HPC workloads."
      },
      {
        "date": "2023-06-22T11:05:00.000Z",
        "voteCount": 5,
        "content": "performance is the main goal. so running HPC in the same AZ is the right choice here"
      },
      {
        "date": "2023-06-26T11:35:00.000Z",
        "voteCount": 1,
        "content": "Thank you @ MoussaNoussa for clarifying. Agreed."
      },
      {
        "date": "2023-06-29T22:28:00.000Z",
        "voteCount": 1,
        "content": "changing my vote to ACF as per below suggestion"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/amazon/view/112885-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-09T07:51:00.000Z",
        "voteCount": 6,
        "content": "The most suitable solution for applying standardized tags across the organization with specific values for each OU is A. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values for each OU. Attach the tag policies to the OUs.\n\nHere's why:\n\nEnforce tag standardization: An SCP applied to the entire organization denies resource creation unless the required tags are present, ensuring consistent tagging across all accounts.\n\nOU-specific tags: Tag policies attached to each OU define the specific tag values for that OU, allowing customization without compromising overall standardization.\n\nGranular control: Attaching tag policies to OUs instead of the management account provides more granular control and flexibility for managing tags within each OU."
      },
      {
        "date": "2023-06-27T09:38:00.000Z",
        "voteCount": 5,
        "content": "You go to the management account -&gt; Organizations console -&gt; Policies -&gt; Tag policies -&gt; \"name of the policy\" -&gt; attach to OU. That's it - A is correct"
      },
      {
        "date": "2023-12-09T07:51:00.000Z",
        "voteCount": 1,
        "content": "A is ans"
      },
      {
        "date": "2023-11-23T15:56:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-07-09T03:43:00.000Z",
        "voteCount": 2,
        "content": "FOR EACH OU's"
      },
      {
        "date": "2023-07-06T15:58:00.000Z",
        "voteCount": 1,
        "content": "it's an A"
      },
      {
        "date": "2023-07-06T06:17:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B.\n\nImagine if you had an AWS Organization with 50+ OUs, it would be very inefficient to manually apply a generic tagging policy to each OU, so that's why there is the concept of policy inheritance: when you attach a policy to the organization root, all OUs and accounts in the organization inherit that policy\n\nWhen you attach a tag policy to your organization root, the tag policy applies to all of that root's member OUs and accounts.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/attach-tag-policy.html\n\nUnderstanding policy inheritance: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance.html"
      },
      {
        "date": "2024-04-30T06:07:00.000Z",
        "voteCount": 2,
        "content": "The Question mentions \"Each of the company's OUs will have unique tag values.\" the values list will change for OU's \n\nMy answer is A"
      },
      {
        "date": "2023-07-22T03:37:00.000Z",
        "voteCount": 3,
        "content": "The question clearly says \"Each of the company's OUs will have unique tag values\", you cannot inherit what is different. The answer is B"
      },
      {
        "date": "2023-07-22T03:38:00.000Z",
        "voteCount": 2,
        "content": "Sorry, I mean cannot be B, and the correct answer is A!"
      },
      {
        "date": "2023-07-04T02:31:00.000Z",
        "voteCount": 1,
        "content": "C and D must be wrong, because of \"allow ... \"\nB is weird."
      },
      {
        "date": "2023-07-02T17:52:00.000Z",
        "voteCount": 1,
        "content": "Each of the company's OUs will have unique tag values.\nThen A because each OU unique tags A is the unique with approved this case"
      },
      {
        "date": "2023-06-26T13:23:00.000Z",
        "voteCount": 3,
        "content": "It's A. The policies are different for each account, so you can't assign it to the management account. Exact same scenario: https://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
      },
      {
        "date": "2023-06-26T11:25:00.000Z",
        "voteCount": 1,
        "content": "MODERATOR - Please remove my previous comment. From the discussion it looks like A is the answer. Looks like the tag policies should be attached at the OU level to ensure that each OU has its own unique tag values."
      },
      {
        "date": "2023-06-24T11:04:00.000Z",
        "voteCount": 2,
        "content": "I think it's A"
      },
      {
        "date": "2023-06-23T14:53:00.000Z",
        "voteCount": 2,
        "content": "GPT 4. 0 says A - I agree. Values per OU"
      },
      {
        "date": "2023-06-23T13:12:00.000Z",
        "voteCount": 1,
        "content": "b-b-b-b-b-b"
      },
      {
        "date": "2023-06-22T11:14:00.000Z",
        "voteCount": 1,
        "content": "option A is the right answer, we need a have a list of allowed tag values per OU"
      },
      {
        "date": "2023-06-21T19:33:00.000Z",
        "voteCount": 3,
        "content": "B - you don't have apply SCPs to each account or OU. Attaching the tag policies to the organization's management account ensures that the policies are applied consistently to all OUs within the organization.\nC is incorrect because SCP are NOT used for ALLOW action. They are used for DENY actions (setting boundaries)"
      },
      {
        "date": "2023-06-29T22:28:00.000Z",
        "voteCount": 1,
        "content": "changing my vote to A. The policies are different for each account, so you can't assign it to the management account."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/amazon/view/112886-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.<br><br>Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-20T20:42:00.000Z",
        "voteCount": 7,
        "content": "Option B is missing the Data Transformation to be done by Lambda"
      },
      {
        "date": "2024-01-14T02:19:00.000Z",
        "voteCount": 5,
        "content": "MSK can't transforms the data"
      },
      {
        "date": "2024-08-16T20:30:00.000Z",
        "voteCount": 1,
        "content": "Must be C.\nhttps://aws.amazon.com/tw/blogs/iot/building-an-iot-solution-to-securely-transmit-mqtt-messages-under-private-networks/"
      },
      {
        "date": "2024-08-01T03:22:00.000Z",
        "voteCount": 1,
        "content": "Option B could also work. The key why it is not correct is because it says there is a single broker, so it will not be HA, therefore it won't prevent the crashing problem. So C is the correct option."
      },
      {
        "date": "2023-12-12T08:44:00.000Z",
        "voteCount": 1,
        "content": "b ANS\nB. Amazon MSK with NLB:\n\nPros:\nHighly available and managed Kafka service.\nScalable to accommodate increasing data volume.\nNLB automatically distributes sensor data across healthy brokers.\nCons:\nRequires migration from on-premises Kafka server.\nPotential cost increase for managed service."
      },
      {
        "date": "2023-11-23T15:58:00.000Z",
        "voteCount": 1,
        "content": "Option C."
      },
      {
        "date": "2023-09-25T08:22:00.000Z",
        "voteCount": 2,
        "content": "C :Anshttps://docs.aws.amazon.com/lambda/latest/dg/services-kinesisfirehose.html"
      },
      {
        "date": "2023-08-15T07:41:00.000Z",
        "voteCount": 5,
        "content": "C, because it said new design and obviously IoT is what aws recommend."
      },
      {
        "date": "2023-08-08T23:02:00.000Z",
        "voteCount": 3,
        "content": "Answer: C\n\nTo me C is still the best option as it is not wrong and there is an uncertainty regarding NLB support for MQTT protocol. You can, yes, however, not out of the box, you would need solutions like HiveMQ, for example: https://github.com/mqtt/mqtt.org/wiki/Server%20support\n\nNow, when I read this part of the question \"Recently, the Kafka server crashed. The company lost sensor data while the server was being restored\", to me it seems that it would be OK for the company to look for different ways in having their data stored in S3, be it using a Kafka server or not.\n\nTherefore and, just because the question doesn't say anything regarding cost effectiveness, least operational overhead, least dev overhead and so on, it's safe to assume (to me) that IoT Core would be the option AWS wants us to think about."
      },
      {
        "date": "2023-07-28T03:50:00.000Z",
        "voteCount": 2,
        "content": "Both B and C will work? \nNLB + MSK is a well defined pattern. MSK is highly available and scaleable. MQTT will pass through NLB as it's just a network port. \nNo changes to the application. \n\nC would also work, but seems to involve more refactoring."
      },
      {
        "date": "2023-07-24T22:06:00.000Z",
        "voteCount": 2,
        "content": "It's B,\nbecause MSK can handle the lightweight MQTT protocol."
      },
      {
        "date": "2023-07-06T16:03:00.000Z",
        "voteCount": 1,
        "content": "its a C\nmqtt-&gt;IoT core"
      },
      {
        "date": "2023-06-29T10:33:00.000Z",
        "voteCount": 2,
        "content": "IoT perfect for MQTT. Option D could have the same problem as on-premises"
      },
      {
        "date": "2023-06-26T13:36:00.000Z",
        "voteCount": 3,
        "content": "It's C. Anytime you see sensors, your best bet is IoT. It's not D because you'll have one Kafka EC2 instance and it's not HA."
      },
      {
        "date": "2023-06-26T11:41:00.000Z",
        "voteCount": 1,
        "content": "MODERATOR - please remove my previous comment. Looks is C is correct answer"
      },
      {
        "date": "2023-06-25T12:18:00.000Z",
        "voteCount": 1,
        "content": "IOT core is designed to handle this. and NLB does not support MQTT."
      },
      {
        "date": "2023-06-24T00:27:00.000Z",
        "voteCount": 2,
        "content": "Agree with C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/amazon/view/112887-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances.<br><br>To meet regulatory and business requirements, the company must make the following changes for data backups:<br><br>\u2022\tBackups must be retained based on custom daily, weekly, and monthly requirements.<br>\u2022\tBackups must be replicated to at least one other AWS Region immediately after capture.<br>\u2022\tThe backup solution must provide a single source of backup status across the AWS environment.<br>\u2022\tThe backup solution must send immediate notifications upon failure of any resource backup.<br><br>Which combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Backup plan with a backup rule for each of the retention requirements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Backup plan to copy backups to another Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up RDS snapshots on each database."
    ],
    "answer": "ABD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABD",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-21T19:46:00.000Z",
        "voteCount": 7,
        "content": "ABD\nE is incorrect because Amazon Data Lifecycle Manager to used to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs. It CANNOT be used for backups for ec2, EFS, RDS\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/snapshot-lifecycle.html"
      },
      {
        "date": "2024-03-15T11:25:00.000Z",
        "voteCount": 1,
        "content": "AWS Backup Plan - https://docs.aws.amazon.com/aws-backup/latest/devguide/about-backup-plans.html \n\nBackup Copy across AWS Regions - https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html\n\nBackup across AWS regions video - https://www.youtube.com/watch?v=qMN18Lpj3PE"
      },
      {
        "date": "2023-11-23T16:02:00.000Z",
        "voteCount": 2,
        "content": "Options A B D"
      },
      {
        "date": "2023-07-06T16:07:00.000Z",
        "voteCount": 2,
        "content": "its ABD"
      },
      {
        "date": "2023-07-02T17:56:00.000Z",
        "voteCount": 2,
        "content": "ABD. You don't need Lambda for cross-region backup. You don't need RDS snaps."
      },
      {
        "date": "2023-06-26T13:43:00.000Z",
        "voteCount": 4,
        "content": "ABD. You don't need Lambda for cross-region backup. You don't need RDS snaps."
      },
      {
        "date": "2023-06-23T13:18:00.000Z",
        "voteCount": 1,
        "content": "a-b-d...a-b-d"
      },
      {
        "date": "2023-06-22T11:38:00.000Z",
        "voteCount": 2,
        "content": "ABD is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/amazon/view/112889-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:<br><br>\u2022\tProvide near-real-time analytics of the inbound genomic data<br>\u2022\tEnsure the data is flexible, parallel, and durable<br>\u2022\tDeliver results of processing to a data warehouse<br><br>Which strategy should a solutions architect use to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T09:30:00.000Z",
        "voteCount": 8,
        "content": "Answer B. \nOption A might be close enough, near-real time, which is Firehose, but the target is RDS but the ask is for Datawarehouse (Redshift)"
      },
      {
        "date": "2024-06-06T05:08:00.000Z",
        "voteCount": 1,
        "content": "Dis the right Answer"
      },
      {
        "date": "2024-08-23T18:50:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-04-18T19:21:00.000Z",
        "voteCount": 1,
        "content": "Kinesis client is a library. Users need to write an application with the Kinesis Client Library to use it. \nBoth A and B states \u201canalyze the data with Kinesis clients\u201d without mentioning how the application is written and deployed. So, both A and B are out, cause the deployment model is the key of the question to satisfy the requirement.\nC has an incorrect statement \u201canalyze the data from Amazon SQS with Kinesis\u201d\nD is a feasible solution."
      },
      {
        "date": "2024-08-23T18:49:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-07-21T15:04:00.000Z",
        "voteCount": 1,
        "content": "SQS is not near real time"
      },
      {
        "date": "2024-03-21T11:06:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B."
      },
      {
        "date": "2024-01-14T02:27:00.000Z",
        "voteCount": 1,
        "content": "'parallel'"
      },
      {
        "date": "2023-11-23T16:05:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-06T16:08:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-26T13:57:00.000Z",
        "voteCount": 4,
        "content": "B. Real-time is either firehose (A) or streams (B). But they require a data warehouse and that's RedShift not RDS."
      },
      {
        "date": "2023-06-23T13:19:00.000Z",
        "voteCount": 1,
        "content": "b=b=b=b=b=b=b"
      },
      {
        "date": "2023-06-23T06:09:00.000Z",
        "voteCount": 1,
        "content": "B is the one for real time"
      },
      {
        "date": "2023-06-22T11:41:00.000Z",
        "voteCount": 2,
        "content": "Answer B is the right one"
      },
      {
        "date": "2023-06-21T19:54:00.000Z",
        "voteCount": 4,
        "content": "B is correct\nB - Kinesis Data Streams is a real-time streaming service and provide near-real-time analytics. Also the question \"Deliver results of processing to a data warehouse\" and this option has redshift cluster which is a powerful data warehousing solution that can handle large-scale analytics workloads.\n\nA - incorrect because Kinesis Data Firehose is NOT ideal for near-real-time analytics and may introduce some latency in the data processing pipeline. Additionally, saving the results to an Amazon RDS instance may not provide the scalability and flexibility required for processing and analyzing large volumes of genomic data."
      },
      {
        "date": "2023-06-29T22:18:00.000Z",
        "voteCount": 1,
        "content": "Between A and B, B is better because questions asks for data warehousing capabilities. So option B has Redshift which is correct answer."
      },
      {
        "date": "2023-06-26T11:52:00.000Z",
        "voteCount": 2,
        "content": "What a worst framed ques. The ques says \"NEAR real time\" which means its Kinesis data firehose. But this option has RDS which is not good for analysis"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/amazon/view/112973-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data layers. The reference architecture must meet the following requirements:<br><br>\u2022\tHigh availability within an AWS Region<br>\u2022\tAble to fail over in 1 minute to another AWS Region for disaster recovery<br>\u2022\tProvide the most efficient solution while minimizing the impact on the user experience<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T09:33:00.000Z",
        "voteCount": 5,
        "content": "not sure how these answers are generated, poor quality!\nCorrect answer - BCE\nHot standby, DynamoDB Global tables, Route53 failover routing policy."
      },
      {
        "date": "2023-11-23T16:09:00.000Z",
        "voteCount": 2,
        "content": "B, C and E"
      },
      {
        "date": "2023-11-22T09:00:00.000Z",
        "voteCount": 1,
        "content": "FDE is incorrect.\nBCE are right options"
      },
      {
        "date": "2023-07-06T16:11:00.000Z",
        "voteCount": 2,
        "content": "BCE for sure"
      },
      {
        "date": "2023-07-04T03:01:00.000Z",
        "voteCount": 1,
        "content": "A and D must be wrong. They cannot meet the performance requirement.\nF is not good. Spot Instances are not reliable."
      },
      {
        "date": "2023-07-02T18:01:00.000Z",
        "voteCount": 1,
        "content": "BCE is correct"
      },
      {
        "date": "2023-06-29T10:40:00.000Z",
        "voteCount": 1,
        "content": "BCE is correct"
      },
      {
        "date": "2023-06-26T14:01:00.000Z",
        "voteCount": 1,
        "content": "A - Failover Rt 53\nC - Global DynamoDB tables to take care of regional replication\nE - Minimum EC2 across regions with reserved and on-demand"
      },
      {
        "date": "2023-06-26T14:02:00.000Z",
        "voteCount": 3,
        "content": "Sorry BCE."
      },
      {
        "date": "2023-06-25T12:28:00.000Z",
        "voteCount": 1,
        "content": "To meet the requirements of high availability within an AWS Region, failover to another AWS Region for disaster recovery, and provide an efficient solution while minimizing user impact, the following three steps should be taken:\n\nStep B: Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.\n\nBy using the failover routing policy in Amazon Route 53, you can configure DNS failover between the primary and disaster recovery Regions. This allows traffic to be redirected to the disaster recovery Region in the event of a failure in the primary Region."
      },
      {
        "date": "2023-06-25T12:28:00.000Z",
        "voteCount": 1,
        "content": "Step C: Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.\n\nAmazon DynamoDB global tables enable automatic multi-region replication, allowing the data to be accessed in both the primary and disaster recovery Regions. This ensures data availability and low-latency access to the data."
      },
      {
        "date": "2023-06-25T12:28:00.000Z",
        "voteCount": 2,
        "content": "Step E: Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.\n\nBy implementing a hot standby model with Auto Scaling groups across multiple Availability Zones in both the primary and disaster recovery Regions, you can ensure high availability within the Region. Using zonal Reserved Instances for the minimum number of servers helps optimize costs, while On-Demand Instances provide flexibility for additional resource provisioning."
      },
      {
        "date": "2023-06-25T12:27:00.000Z",
        "voteCount": 1,
        "content": "B, C and E"
      },
      {
        "date": "2023-06-24T00:33:00.000Z",
        "voteCount": 1,
        "content": "B, C and E"
      },
      {
        "date": "2023-06-23T06:35:00.000Z",
        "voteCount": 1,
        "content": "BCE here as well\nA: 1 hour is too long\nD: just use global table....\nF: hot spot?"
      },
      {
        "date": "2023-06-22T11:48:00.000Z",
        "voteCount": 2,
        "content": "BCE is the right answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/amazon/view/113094-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. Custom applications analyze this data to detect anomalies.<br><br>The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T10:18:00.000Z",
        "voteCount": 8,
        "content": "Option B is correct. Feeltwise Option C requires edge agent to collect the data --&gt; Higher operational overhead to migrate as this will need changes in customer application customer has today."
      },
      {
        "date": "2023-08-20T20:58:00.000Z",
        "voteCount": 6,
        "content": "The confusion seem to be b/w IoTCore and FleetWise (B &amp; C), however for anomaly detection one uses Kinesis Data Analytics(KDA) and other uses Glue ML algorithms. Least overhead is using Random Cut Forest in (KDA) as compared to Glue"
      },
      {
        "date": "2024-04-11T06:43:00.000Z",
        "voteCount": 1,
        "content": "Option C:  How To \n\nhttps://aws.amazon.com/blogs/iot/best-practices-for-ingesting-data-from-devices-using-aws-iot-core-and-or-amazon-kinesis/"
      },
      {
        "date": "2024-04-11T06:44:00.000Z",
        "voteCount": 1,
        "content": "Sorry \" Option B  NOT  C \""
      },
      {
        "date": "2024-02-01T03:54:00.000Z",
        "voteCount": 1,
        "content": "ans is C"
      },
      {
        "date": "2024-08-23T18:53:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-01-03T04:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\nAWS Lookout - Automatically detect anomalies within metrics and identify their root causes.\nhttps://aws.amazon.com/lookout-for-metrics/"
      },
      {
        "date": "2023-12-27T15:05:00.000Z",
        "voteCount": 1,
        "content": "Agree with @duriselvan - Fleetwise is made for this and Glue has machine learning modules"
      },
      {
        "date": "2023-12-09T00:24:00.000Z",
        "voteCount": 3,
        "content": "C ans :-\nAWS IoT FleetWise: This managed service simplifies vehicle data collection and management, reducing operational overhead compared to other options.\nKinesis data stream: This serverless stream allows processing data in real-time, eliminating the need for custom code.\nKinesis Data Firehose: This service automatically stores data in S3, reducing manual intervention.\nGlue machine learning transforms: These built-in features enable anomaly detection directly within Glue, eliminating the need for separate ML models and infrastructure"
      },
      {
        "date": "2023-11-30T09:39:00.000Z",
        "voteCount": 5,
        "content": "Answer B. Straightforward\nC might sound like a good option with Fleetwise, but Glue for anamoly detection?? Also talks about Kinesis integration with Fleetwise not sure. \nFleetwise also needs a Edge agent to communicate with AWS IoT Fleetwise"
      },
      {
        "date": "2023-11-03T03:39:00.000Z",
        "voteCount": 2,
        "content": "its a B...oye! :)"
      },
      {
        "date": "2023-10-12T09:26:00.000Z",
        "voteCount": 2,
        "content": "Here's why option B is the best choice:\n\nSimplicity: This solution leverages AWS IoT Core and Amazon Kinesis Data Firehose, which are fully managed services, making it a simple and low-overhead option.\n\nReal-time Data Streaming: AWS IoT Core efficiently receives the vehicle data using the MQTT protocol, and Kinesis Data Firehose streams the data to Amazon S3. This supports data streaming in real-time.\n\nEasy Anomaly Detection: Amazon Kinesis Data Analytics can easily be set up to process the streaming data in real-time to detect anomalies.\n\nScalability: This architecture is designed to handle a growing number of vehicles and high data volumes, ensuring scalability without operational overhead.\n\nData Storage: Data is reliably stored in Amazon S3, eliminating concerns about on-premises storage limitations."
      },
      {
        "date": "2023-08-09T09:37:00.000Z",
        "voteCount": 1,
        "content": "I agree with everyone. Even olabiba agrees. It's B."
      },
      {
        "date": "2023-07-06T16:29:00.000Z",
        "voteCount": 2,
        "content": "it's a B\nC - there is no Fleetwise to Kinesis integration"
      },
      {
        "date": "2023-06-26T14:18:00.000Z",
        "voteCount": 2,
        "content": "A - too complex\nB - It's B. You se IoT Code, Kinesis Firehose and Kinesis Data Analytics for anomalies https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html\nC - IoT FleetWise is a perfect use case but this solution does not detect anomalies. You need Lookout for this as described here. https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html\nD - This is also possible, but the use case for RabbitMQ is different."
      },
      {
        "date": "2023-06-26T06:32:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-c"
      },
      {
        "date": "2023-06-25T12:37:00.000Z",
        "voteCount": 1,
        "content": "B for me opinion i need use Amazon Kinesis Data Analytics for detect anomalies\nC sounds goood but i don't know how AWS Glue detect anomalies , usually use case is ETL"
      },
      {
        "date": "2023-06-24T14:34:00.000Z",
        "voteCount": 1,
        "content": "Olabiba says 'B'."
      },
      {
        "date": "2023-06-23T18:31:00.000Z",
        "voteCount": 3,
        "content": "AWS IoT Core provides a good way to handle data from IoT devices like these smart vehicles, especially as the MQTT protocol is used. Amazon Kinesis Data Firehose can capture, transform, and load streaming data into data lakes, data stores, and analytics services. It can handle large volumes of data from hundreds of thousands of sources, and it can scale automatically. Amazon Kinesis Data Analytics makes it easy to analyze streaming data in real-time with Java, SQL, or Apache Flink, without having to learn new programming languages or processing frameworks. It could be used to analyze the streaming data and detect anomalies"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/amazon/view/112965-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.<br><br>Which solution will ensure that the credentials are appropriately secured automatically?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T14:26:00.000Z",
        "voteCount": 12,
        "content": "A - AWS Secrets Manager can't rotate the credentials if they are part of the code\nB - You don't store creds in KMS, that's the job of Secrets Manager\nC - Macie can do S3 only. CodeCommit backend is also S3 but it's transparent for us, so you can't use Macie.\nD - Correct. See this use case https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/"
      },
      {
        "date": "2023-12-26T05:57:00.000Z",
        "voteCount": 1,
        "content": "D https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/"
      },
      {
        "date": "2023-11-23T06:26:00.000Z",
        "voteCount": 1,
        "content": "Using lambda to trigger a scan is retrospectively ineffective as Azure can do so with DevOps Organization advanced security (which does code scanning) and provide you an option to remediate if targets are found."
      },
      {
        "date": "2023-11-22T10:59:00.000Z",
        "voteCount": 1,
        "content": "D is right option."
      },
      {
        "date": "2023-10-31T00:35:00.000Z",
        "voteCount": 2,
        "content": "Macie only does S3 -&gt; C is out\nScheduled or nightly script will only detect the problem after a while so damage might has already done --&gt; A, B is out\nPlus KMS doesnt do secrets\nD looks valid technically"
      },
      {
        "date": "2023-07-22T15:21:00.000Z",
        "voteCount": 1,
        "content": "Correct C.\nMacie can scan for credentials in CodeCommit repositories. According to the AWS documentation, Macie supports scanning for credentials in CodeCommit repositories and triggering actions based on the findings. You can use Macie to discover sensitive data such as AWS access keys, AWS secret access keys, private keys, and more in your CodeCommit repositories. You can also configure Macie to send notifications, invoke Lambda functions, or publish findings to AWS Security Hub when it detects sensitive data in CodeCommit repositories. For more information, see Data protection in AWS CodeCommithttps://docs.aws.amazon.com/macie/latest/user/what-is-macie.html and Amazon Macie | AWS Bloghttps://aws.amazon.com/blogs/aws/category/amazon-macie/.https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html: https://docs.aws.amazon.com/codecommit/latest/userguide/data-protection.htmlhttps://aws.amazon.com/blogs/aws/category/amazon-macie/: https://aws.amazon.com/blogs/aws/category/amazon-macie/"
      },
      {
        "date": "2023-07-06T16:32:00.000Z",
        "voteCount": 2,
        "content": "D - https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/"
      },
      {
        "date": "2023-06-26T20:12:00.000Z",
        "voteCount": 1,
        "content": "D can resolve the code that already commit to codecommit"
      },
      {
        "date": "2023-06-28T08:09:00.000Z",
        "voteCount": 1,
        "content": "D says Codecommit trigger to scan new code submissions.... \nhow already commit code will scan ?"
      },
      {
        "date": "2023-06-28T08:11:00.000Z",
        "voteCount": 1,
        "content": "whereas question did not ask for existing code"
      },
      {
        "date": "2023-06-25T12:39:00.000Z",
        "voteCount": 1,
        "content": "Macie sounds good but not is use case is only scans S3.\nThen D is more apropiate in this case , similar question in this exam practice on Tutoriales Dojo"
      },
      {
        "date": "2023-06-24T13:12:00.000Z",
        "voteCount": 2,
        "content": "Macie would be a great choice but at the moment it only scans S3. And even if CodeCommit ends in S3 (according to the AWS documentation) it is not visible for us and therefore I don't believe we an configure Macie to scan. At the moment Lambda remains the best choice"
      },
      {
        "date": "2023-06-23T18:37:00.000Z",
        "voteCount": 1,
        "content": "Need auto-disable and D does it"
      },
      {
        "date": "2023-06-23T18:01:00.000Z",
        "voteCount": 1,
        "content": "D. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.\n\nExplanation:\nThis solution leverages a CodeCommit trigger to automatically invoke an AWS Lambda function whenever new code is submitted to the repository. The Lambda function can scan the code for credentials and if found, take appropriate actions such as disabling those credentials in AWS IAM and notifying the user. This approach ensures that the security vulnerability is automatically identified and remediated as part of the development process, providing a proactive security measure."
      },
      {
        "date": "2023-06-23T06:54:00.000Z",
        "voteCount": 4,
        "content": "I would go with D. reason is ABC are all post event action, meaning the creditential are already leaked AFTER the code submition. \n\nonly D would prevent it from happeninng by doing a check BEFORE it get submitted."
      },
      {
        "date": "2023-06-22T12:05:00.000Z",
        "voteCount": 3,
        "content": "option D is the correct one of course"
      },
      {
        "date": "2023-06-22T10:19:00.000Z",
        "voteCount": 2,
        "content": "C - https://docs.aws.amazon.com/macie/latest/user/managed-data-identifiers.html#managed-data-identifiers-credentials"
      },
      {
        "date": "2023-06-29T22:25:00.000Z",
        "voteCount": 1,
        "content": "change it to D as it would prevent it from happeninng by doing a check BEFORE it get submitted."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/amazon/view/113133-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.<br><br>To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.<br><br>Which combination of steps should the solutions architect take to implement this solution? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application\u2019s VPC. Update the bucket policy to require access from an access point.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-31T00:29:00.000Z",
        "voteCount": 16,
        "content": "For those who struggle on why A but not D as they are almost identical like I did:\nA: Create an S3 access point for each application in THE AWS account\nD: Create an S3 access point for each application in EACH AWS account\n\nNot sure if this is technical or English exam."
      },
      {
        "date": "2024-02-25T16:28:00.000Z",
        "voteCount": 1,
        "content": "A: in the AWS account that owns the S3 bucket"
      },
      {
        "date": "2024-07-24T05:11:00.000Z",
        "voteCount": 1,
        "content": "see details step in below link where 'Create an Amazon S3 gateway endpoint in your VPC'\n\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"
      },
      {
        "date": "2024-05-23T11:02:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/s3-access-bucket-restricted-to-vpc"
      },
      {
        "date": "2024-05-30T12:25:00.000Z",
        "voteCount": 1,
        "content": "The linked post describes the scenario of creating an S3 access point in the data lake account (answer A) and a gateway VPC endpoint in the application's account (answer C)."
      },
      {
        "date": "2024-05-05T05:25:00.000Z",
        "voteCount": 1,
        "content": "A and C in my opinion. Interface Endpoint is for EC2 generally, when we need a private IP. Gateway Endpoint is suitable in 95% cases when there are DynamoDB and S3 secure connectivity."
      },
      {
        "date": "2024-05-01T19:10:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C\nWhy not B?\nInterface endpoints are used for services that require a private IP address within the VPC, such as Amazon EC2, Amazon ECS, or Amazon SNS.\n\nGateway endpoints, on the other hand, are used for services that are accessed using their public endpoint, such as Amazon S3 and Amazon DynamoDB.\n\nSince the scenario involves accessing an S3 bucket, a gateway endpoint is the appropriate choice, not an interface endpoint."
      },
      {
        "date": "2024-04-13T01:59:00.000Z",
        "voteCount": 1,
        "content": "Correct:A,B\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"
      },
      {
        "date": "2024-03-31T10:30:00.000Z",
        "voteCount": 2,
        "content": "Gateway Endpoint only allows resources within the VPC to connect to S3.\nIt is not possible to provide the gateway endpoint across many AWS accounts"
      },
      {
        "date": "2024-03-24T06:42:00.000Z",
        "voteCount": 2,
        "content": "I don't think C can achieve the requirement. At least according to this https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html. Here's why.\n\n\"100's of AWS Accounts\" hints about possibility of cross region access. Gateway Endpoints can't allow access from VPCs in other regions. Gateway endpoint is to access from own VPC."
      },
      {
        "date": "2024-03-21T21:40:00.000Z",
        "voteCount": 3,
        "content": "It's A+B. A sets up S3 Access Points, one for each accessing application, in the data lake account (the S3 account) which are configured with policies giving each application least-privilege access. B then sets up PrivateLink access (==interface endpoints) in each of the application accounts. \n\nC is out because gateway endpoints can't take policies.\nD is less efficient than A+B\nE is too simplistic - one gateway endpoint is not enough.."
      },
      {
        "date": "2024-03-19T09:15:00.000Z",
        "voteCount": 1,
        "content": "A is valid, but C can't be configured for fine-grained access since it involves a gateway endpoint. Therefore: B as this is possible with a PrivateLink (==interface endpoint)"
      },
      {
        "date": "2023-12-10T20:01:00.000Z",
        "voteCount": 3,
        "content": "Answer is A &amp; B. \n\nC is not suitable based on AWS Gateway endpoints documentation - \n\"Endpoint connections cannot be extended out of a VPC. Resources on the other side of a VPN connection, VPC peering connection, transit gateway, or AWS Direct Connect connection in your VPC cannot use a gateway endpoint to communicate with Amazon S3.\"\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
      },
      {
        "date": "2024-02-04T00:30:00.000Z",
        "voteCount": 1,
        "content": "With a gateway endpoint, you can access Amazon S3 from your VPC (https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html)"
      },
      {
        "date": "2023-11-22T11:40:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C are right."
      },
      {
        "date": "2023-10-31T13:23:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"
      },
      {
        "date": "2024-01-03T18:08:00.000Z",
        "voteCount": 2,
        "content": "considering this blog post, do you agree with A&amp;B or A&amp;C?"
      },
      {
        "date": "2023-10-24T16:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is AB, because gateway VPC does not have access to S3 access point.\nAnd interface VPC endpoint allos access to S3 access point.\nNote from ChatGPT:\nAs of my last knowledge update in September 2021, Gateway VPC Endpoints for Amazon S3 do not support direct access to S3 access points. Gateway VPC Endpoints are designed to provide private connectivity from your Amazon Virtual Private Cloud (VPC) to S3, but they do not inherently support access to S3 access points."
      },
      {
        "date": "2023-10-12T09:46:00.000Z",
        "voteCount": 1,
        "content": "A. By creating an S3 access point for each application in the AWS account that owns the S3 bucket and configuring it to be accessible only from the application's VPC, you ensure that each application has the minimum necessary permissions and can access the data lake securely.\n\nC. Creating a gateway endpoint for Amazon S3 in each application's VPC and configuring the endpoint policy to allow access to an S3 access point ensures that traffic from each VPC is directed through the S3 access point and adheres to the security requirements. Specifying the route table that is used to access the access point is an essential part of the configuration.\n\nThis combination of steps helps you meet your security and access requirements by using S3 access points and VPC endpoints for each application. It ensures that the data lake is accessed securely and that access permissions are correctly configured."
      },
      {
        "date": "2023-08-30T20:16:00.000Z",
        "voteCount": 1,
        "content": "Gateway endpoint is public where as S3 access point and Interface endpoint can be private and limited to VPC. https://aws.amazon.com/s3/features/access-points/"
      },
      {
        "date": "2023-08-24T09:56:00.000Z",
        "voteCount": 3,
        "content": "can anyone tell me why B is incorrect\nfrom what i know\ngateway endpoint resolves to Public AWS IP\ninterface endpoint is completely private\nplease correct me if wrong"
      },
      {
        "date": "2023-09-01T06:03:00.000Z",
        "voteCount": 1,
        "content": "interface endpoint is completely private, you are wrong interface endpoint is public"
      },
      {
        "date": "2023-09-01T06:02:00.000Z",
        "voteCount": 1,
        "content": "Because To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application =&gt; using access endpoint instead of interface endpoints"
      },
      {
        "date": "2023-09-07T07:37:00.000Z",
        "voteCount": 1,
        "content": "thanks, got it"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/amazon/view/112977-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.<br><br>The company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.<br><br>How should the solutions architect meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T12:12:00.000Z",
        "voteCount": 13,
        "content": "Answer is B\nQuestion asks for \"near real time\" analysis\nFor near real time  --&gt;use Kinesis Datafirehose. \nFor real time ---&gt; use Kineses data streams\nreal-time is instant, whereas near real-time is delayed"
      },
      {
        "date": "2024-01-02T06:39:00.000Z",
        "voteCount": 3,
        "content": "B:\n\nWhy do they answer the solution backwards. it does no follow the workflow, it is hard to put the picture together. but , anyway."
      },
      {
        "date": "2023-11-22T12:03:00.000Z",
        "voteCount": 1,
        "content": "B is right answer as KDF supports Splunk integration."
      },
      {
        "date": "2023-11-22T12:03:00.000Z",
        "voteCount": 1,
        "content": "and Requirement is Near Real time."
      },
      {
        "date": "2023-10-31T00:07:00.000Z",
        "voteCount": 3,
        "content": "Monitoring solution -&gt; VPC flow logs\nNear real time analysis -&gt; Firehose\nFirehose also can have spunk as destination -&gt; eye on B\nA: giving access key normally a secondary considered option\nC: too complex to get logs while we have vpc flow logs\nD: same"
      },
      {
        "date": "2023-07-22T13:33:00.000Z",
        "voteCount": 1,
        "content": "correct B."
      },
      {
        "date": "2023-07-06T16:57:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-07-05T14:38:00.000Z",
        "voteCount": 1,
        "content": "B, in this link https://docs.aws.amazon.com/firehose/latest/dev/creating-the-stream-to-splunk.html#:~:text=In%20this%20part%20of%20the%20Kinesis%20Data%20Firehose%20tutorial%2C%20you%20create%20an%20Amazon%20Kinesis%20Data%20Firehose%20delivery%20stream%20to%20receive%20the%20log%20data%20from%20Amazon%20CloudWatch%20and%20deliver%20that%20data%20to%20Splunk., the traffic flow is: CW logs-&gt; Kinesis Datafirehose delivery-&gt; Splunk. In our case, we need custom logs, so need to subscribe VPC flow logs to send to splunk for specific monitoring"
      },
      {
        "date": "2023-07-02T18:10:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nQuestion asks for \"near real time\" analysis\nFor near real time --&gt;use Kinesis Datafirehose.\nFor real time ---&gt; use Kineses data streams\nreal-time is instant, whereas near real-time is delayed"
      },
      {
        "date": "2023-06-26T14:37:00.000Z",
        "voteCount": 3,
        "content": "It's B - Rest is too complex. https://docs.aws.amazon.com/firehose/latest/dev/creating-the-stream-to-splunk.html"
      },
      {
        "date": "2023-06-24T01:18:00.000Z",
        "voteCount": 1,
        "content": "B is answer, I think"
      },
      {
        "date": "2023-06-23T23:32:00.000Z",
        "voteCount": 2,
        "content": "B. https://docs.aws.amazon.com/firehose/latest/dev/vpc-splunk-tutorial.html"
      },
      {
        "date": "2023-06-23T18:57:00.000Z",
        "voteCount": 3,
        "content": "GPT - Amazon VPC Flow Logs can be enabled to capture information about the IP traffic going to and from network interfaces in the VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon S3. Once the logs are in CloudWatch, you can create a subscription filter that forwards events to a Kinesis Data Firehose stream.\nAWS Lambda can preprocess records in the Kinesis Data Firehose stream before they are delivered to Splunk.This solution provides near-real-time delivery of VPC Flow Logs to Splunk.Other options are less optimal because they involve unnecessary complexity or do not provide near-real-time monitoring."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/amazon/view/113142-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.<br><br>The company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.<br><br>A solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.<br><br>Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "ABD",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BDF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T14:42:00.000Z",
        "voteCount": 9,
        "content": "B - You need AWS Orgs to manage all other accts\nD - You need to deny creating resources\nE - You create the role in the mgmt acct not in each AWS acct. That's the point of the mgmt acct."
      },
      {
        "date": "2023-08-24T08:20:00.000Z",
        "voteCount": 2,
        "content": "I'm not sure for E. The management account in AWS Organisations is to manage membres account and policies but not roles. I'll go for F instead."
      },
      {
        "date": "2023-07-08T14:28:00.000Z",
        "voteCount": 8,
        "content": "Remember SCP Only deny not allow ( in definition )"
      },
      {
        "date": "2024-05-21T07:10:00.000Z",
        "voteCount": 2,
        "content": "Answer is BDE withouth any doubt!"
      },
      {
        "date": "2024-02-10T07:14:00.000Z",
        "voteCount": 2,
        "content": "Not C because there is no word about default SCP removal.\nFullAWSAccess  - without an explicit deny SCP would not suffice the requirement"
      },
      {
        "date": "2024-02-05T09:28:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\nNotes\nAn Allow statement in an SCP permits the Resource element to only have a \"*\" entry.\nAn Allow statement in an SCP can't have a Condition element at all.\n\nTherefore Option C is not possible"
      },
      {
        "date": "2023-12-20T21:47:00.000Z",
        "voteCount": 1,
        "content": "BCE and it aligns with what ChatGpt thinks"
      },
      {
        "date": "2023-12-08T21:47:00.000Z",
        "voteCount": 1,
        "content": "ABD -ANS\nA. Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.\nB. Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.\nD. Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU."
      },
      {
        "date": "2023-11-30T09:51:00.000Z",
        "voteCount": 1,
        "content": "Answer - BDE"
      },
      {
        "date": "2023-11-22T18:24:00.000Z",
        "voteCount": 5,
        "content": "Explicit Deny is more strict than Explicit Allow - As member account can add allow creation of resources in other regions."
      },
      {
        "date": "2023-07-06T20:35:00.000Z",
        "voteCount": 1,
        "content": "BDE - going with the crowd, although C seems like it'd work too. Is the issue that it can be overriden at account level?"
      },
      {
        "date": "2023-11-16T01:15:00.000Z",
        "voteCount": 2,
        "content": "Not exactly overwritten. If you allow the creation in certain regions in the SCP, all member accounts are allowed to create instances in the region. But each member account can add IAM policies to allow to create them in different regions as well, unless there is an explicit deny. Therefore only D works."
      },
      {
        "date": "2023-07-05T14:43:00.000Z",
        "voteCount": 1,
        "content": "BDE\n\nOrg -&gt; enable all feature-&gt; invite all member account-&gt; member account accept invitation\nOrg-&gt; mgmt account-&gt; create IAM role to access to member account-&gt; login member account assume this role to view billings"
      },
      {
        "date": "2023-07-02T18:27:00.000Z",
        "voteCount": 1,
        "content": "For C, do an allow statement with StringEqual, for D, do a deny statement with StringNotEqual of US region. So C &amp; D are both right.\nCost Explorer has all the reports, creating a S3 is NOT operationally efficient \u2013 A is out\nIAM role is needed to view billing - E"
      },
      {
        "date": "2023-06-29T11:00:00.000Z",
        "voteCount": 1,
        "content": "correct answer is BDE"
      },
      {
        "date": "2023-06-26T07:03:00.000Z",
        "voteCount": 1,
        "content": "b-c-e...b-c-e"
      },
      {
        "date": "2023-06-24T06:27:00.000Z",
        "voteCount": 2,
        "content": "For C, do an allow statement with StringEqual, for D, do a deny statement with StringNotEqual of US region. So C &amp; D are both right.\nCost Explorer has all the reports, creating a S3 is NOT operationally efficient \u2013 A is out\nIAM role is needed to view billing - E"
      },
      {
        "date": "2023-06-24T01:22:00.000Z",
        "voteCount": 1,
        "content": "B, D an E"
      },
      {
        "date": "2023-06-23T23:36:00.000Z",
        "voteCount": 2,
        "content": "it's BDF"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/amazon/view/113012-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.<br><br>How should a solutions architect meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T14:09:00.000Z",
        "voteCount": 5,
        "content": "B is right\nA is incorrect as you CANNOT establish a trust relationship between the IAM policy and account \nC and D does NOT talk about readonly access"
      },
      {
        "date": "2023-07-05T14:48:00.000Z",
        "voteCount": 5,
        "content": "So there is 3 parts, security account, member account, org account\n\nGoal: Security account-&gt; member account\nIn org account, use org crossAccountAccessRole-&gt; create  ReadOnlyRole in member account\nBuild trust: security account &amp; member account\nSecurity account assume member account ReadOnlyRole"
      },
      {
        "date": "2023-12-11T09:12:00.000Z",
        "voteCount": 1,
        "content": "D Ans\nD. STS AssumeRole with OrganizationAccountAccessRole in Member Account:\n\nPros:\nFollows best practices for cross-account access using temporary credentials.\nMinimizes complexity by leveraging the pre-existing OrganizationAccountAccessRole.\nCons:\nSecurity team needs access to each member account to assume the role.\nTherefore, option D, using AWS STS to call the AssumeRole API for the OrganizationAccountAccessRole in each member account from the security account, is the most secure and efficient solution. This approach leverages existing IAM roles, minimizes configuration overhead, and adheres to best practices for cross-account access using temporary credentials."
      },
      {
        "date": "2024-08-24T01:06:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2023-12-14T17:23:00.000Z",
        "voteCount": 3,
        "content": "OrganizationAccountAccessRole by default has AdministratorAccess IAM policy attached. The security team should only get Read Only. Best practice for accounts within an organization is B."
      },
      {
        "date": "2023-11-22T18:29:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-07-22T13:12:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-06T20:40:00.000Z",
        "voteCount": 2,
        "content": "its a b"
      },
      {
        "date": "2023-06-26T15:08:00.000Z",
        "voteCount": 1,
        "content": "B - You need a role."
      },
      {
        "date": "2023-06-26T07:09:00.000Z",
        "voteCount": 1,
        "content": "b-b-b-b-b-b-b"
      },
      {
        "date": "2023-06-25T12:57:00.000Z",
        "voteCount": 1,
        "content": "B is classic usage of Cross Account Role"
      },
      {
        "date": "2023-06-24T15:11:00.000Z",
        "voteCount": 2,
        "content": "oh labiba is 'B'\n\nTo meet the requirements, a solutions architect should choose option B. Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.\n\nBy using the OrganizationAccountAccessRole IAM role, the solutions architect can create a new IAM role with read-only access in each member account. This allows the security team to have read-only access to all accounts from their own AWS account. The trust relationship between the IAM role in each member account and the security account ensures that the security team can assume the IAM role and access the necessary resources."
      },
      {
        "date": "2023-06-24T01:23:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-06-23T19:14:00.000Z",
        "voteCount": 3,
        "content": "GPT: This approach aligns with the AWS best practice of using IAM roles to delegate permissions across AWS accounts. The OrganizationAccountAccessRole is a role that is automatically created when you create a new account in an organization. This role can be assumed by the master account, but it can also be assumed by other accounts if a trust relationship is established."
      },
      {
        "date": "2023-06-23T19:05:00.000Z",
        "voteCount": 2,
        "content": "Option B suggests using the OrganizationAccountAccessRole IAM role to create a new IAM role in each member account. This IAM role will have read-only access permissions. By establishing a trust relationship between the IAM role in each member account and the security account, the security team's AWS account is granted access to the member accounts."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/amazon/view/113010-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.<br><br>A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.<br><br>Which set of additional steps should the solutions architect take to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-31T01:38:00.000Z",
        "voteCount": 1,
        "content": "With hundreds of VPCs you will inevitably face CIDR overlapping conflict so better to use Transit Gateway"
      },
      {
        "date": "2024-05-09T01:45:00.000Z",
        "voteCount": 1,
        "content": "There's a key information that is not mentioned in the question, if the VPCs are in the same region or in different regions, as we're talking of hundreds of AWS accounts the answer will be VPC peering as a single transit gateway doesn\u00b4t support different regions so A. If all VPCs are in the same region, the answer would be transit gateway so B. Saying that I go for A"
      },
      {
        "date": "2024-08-24T01:07:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2023-11-22T18:35:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-10-30T23:08:00.000Z",
        "voteCount": 4,
        "content": "\"hundreds of AWS account\" - think of transit gateway, VPC peering, PrivateLink should be out\noption C: add transit gateway to each account -&gt; out"
      },
      {
        "date": "2023-07-22T13:11:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-06T20:42:00.000Z",
        "voteCount": 1,
        "content": "b for sure"
      },
      {
        "date": "2023-07-05T14:53:00.000Z",
        "voteCount": 4,
        "content": "hundreds of VPCs-&gt; TGW\nthen we only have B and C\nC: create TGW in each account, wrong"
      },
      {
        "date": "2023-06-26T15:37:00.000Z",
        "voteCount": 2,
        "content": "B - Hub and spoke is based on transit GW"
      },
      {
        "date": "2023-06-26T07:11:00.000Z",
        "voteCount": 2,
        "content": "b-b-b-b-b-b-b"
      },
      {
        "date": "2023-06-24T01:25:00.000Z",
        "voteCount": 1,
        "content": "yep, it's B"
      },
      {
        "date": "2023-06-23T19:31:00.000Z",
        "voteCount": 3,
        "content": "Option B suggests creating a transit gateway, which acts as a hub for connectivity between multiple VPCs and on-premises networks. By sharing the transit gateway with the existing AWS accounts, the solutions architect can attach the VPCs, including the spoke VPCs, to the transit gateway. The required routing can then be configured to direct traffic from the spoke VPCs to the transit gateway, which will route it to the egress VPC with the NAT gateway. This allows for centralized routing and connectivity to the internet for the spoke VPCs."
      },
      {
        "date": "2023-06-23T19:19:00.000Z",
        "voteCount": 1,
        "content": "GPT = B; AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. It simplifies the management of network connectivity across a large number of accounts/VPCs."
      },
      {
        "date": "2023-06-23T10:54:00.000Z",
        "voteCount": 1,
        "content": "B is correct because we have hundreds of vpcs and default quota for peering peer vpc is = 50"
      },
      {
        "date": "2023-06-22T14:07:00.000Z",
        "voteCount": 1,
        "content": "SHould be B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/amazon/view/113019-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a weekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts originate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from overwhelming the authentication service.<br><br>Which solution meets these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-12T11:13:00.000Z",
        "voteCount": 5,
        "content": "Option B provides the most operational efficiency to prevent the weekly spike in failed login attempts. Here's why:\n\nAWS WAF (Web Application Firewall) with a rate-based rule allows you to monitor and block traffic based on the rate of requests from different IP addresses.\nThe rate-based rule can help identify and block the excessive login attempts originating from a large number of IP addresses that change weekly.\nBy blocking traffic at the ALB level using AWS WAF, the traffic doesn't reach the application, reducing the load on your authentication service.\nThe rate-based rule can automatically adjust to changing patterns of attack without manual updates, providing an efficient solution.\nAWS WAF is designed for web application protection and allows you to create flexible rules to mitigate various types of attacks, making it a suitable choice for handling this scenario."
      },
      {
        "date": "2023-11-22T18:48:00.000Z",
        "voteCount": 3,
        "content": "Using WAF with ALB is most operationally efficient. This narrows the choices down to B and D. As IP address keeps changing B is most efficient."
      },
      {
        "date": "2023-10-30T22:58:00.000Z",
        "voteCount": 2,
        "content": "The application should be used by users \"around the world\" so policies that IP based are not suitable, as you have to update set of new IPs each week. \nOption B has valid actions, as WAP webACL has rate-basted rule and Block Action."
      },
      {
        "date": "2023-07-22T13:05:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-06T20:43:00.000Z",
        "voteCount": 1,
        "content": "easyu B"
      },
      {
        "date": "2023-07-05T14:54:00.000Z",
        "voteCount": 1,
        "content": "B, if login hit at a certain ratio, block this IP"
      },
      {
        "date": "2023-07-02T18:33:00.000Z",
        "voteCount": 2,
        "content": "B and not D because of \"500 different IP addresses that change each week\""
      },
      {
        "date": "2023-06-26T15:40:00.000Z",
        "voteCount": 3,
        "content": "B and not D because of \"500 different IP addresses that change each week\""
      },
      {
        "date": "2023-06-26T07:12:00.000Z",
        "voteCount": 1,
        "content": "b-b-b-b-b-b"
      },
      {
        "date": "2023-06-24T01:26:00.000Z",
        "voteCount": 1,
        "content": "yep, it's B"
      },
      {
        "date": "2023-06-23T02:02:00.000Z",
        "voteCount": 2,
        "content": "B Is Correct.\nSince IP address keeps changing, WAF can't block on IP/CIDR."
      },
      {
        "date": "2023-06-22T14:30:00.000Z",
        "voteCount": 3,
        "content": "B is the answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/amazon/view/113047-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the SFTP endpoint IP addresses are not permitted.<br><br>The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALStore the files in attached Amazon Elastic Block Store (Amazon EBS) volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the customer-owned block of IP addresses in the company\u2019s AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T18:52:00.000Z",
        "voteCount": 2,
        "content": "Option A is the only possible option."
      },
      {
        "date": "2023-10-30T22:52:00.000Z",
        "voteCount": 2,
        "content": "Option A is valid\nOption D: S3 doen't have support for SFTP option -&gt; out\nB, C: using EC2 to host FTP (not SFTP) while there is a native soltion in option A -&gt; out"
      },
      {
        "date": "2023-08-28T01:04:00.000Z",
        "voteCount": 4,
        "content": "should use AWS Transfer for SFTP"
      },
      {
        "date": "2023-07-30T15:57:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp-servers/"
      },
      {
        "date": "2023-07-22T13:04:00.000Z",
        "voteCount": 1,
        "content": "Correct A."
      },
      {
        "date": "2023-07-09T04:06:00.000Z",
        "voteCount": 1,
        "content": "it's A"
      },
      {
        "date": "2023-07-07T03:21:00.000Z",
        "voteCount": 1,
        "content": "D is too manual"
      },
      {
        "date": "2023-07-06T20:55:00.000Z",
        "voteCount": 1,
        "content": "its an A"
      },
      {
        "date": "2023-06-26T15:43:00.000Z",
        "voteCount": 2,
        "content": "A - AWS Managed SFTP"
      },
      {
        "date": "2023-06-24T07:09:00.000Z",
        "voteCount": 2,
        "content": "AWS Transfer for SFTP, fully managed service, no operational overhead"
      },
      {
        "date": "2023-06-23T19:57:00.000Z",
        "voteCount": 3,
        "content": "Option A suggests using AWS Transfer for SFTP, which is a fully managed service that enables the transfer of files over the Secure File Transfer Protocol (SFTP) directly into and out of Amazon S3. By registering the customer-owned block of IP addresses in the company's AWS account and creating Elastic IP addresses from that address pool, the company can assign those IP addresses to an AWS Transfer for SFTP endpoint. This allows the customers to continue using their existing firewall allow lists without requiring any changes. The files transferred through the SFTP endpoints are stored directly in Amazon S3, reducing operational overhead."
      },
      {
        "date": "2023-06-23T19:29:00.000Z",
        "voteCount": 2,
        "content": "AWS Transfer Family provides fully managed support for Secure File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP). AWS Transfer Family provides a seamless migration experience while preserving authentications and security policies, and it can handle the scale of demanding file transfer workloads. The file transfer can be stored directly into Amazon S3 or Amazon EFS."
      },
      {
        "date": "2023-06-23T00:35:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/amazon/view/113020-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T06:50:00.000Z",
        "voteCount": 1,
        "content": "typical cluster placement group use case"
      },
      {
        "date": "2023-11-22T18:57:00.000Z",
        "voteCount": 4,
        "content": "A is the only option."
      },
      {
        "date": "2023-07-06T20:56:00.000Z",
        "voteCount": 1,
        "content": "easy A"
      },
      {
        "date": "2023-06-26T15:47:00.000Z",
        "voteCount": 4,
        "content": "A - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
      },
      {
        "date": "2023-06-23T19:58:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-06-23T19:33:00.000Z",
        "voteCount": 2,
        "content": "A cluster placement group is a type of placement group that packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of high performance computing (HPC) applications."
      },
      {
        "date": "2023-06-23T01:58:00.000Z",
        "voteCount": 1,
        "content": "A- Provides Low latency and high throughput.\nAuto scaling with additional ENI, spread placement and partition placement won't achieve the requirement."
      },
      {
        "date": "2023-06-22T14:35:00.000Z",
        "voteCount": 3,
        "content": "A - Cluster placement group\nC is incorrect because Partition placement groups are used for large distributed workloads, like Hadoop, Cassandra, and Kafka. They do not offer the same low-latency, high-throughput benefits as cluster placement groups."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/amazon/view/113147-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.<br><br>After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.<br><br>Which approach should the company take to secure its API?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T21:39:00.000Z",
        "voteCount": 15,
        "content": "Ans is Opt D,  A usage plan provides select customers with specific access permissions and request quotas, which helps manage and restrict usage to prevent overuse of resources.\nAPI keys are used for tracking and controlling how the API is used. This additional layer of security ensures that only those with the key can access the API.\nWhy not Opt C, Amazon API Gateway doesn't support request limiting through resource policies. You can set permissions on who can access your API using a resource policy, but rate limiting isn't handled by resource policies.\nAPI keys alone do not provide throttling or rate limiting. For throttling, you typically would need to use them along with usage plans"
      },
      {
        "date": "2024-01-20T10:12:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html"
      },
      {
        "date": "2023-12-12T08:12:00.000Z",
        "voteCount": 1,
        "content": "c ANS\nC. WAF with IP Filtering and Resource Policy:\n\nPros:\nSimple and cost-effective solution.\nWAF rules and resource policy restrict access.\nCons:\nIP filtering might not be effective if partners use dynamic IP addresses.\nResource policy request limit applies to all methods, not just POST."
      },
      {
        "date": "2023-11-22T19:03:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-08-22T12:52:00.000Z",
        "voteCount": 1,
        "content": "def answ D as described here \nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html"
      },
      {
        "date": "2023-07-22T13:00:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-06T20:59:00.000Z",
        "voteCount": 1,
        "content": "D fits"
      },
      {
        "date": "2023-07-05T15:02:00.000Z",
        "voteCount": 1,
        "content": "Amazon API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM role or group) can invoke the API. You can use API Gateway resource policies to allow your API to be securely invoked by:\n\nUsers from a specified AWS account.\n\nSpecified source IP address ranges or CIDR blocks.\n\nSpecified virtual private clouds (VPCs) or VPC endpoints (in any account)."
      },
      {
        "date": "2023-06-26T15:57:00.000Z",
        "voteCount": 3,
        "content": "It's D. The IP filtering is done with the WAF ACL so there is no need to do another IP filtering by using resource policies which can do exactly that. https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html"
      },
      {
        "date": "2023-06-26T07:52:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-d-d"
      },
      {
        "date": "2023-06-25T13:00:00.000Z",
        "voteCount": 2,
        "content": "D is classic use of \"usage plan\" in API Gateway addicionally more apropiate practice is API Key for autentication or other methos"
      },
      {
        "date": "2023-06-24T23:05:00.000Z",
        "voteCount": 2,
        "content": "I vote for D since I couldn't find a way to set up a request limit in resource policy\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html"
      },
      {
        "date": "2023-06-23T20:02:00.000Z",
        "voteCount": 1,
        "content": "Option C provides a cost-effective approach to securing the API while allowing access only to the IP addresses used by the six partners. By creating an AWS WAF web ACL and configuring it to allow access only to the IP addresses of the trusted partners, the company can effectively block requests originating from unauthorized sources. Associating the web ACL with the API ensures that the filtering rules are applied to the API traffic.\n\nAdditionally, creating a resource policy with a request limit allows the company to set a maximum limit on the number of requests that can be made to the API within a given time frame. This helps mitigate the impact of potential botnet traffic, ensuring that the API is not overwhelmed with excessive requests.\n\nRequiring an API key on the POST method adds an extra layer of security by enforcing authentication for accessing the API. This ensures that only authorized partners with valid API keys can successfully make requests to the API."
      },
      {
        "date": "2023-06-23T19:45:00.000Z",
        "voteCount": 2,
        "content": "GPT 4.0: AWS WAF is a web application firewall that lets you monitor HTTP and HTTPS requests that are forwarded to Amazon API Gateway. The solution architect can create a WAF rule that allows access only from the IP addresses of the six partners.\nA usage plan in API Gateway provides throttling and quota limits to manage the rate of requests from your customers and prevent attacks. Setting a request limit that matches the expected usage of the partners would help to protect the API."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/amazon/view/113022-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.<br><br>Which solution will achieve this goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T01:38:00.000Z",
        "voteCount": 5,
        "content": "C achieves the Goal.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Monitoring.html"
      },
      {
        "date": "2024-10-08T09:13:00.000Z",
        "voteCount": 1,
        "content": "For those who think the correct option is B: \"The Aurora DB cluster sends activities to an Amazon Kinesis data stream in near real time.\" It does NOT send to EventBridge.\nhttps://docs.aws.amazon.com/pt_br/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.html"
      },
      {
        "date": "2024-01-03T19:31:00.000Z",
        "voteCount": 1,
        "content": "C seems correct. but why not B?"
      },
      {
        "date": "2023-12-08T09:14:00.000Z",
        "voteCount": 2,
        "content": "B is ans :\nHere's why this solution is the most suitable:\n\nDirect integration: Database activity streams natively integrate with EventBridge, streamlining the process of capturing and routing events.\nRich event filtering: EventBridge offers powerful filtering capabilities, allowing the database team to selectively monitor specific events or patterns of interest.\nFlexible delivery: EventBridge can trigger various targets, including Lambda functions, which provide the ability to process and store events in S3 for further analysis.\nServerless architecture: Lambda functions eliminate the need to manage servers, reducing operational overhead and scaling automatically to handle event volume.\nCost-effective storage: S3 offers durable and cost-effective storage for long-term analysis of database activity logs."
      },
      {
        "date": "2024-08-18T19:20:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2023-11-22T19:05:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-22T12:28:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-06T21:01:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-06-26T16:00:00.000Z",
        "voteCount": 2,
        "content": "C - Correct. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Monitoring.html"
      },
      {
        "date": "2023-06-25T13:01:00.000Z",
        "voteCount": 1,
        "content": "C achieves the Goal.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Monitoring.html"
      },
      {
        "date": "2023-06-23T21:41:00.000Z",
        "voteCount": 1,
        "content": "C indeed"
      },
      {
        "date": "2023-06-23T19:47:00.000Z",
        "voteCount": 4,
        "content": "GPT: Option A and D are incorrect because AWS DMS's Change Data Capture (CDC) functionality captures changes made at the database level, not data activity."
      },
      {
        "date": "2023-06-23T01:04:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2023-06-22T14:47:00.000Z",
        "voteCount": 1,
        "content": "I go with C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/amazon/view/113023-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company deployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's operations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.<br><br>Analysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company expected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that monitors the CPU and memory consumption to dynamically scale the instance fleet. A solutions architect needs to configure the Auto Scaling group to meet demand in the most cost-effective way.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T15:01:00.000Z",
        "voteCount": 10,
        "content": "C . From the question, app is running on memory-optimized instances (r6g.16xlarge) but only utilizing about one quarter of the CPU and memory. So cost-effective to use smaller instances (r6g.4xlarge), which provide a quarter of r6g.16xlarge instances."
      },
      {
        "date": "2023-06-24T14:52:00.000Z",
        "voteCount": 6,
        "content": "16large = 64CPU, \n4Large = 16 CPU\n8Large = 32 CPU\n\u00bc usage of 64 = 16CPU\n\u00bc of 12 EC2 = 3 instance, so C is a better choice."
      },
      {
        "date": "2024-01-27T18:56:00.000Z",
        "voteCount": 1,
        "content": "In regards with Efficiency vs. Headroom: I would choose D over C because there will be less headroom during peak loads."
      },
      {
        "date": "2024-08-18T19:22:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2023-12-14T22:19:00.000Z",
        "voteCount": 1,
        "content": "SORRY c ANS\nr6g.8xlarge\n\nUpfront cost\n0.00 USD\nMonthly cost\n1,248.01 USD\nTotal 12 months cost\n14,976.12 USD\n\n r6g.4xlarge \n\n624.00 USD\n\nTotal 12 months cost\n7,488.00 USD\nhttps://calculator.aws/#/estimate"
      },
      {
        "date": "2023-12-14T22:00:00.000Z",
        "voteCount": 2,
        "content": "I would suggest that option B is the most cost-effective solution that meets the requirements. It uses m6g.4xlarge instances, which are general purpose instances powered by Arm-based AWS Graviton2 processors. These instances offer a balance of compute, memory, and networking resources, and deliver up to 40% better price performance than comparable current generation x86-based instances5 This option can also reduce the number of instances needed to meet the demand, as each m6g.4xlarge instance has 16 vCPUs and 64 GiB of memory, which is equivalent to one quarter of the resources of an r6g.16xlarge instance. This option can also leverage the existing Network Load Balancer and CloudWatch metrics to monitor and distribute the traffic across the instances."
      },
      {
        "date": "2023-12-13T03:26:00.000Z",
        "voteCount": 1,
        "content": "Option D, using r6g.8xlarge instances with a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6, is the most cost-effective solution for this scenario. Here's why:\n\nCost reduction: Lower instance size and smaller fleet size significantly reduce cost compared to the current configuration.\nBalanced memory and cost: R6g.8xlarge still provides sufficient memory for current demand while being cheaper than r6g.16xlarge.\nScalability for peak demand: Doubling the capacity up to 6 instances can cater to potential player spikes while remaining within a controlled budget."
      },
      {
        "date": "2023-11-22T19:14:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-11-18T23:07:00.000Z",
        "voteCount": 1,
        "content": "see  Maria2023's answer"
      },
      {
        "date": "2023-08-09T15:18:00.000Z",
        "voteCount": 1,
        "content": "Initially I was thinking on how the ASG would handle the spikes knowing that each r6g.4xlarge might have troubles handle the load, but the question is to handle the demand in the most cost-effective way.\n\nIn terms of cost, Maria2023 and Nexus2020 made a point that can't be beaten here.\n\nI am still thinking on the load, but if there is something I am learning with these questions is that many of them won't give you enough to make a REAL informed decision, so you should go with your best judgement."
      },
      {
        "date": "2023-07-22T12:20:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-07T10:40:00.000Z",
        "voteCount": 1,
        "content": "C I guess. weird question"
      },
      {
        "date": "2023-06-26T16:03:00.000Z",
        "voteCount": 1,
        "content": "C makes most sense."
      },
      {
        "date": "2023-06-25T00:29:00.000Z",
        "voteCount": 3,
        "content": "1 r6g.4xlarge - $0.8064/h\n1 r6g.8xlarge - $1.6128/h\nDuring peak times both C and D will cost 9.6768/h\nHowever, during non-peak times, C will cost less - 2.4192/h vs 3.2256\nPlus that I think D will be a bit underutilized most of the times if the trends remain the same"
      },
      {
        "date": "2023-06-23T21:48:00.000Z",
        "voteCount": 1,
        "content": "Memory optimized and cost optimized"
      },
      {
        "date": "2023-06-23T20:09:00.000Z",
        "voteCount": 1,
        "content": "The company initially deployed 12 r6g.16xlarge instances but found that the consumption was much lower than expected. To optimize cost, it is necessary to scale down the instance type while still meeting the demand.\n\nOption D suggests configuring the Auto Scaling group to use r6g.8xlarge instances, which have less memory capacity compared to r6g.16xlarge instances. With a minimum capacity of 2, desired capacity of 2, and maximum capacity of 6, the Auto Scaling group will scale up or down based on CPU and memory utilization."
      },
      {
        "date": "2023-06-23T19:55:00.000Z",
        "voteCount": 1,
        "content": "The requirements state that the current set of instances (r6g.16xlarge - memory optimized) are only using about a quarter of the available CPU and memory. Therefore, a smaller instance size would be more cost-effective while still meeting the demand. In this case, the r6g.4xlarge instances would be appropriate, as they are a quarter of the size of the currently used instances (r6g.16xlarge)."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/amazon/view/113025-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.<br><br>Which strategy should a solutions architect recommend to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon ElastiCache cluster in front of the DynamoDB table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse provisioned capacity mode. Purchase Savings Plans in Cost Explorer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-09T15:29:00.000Z",
        "voteCount": 9,
        "content": "It's D. Purchase Savings Plans in Cost Explorer is not for DynamoDB. At least not today."
      },
      {
        "date": "2023-07-30T06:07:00.000Z",
        "voteCount": 9,
        "content": "Repeated lookups = DAX\nAvoid bursts = Provisioned Capacity"
      },
      {
        "date": "2024-03-19T09:35:00.000Z",
        "voteCount": 1,
        "content": "DAX is more expensive than Elasticache. Therefore, A."
      },
      {
        "date": "2024-08-18T19:23:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2023-12-09T01:30:00.000Z",
        "voteCount": 6,
        "content": "DynamoDB Accelerator (DAX): This in-memory cache reduces read latency and improves read throughput for frequently accessed data. Since the application has burst read activity and repeatedly accesses a limited set of keys, DAX can significantly improve performance and reduce costs associated with read throughput on DynamoDB.\nProvisioned capacity mode: While on-demand capacity mode eliminates the need for upfront planning, it can be costly for applications with predictable workloads. Provisioned capacity allows for better cost optimization and predictability by specifying the minimum and maximum capacity required throughout the day.\nDynamoDB auto scaling: This feature automatically adjusts provisioned capacity based on actual usage patterns. This ensures that the table has sufficient capacity during peak hours while avoiding wasted resources during off-peak periods, further reducing costs."
      },
      {
        "date": "2023-11-22T19:19:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2023-07-26T21:17:00.000Z",
        "voteCount": 1,
        "content": "Did you read the question?\nTo reduce costs you can use DAX.\nhttps://aws.amazon.com/dynamodb/dax/\nHere is nothing in the question about saving plans or else."
      },
      {
        "date": "2023-07-26T22:39:00.000Z",
        "voteCount": 1,
        "content": "I now switch to D, because it's an expeded workload."
      },
      {
        "date": "2023-07-30T11:44:00.000Z",
        "voteCount": 2,
        "content": "looool"
      },
      {
        "date": "2023-07-22T12:14:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-16T08:43:00.000Z",
        "voteCount": 3,
        "content": "The D looks like a perfect solution. But the question is only asking to reduce the cost, so I would like to choose C instead."
      },
      {
        "date": "2024-07-27T04:09:00.000Z",
        "voteCount": 1,
        "content": "Saving plans are not applicable  to dynamodb. https://aws.amazon.com/savingsplans/"
      },
      {
        "date": "2023-07-30T11:45:00.000Z",
        "voteCount": 1,
        "content": "what about caching?"
      },
      {
        "date": "2023-07-11T17:20:00.000Z",
        "voteCount": 1,
        "content": "Isn't DAX extremely expensive? Weird question."
      },
      {
        "date": "2023-07-07T10:42:00.000Z",
        "voteCount": 1,
        "content": "its a D"
      },
      {
        "date": "2023-07-05T15:10:00.000Z",
        "voteCount": 1,
        "content": "DAX + Provision Capactiy + Auto Scaling meets the need"
      },
      {
        "date": "2023-06-26T16:10:00.000Z",
        "voteCount": 4,
        "content": "Savings plan is for EC2, B and C are out. A is for read boost. D is correct.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual"
      },
      {
        "date": "2023-06-24T15:03:00.000Z",
        "voteCount": 1,
        "content": "DynamoDB Accelerator (DAX) is an in-memory caching service provided by AWS that is specifically designed to enhance the performance of Amazon DynamoDB. It acts as a caching layer between your application and DynamoDB, reducing the need to directly access the DynamoDB service for frequently accessed data.\n\nD!"
      },
      {
        "date": "2023-06-23T21:53:00.000Z",
        "voteCount": 2,
        "content": "DAX + Provision Capactiy + Auto Scaling meets the need"
      },
      {
        "date": "2023-06-23T19:59:00.000Z",
        "voteCount": 1,
        "content": "Deploying DynamoDB Accelerator (DAX) will help in caching read activity, which can reduce the read cost because DAX is a fully managed, highly available, in-memory cache for DynamoDB that can improve the read performance by up to 10 times, even at millions of requests per second.\n\nThe use of provisioned capacity mode allows you to set the capacity for your table to handle expected workloads, and the table's capacity will not scale up and down based on traffic patterns, which could potentially reduce cost when compared to on-demand capacity mode if your usage is predictable."
      },
      {
        "date": "2023-06-22T23:43:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/80440-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-06-22T15:10:00.000Z",
        "voteCount": 1,
        "content": "D provisioned capacity mode. \nAs per charGPT  company is currently using on-demand capacity mode. On-demand capacity mode is priced higher than provisioned capacity mode because it automatically accommodates your workload's capacity needs based on the volume of reads and writes your application performs. For workloads with predictable capacity needs, provisioned capacity mode can be more cost effective."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/amazon/view/113152-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service.<br><br>In each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.<br><br>Which combination of steps should a solutions architect take to resolve this issue? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-08T05:38:00.000Z",
        "voteCount": 12,
        "content": "When you associate a Network Load Balancer with an endpoint service, the Network Load Balancer forwards requests to the registered target. The requests are forwarded as if the target was registered by IP address. In this case, the source IP addresses are the private IP addresses of the load balancer nodes. If you have access to the Amazon VPC endpoint service, then verify that:\n\n    The Inbound security group rules of the Network Load Balancer\u2019s targets allow communication from the private IP address of the Network Load Balancer nodes\n    The rules within the network ACL associated with the Network Load Balancer\u2019s targets allow communication from the private IP address of the Network Load Balancer nodes\n\nhttps://repost.aws/knowledge-center/security-network-acl-vpc-endpoint"
      },
      {
        "date": "2024-05-09T09:04:00.000Z",
        "voteCount": 2,
        "content": "A and C.\nhttps://repost.aws/knowledge-center/security-network-acl-vpc-endpoint"
      },
      {
        "date": "2024-05-07T01:21:00.000Z",
        "voteCount": 3,
        "content": "A and C.\nThe flow is:\nApplication -&gt; NLB -&gt; Logging Monitor Tool.\nSo we need to check NACL of NLB subnets (in and out from applications client and in and out to EC2 subnet) and Security group (Statefull, so only ingress) of EC2 Instances of Logging Monitor Tool."
      },
      {
        "date": "2024-04-29T18:56:00.000Z",
        "voteCount": 2,
        "content": "B. Network Access Control Lists (NACLs) act as a firewall at the subnet level. To ensure communication between the interface endpoint subnets and the logging service subnets running on EC2 instances, the NACLs attached to both subnets should be configured to allow the necessary traffic.\n\nD. Security groups act as virtual firewalls at the instance level. To allow clients to submit logs to the logging service running on EC2 instances, the security group associated with the EC2 instances should be configured to allow ingress traffic from the clients' IP addresses or security groups."
      },
      {
        "date": "2024-02-09T08:02:00.000Z",
        "voteCount": 3,
        "content": "CE: we only need to allow access from client -&gt; NLB -&gt; application"
      },
      {
        "date": "2024-01-03T21:09:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D are correct answers. Rational:\n\nEC2s and NLB are both in one subnet, so the NACL is associated with one subnet and there is no NACL which controls EC2 and NLB communication --&gt; A is not Valid, C is not Valid.\n\nSecurity groups are attached to EC2s --&gt; E is not Valid"
      },
      {
        "date": "2024-04-29T20:29:00.000Z",
        "voteCount": 1,
        "content": "The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets."
      },
      {
        "date": "2023-12-16T10:12:00.000Z",
        "voteCount": 1,
        "content": "guys .pls B,E ans \ne:-\nThe Inbound security group rules of the Network Load Balancer\u2019s targets allow communication from the private IP address of the Network Load Balancer nodes"
      },
      {
        "date": "2023-12-08T09:54:00.000Z",
        "voteCount": 2,
        "content": "CE is ans\nThe clients are trying to connect to the logging service through the NLB.\nThe NLB needs to forward the requests to the EC2 instances running the logging service.\nTherefore, both the NLB and the EC2 instances need to have security group rules allowing inbound traffic from each other's subnets."
      },
      {
        "date": "2023-12-06T08:41:00.000Z",
        "voteCount": 3,
        "content": "Link below seems to confirm it.  The focus is on the Provider VPC so the question wasn't really that clear.  \n\nhttps://repost.aws/knowledge-center/security-network-acl-vpc-endpoint"
      },
      {
        "date": "2023-11-22T19:36:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2023-11-19T00:09:00.000Z",
        "voteCount": 1,
        "content": "see magmichal05's answer"
      },
      {
        "date": "2023-10-18T15:03:00.000Z",
        "voteCount": 1,
        "content": "B is pretty clear plus E is valid as well since AWS has introduced support for associating security groups with Network Load Balancers (NLBs)."
      },
      {
        "date": "2023-10-15T13:56:00.000Z",
        "voteCount": 1,
        "content": "AC - NLB needs to be allowed to the instances otherwise targets are unhealthy"
      },
      {
        "date": "2023-09-02T22:45:00.000Z",
        "voteCount": 3,
        "content": "AC\n3rd point on https://docs.aws.amazon.com/vpc/latest/privatelink/create-endpoint-service.html#considerations-endpoint-services"
      },
      {
        "date": "2023-08-18T09:29:00.000Z",
        "voteCount": 4,
        "content": "https://www.examtopics.com/discussions/amazon/view/36058-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-07-26T21:29:00.000Z",
        "voteCount": 4,
        "content": "B and C.\nThe NLB is places in the destination Account. That means the EC2 logging instance get traffic from the NLB.\nSo the source for the Logging EC2 instance must be the NLB.\nhttps://aws.amazon.com/de/blogs/architecture/building-saas-services-for-aws-customers-with-privatelink/\nOld but not outdated"
      },
      {
        "date": "2023-07-19T16:33:00.000Z",
        "voteCount": 4,
        "content": "When service consumers send traffic to a service through an interface endpoint, the source IP addresses provided to the application are the private IP addresses of the load balancer nodes, not the IP addresses of the service consumers.\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/create-endpoint-service.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/amazon/view/113153-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).<br><br>A solutions architect reviews the company\u2019s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T07:14:00.000Z",
        "voteCount": 11,
        "content": "This option switches the encryption method from using AWS Key Management Service (AWS KMS) to using server-side encryption with S3 managed keys (SSE-S3). This change can significantly reduce costs because AWS KMS charges per API request, while SSE-S3 does not have additional charges per API request beyond the S3 usage."
      },
      {
        "date": "2024-03-19T08:59:00.000Z",
        "voteCount": 1,
        "content": "100% B"
      },
      {
        "date": "2023-12-21T00:36:00.000Z",
        "voteCount": 3,
        "content": "Bucket key would have been an option here but it is not in the answers."
      },
      {
        "date": "2023-11-22T19:39:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-08-27T02:35:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-bucket-keys-reduce-the-costs-of-server-side-encryption-with-aws-key-management-service-sse-kms/"
      },
      {
        "date": "2023-07-26T21:41:00.000Z",
        "voteCount": 2,
        "content": "B...\nBecause SSE-S3 has no additional costs.\nSSE-C cost per month 0,00040 USD per GB encrypted Data on Top"
      },
      {
        "date": "2023-07-22T11:47:00.000Z",
        "voteCount": 2,
        "content": "Correct B."
      },
      {
        "date": "2023-07-08T08:29:00.000Z",
        "voteCount": 1,
        "content": "this is B"
      },
      {
        "date": "2023-07-07T11:09:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-26T16:34:00.000Z",
        "voteCount": 1,
        "content": "None of this is correct. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html, but let's go with B."
      },
      {
        "date": "2023-06-25T01:27:00.000Z",
        "voteCount": 1,
        "content": "I would actually expect an option with a bucket key as a possible answer since that's the purpose of it. From the available choices, I choose B."
      },
      {
        "date": "2023-06-24T05:34:00.000Z",
        "voteCount": 4,
        "content": "By choosing option B, you can switch the encryption type from SSE-KMS to SSE-S3, which eliminates the need for AWS KMS requests, thereby reducing the associated costs. This solution requires minimal changes to the application and avoids additional operational overhead."
      },
      {
        "date": "2023-06-24T04:56:00.000Z",
        "voteCount": 3,
        "content": "The goal here is to reduce the cost related to the usage of AWS KMS keys for server-side encryption. Using SSE-S3, which uses Amazon S3 managed keys for server-side encryption, would eliminate the additional cost related to KMS key usage while still maintaining a high level of security. Amazon S3 handles key management, which also reduces operational overhead. S3 Batch Operations can be used to efficiently copy the existing objects to the new bucket."
      },
      {
        "date": "2023-06-24T02:13:00.000Z",
        "voteCount": 2,
        "content": "B, SSE-S3 does not incur additional costs."
      },
      {
        "date": "2023-06-23T23:01:00.000Z",
        "voteCount": 1,
        "content": "B is the least operational overhead"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/amazon/view/113105-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon DynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and find that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.<br><br>Which combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvaluate and adjust the RCUs for the DynamoDB tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvaluate and adjust the WCUs for the DynamoDB tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon ElastiCache layer to increase the performance of Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Transfer Acceleration to provide lower latency to users."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T16:37:00.000Z",
        "voteCount": 5,
        "content": "B - because \"performance of DynamoDB when data is saved.\"\nD - you need a queue to slow things down and not loose any uploads"
      },
      {
        "date": "2023-12-11T09:29:00.000Z",
        "voteCount": 2,
        "content": "D. Add an Amazon SQS queue and reprocessing logic between Amazon S3 and the Lambda functions. This decouples photo upload from processing, prevents Lambda overload, and offers retry capabilities.\nA. Evaluate and adjust the RCUs for the DynamoDB tables. This ensures sufficient read capacity for application state retrieval without overspending on unused capacity."
      },
      {
        "date": "2023-11-22T19:47:00.000Z",
        "voteCount": 2,
        "content": "option B and D"
      },
      {
        "date": "2023-07-22T11:46:00.000Z",
        "voteCount": 1,
        "content": "Correct BD"
      },
      {
        "date": "2023-07-07T14:29:00.000Z",
        "voteCount": 1,
        "content": "BD for sure"
      },
      {
        "date": "2023-06-24T07:18:00.000Z",
        "voteCount": 2,
        "content": "SQS and write to DDB."
      },
      {
        "date": "2023-06-24T05:01:00.000Z",
        "voteCount": 2,
        "content": "Adding an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions will help to decouple the Lambda functions from the S3 events and allow the Lambda functions to process photos in batches. This will help to improve the performance of the Lambda functions and reduce the risk of photos not being processed properly.\nEvaluating and adjusting the WCUs for the DynamoDB tables will help to improve the performance of the DynamoDB tables when data is saved. This will help to reduce the risk of Lambda functions experiencing errors when saving data to DynamoDB."
      },
      {
        "date": "2023-06-24T02:20:00.000Z",
        "voteCount": 1,
        "content": "B and D, I think"
      },
      {
        "date": "2023-06-23T23:35:00.000Z",
        "voteCount": 1,
        "content": "WCU &amp; SQS will solve the issue"
      },
      {
        "date": "2023-06-23T08:44:00.000Z",
        "voteCount": 4,
        "content": "B -Ques says app has performance issues when data is SAVED. So this is a write. So increase WCU. \nD- can help decouple"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/amazon/view/113108-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a file server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The company frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.<br><br>Users from across the United States and Canada access the application. Only authenticated users should have the ability to access the application to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application development.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-13T00:39:00.000Z",
        "voteCount": 10,
        "content": "The solution described in Option D leverages AWS Amplify to create a serverless and scalable architecture for media file uploads. Amplify provides an easier development experience and supports integration with Amazon S3 for file storage and Amazon Cognito for user authentication. Hosting the website through Amazon CloudFront ensures low-latency access for users across the United States and Canada. This solution minimizes operational overhead and accelerates application development.\nThis blogpost contains a description of a similar use case:\nhttps://aws.amazon.com/ru/blogs/compute/lifting-and-shifting-a-web-application-to-aws-serverless-part-2/"
      },
      {
        "date": "2023-11-22T20:02:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-10-01T02:49:00.000Z",
        "voteCount": 3,
        "content": "Option D (using AWS Amplify, CloudFront, S3, and Cognito) seems like the best choice. It provides a streamlined development process while ensuring scalability, reliability, and user authentication."
      },
      {
        "date": "2023-07-22T11:40:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-11T17:31:00.000Z",
        "voteCount": 4,
        "content": "Why not C?"
      },
      {
        "date": "2023-07-07T14:33:00.000Z",
        "voteCount": 2,
        "content": "its a D"
      },
      {
        "date": "2023-07-05T15:24:00.000Z",
        "voteCount": 4,
        "content": "key words: \"development\"\n\nAWS Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, and host full-stack applications on AWS"
      },
      {
        "date": "2023-06-26T16:39:00.000Z",
        "voteCount": 2,
        "content": "D - https://aws.amazon.com/amplify/"
      },
      {
        "date": "2023-06-25T05:36:00.000Z",
        "voteCount": 2,
        "content": "LEAST operational overhead"
      },
      {
        "date": "2023-06-25T03:39:00.000Z",
        "voteCount": 4,
        "content": "Option D leverages AWS Amplify, a development platform, to create a static website for uploading media files. Amplify simplifies the process of building and deploying web applications. With Amplify Hosting, the website can be easily served through Amazon CloudFront, which provides low-latency content delivery.\nAmazon S3 is used to store the uploaded media files. S3 is a highly scalable and durable object storage service that can handle large amounts of data. It provides secure storage for the files and allows easy integration with other AWS services.\n\nThis solution requires minimal operational overhead as AWS Amplify abstracts away much of the underlying infrastructure setup and configuration. It enables faster application development and deployment while providing scalability, security, and authentication features needed for the requirements of the application."
      },
      {
        "date": "2023-06-25T01:46:00.000Z",
        "voteCount": 2,
        "content": "Think the key here is this requirement \"accelerate application development.\" Which is one of the things Amplify does"
      },
      {
        "date": "2023-06-24T22:43:00.000Z",
        "voteCount": 1,
        "content": "solution will meet these requirements with the LEAST operational overhead and the company will consider a solution that refactors the application.\nwith those info, I think D is the answer"
      },
      {
        "date": "2023-06-24T07:22:00.000Z",
        "voteCount": 1,
        "content": "AWS Amplify simplifies the process of building, deploying, and hosting web applications, providing a streamlined way to create a new application that would address the company's needs. Amplify Hosting provides fast, global hosting for the static website. Plus S3"
      },
      {
        "date": "2023-06-23T23:46:00.000Z",
        "voteCount": 3,
        "content": "A is least operational overhead. \nD is lot of work upfront"
      },
      {
        "date": "2023-06-23T08:52:00.000Z",
        "voteCount": 1,
        "content": "D aws amplify facilitates the building, deploying, and hosting of the web application. It integrates with Amazon CloudFront for global content delivery and Amazon S3 for file storage"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/amazon/view/113109-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an Auto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company\u2019s development team wants to analyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.<br><br>Which solution will give the development team the ability to view the application logs after a scale-in event?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable access logs for the ALB. Store the logs in an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Auto Scaling group to use a step scaling policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the application with AWS X-Ray tracing."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T08:58:00.000Z",
        "voteCount": 5,
        "content": "B is correct\nOption A - ALB access logs only has details about requests sent to the load balancer, not application\nOption C - change autosclaing behavior would NOT address the problem\nOption D  AWS X-Ray is more suitable for tracing requests as they travel through your application, It doesn't store output logs from your application."
      },
      {
        "date": "2023-11-22T20:03:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-10-13T00:42:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html"
      },
      {
        "date": "2023-07-22T11:35:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-07T14:34:00.000Z",
        "voteCount": 1,
        "content": "easy B"
      },
      {
        "date": "2023-06-26T16:45:00.000Z",
        "voteCount": 1,
        "content": "B - custom logs"
      },
      {
        "date": "2023-06-24T07:24:00.000Z",
        "voteCount": 2,
        "content": "The question states that the development team wants to analyze application logs, and these logs disappear after EC2 instances scale in. To solve this, you can configure the EC2 instances to send their logs to Amazon CloudWatch Logs using the unified CloudWatch agent. This allows you to keep the logs for a longer time period and enables the development team to analyze them at any time, even after the instances have been terminated."
      },
      {
        "date": "2023-06-23T23:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct indeed"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/amazon/view/113184-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the website calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an external API call.<br><br>During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set to www.example.com.<br><br>What should the solutions architect do to resolve the error?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T07:29:00.000Z",
        "voteCount": 8,
        "content": "Cross-Origin Resource Sharing (CORS) is a security measure that allows or denies scripts on webpages from making requests to a different domain than the one the script came from. The CORS policy is configured on the server side, and servers use the Access-Control-Allow-Origin header to tell the browser which domains are allowed to make requests.\n\nIn the scenario provided, the error message is likely occurring because the API Gateway API endpoint used by the static website is not configured to allow www.example.com as an origin for requests."
      },
      {
        "date": "2023-12-16T09:19:00.000Z",
        "voteCount": 3,
        "content": "C : ans https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html"
      },
      {
        "date": "2023-11-22T20:07:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-11-19T01:19:00.000Z",
        "voteCount": 3,
        "content": "we call API Gateway endpoint from a different origin, API Gateway should be able to verify that request comes from the verified origin, hence you should enable CORS in API Gateway and add your website origin to the list of verified origins."
      },
      {
        "date": "2023-07-22T11:32:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-11T17:39:00.000Z",
        "voteCount": 2,
        "content": "I guess it can't be D because lambda doesn't have a Cors setting. However, there are use-cases where you need to return the cors header inside the lambda return.\n\"Configure your REST API integrations to return the required CORS headers\n\nConfigure your backend AWS Lambda function or HTTP server to send the required CORS headers in its response. Keep in mind the following:\""
      },
      {
        "date": "2023-07-07T14:36:00.000Z",
        "voteCount": 1,
        "content": "eaasy C"
      },
      {
        "date": "2023-06-30T01:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-26T16:49:00.000Z",
        "voteCount": 3,
        "content": "C - use case -&gt; https://repost.aws/knowledge-center/api-gateway-cors-errors"
      },
      {
        "date": "2023-06-25T03:47:00.000Z",
        "voteCount": 4,
        "content": "In this case, when the registration form on the static website (hosted on Amazon S3) is submitted and makes a request to the API Gateway API endpoint, a CORS error occurs. This error indicates that the API response lacks the appropriate Access-Control-Allow-Origin header, which specifies the allowed origin domains for the response."
      },
      {
        "date": "2023-06-25T01:54:00.000Z",
        "voteCount": 1,
        "content": "I vote for A since I was not able for find an option to configure CORS on API gateway plus this information\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html"
      },
      {
        "date": "2023-06-30T01:21:00.000Z",
        "voteCount": 1,
        "content": "yes you can\n\nChoose the API:\nChoose the \"Resources\" option in the API Gateway console.\nIn the \"Resources\" pane, choose the resource you want to enable CORS for.\nChoose \"Actions\" -&gt; \"Enable CORS\".\n\nC is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/amazon/view/112796-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the company. The company has a Microsoft Azure Active Directory that is deployed.<br><br>A solutions architect needs to centralize billing and management of the company\u2019s AWS accounts. The company wants to start using identity federation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure each AWS account's email address to be aws+<account id=\"\">@example.com so that account management email messages and invoices are sent to the same place.\n\t\t\t\t\t\t\t\t\t\t</account>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 28,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T07:35:00.000Z",
        "voteCount": 13,
        "content": "Yes ACE - A for a new Management account: C for SSO; E for permissions to IAM"
      },
      {
        "date": "2023-06-28T13:39:00.000Z",
        "voteCount": 5,
        "content": "A) Creating a master account to manage organizations on AWS and invite them sounds like a good idea and is recommended.\nB) Has no sense\nC ) In AWS Single Sign On adding Azure AD as trust sounds like a good idea and it is the usual way to do it as well as creating users and groups\nD ) Create an AD in AWS and share it? it doesn't make sense because there already exists one in azure which we will use\nE ) Creating the corresponding permission set and attaching it to the groups that were created usually makes sense.\nF ) again an AD created in AWS is not necessary because it already exists in Azure and you do not want to have another one again"
      },
      {
        "date": "2023-11-24T07:56:00.000Z",
        "voteCount": 1,
        "content": "ACE make sense."
      },
      {
        "date": "2023-11-22T20:18:00.000Z",
        "voteCount": 1,
        "content": "A C and E options."
      },
      {
        "date": "2023-07-22T11:30:00.000Z",
        "voteCount": 1,
        "content": "Correct ACE."
      },
      {
        "date": "2023-07-08T02:38:00.000Z",
        "voteCount": 1,
        "content": "D must be wrong."
      },
      {
        "date": "2023-07-07T14:38:00.000Z",
        "voteCount": 2,
        "content": "ACE IT!"
      },
      {
        "date": "2023-07-06T02:33:00.000Z",
        "voteCount": 1,
        "content": "this question scored an ACE"
      },
      {
        "date": "2023-06-26T16:56:00.000Z",
        "voteCount": 1,
        "content": "ACE - Management account, AWS SSO with Azure AD and permission sets"
      },
      {
        "date": "2023-06-25T13:24:00.000Z",
        "voteCount": 1,
        "content": "Yes ACE - A for a new Management account: C for SSO; E for permissions to IAM"
      },
      {
        "date": "2023-06-24T02:33:00.000Z",
        "voteCount": 1,
        "content": "A, C and E"
      },
      {
        "date": "2023-06-23T04:21:00.000Z",
        "voteCount": 1,
        "content": "ACE is the right answer"
      },
      {
        "date": "2023-06-21T03:14:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is ACE"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/amazon/view/112797-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by migrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize costs while standardizing by using a single deployment methodology.<br><br>Most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. Average application memory consumption is less than 1 GB. though some applications use as much as 2.5 GB of memory during peak processing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for several hours.<br><br>Which is the MOST cost-effective solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-25T06:13:00.000Z",
        "voteCount": 18,
        "content": "Hours = lambda out\nReserve instance max size = D out\nC: beanstalk still use EC2, if beanstalk = each application, it could be each app get its own EC2, which will cost more than the ECS on EC2 in B.\nSo B is cheaper"
      },
      {
        "date": "2024-04-14T10:33:00.000Z",
        "voteCount": 1,
        "content": "option D seems most cost effective solution"
      },
      {
        "date": "2024-08-19T16:20:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-04-14T10:34:00.000Z",
        "voteCount": 1,
        "content": "This option provides more control over the infrastructure and can accommodate the varying resource requirements of different applications. By utilizing EC2 Auto Scaling and Application Load Balancers, you can efficiently manage resources based on demand. Purchasing Reserved Instances can provide cost savings over the long term."
      },
      {
        "date": "2024-03-12T02:03:00.000Z",
        "voteCount": 2,
        "content": "Go with C as it provides standard deployment process for each App.\nOne can right size each App using appropriate EC2 sizing for each Application and I feel this approach can be as cost effective as using option B (ECS)."
      },
      {
        "date": "2024-02-17T01:29:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\nElastic Beanstalk is a PaaS offering by AWS, which automates the deployment and scaling of web applications. It abstracts the underlying infrastructure, making it easier to manage, but it may have some limitations in terms of customization. \nEC2, on the other hand, is an infrastructure as a service (IaaS) offering that provides more control over the virtual servers running your applications. \nWith EC2, you have the flexibility to customize the infrastructure to your exact needs, but it requires more manual management. In general, if you require more control and customization, EC2 may be more cost-effective in the long run."
      },
      {
        "date": "2024-01-27T01:45:00.000Z",
        "voteCount": 3,
        "content": "Between B &amp; C, I'll go with C .\nBoth options are using EC2, the cost will be the same. Additional requirement is \"standardizing by using a single deployment methodology\" , and this is about Beanstalk."
      },
      {
        "date": "2023-12-09T01:15:00.000Z",
        "voteCount": 2,
        "content": "C: ans \n\nThe most cost-effective solution is C. Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.\n\nHere's why:\n\nCost efficiency:\n\nElastic Beanstalk: Provides managed application deployment and scaling, reducing operational overhead and potential configuration errors.\nAuto Scaling: Ensures that resources are available only when needed, minimizing idle costs.\nReserved Instances: Purchasing 3-year Reserved Instances can offer significant discounts compared to on-demand instances."
      },
      {
        "date": "2023-11-22T20:36:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-10-23T01:24:00.000Z",
        "voteCount": 4,
        "content": "Many of you have already explain the reasons why other options are not a good fit. but i will explain optionD bit further.\nD-&gt; Wrong\nNot only for using Custom Metric but Co-hosting all applications on a single EC2 instance cluster means that the resources (CPU, memory, storage) of the instances would need to be shared among all the applications. This lead to resource contention and inefficient resource allocation, especially when some applications have peak memory requirements of up to 2.5 GB. It may result in underutilization of resources for applications with low usage and performance issues during peak processing times."
      },
      {
        "date": "2023-08-15T23:42:00.000Z",
        "voteCount": 1,
        "content": "B 100% sure"
      },
      {
        "date": "2023-07-22T11:23:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-07T14:46:00.000Z",
        "voteCount": 2,
        "content": "B since the emphasis is on cost, no operational overhead. containers should be a bit more cost-effective as they are more granular per app\na: hours-&gt; no lambda"
      },
      {
        "date": "2023-06-26T16:58:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-06-25T04:28:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-24T02:16:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect due to lambda 15mins constraint\nB is Correct"
      },
      {
        "date": "2023-06-21T03:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/amazon/view/112798-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core nodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.<br><br>The solutions architect must review the architecture and suggest a solution to minimize the compute costs.<br><br>Which solution should the solutions architect recommend to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch all task, primary, and core nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContinue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 80,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 51,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-28T04:26:00.000Z",
        "voteCount": 23,
        "content": "The problem statement says:\n\"The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is not a priority because *the data is not referenced until late in the day.*\"\n\nSo later in the day, clients will be using the cluster to read data. Therefore my understanding is that core and primary nodes need to be available, but the task nodes can be terminated once the tasks have finished their daily run."
      },
      {
        "date": "2023-06-30T01:35:00.000Z",
        "voteCount": 15,
        "content": "Correct Answer is D. In B it has no sense to temrinate primary instance if we have already purchase a saving plan."
      },
      {
        "date": "2024-05-09T07:07:00.000Z",
        "voteCount": 1,
        "content": "The key point here is \"Amazon EMR cluster that is using the EMR File System (EMRFS)\", the EMR File System use S3 as persistent storage, so one the cluster finished the processing of data, the data is ready for the users but the cluster is no longer needed and it can be terminated without any issue."
      },
      {
        "date": "2024-08-19T16:22:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-06-12T06:17:00.000Z",
        "voteCount": 1,
        "content": "Changing my mind to B as the process is business critical and you shouldn\u00b4t use spot instances for any critical processing but the cluster can be terminated as the data is in S3 once it's processed."
      },
      {
        "date": "2024-05-03T02:32:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2024-04-30T17:01:00.000Z",
        "voteCount": 1,
        "content": "B - we should not terminate the cluster.\nD - once task is done can terminate the node.\n\nso my answer is D"
      },
      {
        "date": "2024-04-12T08:18:00.000Z",
        "voteCount": 2,
        "content": "Option D:  How To / Use Case\n\nhttps://aws.amazon.com/blogs/big-data/strategies-for-reducing-your-amazon-emr-costs/"
      },
      {
        "date": "2024-04-07T03:08:00.000Z",
        "voteCount": 2,
        "content": "Terminating all instances make sense as these are not frequent jobs. They are run on once a day \n\nhttps://www.cloudforecast.io/blog/aws-emr-cost-optimization-guide/"
      },
      {
        "date": "2024-04-05T14:00:00.000Z",
        "voteCount": 2,
        "content": "D\nfor the one who chose B, the computer savings plan is a hourly commitment for consistent usage pattern. You will be charged even you shutdown the whole stack"
      },
      {
        "date": "2024-03-22T21:07:00.000Z",
        "voteCount": 4,
        "content": "We can terminate the cluster and then read results from S3.\nRefer below EMR faq:\nQ: How does Amazon EMR use Amazon EC2 and Amazon S3?\n\nYou can upload your input data and a data processing application into Amazon S3. Amazon EMR then launches a number of Amazon EC2 instances that you specified. The service begins the cluster execution while pulling the input data from Amazon S3 using S3 URI scheme into the launched Amazon EC2 instances. Once the cluster is finished, Amazon EMR transfers the output data to Amazon S3, where you can then retrieve it or use as input in another cluster.\nhttps://aws.amazon.com/emr/faqs/"
      },
      {
        "date": "2024-03-19T09:55:00.000Z",
        "voteCount": 3,
        "content": "We _can_ terminate the entire cluster, as EMRFS is specified \u2013 which stores the computational results in S3. Therefore, the cluster is not required after processing."
      },
      {
        "date": "2024-03-12T02:07:00.000Z",
        "voteCount": 2,
        "content": "Option D because processed data is used later in the day."
      },
      {
        "date": "2024-03-04T07:43:00.000Z",
        "voteCount": 3,
        "content": "The difference between D and B is that whether to terminate whole EMR cluster, or do we need the EMR cluster after the 6 hour processing. The answer is yes, \" the data is not referenced until late in the day\" ,  EMRFS can't be access without EMR cluster.  You may argue that you can access the underlying s3 directly.  But, you would loss the benefits of EMR/EMRFS, which provide security control, and most importantly, performance and system throughput related to big data"
      },
      {
        "date": "2024-09-07T01:38:00.000Z",
        "voteCount": 1,
        "content": "Yes, EMRFS can be accessed without an active EMR cluster because EMRFS stores data in Amazon S3, which is a persistent object storage service independent of the EMR cluster.\n\nHere's how it works:\n\nEMRFS is essentially an extension of Amazon S3, allowing EMR clusters to use S3 as a storage layer for data.\nWhen you terminate an EMR cluster, the data in S3 remains intact and accessible"
      },
      {
        "date": "2024-02-22T02:39:00.000Z",
        "voteCount": 2,
        "content": "Once the Amazon EMR cluster completes  processing data in S3 why do you need it ? Does processed data stored on cluster EC2s . There is a specific settings The auto-termination policy terminates the cluster after a specific amount of idle time. \nYou will not need the cluster until the next run ."
      },
      {
        "date": "2024-02-22T02:30:00.000Z",
        "voteCount": 2,
        "content": "Once the Data process is complete is there a need for EMR Cluster ? you can use The auto-termination policy terminates the cluster after a specific amount of idle time.\nThe processed data is in S3 for later queries so my thoughts would be do no need to EMR Cluster till the next run ."
      },
      {
        "date": "2024-02-03T13:31:00.000Z",
        "voteCount": 4,
        "content": "D\nMakes no sense to kill the whole cluster when someone would access it later same day"
      },
      {
        "date": "2024-01-27T01:55:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer. \nTransient clusters is the best suits for this use case. Data processes by EMR stored in s3, and referenced there. No need to keep any nodes up.\n Besides,  EMR File System (EMRFS) is best suited for transient clusters as the data resides irrespective of the lifetime of the cluster."
      },
      {
        "date": "2023-12-10T08:43:00.000Z",
        "voteCount": 2,
        "content": "d ANS\nCost optimization: Using Spot Instances for task nodes significantly reduces costs compared to On-Demand Instances. Spot Instances can offer substantial discounts, especially when running workloads with flexible start and stop times.\n\nMinimal impact: By terminating only the task nodes after processing, the primary and core nodes remain available for future job submissions without requiring a complete cluster restart. This minimizes downtime and maximizes resource utilization.\n\nAvailability and stability: On-Demand Instances for primary and core nodes ensure high availability and stability for critical tasks. This eliminates the risk of interruptions due to Spot Instance price fluctuations or availability constraints.\n\nSavings Plans: Purchasing Compute Savings Plans for On-Demand Instances can provide further cost savings by offering discounts based on a committed level of usage."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/amazon/view/112799-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three Availability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up as targets for an Application Load Balancer (ALB) that is associated with three public subnets.<br><br>The application needs to communicate with on-premises systems. Only traffic from IP addresses in the company's IP address range are allowed to access the on-premises systems. The company\u2019s security team is bringing only one IP address from its internal IP address range to the cloud. The company has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP address.<br><br>A solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution also must be able to mitigate failures automatically.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate the NAT gateway and assign the Elastic IP address to the new NAT gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLTurn on health checks for the NLIn the case of a failed health check, redeploy the NLB in different subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy, invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IP address to the new NAT gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route 53 health check. In the case of a failed health check, recreate the ALB in different subnets."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-22T09:39:00.000Z",
        "voteCount": 8,
        "content": "Isn't NAT Gateway AWS managed\nWhy do we need to check if NAT GW is healthy ?"
      },
      {
        "date": "2023-11-22T21:16:00.000Z",
        "voteCount": 5,
        "content": "This question is little unclear. It does not state whether the communication between on-premise system and AWS is out bond or in bound in nature. If it is outbound then C makes sense."
      },
      {
        "date": "2024-08-08T21:08:00.000Z",
        "voteCount": 1,
        "content": "The design should \"gives the application the ability to communicate with the on-premises systems\", so it is outbound."
      },
      {
        "date": "2024-03-12T02:15:00.000Z",
        "voteCount": 2,
        "content": "Option C is best. As there is only one IP address that can be used Option A = 3 NAT gateways are not needed."
      },
      {
        "date": "2023-11-09T15:53:00.000Z",
        "voteCount": 2,
        "content": "also think about B option to assign an IP address to NLB"
      },
      {
        "date": "2023-07-22T07:12:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-17T02:38:00.000Z",
        "voteCount": 2,
        "content": "All seemed good for option C) till I encountered this sentence - \"The company\u2019s security team is bringing only one IP address from its internal IP address range to the cloud.\" - Please note internal IP not external IP. Which seems to imply there is a connectivity between on-premises &amp; Cloud (either through Site-to-Site VPN or DX), though not explicitly mentioned in the question.\n\nIn such a case, NAT gateway with Public subnet will not help. Option B) will become a viable solution in this case."
      },
      {
        "date": "2023-08-21T01:16:00.000Z",
        "voteCount": 2,
        "content": "Elastic IPs itself are public\nwhether you choose B or C\nOption C is perfect for this use-case unless you associate ALB as target for NLB"
      },
      {
        "date": "2023-07-07T15:13:00.000Z",
        "voteCount": 1,
        "content": "C makes some sense"
      },
      {
        "date": "2023-06-26T17:14:00.000Z",
        "voteCount": 2,
        "content": "C - single NAT if only one Elastic IP is available."
      },
      {
        "date": "2023-06-25T04:39:00.000Z",
        "voteCount": 3,
        "content": "option C provides the most appropriate solution by using a single NAT gateway, monitoring its health with CloudWatch, and invoking a Lambda function to create a new NAT gateway if necessary."
      },
      {
        "date": "2023-06-24T02:43:00.000Z",
        "voteCount": 1,
        "content": "C is the answer single NAT is needed"
      },
      {
        "date": "2023-06-24T02:42:00.000Z",
        "voteCount": 1,
        "content": "I think it's C."
      },
      {
        "date": "2023-06-23T12:24:00.000Z",
        "voteCount": 4,
        "content": "I go with C\nA is incorrect because you dont need 3 nat gateways\nB does not make sense to replace ALB\nD - you cannot assign elastic ip to ALB"
      },
      {
        "date": "2023-06-24T07:49:00.000Z",
        "voteCount": 2,
        "content": "A NAT (Network Address Translation) Gateway enables instances in a private subnet to connect to the internet or other AWS services but prevents the internet from initiating a connection with those instances. By using a single NAT gateway with the provided Elastic IP address, all outbound traffic will appear to come from the single, whitelisted IP address that the company allows."
      },
      {
        "date": "2023-06-21T03:20:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/amazon/view/112800-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There are 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required information so that each account can be operated as a standalone account.<br><br>Which combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer accounts to the new developer organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSign in to the new developer organization's management account and create a placeholder member account that acts as a target for the developer account migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave each developer sign in to their account and confirm to join the new developer organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BEF",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AEF",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ABF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T17:18:00.000Z",
        "voteCount": 18,
        "content": "B - Remove\nE - Invite\nF - Verify\nhttps://repost.aws/knowledge-center/organizations-move-accounts"
      },
      {
        "date": "2023-08-15T04:46:00.000Z",
        "voteCount": 4,
        "content": "Excellent Explanation"
      },
      {
        "date": "2023-10-23T02:49:00.000Z",
        "voteCount": 1,
        "content": "no one talked about;\n\"All accounts are set up with all the required information so that each account can be operated as a standalone account.\"\nWouldnt that make Option B invalid? \ncan some one clarify that plz."
      },
      {
        "date": "2023-11-30T14:34:00.000Z",
        "voteCount": 1,
        "content": "You can remove an account from your organization only if the account is configured with the information required to operate as a standalone account."
      },
      {
        "date": "2023-10-30T10:52:00.000Z",
        "voteCount": 2,
        "content": "No, it's to confirm that B is valid. Removing accounts from organization effectively makes them standalone accounts. The statement you cited, says that they have all info, permissions.. to operate as standalone account thus make B feasible."
      },
      {
        "date": "2023-07-25T10:15:00.000Z",
        "voteCount": 2,
        "content": "BEF is correct.\nhttps://aws.amazon.com/blogs/mt/aws-organizations-moving-an-organization-member-account-to-another-organization-part-1/#:~:text=Moving%20an%20account%20between%20organizations,and%20services%20continue%20to%20operate."
      },
      {
        "date": "2023-07-22T07:11:00.000Z",
        "voteCount": 1,
        "content": "correct BEF."
      },
      {
        "date": "2023-07-13T07:45:00.000Z",
        "voteCount": 2,
        "content": "its BEF"
      },
      {
        "date": "2023-07-07T15:21:00.000Z",
        "voteCount": 1,
        "content": "its BEF"
      },
      {
        "date": "2023-06-25T07:24:00.000Z",
        "voteCount": 2,
        "content": "remove from org, invite from org, verify from invidual. BEF"
      },
      {
        "date": "2023-06-24T07:57:00.000Z",
        "voteCount": 3,
        "content": "GPT 4.0 corrected BEF are the answers. A is not feasible."
      },
      {
        "date": "2023-06-24T07:53:00.000Z",
        "voteCount": 1,
        "content": "GPT: In AWS Organizations, moving an account to a new organization is a two-step process. First, the account has to be removed from the old organization. This can be done using the MoveAccount operation from the old organization's management account (Option A). \nSecond, the account has to be invited to the new organization. The new organization's management account should use the InviteAccountToOrganization operation to send an invitation to the account (Option E).Finally, to accept the invitation to join a new organization, the account owner (in this case, each developer) must sign in to their account and accept the invitation (Option F)."
      },
      {
        "date": "2023-06-24T07:57:00.000Z",
        "voteCount": 2,
        "content": "GPT corrected BEF are the answers."
      },
      {
        "date": "2023-06-24T05:40:00.000Z",
        "voteCount": 1,
        "content": "To move an account between organizations, you need to remove the account from the current organization (using RemoveAccountFromOrganization) and then the individual account holders must accept an invitation to join the new organization (using the MoveAccount operation and then manually confirming the invitation to join the new organization)."
      },
      {
        "date": "2023-06-24T02:56:00.000Z",
        "voteCount": 3,
        "content": "A is incorrect not an option to MoveOperation not across org\nB - remove account from org\nE - Invite the dev account\nF - Confirm"
      },
      {
        "date": "2023-06-24T02:46:00.000Z",
        "voteCount": 1,
        "content": "B, E, and F, I think"
      },
      {
        "date": "2023-06-23T22:16:00.000Z",
        "voteCount": 1,
        "content": "olabiba.ai says BDE"
      },
      {
        "date": "2023-07-30T16:46:00.000Z",
        "voteCount": 1,
        "content": "olabiba.ai is wrong"
      },
      {
        "date": "2023-06-23T12:42:00.000Z",
        "voteCount": 3,
        "content": "I go with BEF\nhttps://aws.amazon.com/blogs/mt/aws-organizations-moving-an-organization-member-account-to-another-organization-part-1/\nThe above doc clearly says \"Moving an account between organizations requires you to remove the account from an organization, making the account standalone, and then you accepting an invite to join another organization\"\nA is incorrect as per above statement\nB Correct\nC is incorrect because  individual account cannot remove itself from an organization. This operation must be performed by the management account of the organization.\nD is incorrect because there is NO need for placeholder\nE is correct . The management account should INVITE its member account\nF is correct - The member account should ACCEPT invitation"
      },
      {
        "date": "2023-06-21T03:21:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is BDE"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/amazon/view/112801-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company\u2019s interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-party tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company has successfully implemented and tested Python logic to detect corrupt images.<br><br>A solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Lambda@Edge function that is invoked by a viewer-response event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Lambda@Edge function that is invoked by an origin-response event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 event notification that invokes an AWS Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 event notification that invokes an AWS Step Functions state machine."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T05:42:00.000Z",
        "voteCount": 16,
        "content": "The requirement here is to catch and deal with the corruption at the time of ingestion. Hence, the logical place to put the check would be where the ingestion is actually happening, which is when the image is put into the S3 bucket. Amazon S3 can be configured to send an event notification when a new object is created (i.e., put into the bucket). This event can then trigger a Lambda function that uses the Python logic to check the image for corruption. This way, you are catching and dealing with any issues as soon as the image is ingested."
      },
      {
        "date": "2024-02-08T08:10:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\nUsing AWS Lambda with Amazon S3\nPDF\nRSS\nYou can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted. You configure notification settings on a bucket, and grant Amazon S3 permission to invoke a function on the function's resource-based permissions policy."
      },
      {
        "date": "2023-12-12T08:34:00.000Z",
        "voteCount": 1,
        "content": "Lambda@Edge triggered by origin-response event:\n\nPros:\nDetects corrupted images closer to the origin, minimizing impact.\nAvoids processing overhead for valid images.\nCons:\nCorrupted images might still be partially downloaded by users before detection."
      },
      {
        "date": "2023-11-22T22:29:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-07T15:23:00.000Z",
        "voteCount": 2,
        "content": "its a C"
      },
      {
        "date": "2023-06-27T14:58:00.000Z",
        "voteCount": 2,
        "content": "Take care of corrupted images as soon as they get uploaded to S3"
      },
      {
        "date": "2023-06-24T08:05:00.000Z",
        "voteCount": 1,
        "content": "D is for more complex and multiple sets of Lambda."
      },
      {
        "date": "2023-06-24T03:17:00.000Z",
        "voteCount": 3,
        "content": "A&amp;B is too late, D is unnecessary\nC is correct"
      },
      {
        "date": "2024-09-24T08:42:00.000Z",
        "voteCount": 1,
        "content": "How is A&amp;B too late if they are done at the edge before the upload?"
      },
      {
        "date": "2023-06-24T02:47:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-06-21T03:22:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/amazon/view/112802-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to deploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.<br><br>When the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and associates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.<br><br>What should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group's launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group\u2019s launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group\u2019s launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-25T04:56:00.000Z",
        "voteCount": 6,
        "content": "This solution automates the deployment process by creating a new Amazon Machine Image (AMI) with the CodeDeploy agent installed. The Auto Scaling group's launch template is then updated to use this new AMI. By associating the CodeDeploy deployment group with the Auto Scaling group, CodeDeploy will automatically deploy the application to any new instances launched by the Auto Scaling group.\n\nThis approach eliminates the need to manually install the CodeDeploy agent on new instances and associate them with the deployment group. It simplifies the deployment process and reduces operational overhead by leveraging the automation capabilities of CodeDeploy and the Auto Scaling group."
      },
      {
        "date": "2024-03-12T02:35:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-22T22:33:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-19T01:56:00.000Z",
        "voteCount": 1,
        "content": "CodeDeploy deployment group should be associated with ASG"
      },
      {
        "date": "2023-08-28T07:45:00.000Z",
        "voteCount": 3,
        "content": "D as per this rather old blog post - https://aws.amazon.com/blogs/devops/under-the-hood-aws-codedeploy-and-auto-scaling-integration/"
      },
      {
        "date": "2023-08-28T07:13:00.000Z",
        "voteCount": 1,
        "content": "It seems really unnecessary to have to install an app on the fly during scale-out of an ASG. Just launching the EC2 instances from a pre-installed AMI is so much faster, and removes sources of error.\n\nI am a little frustrated never to have encountered AWS Image Builder in a question, or in course material..."
      },
      {
        "date": "2023-08-28T07:19:00.000Z",
        "voteCount": 2,
        "content": "https://dev.to/aws-builders/how-to-create-a-custom-ami-with-image-pipeline-and-automate-its-creation-using-ec2-image-builder-108m"
      },
      {
        "date": "2023-08-27T23:37:00.000Z",
        "voteCount": 2,
        "content": "AWS CodeDeploy is a deployment service that enables developers to automate the deployment of applications to instances and to update the applications as required."
      },
      {
        "date": "2023-07-30T17:20:00.000Z",
        "voteCount": 1,
        "content": "Bake AMI with agent already installed"
      },
      {
        "date": "2023-07-16T02:35:00.000Z",
        "voteCount": 1,
        "content": "D is not correct since it is considering about the code change."
      },
      {
        "date": "2023-07-30T17:20:00.000Z",
        "voteCount": 1,
        "content": "CodeBuild cant create a new AMI?"
      },
      {
        "date": "2023-07-07T16:06:00.000Z",
        "voteCount": 1,
        "content": "It's a D"
      },
      {
        "date": "2023-06-27T07:22:00.000Z",
        "voteCount": 3,
        "content": "D - correct. You want the agent baked in the AMI."
      },
      {
        "date": "2023-06-24T08:07:00.000Z",
        "voteCount": 3,
        "content": "GPT: This option provides the least amount of operational overhead by associating the CodeDeploy deployment group with the Auto Scaling group rather than individual EC2 instances. This enables any new instances launched by the Auto Scaling group to be automatically included in deployments, eliminating the need for manual intervention or additional automation to add new instances to the deployment group. The creation of an AMI with the CodeDeploy agent pre-installed ensures that all new instances launched by the Auto Scaling group will have the necessary components to participate in CodeDeploy deployments."
      },
      {
        "date": "2023-06-21T03:24:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/amazon/view/112803-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that an EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then manually adds a new EC2 instance behind the ALB.<br><br>A solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company needs to minimize downtime during the switch to the new solution.<br><br>Which set of steps should the solutions architect take to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the existing ALB. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Attach the existing EC2 instances to the Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALAttach the existing EC2 instances to the Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimum number of EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for the existing ALB to register the existing EC2 instances with the Auto Scaling group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-21T07:04:00.000Z",
        "voteCount": 9,
        "content": "Deleting the ALB will increase downtime, so A &amp; C eliminated. B &amp; D are similar but D suggests wait for ALB to register EC2 instances, again causing delay so eliminated"
      },
      {
        "date": "2024-01-20T14:56:00.000Z",
        "voteCount": 1,
        "content": "Agreed\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html#create-asg-from-instance-console"
      },
      {
        "date": "2024-07-31T11:45:00.000Z",
        "voteCount": 1,
        "content": "B and D are similar but\nB is wrong since you cannot add ec2 instances to autoscaling group. You can only add ec2 to ALB target group."
      },
      {
        "date": "2024-08-19T16:34:00.000Z",
        "voteCount": 1,
        "content": "Just B"
      },
      {
        "date": "2024-08-10T03:43:00.000Z",
        "voteCount": 3,
        "content": "You are wrong. Don't misguide others here with wrong argument.\n\nYou can attach EC2 to ASG for sure\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-detach-attach-instances.html\n\njust simply google `attach ec2 to asg` and you will find the doc above. I don't understand why ppl just make random wrong assumptions and misguide others while it only take a few seconds to verify on google/chatgpt"
      },
      {
        "date": "2023-11-22T22:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer."
      },
      {
        "date": "2023-11-02T10:38:00.000Z",
        "voteCount": 1,
        "content": "why should we attach the current one, why not leaving it to the ASG?"
      },
      {
        "date": "2023-11-03T08:54:00.000Z",
        "voteCount": 2,
        "content": "if you read @SK_Tyagi , i think he made a fair point. :)"
      },
      {
        "date": "2023-12-30T06:22:00.000Z",
        "voteCount": 1,
        "content": "Is ALB even capable of automatically registering existing EC2 instances with an ASG? I don't think so."
      },
      {
        "date": "2023-10-01T10:49:00.000Z",
        "voteCount": 1,
        "content": "Does it require Attaching the existing EC2 instances to the Auto Scaling group? Why is D incorrect or Why B is a better response than D?"
      },
      {
        "date": "2023-07-22T06:50:00.000Z",
        "voteCount": 1,
        "content": "Correct B"
      },
      {
        "date": "2023-07-07T16:10:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-06-27T07:24:00.000Z",
        "voteCount": 1,
        "content": "B - correct. Attach the EC2s"
      },
      {
        "date": "2023-06-25T13:44:00.000Z",
        "voteCount": 1,
        "content": "New AS Group - assign to existing ALB and attach EC2s to new Scaling group."
      },
      {
        "date": "2023-06-24T08:16:00.000Z",
        "voteCount": 2,
        "content": "New AS Group - assign to existing ALB and attach EC2s to new Scaling group."
      },
      {
        "date": "2023-06-24T05:48:00.000Z",
        "voteCount": 4,
        "content": "Auto Scaling groups are designed to ensure that you are running your desired number of Amazon EC2 instances. It also can automatically replace any instances that fail or are unhealthy based on health checks. You can specify the minimum, maximum, and desired number of instances in your Auto Scaling group. By attaching a new launch template to the Auto Scaling group, the Auto Scaling group knows what configuration to use for the new instances it launches.\n\nThere's no need to delete the existing ALB as suggested in options A and C. The ALB is still functional and will work with the newly created Auto Scaling group. You can directly attach the Auto Scaling group to the existing ALB."
      },
      {
        "date": "2023-06-21T03:24:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/amazon/view/112804-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS Organizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve approximately 1 TB of data each day from Amazon S3.<br><br>The developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3 buckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC infrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at which the developers can perform their tasks.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation template to deploy an approved VPC configuration with S3 interface endpoints. Scope the developers' IAM permissions so that the developers can launch VPC resources only with CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer accounts. When the forecasted cost is 75% of the actual budget cost, send an alert to the developer teams. If the actual budget cost is 100%, create a budget action to terminate the developers' EC2 instances and VPC infrastructure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and approved EC2 instances. Share the portfolio with the developer accounts. Configure an AWS Service Catalog launch constraint to use an approved IAM role. Scope the developers' IAM permissions to allow access only to AWS Service Catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers launch unapproved EC2 instances or if developers create VPCs without S3 gateway endpoints, perform a remediation action to terminate the unapproved resources."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T13:23:00.000Z",
        "voteCount": 1,
        "content": "A. Interface endpoints are cheaper than Gateway endpoints if the resources are in the same region. The question specifically said one region."
      },
      {
        "date": "2024-08-12T19:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct and least affects the speed at which the developers can perform their tasks\nC denies the developers access to any AWS services except AWS Service Catalog, therefore it would limit access to all other services."
      },
      {
        "date": "2024-08-19T16:36:00.000Z",
        "voteCount": 2,
        "content": "just C"
      },
      {
        "date": "2023-11-22T22:46:00.000Z",
        "voteCount": 3,
        "content": "C is least disruptive option for Developers productivity."
      },
      {
        "date": "2023-08-31T11:11:00.000Z",
        "voteCount": 2,
        "content": "Why not D?"
      },
      {
        "date": "2023-07-07T16:17:00.000Z",
        "voteCount": 2,
        "content": "C works"
      },
      {
        "date": "2023-06-27T07:26:00.000Z",
        "voteCount": 3,
        "content": "C - let the devs choose what they want but they still adhere to standards. Service catalog does that."
      },
      {
        "date": "2023-06-25T13:52:00.000Z",
        "voteCount": 3,
        "content": "C is correct. Service catalog solves all issues.\nS3 Gateway endpoint more cost efective with data transfer in VPC on AWS"
      },
      {
        "date": "2023-06-24T08:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct. Service catalog solves all issues."
      },
      {
        "date": "2023-06-23T13:42:00.000Z",
        "voteCount": 4,
        "content": "C is the effective way. \nA is incorrect because it can allow users to create resources that are defined outside of cloudformation"
      },
      {
        "date": "2023-06-21T03:25:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/amazon/view/112805-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A solutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-19T00:26:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect as it is too difficult to maintain. C is correct answer."
      },
      {
        "date": "2023-08-20T15:39:00.000Z",
        "voteCount": 2,
        "content": "my bad, \"attach a policy to each user\" its a tedious tasks. ignore my previous message."
      },
      {
        "date": "2023-08-20T15:38:00.000Z",
        "voteCount": 1,
        "content": "can someone please detail why the answer cannot be B?"
      },
      {
        "date": "2023-10-29T13:45:00.000Z",
        "voteCount": 4,
        "content": "For this type of question (organization and policy for many accounts), we avoid options that require actions on each account/user. There's always better option to set policies at one place."
      },
      {
        "date": "2023-07-07T16:19:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-06-27T07:32:00.000Z",
        "voteCount": 4,
        "content": "AWS Org, Control Tower and SCPs."
      },
      {
        "date": "2023-06-25T05:04:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-24T08:23:00.000Z",
        "voteCount": 2,
        "content": "Control Tower with SCP (deny ) solves the issues"
      },
      {
        "date": "2023-06-23T13:43:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-06-21T03:26:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/amazon/view/112806-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fleet for web hosting, database API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizing operational costs.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic with Amazon SQS long polling for retaining failed orders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic with Amazon S3 Glacier Deep Archive for retaining failed orders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for retaining failed orders."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T14:58:00.000Z",
        "voteCount": 4,
        "content": "Answer - C\nS3 for Web hosting,\nAppsync for DB API services\nSQS DLQ for Failed orders"
      },
      {
        "date": "2023-11-19T00:53:00.000Z",
        "voteCount": 3,
        "content": "SQS Dead letter Queue is key"
      },
      {
        "date": "2023-07-07T16:24:00.000Z",
        "voteCount": 2,
        "content": "C\nA would work with Lambda/SQS vs ECS/SQS"
      },
      {
        "date": "2023-06-28T14:19:00.000Z",
        "voteCount": 1,
        "content": "S3 + Appsync DB API (Manged service) and SQS and Deal letter queue for failed orders"
      },
      {
        "date": "2023-06-27T07:38:00.000Z",
        "voteCount": 2,
        "content": "C - You don't use \"Amazon SQS long polling for retaining failed orders\""
      },
      {
        "date": "2023-06-26T16:14:00.000Z",
        "voteCount": 3,
        "content": "Option C combines Amazon S3 for web hosting, AWS AppSync for database API services, and AWS Lambda for business logic. This combination provides a decoupled and scalable architecture. Using Amazon SQS for order queuing ensures reliable message delivery, and utilizing an SQS dead-letter queue allows for retaining failed orders. This solution meets the requirements of the scenario while minimizing operational costs"
      },
      {
        "date": "2023-06-25T08:31:00.000Z",
        "voteCount": 2,
        "content": "C is a good answer, but is it the cheapest? hard to tell"
      },
      {
        "date": "2023-06-25T03:54:00.000Z",
        "voteCount": 4,
        "content": "Checking a bit more for AWS AppSync - AWS AppSync enables developers to connect their applications and services to data and events with secure, serverless and high-performing GraphQL and Pub/Sub APIs. GraphQL is an open-source query language that describes how a client should request information through an API\nI don't believe this is the intent of the exercise here by saying \"Database API\""
      },
      {
        "date": "2024-06-08T05:18:00.000Z",
        "voteCount": 2,
        "content": "From AppSync page - Access data from multiple sources with a single request. Instantly create APIs for your databases. Combine APIs into a single Merged API"
      },
      {
        "date": "2024-06-02T08:36:00.000Z",
        "voteCount": 1,
        "content": "SQS long polling does not solve \"retaining failed orders\" - it's dead-letter queue's responsibility."
      },
      {
        "date": "2023-06-24T08:29:00.000Z",
        "voteCount": 3,
        "content": "S3 + Appsync DB API (Manged service) and SQS and Deal letter queue for failed orders"
      },
      {
        "date": "2023-06-23T06:21:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2023-06-21T03:26:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/amazon/view/112807-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind an Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a cross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions architect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.<br><br>Which additional step should the solutions architect take?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a MySQL standby database on an Amazon EC2 instance in us-west-2."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T08:33:00.000Z",
        "voteCount": 7,
        "content": "B- Aurora provides the minimum RTO and RPO (1 min)"
      },
      {
        "date": "2023-08-19T03:12:00.000Z",
        "voteCount": 5,
        "content": "B is correct. RTO of A is Usually minutes, not sure will be less than 5p\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-disaster-recovery/choosing-database.html"
      },
      {
        "date": "2024-03-21T12:47:00.000Z",
        "voteCount": 1,
        "content": "B. Not the cheapest, but the other ones are either not cross-regional or can't handle the RTO/RPO."
      },
      {
        "date": "2024-03-06T14:06:00.000Z",
        "voteCount": 1,
        "content": "Amazon's documentation states that for Multi-AZ deployments, the typical RTO for failing over to the standby is 60-120 seconds. For read replicas, since the lag is typically larger, the RTO is often cited as around 5-10 minutes under normal conditions."
      },
      {
        "date": "2023-12-06T19:59:00.000Z",
        "voteCount": 3,
        "content": "The question doesn't mention cost,  so usually it will be the best performance choice. In this case is B"
      },
      {
        "date": "2023-11-19T00:57:00.000Z",
        "voteCount": 2,
        "content": "B is right answer"
      },
      {
        "date": "2023-11-12T05:05:00.000Z",
        "voteCount": 2,
        "content": "A\nRDS Read Replica is more cost effective and can be promoted as Primary within 5 mins"
      },
      {
        "date": "2023-11-02T06:31:00.000Z",
        "voteCount": 2,
        "content": "Why not A? Promoting a read replica will still meet the RTO of 5 minutes while being cheaper than using aurora."
      },
      {
        "date": "2023-08-10T07:34:00.000Z",
        "voteCount": 2,
        "content": "but A also meet requirement actually according to https://aws.amazon.com/blogs/database/how-to-choose-the-best-disaster-recovery-option-for-your-amazon-aurora-mysql-cluster/"
      },
      {
        "date": "2023-07-07T16:25:00.000Z",
        "voteCount": 5,
        "content": "B for Baurora"
      },
      {
        "date": "2023-06-24T10:07:00.000Z",
        "voteCount": 2,
        "content": "B global database is correct"
      },
      {
        "date": "2023-06-23T13:56:00.000Z",
        "voteCount": 1,
        "content": "B Aurora is the right choice"
      },
      {
        "date": "2023-06-21T03:27:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/amazon/view/112808-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific member accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced based on a group standard, and centrally managed with minimal configuration.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T15:01:00.000Z",
        "voteCount": 2,
        "content": "Answer - D\nAlways SCPs for OUs to confine accounts from using services"
      },
      {
        "date": "2023-11-19T00:59:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2023-10-29T04:36:00.000Z",
        "voteCount": 1,
        "content": "Cant be anything else than D"
      },
      {
        "date": "2023-07-22T06:40:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-09T09:21:00.000Z",
        "voteCount": 2,
        "content": "D will only apply to the specific account in the new OU while C will apply SCP to the whole accounts with the organization"
      },
      {
        "date": "2023-07-07T20:14:00.000Z",
        "voteCount": 1,
        "content": "easy D"
      },
      {
        "date": "2023-06-27T07:44:00.000Z",
        "voteCount": 2,
        "content": "D - Correct. SCPs applied to OU."
      },
      {
        "date": "2023-06-24T10:06:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-06-24T08:37:00.000Z",
        "voteCount": 1,
        "content": "OU and SCP to have Tags and regions denied"
      },
      {
        "date": "2023-06-21T03:29:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/amazon/view/112809-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application generates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that anyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.<br><br>Which set of actions will immediately remediate the security issue without impacting the application's normal workflow?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the AWS Trusted Advisor bucket permissions check and implement the recommended actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a script that puts a private ACL on all of the objects in the bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs option to TRUE on the bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-30T18:17:00.000Z",
        "voteCount": 7,
        "content": "Script is never AWS answers"
      },
      {
        "date": "2024-06-28T08:06:00.000Z",
        "voteCount": 1,
        "content": "Not C due to the following:\n\nC. Script for Private ACL (Potentially Disruptive):\n\nSetting a private ACL on all objects might disrupt existing download mechanisms that rely on signed URLs."
      },
      {
        "date": "2024-08-23T00:08:00.000Z",
        "voteCount": 1,
        "content": "Also running script is only temporary solution since files uploaded after script it executed can still be publicly accessible. So D is better selection."
      },
      {
        "date": "2024-01-20T15:32:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html"
      },
      {
        "date": "2023-11-19T01:01:00.000Z",
        "voteCount": 2,
        "content": "Public Block access"
      },
      {
        "date": "2023-10-16T17:18:00.000Z",
        "voteCount": 4,
        "content": "D. \nIgnorePulicAcls : Setting this option to TRUE causes Amazon S3 to ignore all public ACLs on a bucket and any objects that it contains. This setting enables you to safely block public access granted by ACLs while still allowing PUT Object calls that include a public ACL (as opposed to BlockPublicAcls, which rejects PUT Object calls that include a public ACL). Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html"
      },
      {
        "date": "2023-07-22T06:28:00.000Z",
        "voteCount": 3,
        "content": "Correct D.\nUses the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on the bucket. This would immediately block public access to the files in the S3 bucket without affecting the application's normal workflow. The application can still generate signed URLs to allow users to download their reports. The IgnorePublicAcls setting ignores any public ACLs on objects in this bucket and any objects that are added to this bucket in the future."
      },
      {
        "date": "2023-07-07T20:20:00.000Z",
        "voteCount": 1,
        "content": "its a D"
      },
      {
        "date": "2023-06-27T07:47:00.000Z",
        "voteCount": 1,
        "content": "D - yank the cable from the switch. Check this -&gt; https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html"
      },
      {
        "date": "2023-06-26T16:19:00.000Z",
        "voteCount": 1,
        "content": "D is the most appropriate solution as it directly addresses the security issue by using the Block Public Access feature in Amazon S3. By setting the IgnorePublicAcIs option to TRUE, it ensures that public access to the bucket and its objects is blocked, preventing unauthorized downloads. This solution is immediate, doesn't require modifying the application code or workflow, and provides an effective security control."
      },
      {
        "date": "2023-06-26T09:38:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-d-d"
      },
      {
        "date": "2023-06-25T15:29:00.000Z",
        "voteCount": 2,
        "content": "IF the purpose is block pre-signed URL access to bucket, none of the options will work. \n\nIf we are just blocking non pre-signed URL access, then both C and D will work.\n\nCorrect me if I am wrong here."
      },
      {
        "date": "2023-10-29T01:55:00.000Z",
        "voteCount": 2,
        "content": "Then you know to choose D since \"running a script\" never be the answer in aws exam"
      },
      {
        "date": "2023-06-24T10:06:00.000Z",
        "voteCount": 1,
        "content": "C indeed"
      },
      {
        "date": "2023-06-24T08:41:00.000Z",
        "voteCount": 2,
        "content": "Amazon S3 Block Public Access provides settings for access points, buckets, and accounts to help you manage public access to Amazon S3 resources. By default, new buckets, access points, and objects don't allow public access, but users or applications can modify bucket policies or object permissions to allow public access. S3 Block Public Access settings override these public access settings. You can use S3 Block Public Access to block existing public access, whether specified by an ACL or a policy, and to ensure that public access isn't granted to newly created items. Using signed URLs to grant temporary access to the S3 objects is a secure way to share files. It allows the company to continue using their current workflow without affecting its users while also maintaining the privacy and security of the files in the bucket."
      },
      {
        "date": "2023-06-24T03:24:00.000Z",
        "voteCount": 2,
        "content": "D - Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs"
      },
      {
        "date": "2023-06-21T03:29:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/amazon/view/112810-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions architect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the migration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must be identical to the source database at completion of the migration process.<br><br>All applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The RDS for Oracle DB instance is in a private subnet.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "BEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-27T07:57:00.000Z",
        "voteCount": 16,
        "content": "ace - correct\nb - AWS SCT can't create RDS\nd - never make anything publicly accessible even if temporary\nf - you need initial data, not just changes"
      },
      {
        "date": "2024-03-18T09:12:00.000Z",
        "voteCount": 1,
        "content": "Option E - https://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/\n\nOption A - https://docs.aws.amazon.com/dms/latest/sbs/chap-oracle-postgresql.migration-process.database-schema-conversion.html"
      },
      {
        "date": "2023-12-08T10:01:00.000Z",
        "voteCount": 1,
        "content": "B. Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.\nC. Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account.\nE. Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
      },
      {
        "date": "2023-11-30T15:10:00.000Z",
        "voteCount": 1,
        "content": "Answer - ACE\nCreate target DB and use SCT for Schema conversion\nVPC peering and Open DB access ports via SGs\nAWS DMS to fully load + CDC"
      },
      {
        "date": "2023-11-19T01:08:00.000Z",
        "voteCount": 1,
        "content": "A, C, E right options"
      },
      {
        "date": "2023-10-29T01:22:00.000Z",
        "voteCount": 1,
        "content": "Choices are between A vs B, C vs D, E vs F.\nB: SCT cannot create RDS\nD: When you see making database publicly accessible, you don't need to read more\nF: only perform on changed data while E also do the full load"
      },
      {
        "date": "2023-07-22T06:21:00.000Z",
        "voteCount": 1,
        "content": "Correct ACE."
      },
      {
        "date": "2023-07-07T20:23:00.000Z",
        "voteCount": 1,
        "content": "ACE it"
      },
      {
        "date": "2023-06-25T14:03:00.000Z",
        "voteCount": 1,
        "content": "ACE are correct\nB is inorrect because SCT cannot create RDS instance"
      },
      {
        "date": "2023-06-25T04:22:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/sbs/chap-oracle-postgresql.migration-process.data-migration.html"
      },
      {
        "date": "2023-06-24T10:17:00.000Z",
        "voteCount": 1,
        "content": "ACE is correct"
      },
      {
        "date": "2023-06-24T08:50:00.000Z",
        "voteCount": 1,
        "content": "A. Use SCT; C- Peering; E - DMS with full and change"
      },
      {
        "date": "2023-06-24T03:27:00.000Z",
        "voteCount": 1,
        "content": "A, C and E"
      },
      {
        "date": "2023-06-23T17:20:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is ACE"
      },
      {
        "date": "2023-06-23T14:04:00.000Z",
        "voteCount": 3,
        "content": "ACE are correct\nB is inorrect because SCT cannot create RDS instance"
      },
      {
        "date": "2023-06-23T06:42:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is ACE"
      },
      {
        "date": "2023-06-21T03:30:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is BEF"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/amazon/view/112811-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders. Further log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on the backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing timeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process subsequent messages.<br><br>Which step should the solutions architect take to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the backend processing timeout to 30 seconds to match the visibility timeout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the visibility timeout of the queue to automatically remove the faulty message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T19:52:00.000Z",
        "voteCount": 15,
        "content": "It's D - can't be C because the queue is standard queue.\n\"The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.\"\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "date": "2024-05-01T15:34:00.000Z",
        "voteCount": 1,
        "content": "ANS: D\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue.html"
      },
      {
        "date": "2024-01-25T15:23:00.000Z",
        "voteCount": 1,
        "content": "C is the ans, \nThe dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.\nin the question, we have SQS standard queue. hence ans is C\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "date": "2024-01-25T15:24:00.000Z",
        "voteCount": 1,
        "content": "sry typo its \"D\""
      },
      {
        "date": "2023-12-10T09:10:00.000Z",
        "voteCount": 2,
        "content": "ANS c\nConfigure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.\n\nFault isolation: A dead-letter queue (DLQ) provides a dedicated location for storing messages that cannot be processed successfully. This isolates the faulty message from the main queue, allowing subsequent messages to be processed without interruption.\n\nFIFO processing: Since the faulty message is causing an error on the backend, it's crucial to retain the original order of messages. A FIFO queue preserves the order in which messages were received, ensuring proper processing order after resolving the issue with the faulty message.\n\nMessage analysis: Placing the faulty message in the DLQ facilitates further analysis to identify the cause of the error and update the backend to handle such messages in the future."
      },
      {
        "date": "2023-11-19T01:23:00.000Z",
        "voteCount": 4,
        "content": "Option D is most logical right answer.\nThis question description looks little confusing. Why in a standard SQS one faulty message can block other message processing. It must be a FIFO queue. Processing logic should continue reading other arriving messages that are not faulty. One faulty message may keep failing after every 30 sec of visibility timeout."
      },
      {
        "date": "2023-11-14T10:41:00.000Z",
        "voteCount": 2,
        "content": "It is C.  In an ordering system, it is important to receive the orders in order, so FIFO.  Both C and D are new SQS queues - doesn't matter what the original was."
      },
      {
        "date": "2023-10-29T01:08:00.000Z",
        "voteCount": 4,
        "content": "Somehow I read the option D as create a new queue (to replace the current one) and so confused of what's going on. Wording for this exam is really disaster."
      },
      {
        "date": "2023-07-22T06:14:00.000Z",
        "voteCount": 1,
        "content": "Correct D."
      },
      {
        "date": "2023-07-07T20:25:00.000Z",
        "voteCount": 2,
        "content": "it's a D"
      },
      {
        "date": "2023-07-03T09:54:00.000Z",
        "voteCount": 3,
        "content": "The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue."
      },
      {
        "date": "2023-06-27T13:19:00.000Z",
        "voteCount": 2,
        "content": "Configuring a new SQS standard queue as a dead-letter queue (option D) is not the best choice in this scenario because a standard queue does not provide the strict ordering and exactly-once processing semantics needed for isolating faulty messages. The use of a FIFO queue ensures that the ordering of messages is preserved, which is crucial for troubleshooting and analysis."
      },
      {
        "date": "2023-06-27T13:15:00.000Z",
        "voteCount": 1,
        "content": "C\nits a C"
      },
      {
        "date": "2023-06-27T08:01:00.000Z",
        "voteCount": 4,
        "content": "It's D - can't be C because the queue is standard queue.\n \"The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.\"\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "date": "2023-06-25T14:07:00.000Z",
        "voteCount": 2,
        "content": "D dead letter queu"
      },
      {
        "date": "2023-06-24T10:23:00.000Z",
        "voteCount": 2,
        "content": "D indeed \nC incorrect FIFO will slow down the process"
      },
      {
        "date": "2023-06-24T08:54:00.000Z",
        "voteCount": 3,
        "content": "SQS - dead letter queue is designed for failures and needs to be addressed by the developers. We use it all teh time."
      },
      {
        "date": "2023-06-24T06:08:00.000Z",
        "voteCount": 2,
        "content": "Amazon Simple Queue Service (SQS) allows you to set up Dead-Letter Queues (DLQs) to isolate messages that can't be processed correctly. This option is useful when you want to set aside and isolate messages that can't be processed (consumed) successfully to examine them later. When using standard queues, the DLQ should also be a standard queue."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/amazon/view/112812-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workflow consists of multiple steps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workflow.<br><br>A review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to improve the workflow so that notifications are sent for all types of failures in the retraining process.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\" that targets the team's mailing list.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a task named \"Email\" that forwards the input arguments to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next\u201d: \"Email\".\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a task named \"Email\" that forwards the input arguments to the SES email address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.Runtime\" ] and \"Next\": \"Email\"."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-25T04:36:00.000Z",
        "voteCount": 6,
        "content": "\"notifications are sent for all types of failures in the retraining process\" - that means States.ALL. The rest is common sense.\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html"
      },
      {
        "date": "2024-03-06T03:54:00.000Z",
        "voteCount": 1,
        "content": "I think that ABC makes the most sense here but look what I found reading this: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\n\n\"A retry or catch on States.ALL won't catch States.Runtime errors.\"\n\nReally does this need be so convoluted? Do we need to be tested in such nitty-gritty details? :-("
      },
      {
        "date": "2024-03-06T03:57:00.000Z",
        "voteCount": 1,
        "content": "Correcting my poor English grammar:\n\n\"Really, does this need to be so convoluted? Do we need to be tested in such nitty-gritty details?\""
      },
      {
        "date": "2023-11-19T01:42:00.000Z",
        "voteCount": 2,
        "content": "A, B, C - Right answers"
      },
      {
        "date": "2023-10-29T01:00:00.000Z",
        "voteCount": 2,
        "content": "I love how this question formulated, wish all SAP question can be of this type. With only knowing SES is not for notification, you can rule out D and E. We have to choose three among A B C F, which easily can narrow down to choose between C and F as they are similar. Then yeah, State.ALL vs State.Runtime determines it, A B C it is \ud83d\ude02"
      },
      {
        "date": "2023-07-07T20:33:00.000Z",
        "voteCount": 1,
        "content": "simple as ABC"
      },
      {
        "date": "2023-06-30T04:10:00.000Z",
        "voteCount": 1,
        "content": "ABC is the right answer"
      },
      {
        "date": "2023-06-27T08:06:00.000Z",
        "voteCount": 2,
        "content": "ABC\nD, E - SES - not good\nF - States.runtime, doesn't catch all errors"
      },
      {
        "date": "2023-06-24T15:57:00.000Z",
        "voteCount": 3,
        "content": "From GPT 4 now - Changed to ABC - A to create SNS, Create a task named \"Email\" that forwards the input arguments to the SNS topic.C for Errorr- F is bad since \"States.Runtime\" is not correct."
      },
      {
        "date": "2023-06-24T10:28:00.000Z",
        "voteCount": 1,
        "content": "ABC is correct"
      },
      {
        "date": "2023-06-24T09:07:00.000Z",
        "voteCount": 1,
        "content": "GPT 4.0 is more accurate than 3.5. But has a limit.  A is to create SNS; C to create a Task -This step adds error handling to the states in the workflow. If any step fails, the workflow will transition to the \"Email\" task to send a notification.  E. Create a task named \"Email\" that forwards the input arguments to the SNS email address. E This step creates an AWS Lambda function or an AWS Step Functions task that sends an email notification using the SNS topic created in step A."
      },
      {
        "date": "2023-06-24T06:48:00.000Z",
        "voteCount": 2,
        "content": "In AWS Step Functions, each state reports heartbeat failure, timeout failure, and all other types of failures. Therefore, to catch all errors, the solutions architect should add a Catch field to all Task, Map, and Parallel states with a statement of \"ErrorEquals\": [ \"States.ALL\" ], and \"Next\": \"Email\".\n\nThen, a task named \"Email\" can be created to forward the input arguments to an SNS topic that sends notifications to the team's email."
      },
      {
        "date": "2023-06-24T03:32:00.000Z",
        "voteCount": 1,
        "content": "A, B and C"
      },
      {
        "date": "2023-06-23T14:16:00.000Z",
        "voteCount": 2,
        "content": "ABC are right\nDE are incorrect because SES cannot be used here. SES can be good fir for Bulk/Marketing emails\nF is incorrect because  the error type \"States.Runtime\"  doesn't catch all types of errors. The ques asks \"notifications are sent for all types of failures \""
      },
      {
        "date": "2023-06-23T07:00:00.000Z",
        "voteCount": 2,
        "content": "ABC is the right answer"
      },
      {
        "date": "2023-06-21T03:32:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is ACF"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/amazon/view/112813-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to the company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are accessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is available only on the company's private network.<br><br>A solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing services.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises company.example zone that points to the authoritative name servers for the new private zone in Route 53.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward requests for company.example to the on-premises name servers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on DNS hostnames for the VPConfigure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configur&amp;the on-premises DNS server to forward requests for company.example to the new resolver.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon EventBridge rule to run the document when an instance is entering the running state."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T14:23:00.000Z",
        "voteCount": 12,
        "content": "Outbound resolver endpoints will let you query your onprem DNS\nInbound resolver endpoints will let your onprem DNS server to query the AWS VPC DNS server"
      },
      {
        "date": "2023-06-24T09:15:00.000Z",
        "voteCount": 4,
        "content": "Option B leverages Amazon Route 53 Resolver to handle DNS resolution between the VPC and the on-premises network. By turning on DNS hostnames for the VPC, the EC2 instances will have DNS resolution capabilities. Setting up an outbound endpoint with Route 53 Resolver enables the VPC to resolve DNS queries for external domains. Creating a Resolver rule specifically for the company.example domain allows forwarding of requests for that domain to the on-premises name servers."
      },
      {
        "date": "2023-11-19T15:08:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect. B is right answer."
      },
      {
        "date": "2023-08-21T09:10:00.000Z",
        "voteCount": 1,
        "content": "bhanus explanation spot on"
      },
      {
        "date": "2023-07-21T07:59:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-07T20:35:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-06-27T13:13:00.000Z",
        "voteCount": 1,
        "content": "b\nits a B"
      },
      {
        "date": "2023-06-27T10:50:00.000Z",
        "voteCount": 1,
        "content": "B - Outbound. \nhttps://catalog.us-east-1.prod.workshops.aws/workshops/b4a4be0e-d4f9-4ff5-af82-ebfb86dbe46a/en-US/4-route-53-resolvers-with-active-directory/endpoints"
      },
      {
        "date": "2023-06-24T10:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-23T14:21:00.000Z",
        "voteCount": 2,
        "content": "Outbound resolver endpoints will let you query your onprem DNS\nInbound resolver endpoints will let onprem DNS query the AWS default DNS server of VPC (.2)"
      },
      {
        "date": "2023-06-21T03:32:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/amazon/view/112814-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends traffic to the public internet must send the traffic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and the traffic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.<br><br>A security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any of the company's other VPCs. A solutions architect needs to limit the traffic between the VPCs. Each VPC must be able to communicate only with a predefined, limited set of authorized VPCs.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T09:22:00.000Z",
        "voteCount": 8,
        "content": "C is correct. Option C suggests creating a dedicated transit gateway route table for each VPC attachment. This allows fine-grained control over the routing of traffic between VPCs. By creating separate route tables, the architect can specify the allowed routes for each VPC attachment and limit traffic to only the authorized VPCs. This approach ensures that communication between VPCs is restricted and provides a secure and controlled network environment."
      },
      {
        "date": "2023-06-26T06:00:00.000Z",
        "voteCount": 5,
        "content": "The wording for C is bad though, if ec2 in one VPC can communicate to another EC2 in any VPC, then TGW is the one linking them together, aka TGW already has a route table. \n\nNow, creating a new route table? so the TGW will not look at the old route table? bad wording though"
      },
      {
        "date": "2024-03-15T23:33:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-11-30T15:29:00.000Z",
        "voteCount": 3,
        "content": "Answer C. \nSince TGW is responsible for VPCs communicating with each other, there should be default routes for each VPC attachment on the TGW route table limiting access to VPCs"
      },
      {
        "date": "2023-07-21T07:58:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-07T20:38:00.000Z",
        "voteCount": 1,
        "content": "it's a C"
      },
      {
        "date": "2023-06-27T10:53:00.000Z",
        "voteCount": 2,
        "content": "C - Correct. Static routes on TGW."
      },
      {
        "date": "2023-06-24T20:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-23T07:06:00.000Z",
        "voteCount": 1,
        "content": "C is the right aanswer"
      },
      {
        "date": "2023-06-21T03:33:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 280,
    "url": "https://www.examtopics.com/discussions/amazon/view/112815-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently acquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to migrate and rehost the Windows-based desktop application to AWS.<br><br>All employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a simplified way to manage access to the application on AWS for all the employees.<br><br>Which solution will rehost the application on AWS with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito identity pools. Instruct employees to run the application from their provisioned Workspaces virtual desktops.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company\u2019s Active Directory domain. Implement authentication by using the Active Directory that is running on premises. Instruct employees to run the application by using a Windows remote desktop.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an AppStream 2.0 On-Demand fleet with dynamic Fleet Auto Scaling policies for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct the employees to access the application by starting browser-based AppStream 2.0 streaming sessions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies. Implement authentication by using Amazon Cognito user pools. Instruct the employees to run the application from their browsers."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-10T22:52:00.000Z",
        "voteCount": 1,
        "content": "typical use case of appstream"
      },
      {
        "date": "2023-11-19T15:29:00.000Z",
        "voteCount": 2,
        "content": "Option C. B is possible but it needs RDP connectivity to Windows server and so will be more complex than C"
      },
      {
        "date": "2023-08-11T12:57:00.000Z",
        "voteCount": 1,
        "content": "Answer: C - Don't even think in any other option. It's AppStream what they need to provision."
      },
      {
        "date": "2023-07-21T07:55:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-07T20:40:00.000Z",
        "voteCount": 3,
        "content": "it's C, so Linux desktops can access via browser"
      },
      {
        "date": "2023-06-27T10:55:00.000Z",
        "voteCount": 1,
        "content": "C - Correct. AppStream is what is Citrix XenDesktop."
      },
      {
        "date": "2023-06-26T06:43:00.000Z",
        "voteCount": 2,
        "content": "Amazon Cognito identity pools does not support AD. however WorkSpace is a right choise forthis use case though."
      },
      {
        "date": "2023-06-25T15:26:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-24T21:00:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2023-06-24T09:36:00.000Z",
        "voteCount": 4,
        "content": "Option C leverages Amazon AppStream 2.0, a fully managed application streaming service. With AppStream 2.0, you can create an image that includes the Windows-based desktop application and the required configurations."
      },
      {
        "date": "2023-06-24T03:41:00.000Z",
        "voteCount": 1,
        "content": "C seems correct answer."
      },
      {
        "date": "2023-06-23T14:28:00.000Z",
        "voteCount": 1,
        "content": "C\nUse appstream"
      },
      {
        "date": "2023-06-21T03:33:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2023-06-25T15:26:00.000Z",
        "voteCount": 22,
        "content": "Stop putting wrong answers in every question"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 281,
    "url": "https://www.examtopics.com/discussions/amazon/view/112779-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is collecting a large amount of data from a fleet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop Distributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache Presto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM and 10 PM.<br><br>The company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective solution that will allow SQL data queries.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore data in Amazon S3. Use Amazon Redshift Spectrum to query data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore data in Amazon Redshift. Use Amazon Redshift to query data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T15:41:00.000Z",
        "voteCount": 6,
        "content": "Storing the data in Amazon S3 is a cost-effective solution compared to running a persistent EMR cluster with HDFS.\nThe AWS Glue Data Catalog provides a centralized metadata repository for organizing and cataloging data in S3.\nAmazon Athena is a serverless query service that allows you to run SQL queries directly against data in S3 without the need for a dedicated cluster or infrastructure.\nBy using Amazon Athena, you only pay for the queries you run, which aligns with the requirement of cost-effectiveness."
      },
      {
        "date": "2024-05-12T11:57:00.000Z",
        "voteCount": 1,
        "content": "Why not D , Is it because it is expensive?"
      },
      {
        "date": "2024-08-19T20:03:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-08-10T06:59:00.000Z",
        "voteCount": 1,
        "content": "Yeah, you don't wanna build a Redshift cluster for it. You store data in S3, and  use Athena to query it, so you just pay for the query you run rather than paying for the whole Redshift cluster"
      },
      {
        "date": "2024-03-19T05:21:00.000Z",
        "voteCount": 2,
        "content": "Option B -  Athena can connect to your data stored in Amazon S3 using the AWS Glue Data Catalog to store metadata such as table and column names. After the connection is made, your databases, tables, and views appear in Athena's query editor.\n\nhttps://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html"
      },
      {
        "date": "2024-01-21T12:55:00.000Z",
        "voteCount": 1,
        "content": "The question doesn't provide enough info to calculate the answer.  We need to know how large the emr cluster is, how many queries, and how many TBs/PBs of data per query per day.  However I'm leaning towards...\n\nAnswer C: Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data.\n\nEMRFS is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3.\n\nThe company could switch to EMRFS and continue to use Presto which comes included in EMR and turn off the clusters when not in use while the data persists in EMRFS(S3).\n\nEMR comes in many flavors with different price points (EC2, Serverless) and is geared more towards daily data pipelines like this company is running.\n\nRegarding B: Athena is serverless and great for ad-hoc queries, but it is not cheap."
      },
      {
        "date": "2023-12-27T12:02:00.000Z",
        "voteCount": 2,
        "content": "significantly more expensive to store data in Redshift compared to S3 HOWEVER \n https://docs.aws.amazon.com/redshift/latest/gsg/data-lake.html  You can use Amazon Redshift Spectrum to query data in Amazon S3 files without having to load the data into Amazon Redshift tables.  Athena: While cost-effective for occasional ad-hoc queries, Athena's serverless architecture may not be as performant for frequent, resource-intensive queries  [Queries scan large amounts of data]"
      },
      {
        "date": "2023-11-19T15:36:00.000Z",
        "voteCount": 3,
        "content": "B is most cost effective. A Redshift Spectrum can be a good option but then it needs Reshift cluster which my be more expensive. One information missing in the question is many queries/sec. If there are large number queries/sec then A can be better choice."
      },
      {
        "date": "2023-07-21T07:50:00.000Z",
        "voteCount": 1,
        "content": "Correct B"
      },
      {
        "date": "2023-07-07T20:42:00.000Z",
        "voteCount": 2,
        "content": "it's a B"
      },
      {
        "date": "2023-07-02T20:13:00.000Z",
        "voteCount": 4,
        "content": "Clasic ServerLess \nS3 Datalake\nGlue for ETL\nAthena for Query"
      },
      {
        "date": "2023-06-27T10:57:00.000Z",
        "voteCount": 1,
        "content": "B - S3 , GDC and Athena for sure is the cheapest."
      },
      {
        "date": "2023-06-24T21:06:00.000Z",
        "voteCount": 1,
        "content": "B is most cost effective"
      },
      {
        "date": "2023-06-24T09:41:00.000Z",
        "voteCount": 1,
        "content": "S3 with Glue and Athena will do the trick"
      },
      {
        "date": "2023-06-24T03:47:00.000Z",
        "voteCount": 1,
        "content": "B could be the answer"
      },
      {
        "date": "2023-06-23T14:30:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-06-21T03:00:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 282,
    "url": "https://www.examtopics.com/discussions/amazon/view/112780-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase visibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many development and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and project ID numbers for all existing and future DynamoDB tables and RDS instances.<br><br>Which strategy should the solutions architect provide to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged RDS databases and DynamoDB resources every hour using a cross-account role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update existing federated roles to restrict privileges to provision resources that do not include the cost center and project ID on the resource."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T05:28:00.000Z",
        "voteCount": 2,
        "content": "Option C:  Expanding use of tag policies in AWS Organization \n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies-getting-started.html"
      },
      {
        "date": "2023-11-19T15:40:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2023-07-21T07:49:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-07T20:45:00.000Z",
        "voteCount": 2,
        "content": "C of course"
      },
      {
        "date": "2023-07-07T01:43:00.000Z",
        "voteCount": 2,
        "content": "C.\nA will meet only 1 of the 2 points which is the Tag. A wont prevent it in the future."
      },
      {
        "date": "2023-06-27T10:58:00.000Z",
        "voteCount": 2,
        "content": "C - Apply tags and prevent future untagged resources to be created with SCPs."
      },
      {
        "date": "2023-06-25T14:17:00.000Z",
        "voteCount": 1,
        "content": "C , adicionally use SCP for denied not create resource without tag in the future"
      },
      {
        "date": "2023-06-25T04:59:00.000Z",
        "voteCount": 3,
        "content": "Requirement \"There is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using AWS CloudFormation with consistent tagging.\" equals SCP, so answer C"
      },
      {
        "date": "2023-06-24T21:13:00.000Z",
        "voteCount": 2,
        "content": "C is correct. \nA only takes care of existing resources not future resources"
      },
      {
        "date": "2023-06-24T09:49:00.000Z",
        "voteCount": 1,
        "content": "Option A suggests using the Tag Editor feature in AWS Billing and Cost Management to tag existing resources. By using consistent tagging through cost allocation tags, the cost center and project ID can be defined and associated with the DynamoDB tables and RDS instances. Allowing 24 hours for tags to propagate ensures that the existing resources are appropriately tagged."
      },
      {
        "date": "2023-06-24T03:51:00.000Z",
        "voteCount": 1,
        "content": "C makes sense, using SCP"
      },
      {
        "date": "2023-06-23T14:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct use SCPs"
      },
      {
        "date": "2023-06-23T07:28:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2023-06-21T11:09:00.000Z",
        "voteCount": 3,
        "content": "Why not C, C will take care of existing and SCP will ensure future resources are tagged"
      },
      {
        "date": "2023-06-21T03:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-11-02T12:29:00.000Z",
        "voteCount": 1,
        "content": "wrong answer. you need the scp for future resources."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 283,
    "url": "https://www.examtopics.com/discussions/amazon/view/112781-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different accounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated connectivity to AWS.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a public VIF between the on-premises environment and the private VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 interface endpoint in the networking account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 gateway endpoint in the networking account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the accounts that host the S3 buckets with the VPC in the network account."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-07T21:17:00.000Z",
        "voteCount": 7,
        "content": "You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints (by using AWS PrivateLink). A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region by using VPC peering or AWS Transit Gateway."
      },
      {
        "date": "2023-11-19T15:47:00.000Z",
        "voteCount": 5,
        "content": "S3 Gateway endpoint is for access inside VPC and not from on-premise."
      },
      {
        "date": "2024-10-08T11:50:00.000Z",
        "voteCount": 1,
        "content": "Why not A and D: \"Currently, gateway VPC endpoints for Amazon S3 do not support accessing resources in a different Region, in a different VPC, or from an on-premises data center (environment outside of AWS).\""
      },
      {
        "date": "2024-07-13T02:04:00.000Z",
        "voteCount": 1,
        "content": "A, C for sure.\nInterface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region by using VPC peering or AWS Transit Gateway."
      },
      {
        "date": "2024-01-29T07:02:00.000Z",
        "voteCount": 4,
        "content": "Really, really awful question. Agree that the answer they're looking for is AC. However, technically, this element of B if done in isolation will also work and might actually be better: \"Set up an AWS Direct Connect connection with a public VIF between the on-premises environment and the private VPC\". Just because you're accessing S3 using its public IPs, doesn't mean you're routing over the \"public internet\". Plus, accessing S3 via its regular public prefixes means no mucking around with `--endpoint-url https://bucket.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com` command line options. Your devs can just use S3 normally with normal DNS hostnames. If they forget then the traffic will route via the internet - oops. So B+anything-else is technically also correct, and arguably preferable."
      },
      {
        "date": "2024-01-29T07:08:00.000Z",
        "voteCount": 3,
        "content": "And yes, I know that technically a public VIF has nothing to do with nor are they attached to VPCs, but the core tenet of B is to \"use public VIF\", i.e. public peering. So, if I was faced with this situation in real life, I'd consider that. The downside of the public VIF approach is missing out on VPC endpoint policies. Maybe the optimal solution is to deploy EC2 forward proxies in a VPC with an S3 gateway endpoint?"
      },
      {
        "date": "2023-12-10T08:54:00.000Z",
        "voteCount": 1,
        "content": "A. Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC.\n\nThis creates a dedicated, private connection between the on-premises systems and the AWS VPC, ensuring data remains secure and isolated from the public internet. The private VIF further enhances security by preventing access to the S3 buckets from the public internet.\n\nE. Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the accounts that host the S3 buckets with the VPC in the network account.\n\nThis establishes connectivity between the private VPC and the VPCs containing the S3 buckets, enabling private data transfer without crossing the public internet. Peering allows resources in both VPCs to communicate directly, maintaining data security and privacy."
      },
      {
        "date": "2023-12-17T02:48:00.000Z",
        "voteCount": 3,
        "content": "S3 doesn't live in a customer VPC.  Its a public service.  So you either connect to it over the Internet or through a VPC Gateway endpoint of Interface Endpoint depending on the setup."
      },
      {
        "date": "2023-11-13T15:16:00.000Z",
        "voteCount": 2,
        "content": "C: needs to be an endpoint\nE: Company does NOT have a dedicated network connection so DX answers are out, so peer the VPC's."
      },
      {
        "date": "2023-11-05T19:33:00.000Z",
        "voteCount": 4,
        "content": "AC:\n\"The company must send the data privately\" = Interface endpoints\n\nGateway endpoints, do not allow access from on premises."
      },
      {
        "date": "2023-09-03T01:18:00.000Z",
        "voteCount": 3,
        "content": "AC - DX+Interface endpoint.\n\nBoth gateway and interface endpoints will use aws backbone, so not internet. However, you cannot access a GW endpoint from onprem. Therefore needs interface (ENIs) endpoints."
      },
      {
        "date": "2023-07-20T17:36:00.000Z",
        "voteCount": 1,
        "content": "Correct AC."
      },
      {
        "date": "2023-07-07T20:52:00.000Z",
        "voteCount": 1,
        "content": "AC of course. see links below"
      },
      {
        "date": "2023-06-28T05:34:00.000Z",
        "voteCount": 1,
        "content": "AC - links provided by other members provide very good explanation."
      },
      {
        "date": "2023-06-27T11:08:00.000Z",
        "voteCount": 4,
        "content": "AC  - detailed steps under use case 2 -&gt; https://repost.aws/knowledge-center/s3-bucket-access-direct-connect"
      },
      {
        "date": "2023-06-26T13:44:00.000Z",
        "voteCount": 3,
        "content": "Endpoint comparison: https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3"
      },
      {
        "date": "2023-06-27T18:52:00.000Z",
        "voteCount": 1,
        "content": "Thank you. Perfect explanation"
      },
      {
        "date": "2023-06-25T18:33:00.000Z",
        "voteCount": 2,
        "content": "AC - Access from on-prem is using S3 Interface Endpoint + Private VIF.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/secure-hybrid-access-to-amazon-s3-using-aws-privatelink/"
      },
      {
        "date": "2023-06-24T21:23:00.000Z",
        "voteCount": 1,
        "content": "Seems AC"
      },
      {
        "date": "2023-06-24T10:07:00.000Z",
        "voteCount": 1,
        "content": "Amazon S3: interface VPC endpoint and gateway VPC endpoint. Difference : \nWhen you configure an interface VPC endpoint, an elastic network interface (ENI) with a private IP address is deployed in your subnet. An Amazon EC2 instance in the VPC can communicate with an Amazon S3 bucket through the ENI and AWS network. Using the interface endpoint, applications in your on-premises data center can easily query S3 buckets over AWS Direct Connect or Site-to-Site VPN. Interface endpoint supports a growing list of AWS services. Consult our documentation to find AWS services compatible with interface endpoints powered by AWS PrivateLink."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 284,
    "url": "https://www.examtopics.com/discussions/amazon/view/112782-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company operates quick-service restaurants. The restaurants follow a predictable model with high sales traffic for 4 hours daily. Sales traffic is lower outside of those peak hours.<br><br>The point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database table uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.<br><br>The company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the provisioned RCUs and WCUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the DynamoDB table to use on-demand capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Dynamo DB auto scaling for the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T08:09:00.000Z",
        "voteCount": 8,
        "content": "The correct answer is B: On Demand\nAutoscaling with the current RCU and WCU will not make sense since it is defined for peak loads"
      },
      {
        "date": "2024-08-20T22:39:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-04-10T14:40:00.000Z",
        "voteCount": 1,
        "content": "\"C\" is correct, because the question states a \"\n\nYou can use auto scaling to adjust your table\u2019s provisioned capacity automatically in response to traffic changes. Provisioned mode is a good option if any of the following are true:\n- You have PREDICTABLE application traffic.\n\nOn-demand mode is a good option if any of the following are true:\n- You have unpredictable application traffic.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand"
      },
      {
        "date": "2023-11-19T16:12:00.000Z",
        "voteCount": 7,
        "content": "Question itself is bit unclear as it does not state difference in load for peak vs non-peak. Choice of most cost-effective depends on this between Reserved vs on-demands vs autoscaling. Overall autoscaling looks safest option."
      },
      {
        "date": "2024-07-24T00:21:00.000Z",
        "voteCount": 1,
        "content": "C is  correct one\nDynamoDB auto-scaling for predictable \nDynamoDB on-demand for un-predictable"
      },
      {
        "date": "2024-07-13T00:40:00.000Z",
        "voteCount": 2,
        "content": "By implementing DynamoDB Auto Scaling, you can achieve the following benefits:\n\n    Cost savings: During non-peak hours, the provisioned capacity will automatically scale down, reducing the cost of provisioned throughput.\n    Consistent performance: During peak hours, the provisioned capacity will automatically scale up to handle the increased workload, ensuring consistent performance.\n    Reduced operational overhead: Auto Scaling eliminates the need for manual capacity management, reducing the operational burden on the IT staff."
      },
      {
        "date": "2024-05-29T02:04:00.000Z",
        "voteCount": 2,
        "content": "If you know that the peak is in the 4hour you can use autoscaling.\nBUT, if you want to reduce operation IT, and if the peak goes higher in other moment, on-demand is the way"
      },
      {
        "date": "2024-04-10T14:39:00.000Z",
        "voteCount": 2,
        "content": "\"C\" is correct, because the question states \"a predictable module\".\n\nYou can use auto scaling to adjust your table\u2019s provisioned capacity automatically in response to traffic changes. Provisioned mode is a good option if any of the following are true:\n- You have PREDICTABLE application traffic.\n\nOn-demand mode is a good option if any of the following are true:\n- You have unpredictable application traffic.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand"
      },
      {
        "date": "2024-03-07T08:52:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer. On Demand is out since it is only fully used for 4 hours daily"
      },
      {
        "date": "2024-01-21T14:56:00.000Z",
        "voteCount": 2,
        "content": "Answer C:\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"
      },
      {
        "date": "2023-12-16T09:47:00.000Z",
        "voteCount": 1,
        "content": "B ia ans\nhttps://dynobase.dev/dynamodb-on-demand-vs-provisioned-scaling/"
      },
      {
        "date": "2023-08-30T01:02:00.000Z",
        "voteCount": 4,
        "content": "When it's predictable i go for reserved capacity that have up to 77% cost reduction. https://aws.amazon.com/dynamodb/reserved-capacity/. I'll go for D."
      },
      {
        "date": "2023-12-17T02:59:00.000Z",
        "voteCount": 1,
        "content": "You are right but if you reserve the capacity based on the peak requirement, you only use that capacity for 4 / 24 hours per day.  Whilst if you provision to guarantee availability and auto-scale to that level you will save 20 hours of low usage.  As @career360guru said, we will need more information as to what that balance of 72% savings on 4 hours would be when compared to provisioned+auto-scaled means for the savings on 20 hours (per day)."
      },
      {
        "date": "2023-07-07T20:54:00.000Z",
        "voteCount": 3,
        "content": "its C for predictable scaling"
      },
      {
        "date": "2023-07-02T20:23:00.000Z",
        "voteCount": 5,
        "content": "C - Autoscaling. \"In addition, you can leverage auto-scaling to adjust the table's capacity based on the application\u2019s utilization, thereby enforcing cost optimization measures. It is a good fit for workloads with predictable traffic. \"\nhttps://www.finout.io/blog/how-to-optimize-usage-and-reduce-dynamodb-pricing"
      },
      {
        "date": "2023-06-27T11:14:00.000Z",
        "voteCount": 5,
        "content": "C - Autoscaling. \"In addition, you can leverage auto-scaling to adjust the table's capacity based on the application\u2019s utilization, thereby enforcing cost optimization measures. It is a good fit for workloads with predictable traffic. \"\nhttps://www.finout.io/blog/how-to-optimize-usage-and-reduce-dynamodb-pricing"
      },
      {
        "date": "2023-06-24T21:28:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer with predictable pattern auto scaling is good enough and not on demand"
      },
      {
        "date": "2023-06-21T11:32:00.000Z",
        "voteCount": 2,
        "content": "C : https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
      },
      {
        "date": "2023-06-24T10:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct A, B and D do not meet needs."
      },
      {
        "date": "2023-06-21T03:03:00.000Z",
        "voteCount": 2,
        "content": "C is the correct Option"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 285,
    "url": "https://www.examtopics.com/discussions/amazon/view/112783-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently does not use API keys to authorize requests. The API model is as follows:<br><br>GET /posts/{postId}: to get post details<br>GET /users/{userId}: to get user details<br>GET /comments/{commentId}: to get comments details<br><br>The company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by making the comments appear in real time.<br><br>Which design should be used to reduce comment latency and improve user experience?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse edge-optimized API with Amazon CloudFront to cache API responses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the blog application code to request GET/comments/{commentId} every 10 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS AppSync and leverage WebSockets to deliver comments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the concurrency limit of the Lambda functions to lower the API response time."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-25T15:49:00.000Z",
        "voteCount": 7,
        "content": "Option C (Use AWS AppSync and leverage WebSockets to deliver comments) is the most appropriate solution for real-time comments. AWS AppSync is a fully managed service that simplifies real-time data synchronization and offline capabilities for applications. It supports WebSockets, which enables real-time communication between clients and the server. By leveraging AppSync and WebSockets, the comments can be delivered instantly to users as they are posted, reducing comment latency and improving user engagement."
      },
      {
        "date": "2023-07-07T20:57:00.000Z",
        "voteCount": 6,
        "content": "C. websockets ==realtime"
      },
      {
        "date": "2024-01-21T15:02:00.000Z",
        "voteCount": 3,
        "content": "Answer C:\nhttps://docs.aws.amazon.com/appsync/latest/devguide/aws-appsync-real-time-data.html"
      },
      {
        "date": "2023-06-27T11:17:00.000Z",
        "voteCount": 1,
        "content": "C - Correct. https://advancedweb.hu/real-time-data-with-appsync-subscriptions/"
      },
      {
        "date": "2023-06-24T21:34:00.000Z",
        "voteCount": 2,
        "content": "C is correct others are not real time and cost effective"
      },
      {
        "date": "2023-06-24T10:17:00.000Z",
        "voteCount": 3,
        "content": "AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need. With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda."
      },
      {
        "date": "2023-06-21T03:04:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 286,
    "url": "https://www.examtopics.com/discussions/amazon/view/112784-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product teams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the internet.<br><br>What is the MOST operationally efficient way to enforce this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-27T11:27:00.000Z",
        "voteCount": 5,
        "content": "B - Since you have 100s of accounts. If it was a single account, then A.\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"
      },
      {
        "date": "2023-08-16T06:30:00.000Z",
        "voteCount": 1,
        "content": "don't think there is so called \"S3 access point resource policy\" no matter it is 1 or 100 accounts. it is either identity or bucket resource policy"
      },
      {
        "date": "2023-12-27T12:48:00.000Z",
        "voteCount": 1,
        "content": "@duriselvan  \"the\" access point \nwhich one bro.. all of them ? ==&gt;\nhundreds of AWS accounts centrally in an organization in AWS Organizations. company recently started to allow product teams to create and manage their own S3 access points in their accounts. \nregarding Minimal impact? was that constraint perhaps from some other question ?\nMOST operationally efficient way to enforce this requirement\nLastly  Resource policies inherently apply to actions performed on a specific resource. To control the creation of a resource like an access point, a broader policy mechanism is needed."
      },
      {
        "date": "2023-12-13T02:16:00.000Z",
        "voteCount": 1,
        "content": "A. Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.\n\nHere's why:\n\nGranularity: Enforcing the restriction within the access point resource policy itself offers the most granular control. It applies directly to the access point creation action, preventing unauthorized configuration at the source.\nCentralized management: Implementing the policy at the access point level allows for centralized management and avoids the need to manage individual IAM policies in each account. This simplifies operation and reduces maintenance overhead.\nMinimal impact: This approach doesn't require additional infrastructure or services like Service Control Policies (SCPs) or CloudFormation StackSets, minimizing setup and complexity."
      },
      {
        "date": "2023-11-19T16:30:00.000Z",
        "voteCount": 2,
        "content": "As customer is using Organizations B is right."
      },
      {
        "date": "2023-07-07T20:59:00.000Z",
        "voteCount": 3,
        "content": "B. SCP for scale"
      },
      {
        "date": "2023-06-25T14:21:00.000Z",
        "voteCount": 3,
        "content": "B is correct SCP at Org level"
      },
      {
        "date": "2023-06-24T22:28:00.000Z",
        "voteCount": 3,
        "content": "B is correct SCP at Org level"
      },
      {
        "date": "2023-06-24T10:23:00.000Z",
        "voteCount": 2,
        "content": "SCP is a type of policy that you can use to manage permissions in your organization, allowing you to control AWS service actions across multiple AWS accounts. By creating the SCP at the root level, you ensure that all accounts within the organization are subjected to this policy. This is an efficient way to enforce the requirement across all accounts as it requires a single policy change instead of individual changes in every account."
      },
      {
        "date": "2023-06-24T04:08:00.000Z",
        "voteCount": 2,
        "content": "B\nwhen the question mention AWS Organizations, use SCP always the good choice."
      },
      {
        "date": "2023-06-23T07:56:00.000Z",
        "voteCount": 1,
        "content": "of course answer B"
      },
      {
        "date": "2023-06-21T11:52:00.000Z",
        "voteCount": 2,
        "content": "B - This approach ensures centralized policy management and consistent enforcement across all AWS accounts within the organization. It avoids the need for configuring bucket policies or access point resource policies in each individual account, making it operationally efficient."
      },
      {
        "date": "2023-06-21T03:05:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2024-08-20T22:42:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2023-07-31T13:47:00.000Z",
        "voteCount": 4,
        "content": "yeah be careful, you skewing the numbers on the vote, we are trying to help others."
      },
      {
        "date": "2023-06-24T04:07:00.000Z",
        "voteCount": 9,
        "content": "you always provided wrong answer, not sure if you do that on purpose."
      },
      {
        "date": "2023-06-25T15:53:00.000Z",
        "voteCount": 7,
        "content": "Why do you always provide wrong answers? Please do your research before making a comment, as you re misleading others"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 287,
    "url": "https://www.examtopics.com/discussions/amazon/view/112785-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The solutions architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.<br><br>What should be done next to complete the update?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedirect to the new environment using Amazon Route 53.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the Swap Environment URLs option.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the Auto Scaling launch configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the DNS records to point to the green environment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T10:26:00.000Z",
        "voteCount": 8,
        "content": "AWS Elastic Beanstalk provides a Swap Environment URLs option for performing a blue/green deployment. This operation swaps the CNAME records of two environments, thus rerouting traffic from the original environment (blue) to the new environment (green)."
      },
      {
        "date": "2023-12-16T10:33:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html"
      },
      {
        "date": "2023-11-19T16:35:00.000Z",
        "voteCount": 3,
        "content": "Option B."
      },
      {
        "date": "2023-07-20T17:31:00.000Z",
        "voteCount": 1,
        "content": "Correct B."
      },
      {
        "date": "2023-07-07T21:00:00.000Z",
        "voteCount": 2,
        "content": "its a B"
      },
      {
        "date": "2023-06-27T12:57:00.000Z",
        "voteCount": 2,
        "content": "B\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html"
      },
      {
        "date": "2023-06-27T11:30:00.000Z",
        "voteCount": 2,
        "content": "B - Look at the link, step 5 -&gt; https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html"
      },
      {
        "date": "2023-06-25T14:25:00.000Z",
        "voteCount": 2,
        "content": "B. Select the Swap Environment URLs option."
      },
      {
        "date": "2023-06-24T22:46:00.000Z",
        "voteCount": 1,
        "content": "B to swap from blue to green"
      },
      {
        "date": "2023-06-23T16:02:00.000Z",
        "voteCount": 2,
        "content": "B elastic beanstalk has Swap Environment URLs feature\nhttps://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html"
      },
      {
        "date": "2023-06-23T07:59:00.000Z",
        "voteCount": 1,
        "content": "B of course"
      },
      {
        "date": "2023-06-21T03:06:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 288,
    "url": "https://www.examtopics.com/discussions/amazon/view/112786-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users worldwide will upload their images. The will then overlay text on the uploaded images, which will then be published on the company website.<br><br>Which design should a solutions architect implement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fleet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fleet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet of EC2 instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T08:01:00.000Z",
        "voteCount": 5,
        "content": "ALB \u2013 B is out\nS3 is good enough, EFS and EBS are too much for image processing"
      },
      {
        "date": "2023-06-25T15:57:00.000Z",
        "voteCount": 5,
        "content": "Option C (Store the uploaded images in an S3 bucket and use S3 event notification with SQS queue) is the most suitable design. Amazon S3 provides highly scalable and durable storage for the uploaded images. Configuring S3 event notifications to send messages to an SQS queue allows for decoupling the processing of images from the upload process. A fleet of EC2 instances can pull messages from the SQS queue to process the images and store them in another S3 bucket. Scaling out the EC2 instances based on SQS queue depth using CloudWatch metrics ensures efficient utilization of resources. Enabling Amazon CloudFront with the origin set to the S3 bucket containing the processed images improves the global availability and performance of image delivery."
      },
      {
        "date": "2023-11-19T16:39:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-07-20T17:29:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-07-07T21:02:00.000Z",
        "voteCount": 1,
        "content": "its a C"
      },
      {
        "date": "2023-06-27T11:34:00.000Z",
        "voteCount": 4,
        "content": "C - no doubt, SQS and CloudFront for processed image retrieval"
      },
      {
        "date": "2023-06-25T14:30:00.000Z",
        "voteCount": 1,
        "content": "C without doubt"
      },
      {
        "date": "2023-06-24T23:08:00.000Z",
        "voteCount": 1,
        "content": "C indeed"
      },
      {
        "date": "2023-06-23T08:01:00.000Z",
        "voteCount": 2,
        "content": "C without doubt"
      },
      {
        "date": "2023-06-21T03:07:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 289,
    "url": "https://www.examtopics.com/discussions/amazon/view/112787-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-wes&amp;1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 49,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-20T17:27:00.000Z",
        "voteCount": 19,
        "content": "Correct D.\nYou cannot convert RDS MySQL to Aurora MySQL natively, but you can create an Aurora read replica of the RDS MySQL DB instance and then promote it to a standalone Aurora MySQL DB clusterhttps://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.html. This is the first step of option A in the question. However, this option also requires pausing application writes and reconfiguring the application, which can cause downtime and data inconsistency. Therefore, option A is not the best solution for the given requirements. Option D is still the correct answer because it does not require pausing writes or reconfiguring the application, and it enables cross-Region replication and write forwarding for the database."
      },
      {
        "date": "2024-08-10T18:53:00.000Z",
        "voteCount": 1,
        "content": "D is wrong, you should choose A.\n\nlook at this blog: https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/\n\nIn step 4 you do need to pause write"
      },
      {
        "date": "2024-08-10T18:54:00.000Z",
        "voteCount": 1,
        "content": "Sorry, typo, step 4"
      },
      {
        "date": "2023-12-17T03:10:00.000Z",
        "voteCount": 1,
        "content": "You need the pause of writing to the old db because of the lag in the replication."
      },
      {
        "date": "2023-10-16T21:47:00.000Z",
        "voteCount": 7,
        "content": "You cannot natively convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Instead, you can create an Amazon Aurora MySQL replica of the RDS MySQL RDS DB instance:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html\nhttps://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/"
      },
      {
        "date": "2024-08-10T18:54:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/\n\nA is correct. You do need to pause write before creating replica."
      },
      {
        "date": "2024-08-10T04:58:00.000Z",
        "voteCount": 1,
        "content": "Aurora supports cross-region replication and write forwarding. Only need to promote DB Custer in the failover scenario, not for migration."
      },
      {
        "date": "2024-07-08T19:39:00.000Z",
        "voteCount": 2,
        "content": "Vote D.\nOn top of ggrodskiy's point, standalone DB cluster only support 1 regoin. If multiple regions are requireed then DB cluster won't be standalone."
      },
      {
        "date": "2024-05-03T02:39:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2024-03-07T10:37:00.000Z",
        "voteCount": 3,
        "content": "This approach leverages Amazon Aurora's Global Database capability, which allows for a single database to span multiple AWS regions, thus enabling low-latency reads and writes in multiple regions and providing data replication across regions with minimal latency. By converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster and enabling write forwarding, the solution supports writes in multiple regions and ensures that the data is synchronized across the regions in real time. This setup allows customers in both the US and Europe to see updates from each other as they happen, meeting the requirement for real-time data consistency and low application latency."
      },
      {
        "date": "2024-01-29T10:25:00.000Z",
        "voteCount": 1,
        "content": "Galera + ProxySQL ftw"
      },
      {
        "date": "2024-01-14T20:47:00.000Z",
        "voteCount": 5,
        "content": "D said 'Convert' but not 'Mirgrate'. You cannot convert RDS MySQL to Aurora MySQL natively."
      },
      {
        "date": "2023-12-11T08:51:00.000Z",
        "voteCount": 3,
        "content": "D CORRECT\nD. Aurora Global Database with Write Forwarding:\n\nThis solution addresses all requirements:\nReal-time data access and updates: Aurora provides global secondary databases in the chosen region (eu-west-1) for low latency and consistent data.\nMinimal downtime: Aurora automatically handles failovers and data synchronization between regions.\nWrite forwarding: Both regions can perform write operations, ensuring real-time updates for all users.\nHigh availability: Aurora offers automatic backups and failover capabilities.\nTherefore, D. Converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster with a secondary Region in eu-west-1 and enabling write forwarding is the most suitable solution. It meets all requirements for data availability, minimal latency, real-time updates, and high availability for both US and European customers."
      },
      {
        "date": "2023-12-17T03:13:00.000Z",
        "voteCount": 1,
        "content": "The first statement in D (\"Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster.\") is wrong, therefore D is wrong.  The multiple choice is based on these tricks.  Real life is a different matter when we say \"Convert\" to mean go through the process of replacing by replicating, etc."
      },
      {
        "date": "2023-11-19T17:01:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-11-19T04:04:00.000Z",
        "voteCount": 3,
        "content": "yes, migration is done through the replica promotion"
      },
      {
        "date": "2023-11-02T12:59:00.000Z",
        "voteCount": 3,
        "content": "RDS MySQL to aurora replica, then promote the replica as aurora cluster."
      },
      {
        "date": "2023-10-02T21:58:00.000Z",
        "voteCount": 1,
        "content": "A is ans:"
      },
      {
        "date": "2023-09-24T17:49:00.000Z",
        "voteCount": 5,
        "content": "Write forwarding is a feature of Aurora that allows writes to be directed to the primary cluster while maintaining read access to the replica cluster, ensuring data consistency and low latency."
      },
      {
        "date": "2023-08-24T05:51:00.000Z",
        "voteCount": 2,
        "content": "It's clearly A , not any other option"
      },
      {
        "date": "2023-07-31T10:46:00.000Z",
        "voteCount": 4,
        "content": "A, \u2018cause of the conversion which is not possible"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 290,
    "url": "https://www.examtopics.com/discussions/amazon/view/112788-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for authentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.<br><br>A solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. The solution must not change the way customers connect.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T20:36:00.000Z",
        "voteCount": 6,
        "content": "B\nQuestion say \" The EC2 instance also has an attached security group that allows access from all customer IP addresses.\"\n\nB say  \"Attach the security group with customer IP addresses to the new endpoint\"\n\nShould be Security Group for working with security for customer"
      },
      {
        "date": "2024-10-08T12:30:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/aws-sftp-endpoint-type"
      },
      {
        "date": "2024-09-09T10:58:00.000Z",
        "voteCount": 1,
        "content": "B is wrong, it's similar to A but uses a VPC-hosted endpoint, which is unnecessary for this public-facing scenario and adds complexity without any clear benefit."
      },
      {
        "date": "2023-12-18T04:28:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/transfer/latest/userguide/create-server-in-vpc.html  -b ans"
      },
      {
        "date": "2023-12-08T09:41:00.000Z",
        "voteCount": 1,
        "content": "S Fargate and a Network Load Balancer provides the most efficient and secure solution, meeting all the requirements without compromising availability, introducing unnecessary complexity, or disrupting existing customer access."
      },
      {
        "date": "2023-11-19T17:06:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-10-18T03:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nhttps://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp-servers/"
      },
      {
        "date": "2023-07-07T21:12:00.000Z",
        "voteCount": 1,
        "content": "B of course. need SG to whitelist IPs"
      },
      {
        "date": "2023-07-06T04:47:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/aws-sftp-endpoint-type"
      },
      {
        "date": "2023-06-27T11:44:00.000Z",
        "voteCount": 4,
        "content": "It's B. You can't attach elastic IP with A). -&gt; https://repost.aws/knowledge-center/aws-sftp-endpoint-type - look at the table"
      },
      {
        "date": "2023-06-24T22:12:00.000Z",
        "voteCount": 4,
        "content": "It's B: https://repost.aws/knowledge-center/aws-sftp-endpoint-type"
      },
      {
        "date": "2023-06-24T10:44:00.000Z",
        "voteCount": 1,
        "content": "A is public access; the requirement says need Security Group with Ip addresses - B is correct"
      },
      {
        "date": "2023-06-22T16:21:00.000Z",
        "voteCount": 2,
        "content": "Olabiba.ai Says B:\n\nOption B suggests disassociating the Elastic IP address from the EC2 instance and creating an Amazon S3 bucket for SFTP file hosting. An AWS Transfer Family server is then created and configured with a VPC-hosted, internet-facing endpoint. The SFTP Elastic IP address is associated with the new endpoint, and the security group with customer IP addresses is attached to the endpoint. The Transfer Family server is pointed to the S3 bucket, and all files from the SFTP server are synced to the S3 bucket."
      },
      {
        "date": "2023-06-21T03:08:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-07-31T14:38:00.000Z",
        "voteCount": 2,
        "content": "again wrong, dont be quick and wrong."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 291,
    "url": "https://www.examtopics.com/discussions/amazon/view/112789-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4 hours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.<br><br>The current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and store the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances each night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script terminates the instances when the processing is complete.<br><br>The Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new design.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fleet of EC2 On-Demand Instances each night to perform the batch processing of the S3 data. Configure the script to terminate the instances when the processing is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ingestion process to use a fleet of EC2 Reserved Instances with 3-year reservations behind a Network LoadBalancer. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule an AWS Lambda function to run nightly to query Amazon Redshift to generate the daily statistics."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-11T07:02:00.000Z",
        "voteCount": 5,
        "content": "A=&gt;  Use a scheduled script to launch a fleet of EC2 On-Demand wrong\nC=&gt; Update the ingestion process to use a fleet of EC2 Reserved Instances wrong\nD=&gt; lambda wrong"
      },
      {
        "date": "2024-01-21T16:14:00.000Z",
        "voteCount": 2,
        "content": "Answer B:\nhttps://docs.aws.amazon.com/batch/latest/userguide/best-practices.html"
      },
      {
        "date": "2023-12-27T05:00:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2023-11-19T21:04:00.000Z",
        "voteCount": 4,
        "content": "B is right answer. In C in addition to 3 year reserved instances NLB is extra cost."
      },
      {
        "date": "2023-11-19T21:07:00.000Z",
        "voteCount": 2,
        "content": "Compared to on-demand, Reserved instances can be upto 73% reduction but Spot can go upto 90%."
      },
      {
        "date": "2023-08-07T07:54:00.000Z",
        "voteCount": 1,
        "content": "For a stable rate of ingestion I choose EC2 with 3yr reservation over Firehose &amp; S3API costs. Using Spot instances for the low priority aggregation will lower the costs further"
      },
      {
        "date": "2023-07-08T08:33:00.000Z",
        "voteCount": 1,
        "content": "its a B"
      },
      {
        "date": "2023-06-27T12:44:00.000Z",
        "voteCount": 4,
        "content": "B - Correct. And only because of this -&gt; \" The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.\"\nSpot instances are not guaranteed and if the condition above was not there, than probably C."
      },
      {
        "date": "2023-06-26T12:39:00.000Z",
        "voteCount": 2,
        "content": "b-b-b-b-b-b-b"
      },
      {
        "date": "2023-06-24T10:50:00.000Z",
        "voteCount": 2,
        "content": "S3 + Batch with SOT servers"
      },
      {
        "date": "2023-06-21T18:56:00.000Z",
        "voteCount": 1,
        "content": "Support B as answer. MOST cost effective"
      },
      {
        "date": "2023-06-21T03:08:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 292,
    "url": "https://www.examtopics.com/discussions/amazon/view/112790-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to downstream applications through an NFS share.<br><br>As part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of static public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data center and its VPC.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T07:39:00.000Z",
        "voteCount": 7,
        "content": "A is correct\nB is incorrect because for Publicly accessible endpoints for AWS Transfer Family you can\u2019t attach a static IP address. AWS provides IP addresses that are subject to change. IPs are provided via AWS Global Accelerator, which uses static Anycast IP addresses\nhttps://repost.aws/knowledge-center/aws-sftp-endpoint-type"
      },
      {
        "date": "2024-09-09T11:12:00.000Z",
        "voteCount": 1,
        "content": "A is wrong. The use of an internet-facing VPC endpoint for AWS Transfer Family is not necessary here. The publicly accessible endpoint provided by AWS Transfer Family itself meets the requirement for external access. Also, assigning Elastic IPs to subnets is unnecessary because AWS Transfer Family manages public IPs."
      },
      {
        "date": "2023-12-08T09:24:00.000Z",
        "voteCount": 3,
        "content": "a IS ANS\nHere's why this solution is optimal:\n\nManaged SFTP: AWS Transfer Family eliminates the need to manage and maintain SFTP servers, reducing operational overhead compared to EC2-based solutions.\nHigh availability: It provides built-in high availability, ensuring continuous access to SFTP services even in case of component failures.\nStatic IP addresses: The internet-facing VPC endpoint with Elastic IP addresses provides fixed IPs for external vendors, meeting their security requirements.\nSecure file storage: EFS offers a managed, scalable, and highly available file system, ensuring secure file storage and access for downstream applications.\nNFS compatibility: EFS integrates seamlessly with NFS, allowing easy migration of downstream applications to the new file system."
      },
      {
        "date": "2023-11-19T21:12:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-07-20T17:13:00.000Z",
        "voteCount": 2,
        "content": "Correct A."
      },
      {
        "date": "2023-07-15T06:46:00.000Z",
        "voteCount": 2,
        "content": "AAAAAAAAA"
      },
      {
        "date": "2023-07-08T08:35:00.000Z",
        "voteCount": 1,
        "content": "its an A.. static IPs"
      },
      {
        "date": "2023-06-27T12:46:00.000Z",
        "voteCount": 2,
        "content": "It's A. You can't have elastic IP with B."
      },
      {
        "date": "2023-06-27T12:46:00.000Z",
        "voteCount": 3,
        "content": "A https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-sftp-server-to-aws-using-aws-transfer-for-sftp.html"
      },
      {
        "date": "2023-06-25T16:22:00.000Z",
        "voteCount": 3,
        "content": "Option A suggests creating an AWS Transfer Family server and configuring an internet-facing VPC endpoint for it. By specifying an Elastic IP address for each subnet, the company can provide a set of static public IP addresses to external vendors. The Transfer Family server can be configured to place files into an Amazon Elastic File System (Amazon EFS) file system, which provides a scalable and highly available storage solution across multiple Availability Zones. This allows the company to maintain high availability for the SFTP site and its downstream applications without the need for manual intervention or additional operational overhead."
      },
      {
        "date": "2023-06-24T11:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct for Pvt IP addresses."
      },
      {
        "date": "2023-06-24T07:34:00.000Z",
        "voteCount": 2,
        "content": "A is correct\nIn B there is NO mention of elasticIPs. the question asks \"The solution must provide external vendors with a set of static public IP addresses that the vendors can allow\""
      },
      {
        "date": "2023-06-21T03:09:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 293,
    "url": "https://www.examtopics.com/discussions/amazon/view/112791-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:<br><br><br>VPC CIDR: 10.0.0.0/23 -<br><br>AZ1 subnet CIDR: 10.0.0.0/24 -<br><br>AZ2 subnet CIDR: 10.0.1.0/24 -<br><br>Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTerminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-06T06:07:00.000Z",
        "voteCount": 45,
        "content": "This question was painful to read."
      },
      {
        "date": "2023-11-30T17:08:00.000Z",
        "voteCount": 12,
        "content": "Answer - A\nD is closest, but wrong as you subnets cannot be modified. They have to be deleted and re-created."
      },
      {
        "date": "2024-09-09T11:20:00.000Z",
        "voteCount": 1,
        "content": "A is wrong. It involves a complex process of deleting and recreating subnets, which could lead to downtime and operational complexity. Also, the approach of creating new subnets from the old address space is risky and can be prone to errors."
      },
      {
        "date": "2023-12-12T23:15:00.000Z",
        "voteCount": 2,
        "content": "D is ans\nere's why this option is the most suitable:\n\nMinimal downtime: It minimizes downtime by gradually shifting instances between subnets within the same VPC, ensuring continuous connectivity to the on-premises environment.\nNo additional address space: It utilizes the existing IPv4 address space by splitting the subnets, avoiding the need for additional resources.\nPhased approach: It implements the changes in manageable steps, minimizing risk and allowing for rollback if necessary."
      },
      {
        "date": "2023-11-26T22:52:00.000Z",
        "voteCount": 1,
        "content": "For the CIDR range, what's after '-'? Is something missing?"
      },
      {
        "date": "2023-11-19T10:22:00.000Z",
        "voteCount": 8,
        "content": "B do not follow the need of no downtime\nC will force you to migrate to a new CIDR\n\nA and D are similar except that in A you recreate the subnets while in D you update the subnets.\nBut you cannot update the subnets, you have to remove and recreate them.\n\nSo A is the correct answer."
      },
      {
        "date": "2023-10-17T00:23:00.000Z",
        "voteCount": 4,
        "content": "In a scenario where you must add a new AZ without service downtime, option A, which progressively transitions to new subnets in the new AZ while keeping the existing infrastructure running, is a better choice. This approach ensures high availability and minimal disruption to your services.\nOption D is not correct. You cannot update the CIDR block of an existing Amazon VPC subnet without recreating it."
      },
      {
        "date": "2023-09-22T02:58:00.000Z",
        "voteCount": 2,
        "content": "The question though lol had to look for the difference in the options to remember the answer. When it comes to a \u201cdelete \u201c"
      },
      {
        "date": "2023-08-30T07:04:00.000Z",
        "voteCount": 2,
        "content": "D is easier, no need to delete the subnet. https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html"
      },
      {
        "date": "2023-08-21T11:30:00.000Z",
        "voteCount": 3,
        "content": "Surely wasn't a 3 min ques. Thankfully they did not throw CIDR reservations into the mix"
      },
      {
        "date": "2023-07-08T08:40:00.000Z",
        "voteCount": 3,
        "content": "A. can't update subnet"
      },
      {
        "date": "2023-07-07T21:51:00.000Z",
        "voteCount": 5,
        "content": "These answers are big pain to read"
      },
      {
        "date": "2023-06-27T12:49:00.000Z",
        "voteCount": 2,
        "content": "A - Correct. You can't modify subnet as D says."
      },
      {
        "date": "2023-06-26T17:25:00.000Z",
        "voteCount": 4,
        "content": "D: \"Update the AZ1 subnet\" in D is not possible. you have to delete and recreate a subnet, there is no update option\nB: service intruption \nC: is a joke....."
      },
      {
        "date": "2023-06-24T10:21:00.000Z",
        "voteCount": 1,
        "content": "olabiba.ai says \"A\". Chatgpt kept bouncing between \"B\" &amp; \"D\"."
      },
      {
        "date": "2023-06-24T07:43:00.000Z",
        "voteCount": 2,
        "content": "A is answer"
      },
      {
        "date": "2023-06-24T05:10:00.000Z",
        "voteCount": 2,
        "content": "yep, A is correct."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 294,
    "url": "https://www.examtopics.com/discussions/amazon/view/112695-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to deploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using a predefined list of project values.<br><br>When the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant project values. The company wants to enforce the use of project tags for new resources.<br><br>Which solution will meet these requirements with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T07:48:00.000Z",
        "voteCount": 7,
        "content": "A is correct BUT I did NOT like the last line in option A. It says \"Attach the SCP to each OU\". Why should you attach SCP to each OU. Can't you just attach to RootOU so it gets inherited to child OUs"
      },
      {
        "date": "2023-07-04T05:29:00.000Z",
        "voteCount": 4,
        "content": "The tags are different for each OU."
      },
      {
        "date": "2023-12-17T04:03:00.000Z",
        "voteCount": 6,
        "content": "The key to the answer is in the first sentence of A and B.  You can create a Tag Policy in the Management Account not OU since the OU is not an \"Account\" but a target where a policy is applied.   Tag Policy is not the same as an SCP.  \n\nSee: https://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
      },
      {
        "date": "2023-11-19T11:05:00.000Z",
        "voteCount": 1,
        "content": "Ok this is strange if you do not use this stuff regularly as AWS uses \"tag policy\" for several different configuration services.\n\nYou can apply a tag policy on the management account through AWS Organization. If you do it all child OUs will inherit the tag policy.\n\nIf you do the same \"tag policy\" on the management account using AWS Resource Groups Tag Editor it will not be inherited. \n\nB was a very seductive answer, even chatGPT made a mistake here by defining this answer as good in first occurence.\n\nBut considering we use AWS Organization to manage everything, it's clearly an AWS Organization Tag Policy which is used here. So a tag policy applied on the management account will be inherited by the child OUs.\n\nAnswer is A.\n\nAWS terminology can be really bad."
      },
      {
        "date": "2023-07-20T17:09:00.000Z",
        "voteCount": 1,
        "content": "Correct A."
      },
      {
        "date": "2023-07-08T08:51:00.000Z",
        "voteCount": 3,
        "content": "A. tag policy create in management account"
      },
      {
        "date": "2023-06-28T07:44:00.000Z",
        "voteCount": 2,
        "content": "A) in management account for tag policy and SCP , Sounds Good\nB) for each account ? more overhead\nC ) IAM for account in cloudformation ? is incorrect in this case\nD) AWS Service Catalog ? why ? incorrect"
      },
      {
        "date": "2023-06-27T13:08:00.000Z",
        "voteCount": 1,
        "content": "A - Correct. You create an SCP with allowed tags in the root OU and then attach the SCP to all OUs."
      },
      {
        "date": "2023-06-27T12:39:00.000Z",
        "voteCount": 1,
        "content": "AAAAAAAAAAAAA"
      },
      {
        "date": "2023-06-23T18:11:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-06-23T11:29:00.000Z",
        "voteCount": 3,
        "content": "A) Is correct in the master account of all organization use SCP is less overhead than B\nB ) is more overhead than A because in each OU create SCP\nC ) IAM in all account is more overhead\nD) is valid but not restrict other options o create with CLI or console the rest service without tags\n\nThen A is correct"
      },
      {
        "date": "2023-06-22T15:46:00.000Z",
        "voteCount": 1,
        "content": "olabiba.ai says 'A'"
      },
      {
        "date": "2023-06-21T03:11:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-06-20T10:46:00.000Z",
        "voteCount": 1,
        "content": "What not use SCP?"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 295,
    "url": "https://www.examtopics.com/discussions/amazon/view/112792-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type of instance.<br><br>CPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently reduce the EC2 cost and increase the utilization.<br><br>Which solution will meet these requirements with the LEAST number of configuration changes in the future?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tList instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's launch template configuration to use multiple instance types from the list.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group's configuration by adding the new instance type. Remove the current instance type from the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group's launch template."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-27T13:16:00.000Z",
        "voteCount": 13,
        "content": "It's C. You change the instance type/size in the launch template not the ASG. ASG can change the min/max size, not instance type."
      },
      {
        "date": "2024-04-12T12:10:00.000Z",
        "voteCount": 4,
        "content": "I\u2019ve tested it myself in the AWS Console \u2013 correct answer is \u201cB\u201d. To change the instance type you have 3 options, and all of them require modifying the ASG\u2019s config:\n1.\tCreate a new revision of the current launch template, then change the ASG config to use it.\n2.\tCreate a new launch template, then change the ASG config to use it.\n3.\tUse the option \u201cOverride launch template\u201d in the ASG config.\nIf you only create a new revision of the launch template, the ASG will continue to use the old revision. The state that you cannot change the instance type from the ASG config is NOT true and anybody can verify it in the AWS Console."
      },
      {
        "date": "2024-08-21T01:23:00.000Z",
        "voteCount": 1,
        "content": "just C"
      },
      {
        "date": "2024-01-29T11:54:00.000Z",
        "voteCount": 7,
        "content": "The answer used to be C, but now it's B. But not for the reasons others here have mentioned. The question states that \"The Auto Scaling group configuration uses only one type of instance\". This implies the ASG config has implemented instance overrides, which - you guessed it - overrides the instance type that's specified in the launch template. You could cut new versions of launch templates until you're blue in the face, it wont make a lick of difference if the ASG config is overriding the instance type. And because ASGs can be modified, I reckon that puts a nail in C's coffin, making B the new correct answer. I think this is the first question (out of 400+) where the moderator-selected solution was correct and the community-voted solution was incorrect."
      },
      {
        "date": "2024-06-03T07:35:00.000Z",
        "voteCount": 1,
        "content": "I believe \"The Auto Scaling group configuration uses only one type of instance.\" is just a badly phrased sentence and the question designer only meant that instances running under AutoScaling Group are of one instance type. I understand the sentence as a suggestion to improve the state by specifying multiple types instead.\n\nApart from the wording, there's nothing wrong with answer C. It lets you stop worrying about the future instance generations, too, compared to B where you have to modify the instance type whenever a new generation is released. Also, as specific instance types can be temporarily unavailable (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity), C can smoothly use another available instance automatically."
      },
      {
        "date": "2024-09-09T11:59:00.000Z",
        "voteCount": 1,
        "content": "Why Option C is Less Optimal:\nSingle Instance Type Limitation: By specifying CPU and memory requirements for a single instance type, you limit the flexibility of your Auto Scaling group. If the chosen instance type does not fully match the application's varying load, it may result in either underutilization or performance issues.\n\nNo Cost Optimization: This option does not take advantage of cost differences among different instance types. Without multiple instance types, you miss out on opportunities to select more cost-effective options based on current pricing and utilization needs.\n\nFuture Configuration Changes: While specifying the right instance type initially is good, it doesn't address changing application requirements or price fluctuations over time. It could still require adjustments in the future if the chosen instance type becomes less cost-effective or if application requirements change."
      },
      {
        "date": "2024-08-24T02:51:00.000Z",
        "voteCount": 1,
        "content": "Perhaps it\u2019s an older question that AWS intended for us to solve using C.\nBut nowadays days its B for the reasons explained by some of the people in this thread.\n\nI would be really surprised to see this question in the exam, but if I will - I'll definitely go with B"
      },
      {
        "date": "2024-07-03T05:19:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A:\nThis solution allows for a mix of instance types, which can help optimize costs and increase utilization.\nBy using similar instance types, it ensures compatibility with the application's requirements.\nThis approach requires the least number of configuration changes in the future as it provides flexibility to automatically use different instance types as they become available or as prices change.\n\nB. This option limits the Auto Scaling group to a single instance type again, which doesn't provide flexibility for future changes.\nC. Specifying CPU and memory requirements without instance types may lead to unexpected instance selections and potential compatibility issues.\nD. Using a script with the AWS Price List Bulk API could lead to frequent changes and may select instance types that aren't optimal for the application's needs."
      },
      {
        "date": "2024-05-03T02:45:00.000Z",
        "voteCount": 2,
        "content": "C for me"
      },
      {
        "date": "2024-04-12T12:11:00.000Z",
        "voteCount": 4,
        "content": "I\u2019ve tested it myself in the AWS Console \u2013 correct answer is \u201cB\u201d. To change the instance type you have 3 options, and all of them require modifying the ASG\u2019s config:\n1.\tCreate a new revision of the current launch template, then change the ASG config to use it.\n2.\tCreate a new launch template, then change the ASG config to use it.\n3.\tUse the option \u201cOverride launch template\u201d in the ASG config.\nIf you only create a new revision of the launch template, the ASG will continue to use the old revision. The state that you cannot change the instance type from the ASG config is NOT true and anybody can verify it in the AWS Console."
      },
      {
        "date": "2024-03-16T00:38:00.000Z",
        "voteCount": 3,
        "content": "Option C"
      },
      {
        "date": "2024-02-22T13:39:00.000Z",
        "voteCount": 1,
        "content": "C:\n\nAuto scaling group is built on top of launch template, you can reference AMI in template, but not in auto scaling group"
      },
      {
        "date": "2024-02-10T06:18:00.000Z",
        "voteCount": 2,
        "content": "AWS does not allow to edit launch configuration. If you notice, we define instance type at time of launch configuration. So if you want to change instance type in Auto Scaling group than you need to create new launch configuration for that."
      },
      {
        "date": "2024-01-14T21:28:00.000Z",
        "voteCount": 2,
        "content": "C is wrong.\n\nLet's assume a scenario where the optimal hardware requirement for a program under load is 4GB of RAM for every 1 CPU. \n\nHowever, you have specified only one type of instance with 1CPU and 1GB RAM.\n\nEven if you choose Option C and apply load balancing, having 4 instances of 1CPU and 1GB RAM (totaling 4CPU and 4GB RAM) will still result in an issue of low CPU utilization."
      },
      {
        "date": "2023-12-17T04:12:00.000Z",
        "voteCount": 6,
        "content": "Key to the Answer is \"Modify\".  Launch templates are immutable; after you create a launch template, you can't modify it. Instead, you can create a new version of the launch template that includes any changes you require.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html"
      },
      {
        "date": "2023-12-09T08:27:00.000Z",
        "voteCount": 1,
        "content": "b IS ANS\nMinimal configuration changes: This solution only requires modifying the Auto Scaling group configuration to add the new, more efficient instance type and remove the old, underutilized type. This minimizes future maintenance and reduces the risk of introducing errors.\n\nScalability and flexibility: The Auto Scaling group will automatically scale up and down based on demand, even with the new instance type. This ensures high availability and cost-effectiveness.\n\nFuture-proof: This approach doesn't rely on specific instance types or the AWS Price List Bulk API, making it more adaptable to future changes and updates in the AWS ecosystem."
      },
      {
        "date": "2023-11-19T04:45:00.000Z",
        "voteCount": 4,
        "content": "we cannot change/modify launch config or launch template"
      },
      {
        "date": "2023-09-03T02:58:00.000Z",
        "voteCount": 5,
        "content": "It could be B or C, but \"LEAST number of configuration changes in the future\" makes it C."
      },
      {
        "date": "2023-08-29T06:10:00.000Z",
        "voteCount": 7,
        "content": "In the launch template, you can only select one instance type. You can however override the Launch Template in the ASG configuration and specify multiple instance types."
      },
      {
        "date": "2023-08-16T07:31:00.000Z",
        "voteCount": 4,
        "content": "attribute-based instance types"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 296,
    "url": "https://www.examtopics.com/discussions/amazon/view/112793-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.<br><br>A solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-16T23:50:00.000Z",
        "voteCount": 21,
        "content": "I think the key here is to focus on the requirements. It is clearly stated that the requirement is that the strategy meet an RPO of 2 hours and an RTO of 4 hours. Even though option C is the most cost-effective, it is contingent on a few external factors, like the size of the data, the data change rate, etc., which cannot be assumed at the risk of breaching RPO and RTO requirements. So based on that, the most effective option is D."
      },
      {
        "date": "2023-10-17T16:44:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2024-04-12T12:34:00.000Z",
        "voteCount": 2,
        "content": "\"C\" does not mention a restore operation at all. Where will Route 53 route the traffic in the secondary Region: to the DB snapshots in the AWS Backup vault maybe?\nSo, D should be the right option.\n\nP.S. Very badly written question btw."
      },
      {
        "date": "2023-08-11T14:22:00.000Z",
        "voteCount": 7,
        "content": "Answer: C\nWeird question. Sometimes I think there is no BEST answer and that they were created just to confuse people.\nAnyway, thinking on cost and the mentioned RPO and RTO, I would still go with C (if they were longer, it would be easier to choose among the questions)."
      },
      {
        "date": "2024-08-21T01:27:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-09-30T16:56:00.000Z",
        "voteCount": 1,
        "content": "The ONLY reason I am going with C is that AWS Backup is generally more cost-effective compared to continuous replication setups like Aurora Global Database or DynamoDB Global Tables1.\n\nIt allows you to create point-in-time backups and restore them in a secondary region, meeting the RPO requirement of 2 hours and RTO requirement of 4 hours"
      },
      {
        "date": "2024-08-13T19:34:00.000Z",
        "voteCount": 2,
        "content": "vote for D. Global table"
      },
      {
        "date": "2024-05-03T02:46:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2024-04-30T19:58:00.000Z",
        "voteCount": 1,
        "content": "My answer is B.\nB is cheapest and it will create only when event occurs. it will complete within 2hours.\nC and D are costly options compare to BH"
      },
      {
        "date": "2024-03-21T21:06:00.000Z",
        "voteCount": 4,
        "content": "AWS often publish this kind of bad framed question. The question is looking for most cost effective solution. So I believe C is the expected answer even it is not complete answer. But C has three big problems: \n1. a backup is a backup, if it doesn't provide a way to restore, it is only a backup and is not a complete DR. \n2. It doesn't mention the frequency of the backup nor the continuous backup, which means we don't know whether it can meet the 2hr RPO. \n3. It doesn\u2019t mention the ECS DR. Well, neither does the other answers.\n.\nAurora global db and DynamoDB global table are apparently more expensive. With the question design, they should be excluded even they are actually complete answers."
      },
      {
        "date": "2024-03-15T04:27:00.000Z",
        "voteCount": 2,
        "content": "Github Copilot answer:\nThe solution you proposed is a good approach for implementing a disaster recovery (DR) strategy. Here's a breakdown of how it works:\n1. **AWS Database Migration Service (DMS)**: This service can be used to replicate data from your Amazon Aurora databases in the primary region to the secondary region. This ensures that you have a backup of your data in case of a disaster in the primary region.\n2. **Amazon EventBridge and AWS Lambda**: These services can be used together to trigger the replication process whenever there is a change in the Aurora databases.\n3. **DynamoDB Streams, EventBridge, and Lambda**: DynamoDB Streams capture table activity, and you can use Lambda functions triggered by EventBridge to process the stream and replicate the changes to DynamoDB tables in the secondary region."
      },
      {
        "date": "2024-03-06T09:49:00.000Z",
        "voteCount": 1,
        "content": "We have no idea the size of the db thus we can't assume we can reach an RTO of 4 hours using backups. D is the cheapest solution out of A, B and D."
      },
      {
        "date": "2024-02-29T01:52:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\nThus the correct answer is C, as the minimum RPO for AWS Backup (unless you use Point-in-time recovery) is exactly 2 hours."
      },
      {
        "date": "2024-02-10T00:14:00.000Z",
        "voteCount": 3,
        "content": "Backup restore can take more than 4 hours, so D"
      },
      {
        "date": "2024-02-17T03:22:00.000Z",
        "voteCount": 1,
        "content": "In general time to restore from a recovery point using AWS Backup depends on the size of the data and type of resource being restored, is it a single DB, or an entire aurora cluster, a time frame cannot be estimated, it may take 5 minutes or 1 hour"
      },
      {
        "date": "2024-02-06T06:03:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is in fact D. Though the question asks for a cost effective option. Option C does not guarantee on the mentioned RPO and RTO.\nSo between A and D what is the most cost effective way.\nD wins as it does not have cost of Cloudfront"
      },
      {
        "date": "2023-12-30T14:25:00.000Z",
        "voteCount": 5,
        "content": "AWS often publish this kind of bad framed question. The question is looking for most cost effective solution. So I believe C is the expected answer even it is not complete answer. But C has two big problems: \n1. a backup is a backup, if it doesn't provide a way to restore, it is only a backup and is not a complete DR. \n2. It doesn't mention the frequency of the backup nor the continues backup, which means we don't whether it can meet the 2hr RPO. \n.\nAurora global db and DynamoDB global table are apparently more expensive. With the question design, they should be excluded even they are actually complete answers."
      },
      {
        "date": "2023-12-08T08:58:00.000Z",
        "voteCount": 1,
        "content": "D is ans\nDatabase replication: Aurora global databases and DynamoDB global tables provide automatic, continuous replication across Regions, ensuring an RPO of 2 hours or less. This eliminates the need for manual database setup or complex replication processes.\nRegional API endpoints: Configuring API Gateway APIs with Regional endpoints in both Regions ensures availability in either Region, supporting a quick RTO of 4 hours.\nRoute 53 failover routing: Route 53 provides a cost-effective and efficient way to switch traffic between Regions during a DR event. It eliminates the need for more expensive services like CloudFront for failover."
      },
      {
        "date": "2023-12-08T08:57:00.000Z",
        "voteCount": 1,
        "content": "d  ANS"
      },
      {
        "date": "2023-12-08T04:35:00.000Z",
        "voteCount": 3,
        "content": "Its C based on the: \"Which solution will meet these requirements MOST cost-effectively?\""
      },
      {
        "date": "2023-11-20T02:13:00.000Z",
        "voteCount": 4,
        "content": "The AWS backup based approach is highly cost-effective, employs a backup and restore strategy, and can be designed to comply with cross region backup regulatory requirements. I also explained Aurora Global Database, an Aurora feature which can be utilized when you have strict RTO and RPO requirements.\nhttps://aws.amazon.com/es/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 297,
    "url": "https://www.examtopics.com/discussions/amazon/view/112794-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that the web application is slowing down.<br><br>The company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that query strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.<br><br>Which set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFront distribution to disable caching based on query string parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFront distribution to specify casing-insensitive query string processing."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-12T06:30:00.000Z",
        "voteCount": 11,
        "content": "A. Yes, because Amazon CloudFront considers the case of parameter names and values when caching based on query string parameters , thus inconsistent query strings may cause CloudFront to forward mixed-cased/misordered requests to the origin.\nTriggering a Lambda@Edge function based on a viewer request event to sort parameters by name and force them to be lowercase is the best choice.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html#query-string-parameters-optimizing-caching\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html\n\nB. No, because this will exacerbate the caching issue by sending all query string parameters requests to the origin\nC. No, because this won't help increase the cache hit ratio\nD. No, because a CloudFront distribution specifies information about the origin/source of your content and how to track and manage content delivery."
      },
      {
        "date": "2024-06-25T02:48:00.000Z",
        "voteCount": 1,
        "content": "Answer:D is best and cheap."
      },
      {
        "date": "2024-08-21T01:29:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2024-01-29T12:23:00.000Z",
        "voteCount": 1,
        "content": "OMG so now I have to invoke and pay for a Lambda for every single GET request that traverses my CDN? No, F*** that. If D isn't supported then ciao bella s/Cloudfront/Cloudflare/g and say hello to Apache running mod_substitute thank you very much."
      },
      {
        "date": "2023-12-12T21:56:00.000Z",
        "voteCount": 1,
        "content": "Caching based on query string parameters\n\nIf you configure CloudFront to cache based on query string parameters, you can improve caching if you do the following:\n\nConfigure CloudFront to forward only the query string parameters for which your origin will return unique objects."
      },
      {
        "date": "2023-12-12T21:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html"
      },
      {
        "date": "2023-12-12T21:53:00.000Z",
        "voteCount": 1,
        "content": "D is ans\nD. Casing-insensitive query string processing:\n\nThis is the simplest and fastest solution to implement.\nIt will treat requests with the same query string but different character casing as identical, boosting the cache hit ratio.\nIt utilizes built-in functionality of CloudFront without requiring additional services or configurations.\nRemember, while other options might offer additional functionalities, the primary goal is to quickly improve the cache hit ratio. Specifying casing-insensitive query string processing achieves this with minimal impact and complexity."
      },
      {
        "date": "2023-07-20T16:58:00.000Z",
        "voteCount": 2,
        "content": "Correct A."
      },
      {
        "date": "2023-07-08T09:10:00.000Z",
        "voteCount": 2,
        "content": "its an A\nD would be nice if was supported"
      },
      {
        "date": "2023-06-27T13:35:00.000Z",
        "voteCount": 3,
        "content": "A - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters"
      },
      {
        "date": "2023-06-27T13:34:00.000Z",
        "voteCount": 1,
        "content": "A - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters"
      },
      {
        "date": "2023-06-26T18:02:00.000Z",
        "voteCount": 1,
        "content": "D is out: CloudFront distributions do not have built-in support for specifying a case-insensitive query string. By default, CloudFront treats query strings as case-sensitive, meaning that a URL with a different case in the query string parameter would be treated as a separate object and potentially result in a cache miss."
      },
      {
        "date": "2023-06-25T14:36:00.000Z",
        "voteCount": 2,
        "content": "A , same questions this version 1 \nhttps://www.examtopics.com/discussions/amazon/view/27789-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-06-24T11:39:00.000Z",
        "voteCount": 2,
        "content": "A is the answer -to sort parameters by name and force them to be lowercase"
      },
      {
        "date": "2023-06-24T08:14:00.000Z",
        "voteCount": 1,
        "content": "A\ncheck for the example in the below documentation\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html"
      },
      {
        "date": "2023-06-24T05:28:00.000Z",
        "voteCount": 1,
        "content": "A is answer"
      },
      {
        "date": "2023-06-21T03:13:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 298,
    "url": "https://www.examtopics.com/discussions/amazon/view/112795-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store information about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.<br><br>The company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an RPO of 1 hour.<br><br>Which solution will meet these requirements with the LOWEST cost?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 24,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-06T06:05:00.000Z",
        "voteCount": 27,
        "content": "Good luck for the exams. I know I'm gonna fail coz it takes me 3 hours just to read the questions. &gt;:("
      },
      {
        "date": "2024-03-14T07:00:00.000Z",
        "voteCount": 5,
        "content": "The trick is not to read the question first. Here is what I do:\n1. Read all options. \n2. Eliminate the incorrect ones, and settle on 1 or 2 options.\n3. Scroll through the question last."
      },
      {
        "date": "2023-10-23T11:10:00.000Z",
        "voteCount": 1,
        "content": "any news about your exam?"
      },
      {
        "date": "2024-02-03T02:09:00.000Z",
        "voteCount": 1,
        "content": "I failed just cuz of time mngmnt ,now I'm here again to give myself one more \nchance,...:-(,.....\nAnyone here's to help me with exam by sharing their experience??"
      },
      {
        "date": "2024-02-05T22:35:00.000Z",
        "voteCount": 1,
        "content": "did you actually see any of the same questions on the test?"
      },
      {
        "date": "2023-07-02T21:11:00.000Z",
        "voteCount": 19,
        "content": "if you got far it means you are persistent, good luck on your exam"
      },
      {
        "date": "2024-09-05T02:17:00.000Z",
        "voteCount": 1,
        "content": "For those who are voting for D, here are a few simple facts:\n\n- You can\u2019t disabled Aurora automated backup;\n- You can\u2019t change Aurora backup frequency - it\u2019s automated and point in time, you just need to define retention period which is the period for which you can perform a point-in-time recovery(1 day for free, max 35)"
      },
      {
        "date": "2024-08-11T07:23:00.000Z",
        "voteCount": 1,
        "content": "Between A and C, I choose A.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-getting-started.html#aurora-global-database-attaching\nFor C, note there's high volume of write daily. Can CDC to S3 bucket in another region keep up with 1 hour RPO? Although C has considered lowest cost for backup, does it consider the time taken to restore to full aurora database in the second region up and running?"
      },
      {
        "date": "2024-08-10T21:16:00.000Z",
        "voteCount": 2,
        "content": "Ask for 1 hour RPO, cheapest\n\nAurora Global can do 1 second RPO, but expensive\n\nDMS with CDC can do 1-hour RPO and cheaper"
      },
      {
        "date": "2024-06-11T00:03:00.000Z",
        "voteCount": 2,
        "content": "Option A will replicate \"the whole database\", we only need \"data\""
      },
      {
        "date": "2024-04-29T18:18:00.000Z",
        "voteCount": 4,
        "content": "option D provides the most cost-effective solution by leveraging Aurora backups with a 1-hour frequency and cross-Region replication to meet the disaster recovery requirements with the desired RPO."
      },
      {
        "date": "2024-04-28T12:20:00.000Z",
        "voteCount": 1,
        "content": "RPO is a requirement and not RTO. They are talking about replicating the data in the Aurora database to another Region to meet disaster recovery requirements."
      },
      {
        "date": "2024-03-19T08:01:00.000Z",
        "voteCount": 5,
        "content": "My head hurts after the reading the last 2 questions and 45 mins later, still confuse.  I am looking at the key requirements, RPO &lt;1h, meet DR requirements, and LOWEST Cost.  After reading the link below and all of the comments, I think  Option A fulfil all the requirements in the question.  \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html"
      },
      {
        "date": "2024-03-15T04:51:00.000Z",
        "voteCount": 6,
        "content": "Github copilot\nWhile AWS Database Migration Service (DMS) can be used to replicate ongoing changes from the Aurora database to an Amazon S3 bucket in another region using a change data capture (CDC) task, it's important to note that DMS does not create a standard SQL dump or backup file that can be directly restored to an Aurora database.\nThe data migrated to S3 by DMS is in Apache Parquet format, a columnar storage file format optimized for speed and for a small footprint. This format is not directly restorable to an Aurora database.\nIf you need to restore the data to an Aurora database, you would need to use a service like AWS Glue or Amazon Athena to read the data from S3 and then insert it into Aurora. This process could be complex and time-consuming, and might not meet your RPO of 1 hour."
      },
      {
        "date": "2024-03-15T04:49:00.000Z",
        "voteCount": 1,
        "content": "Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region. Is it possible to restore data from the S3 bucket to an Aurora Database?"
      },
      {
        "date": "2024-02-22T12:31:00.000Z",
        "voteCount": 1,
        "content": "D:\n\nthe auto back can be disabled, see the link below:\nhttps://repost.aws/questions/QU1FrG5tQkQwi-yHbhT_EdvA/easily-turn-off-the-auto-backups-snapshots-for-rds"
      },
      {
        "date": "2024-03-08T01:06:00.000Z",
        "voteCount": 1,
        "content": "The link is for RDS not Aurora."
      },
      {
        "date": "2024-02-17T10:17:00.000Z",
        "voteCount": 2,
        "content": "C is the least worst answer"
      },
      {
        "date": "2024-02-11T12:52:00.000Z",
        "voteCount": 4,
        "content": "not C because.. DMS is not cheap and moreover, DMS S3 Target support either .csv or parquet format.. good luck with restoring this data into a database from s3\nthis is not a Disaster Recovery this is purely \"playing around with data in a Disaster situation\""
      },
      {
        "date": "2023-11-21T22:51:00.000Z",
        "voteCount": 2,
        "content": "As there is no RTO C is best and most cost-effective."
      },
      {
        "date": "2023-11-19T05:14:00.000Z",
        "voteCount": 1,
        "content": "we assume that dms instance is deployed in a different region and somehow accesses the aurora instance, through the public endpoint or with vpc connection or any other way, and then replicates changes to the bucket in the same region it resides(different region for aurora)"
      },
      {
        "date": "2023-10-17T17:47:00.000Z",
        "voteCount": 4,
        "content": "I thought it has 304 questions, how come there is no more next page?"
      },
      {
        "date": "2023-10-23T11:18:00.000Z",
        "voteCount": 2,
        "content": "i wonder myself"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 299,
    "url": "https://www.examtopics.com/discussions/amazon/view/124742-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company's solutions architect is evaluating an AWS workload that was deployed several years ago. The application tier is stateless and runs on a single large Amazon EC2 instance that was launched from an AMI. The application stores data in a MySQL database that runs on a single EC2 instance.<br><br>The CPU utilization on the application server EC2 instance often reaches 100% and causes the application to stop responding. The company manually installs patches on the instances. Patching has caused downtime in the past. The company needs to make the application highly available.<br><br>Which solution will meet these requirements with the LEAST development me?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the application tier to AWS Lambda functions in the existing VPC. Create an Application Load Balancer to distribute traffic across the Lambda functions. Use Amazon GuardDuty to scan the Lambda functions. Migrate the database to Amazon DocumentDB (with MongoDB compatibility.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the EC2 instance type to a smaller Graviton powered instance type. Use the existing AMI to create a launch template for an Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the application tier to containers by using Docker. Run the containers on Amazon Elastic Container Service (Amazon ECS) with EC2 instances. Create an Application Load Balancer to distribute traffic across the ECS cluster. Configure the ECS cluster to scale based on CPU utilization. Migrate the database to Amazon Neptune.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a now AMI that is configured with AWS Systems Manager Agent (SSM Agent). Use the new AMI to create a launch template for an Auto Scaling group. Use smaller instances in the Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon Aurora MySQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-02T20:33:00.000Z",
        "voteCount": 5,
        "content": "No development effort needed so no need to migrate nonSQL or to Neptune. and no need to rework it based on lambda."
      },
      {
        "date": "2023-11-01T13:48:00.000Z",
        "voteCount": 2,
        "content": "A: No guarantee that the work can finish within 15 minutes limit of Lambda\nB, C: Migrate MySQL to DynamoDB or Neptune? Big no no to migrate to different type of database unless the requirement says so.\nD: Classic architecture: ALB + ASG + EC2, scale based on CPU Utilization for cost optimization. The use of SSM to create AMI for launch template of ASG is correct. Aurora MySQL is compatible with current MySQL database."
      },
      {
        "date": "2023-10-31T13:18:00.000Z",
        "voteCount": 1,
        "content": "A - you will spend some time to adapt an old platform to a lamdba function + the application works with mysql not with mongodb\nB - You do not want a smaller instance when you have a performance problem\nC - you will have to readapt a whole application to containerization on ECS which is not even the most flexible virtualization platform even if it theorically requires less maintenance\n\nD - The most classic way of migrating such application : you create a new platform, you make the application more scalable by using an ASG + you migrate your MySQL server from an overloaded EC2 instance to a managed service."
      },
      {
        "date": "2023-10-28T05:01:00.000Z",
        "voteCount": 1,
        "content": "With least development time, from MySQL to Amazon Aurora MySQL."
      },
      {
        "date": "2023-10-28T03:25:00.000Z",
        "voteCount": 2,
        "content": "Answer: D    \n    Because MySQL Database will be compatible with Aurora MySQL    \n    A. Lambda is not the  correct solution as it will be more development effort    \n    B. Changing to DynamoDB from MySQL (Relational Database) will be more development effort.\n    C. More development effort to convert to Docker."
      },
      {
        "date": "2023-10-27T11:21:00.000Z",
        "voteCount": 1,
        "content": "I think D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 300,
    "url": "https://www.examtopics.com/discussions/amazon/view/124796-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate several applications to AWS. The company does not have a good understanding of its entire application estate. The estate consists of a mixture of physical machines and VMs.<br><br>One application that the company will migrate has many dependencies that are sensitive to latency. The company is unsure what all the dependencies are. However the company knows that the low-latency communications use a custom IP-based protocol that runs on port 1000. The company wants to migrate the application and these dependencies together to move all the low-latency interfaces to AWS at the same time.<br><br>The company has installed the AWS Application Discovery Agent and has been collecting data for several months.<br><br>What should the company do to identify the dependencies that need to be migrated in the same phase as the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Configure Application Migration Service to launch test instances for all the servers that interact with the application. Perform acceptance tests on the test instances. If no issues are identified, create a move group that is based on the tested servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Migration Hub and select the servers that host the application. Turn on data exploration in Network Access Analyzer. Use the Network Access Analyzer console to select the servers that host the application. Select a Network Access Scope of port 1000 and note the matching servers. Return to Migration Hub. Create a move group that is based on the findings from Network Access Analyzer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Migration Hub and select the servers that host the application. Push the Amazon CloudWalch agent to the identified servers by using the AWS Application Discovery Agent. Export the CloudWatch logs that the agents collect to Amazon S3. Use Amazon Athena to query the logs to find servers that communicate on port 1000. Return to Migration Hub Create a move group that is based on the findings from the Athena queries."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-29T19:25:00.000Z",
        "voteCount": 12,
        "content": "Answer A . Network access analyzer is to valid network usage OF aws services and not on-prem\n\nMigration hub has feature for network visualization and Athena can be used to query data\n\nhttps://aws.amazon.com/blogs/mt/using-aws-migration-hub-network-visualization-to-overcome-application-and-server-dependency-challenges/\n\nhttps://aws.amazon.com/about-aws/whats-new/2020/11/aws-migration-hub-includes-network-visualization/"
      },
      {
        "date": "2023-11-01T14:02:00.000Z",
        "voteCount": 7,
        "content": "Architecture pattern is Discovery Service + Migration Hub + Athena for data exploration:\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/explore-data.html\n\nA: looks fine \nB: AWS Application Migration Service is for lift and shift, not for dependency mapping\nC: Network Access Analyzer only for AWS resource, not for on prem\nD: not the use case of CloudWatch."
      },
      {
        "date": "2024-09-09T12:26:00.000Z",
        "voteCount": 1,
        "content": "C seems to be more suitable"
      },
      {
        "date": "2024-06-01T14:07:00.000Z",
        "voteCount": 1,
        "content": "Option C - dentifying Servers Communicating on Port 1000: In the Network Access Analyzer console, you can select the servers that host the application and specify a Network Access Scope of port 1000. This will allow you to identify the servers that communicate with the application using the custom IP-based protocol on port 1000, which are the dependencies that need to be migrated together."
      },
      {
        "date": "2024-08-21T01:35:00.000Z",
        "voteCount": 1,
        "content": "just A"
      },
      {
        "date": "2023-12-17T06:11:00.000Z",
        "voteCount": 2,
        "content": "See: https://docs.aws.amazon.com/migrationhub/latest/ug/network-diagram.html\n\nand https://aws.amazon.com/about-aws/whats-new/2020/11/aws-migration-hub-includes-network-visualization/"
      },
      {
        "date": "2023-11-18T17:17:00.000Z",
        "voteCount": 1,
        "content": "A is right answer."
      },
      {
        "date": "2023-10-29T05:33:00.000Z",
        "voteCount": 1,
        "content": "Should work with c"
      },
      {
        "date": "2023-10-28T03:31:00.000Z",
        "voteCount": 3,
        "content": "Answer: C\nTo identify the dependencies that need to be migrated in the same phase as the application, the company can use the AWS Application Discovery Agent data. In this case, the sensitive low-latency communications use a custom IP-based protocol that runs on port 1000. The goal is to find servers that communicate on port 1000. Option C would be the most appropriate approach"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 301,
    "url": "https://www.examtopics.com/discussions/amazon/view/124801-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rale-based rule that includes an appropriate request quota."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-08T06:54:00.000Z",
        "voteCount": 7,
        "content": "REST APIs and HTTP APIs are both RESTful API products. REST APIs support more features than HTTP APIs, while HTTP APIs are designed with minimal features so that they can be offered at a lower price. Choose REST APIs if you need features such as API keys, per-client throttling, request validation, AWS WAF integration, or private API endpoints. Choose HTTP APIs if you don't need the features included with REST APIs."
      },
      {
        "date": "2023-11-18T17:43:00.000Z",
        "voteCount": 5,
        "content": "Option A answer is little confusing because it talks about Quota but not about Throttle limits.  Option B mentions route-level throttling that is also not correct. Route-level throttling can not be applied at per user basis. \nSo option A is right answer."
      },
      {
        "date": "2023-11-09T03:21:00.000Z",
        "voteCount": 2,
        "content": "Option A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html"
      },
      {
        "date": "2023-10-30T08:39:00.000Z",
        "voteCount": 3,
        "content": "In order to achieve \"Some customers must receive a higher quota for a shorter time period.\", throttling should be set with rate and burst can be set using Throttling"
      },
      {
        "date": "2023-10-29T08:35:00.000Z",
        "voteCount": 4,
        "content": "Option A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html"
      },
      {
        "date": "2023-10-29T05:35:00.000Z",
        "voteCount": 1,
        "content": "Its A, you dont need route level throttling"
      },
      {
        "date": "2023-10-28T18:55:00.000Z",
        "voteCount": 1,
        "content": "Option B\nroute-level throttling for each usage plan"
      },
      {
        "date": "2023-10-28T12:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html \n HTTP API doesn't include USAGE feature."
      },
      {
        "date": "2023-10-28T03:39:00.000Z",
        "voteCount": 1,
        "content": "Option A\nCreate REST API with Proxy Integration and for each customer set the usage plan and Create API Key.\nhttps://medium.com/geekculture/api-key-and-usage-plan-integration-with-aws-api-gateway-2d07bbb9a2a4"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 302,
    "url": "https://www.examtopics.com/discussions/amazon/view/124802-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration.<br><br>Which solution will complete the migration to AWS in the LEAST amount of time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMS. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-02T21:03:00.000Z",
        "voteCount": 6,
        "content": "I'll go with B as 10G direct connection is faster than enough for workload not so big.\nEFS and DataSync are feasible as well."
      },
      {
        "date": "2024-04-12T13:49:00.000Z",
        "voteCount": 1,
        "content": "Ans D.\nThe questions states \"The VMs have many different operating systems and many custom software packages installed\". However, the AWS AMS supports only limited and commonly used OSes and apps.\n\nQ: What operating systems and applications are supported by AWS Application Migration Service?\nAWS Application Migration Service allows you to migrate physical, virtual, and cloud source servers to AWS for a variety of supported operating systems (OS). AWS Application Migration Service supports commonly used applications such as SAP, Oracle, and Microsoft SQL Server.\n\nhttps://aws.amazon.com/application-migration-service/faqs/#:~:text=AWS%20Application%20Migration%20Service%20allows,Oracle%2C%20and%20Microsoft%20SQL%20Server."
      },
      {
        "date": "2024-08-14T22:46:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-03-21T13:25:00.000Z",
        "voteCount": 1,
        "content": "A is too time-consuming.\nB is viable\nC is viable, but overengineered as the DC connection has enough capacity."
      },
      {
        "date": "2024-03-21T13:26:00.000Z",
        "voteCount": 1,
        "content": "Typo:\nC is too time-consuming\nD is viable, but overengineered as the DC connection has enough capacity."
      },
      {
        "date": "2024-01-26T20:24:00.000Z",
        "voteCount": 1,
        "content": "10Gbps = 1.25GB/s = 4.5TB/H"
      },
      {
        "date": "2023-11-18T17:47:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2023-11-09T03:24:00.000Z",
        "voteCount": 4,
        "content": "Option B.\n10 Gbps AWS Direct Connect connection"
      },
      {
        "date": "2023-10-29T06:53:00.000Z",
        "voteCount": 3,
        "content": "B, 13mins to transfer 10tb"
      },
      {
        "date": "2023-12-30T13:21:00.000Z",
        "voteCount": 2,
        "content": "More like 2.5 hrs, but still a lot faster than shippings Snowballs back and forth..."
      },
      {
        "date": "2023-10-28T18:59:00.000Z",
        "voteCount": 4,
        "content": "Option B.\n10 Gbps AWS Direct Connect connection"
      },
      {
        "date": "2023-10-28T03:53:00.000Z",
        "voteCount": 2,
        "content": "Option D is the correct answer by using Snowball Edge each have 80TB capacity.\nA - Does not make sense to use only 1 Snowball Edge, also NFS to NFS server in EC2 it is not correct! Use AWS EFS\nB - Using Replication will be slow, there is not parellasim especially with additional NFS data transfer\nC - Install required software, as it is custom software, it may be time consuming os 120 VMs"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 303,
    "url": "https://www.examtopics.com/discussions/amazon/view/124803-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution.<br><br>The company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the data<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RSA key pair that is dedicated to the data-handing microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-29T09:13:00.000Z",
        "voteCount": 12,
        "content": "Please have a look at: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\nsteps:\n- Get a public key-private key pair\n- Create a field-level encryption profile\n- Create a field-level encryption configuration\n- Link to a cache behavior\n\nAn RSA key pair includes a private and a public key (asymmetric)"
      },
      {
        "date": "2023-11-18T21:24:00.000Z",
        "voteCount": 6,
        "content": "You need to RSA key for Field levell Encryption and not KMS Symmetric Key so B is the right answer"
      },
      {
        "date": "2024-09-27T23:16:00.000Z",
        "voteCount": 1,
        "content": "The following steps provide an overview of setting up field-level encryption. For specific steps, see Set up field-level encryption.\n-Get a public key-private key pair.\n-Create a field-level encryption profile\n-Create a field-level encryption configuration.\n-Link to a cache behavior."
      },
      {
        "date": "2024-06-01T14:20:00.000Z",
        "voteCount": 3,
        "content": "Option B - field-level encryption requires public/private key pair"
      },
      {
        "date": "2024-04-01T06:36:00.000Z",
        "voteCount": 1,
        "content": "field-level encryption in CloudFront uses asymmetric encryption with RSA key"
      },
      {
        "date": "2024-01-24T17:32:00.000Z",
        "voteCount": 4,
        "content": "CloudFront only supports field-level encryption with symmetric KMS keys, not with RSA keys.  in this specific scenario, Option A would be the correct answer because it leverages the native capabilities of CloudFront and meets the requirement of centralized key management for decrypting sensitive data."
      },
      {
        "date": "2024-01-24T17:35:00.000Z",
        "voteCount": 2,
        "content": "B is correct, Not A"
      },
      {
        "date": "2023-11-06T03:48:00.000Z",
        "voteCount": 3,
        "content": "ALGORITHM: CloudFront uses RSA/ECB/OAEPWithSHA-256AndMGF1Padding as the algorithm for encrypting, so you must use the same algorithm to decrypt the data.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-decrypt"
      },
      {
        "date": "2023-10-29T05:47:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html"
      },
      {
        "date": "2023-10-28T04:06:00.000Z",
        "voteCount": 1,
        "content": "Answer:  A\nuse field-level encryption with AWS Key Management Service (KMS) so that it can encrypt when send through Cloud Distribution and only the specific microservice with access to the appropriate KMS key can decrypt it.\nRSA does not work as Microservice data handling cannot decrypt it.\nIt does not require Lambda @Edge to perform to encrypt the data, just Associate the KMS key and the configuration with the CloudFront cache behavio"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 304,
    "url": "https://www.examtopics.com/discussions/amazon/view/124950-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally instances that have public IP addresses must receive corresponding public hostnames<br><br>Which solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeactivate the enableDnsSupport attribute for the VPActivate the enableDnsHostnames attribute for the VPCreate a new VPC DHCP options set, and configure doman-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-30T05:56:00.000Z",
        "voteCount": 5,
        "content": "Both settings need to be enabled to allow assigning of public DNS names and use of Amazon DNS, see https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#AmazonDNS"
      },
      {
        "date": "2024-01-13T23:54:00.000Z",
        "voteCount": 5,
        "content": "A is wrong because the question says it use AWS DNS rather than 10.24.34.2 custom DNS server.\nC is wrong because same reason with A.\nD is wrong because we need to actvate DnsSupport and DnsHostnames.\n\nPlease correct me if I am wrong."
      },
      {
        "date": "2023-11-18T21:48:00.000Z",
        "voteCount": 4,
        "content": "Enable both the dns options."
      },
      {
        "date": "2023-11-17T05:16:00.000Z",
        "voteCount": 2,
        "content": "B is the best answer"
      },
      {
        "date": "2023-11-13T21:08:00.000Z",
        "voteCount": 2,
        "content": "B enables both settings"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 305,
    "url": "https://www.examtopics.com/discussions/amazon/view/124804-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive.<br><br>Business requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Amazon EMR cluster Offload the complex data processing tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the Concurrency Scaling feature for the Amazon Redshift cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-25T15:53:00.000Z",
        "voteCount": 3,
        "content": "\u201c With the Concurrency Scaling feature, you can support thousands of concurrent users and concurrent queries, with consistently fast query performance. When you turn on concurrency scaling, Amazon Redshift automatically adds additional cluster capacity to process an increase in both read and write queries.\u201d \nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"
      },
      {
        "date": "2023-11-19T05:50:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2024-08-14T22:52:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2023-11-18T21:51:00.000Z",
        "voteCount": 2,
        "content": "Best option is D"
      },
      {
        "date": "2023-11-17T05:20:00.000Z",
        "voteCount": 2,
        "content": "The most cost-effective solution for addressing bursts of usage and accommodating complex queries in Amazon Redshift is to turn on the Concurrency Scaling feature for the Amazon Redshift cluster."
      },
      {
        "date": "2023-11-02T21:19:00.000Z",
        "voteCount": 1,
        "content": "Simply D"
      },
      {
        "date": "2023-10-28T16:18:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/big-data/scale-amazon-redshift-to-meet-high-throughput-query-requirements/#:~:text=Use%20concurrency%20scaling%20to%20dynamically,data%20warehouse%20using%20Amazon%20Redshift."
      },
      {
        "date": "2023-10-28T04:23:00.000Z",
        "voteCount": 4,
        "content": "Answer: D\nThe most cost-effective solution for addressing bursts of usage and accommodating complex queries in Amazon Redshift is to turn on the Concurrency Scaling feature for the Amazon Redshift cluster."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 306,
    "url": "https://www.examtopics.com/discussions/amazon/view/124882-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to an Amazon S3 bucket. One hundred scientists are using this object storage to store their work-related documents. Each scientist has a personal folder on the object store. All the scientists are members of a single IAM user group.<br><br>The research center's compliance officer is worried that scientists will be able to access each other's work. The research center has a strict obligation to report on which scientist accesses which documents. The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead.<br><br>Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with $(aws:username). Apply the policy on the scientists\u2019 IAM user group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 server access logging. Configure another S3 bucket as the target for log delivery. Use Amazon Athena to query the logs and generate reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket policy that grants read and write access to users in the scientists\u2019 IAM user group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write the events to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs and generate reports."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T16:47:00.000Z",
        "voteCount": 1,
        "content": "Option E problem is cloudwatch need to be exported to s3 bucket to be queried by Athena."
      },
      {
        "date": "2024-06-01T14:29:00.000Z",
        "voteCount": 1,
        "content": "Option AB"
      },
      {
        "date": "2024-02-17T04:06:00.000Z",
        "voteCount": 4,
        "content": "Not C: Server access log records are delivered on a best-effort basis."
      },
      {
        "date": "2024-03-01T12:14:00.000Z",
        "voteCount": 1,
        "content": "To elaborate further, \"The completeness and timeliness of server logging is not guaranteed. The log record for a particular request might be delivered long after the request was actually processed, or it might not be delivered at all. \""
      },
      {
        "date": "2024-02-08T16:36:00.000Z",
        "voteCount": 2,
        "content": "AB is correct. They key here is that the logs are required to be accurate for compliance reasons. Server access isn't good enough here. \"Server access log records are delivered on a best-effort basis. Most requests for a bucket that is properly configured for logging result in a delivered log record. Most log records are delivered within a few hours of the time that they are recorded, but they can be delivered more frequently\""
      },
      {
        "date": "2024-01-18T00:36:00.000Z",
        "voteCount": 1,
        "content": "\"The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead.\""
      },
      {
        "date": "2024-01-18T00:34:00.000Z",
        "voteCount": 1,
        "content": "A and C\n\"The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead.\""
      },
      {
        "date": "2024-01-16T14:08:00.000Z",
        "voteCount": 1,
        "content": "Answer: AB\nOption C is incorrect because enabling S3 server access logging and delivering the logs to another S3 bucket does not directly address the requirement to report on which scientist accesses which documents. While the logs can be queried, it does not provide a straightforward solution for generating the required reports.\nOption D is incorrect because creating an S3 bucket policy that grants read and write access to users in the scientists' IAM user group does not address the compliance officer's concern about scientists being able to access each other's work. It also does not provide a solution for reporting on which scientist accesses which documents."
      },
      {
        "date": "2023-11-26T18:30:00.000Z",
        "voteCount": 4,
        "content": "Answer: AB\nhttps://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/"
      },
      {
        "date": "2023-11-20T04:24:00.000Z",
        "voteCount": 1,
        "content": "In Amazon S3, you can identify requests using an AWS CloudTrail event log. AWS CloudTrail is the preferred way of identifying Amazon S3 requests, but if you are using Amazon S3 server access logs, see Using Amazon S3 access logs to identify requests.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-request-identification.html"
      },
      {
        "date": "2023-11-13T14:46:00.000Z",
        "voteCount": 2,
        "content": "It doesn't mention that the folder name is the AWS username. There is no guarantee that alternative \u201cA\u201d will be effective."
      },
      {
        "date": "2023-11-29T05:58:00.000Z",
        "voteCount": 1,
        "content": "I think BD as well"
      },
      {
        "date": "2023-11-11T05:54:00.000Z",
        "voteCount": 2,
        "content": "CloudTrail + Identify so A and B, there was another question on CloudTrail vs. S3 Server Access logging, always CloudTrail wins"
      },
      {
        "date": "2023-11-09T03:38:00.000Z",
        "voteCount": 1,
        "content": "AB\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"
      },
      {
        "date": "2023-11-06T12:00:00.000Z",
        "voteCount": 2,
        "content": "AB\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"
      },
      {
        "date": "2023-11-17T04:18:00.000Z",
        "voteCount": 1,
        "content": "Look for\nTurn on logs for a subset of objects (prefix)\n-&gt; Only possible for CloudTrail"
      },
      {
        "date": "2023-11-02T21:27:00.000Z",
        "voteCount": 1,
        "content": "To Audit: B is the correct one\nTo Act: A is the correct one but not so effective."
      },
      {
        "date": "2023-11-02T11:07:00.000Z",
        "voteCount": 2,
        "content": "cloudtrail always for compliance"
      },
      {
        "date": "2023-10-30T06:18:00.000Z",
        "voteCount": 2,
        "content": "CloudTrail provides more detailed logging than S3 server access logging\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html"
      },
      {
        "date": "2023-10-30T00:52:00.000Z",
        "voteCount": 2,
        "content": "Why not c? As objects are being accessed within the bucket."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 307,
    "url": "https://www.examtopics.com/discussions/amazon/view/124883-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations to manage a multi-account structure. The company has hundreds of AWS accounts and expects the number of accounts to increase. The company is building a new application that uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry (Amazon ECR). Only accounts that are within the company\u2019s organization should have access to the images.<br><br>The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images. However, the company wants to retain only the five most recent untagged images.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company\u2019s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a public repository in Amazon ECR. Create an IAM role in the ECR account. Set permissions so that any account can assume the role if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company\u2019s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private repository in Amazon ECR. Create a permissions policy for the repository that includes only required ECR operations. Include a condition to allow the ECR operations for all account IDs in the organization Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a public repository in Amazon ECR. Configure Amazon ECR to use an interface VPC endpoint with an endpoint policy that includes the required permissions for images that the company needs to pull. Include a condition to allow the ECR operations for all account IDs in the company\u2019s organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T00:24:00.000Z",
        "voteCount": 1,
        "content": "How to associate the policy in ECR repository ? \nI think A is also wrong...."
      },
      {
        "date": "2023-12-03T03:13:00.000Z",
        "voteCount": 4,
        "content": "Answer A. Use ECR Lifecycle policy. Also using OrgId is more scalable with more accounts will be added than adding accounts individually. Less operational overhead."
      },
      {
        "date": "2023-11-18T22:25:00.000Z",
        "voteCount": 2,
        "content": "A is right option."
      },
      {
        "date": "2023-11-17T05:29:00.000Z",
        "voteCount": 2,
        "content": "Only A is a good idea"
      },
      {
        "date": "2023-11-01T15:40:00.000Z",
        "voteCount": 4,
        "content": "B, D: stop reading at \"public repository\" \nA: policy specific to aws:PrincipalOrgId equal company's organization ID\nC: policy allow all account ID (effectively the same actually) but use Eventbridge + lambda while ECR has lifecycle policy."
      },
      {
        "date": "2023-10-30T06:20:00.000Z",
        "voteCount": 1,
        "content": "Also A"
      },
      {
        "date": "2023-10-29T06:29:00.000Z",
        "voteCount": 1,
        "content": "A works for all requirements"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 308,
    "url": "https://www.examtopics.com/discussions/amazon/view/124807-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days.<br><br>The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-29T06:34:00.000Z",
        "voteCount": 6,
        "content": "Crossaccount management is a feature of only the aws backup service."
      },
      {
        "date": "2023-11-18T22:28:00.000Z",
        "voteCount": 3,
        "content": "Option A"
      },
      {
        "date": "2023-10-30T06:23:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html"
      },
      {
        "date": "2023-10-28T16:45:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html"
      },
      {
        "date": "2023-10-28T05:00:00.000Z",
        "voteCount": 2,
        "content": "Answer: A\nUser BAWS BAckup &gt; Cross Accounr &gt; backup plan for frequency and retention and health of the backup"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 309,
    "url": "https://www.examtopics.com/discussions/amazon/view/124808-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS Organizations with a multi-account architecture. The company's current security configuration for the account architecture includes SCPs, resource-based policies, identity-based policies, trust policies, and session policies.<br><br>A solutions architect needs to allow an IAM user in Account A to assume a role in Account B.<br><br>Which combination of steps must the solutions architect take to meet this requirement? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the SCP for Account A to allow the action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the resource-based policies to allow the action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the identity-based policy on the user in Account A to allow the action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the identity-based policy on the user in Account B to allow the action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the trust policy on the target role in Account B to allow the action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "CEF",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "CDE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-28T05:06:00.000Z",
        "voteCount": 11,
        "content": "Answer: C, E, F\nAttach a policy to the IAM user in Account A &gt; Trust Policy in Account B &gt;  GetSessionToken API operation"
      },
      {
        "date": "2024-02-17T04:20:00.000Z",
        "voteCount": 1,
        "content": "F is wrong, you cannot use GetSessionToken to configure session policy. \nYou can pass a single inline session policy programmatically by using the policy parameter with the AssumeRole, AssumeRoleWithSAML, AssumeRoleWithWebIdentity, and GetFederationToken API operations.\nACE is correct answer."
      },
      {
        "date": "2023-11-07T09:23:00.000Z",
        "voteCount": 9,
        "content": "- C) Attach an identity-based policy to the IAM user in Account A (allowed to assume IAM role in Acccount B)\n- E) Configure the trust policy on the target role in Account B (accountID of the trusted account which is Account A)\n- B) Configure a resource-based policy which allows certain actions on resources which reside in Account B)\n\nreference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
      },
      {
        "date": "2024-01-04T01:18:00.000Z",
        "voteCount": 1,
        "content": "IAM roles and resource-based policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard aws partition. You also have an account in China (Beijing) in the aws-cn partition. You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard aws account.\n\nSo B can't be answer."
      },
      {
        "date": "2024-10-01T06:49:00.000Z",
        "voteCount": 1,
        "content": "SCPs are not necessary at all here..."
      },
      {
        "date": "2024-09-27T17:43:00.000Z",
        "voteCount": 1,
        "content": "the ask is steps to \"ASSUME A ROLE\" not to \"access the resource\" . so option B and F are wrong as with A, C, E I can still assume the role regardless of the configuration of resource policy and session policy who can still deny access to the resource"
      },
      {
        "date": "2024-08-13T05:42:00.000Z",
        "voteCount": 1,
        "content": "To allow an IAM user in Account A to assume a role in Account B - we only need identity-based , resource-based and trust policies. Session policy and SCP not required."
      },
      {
        "date": "2024-08-11T02:19:00.000Z",
        "voteCount": 1,
        "content": "A (SCP) is more relevant than B (resource-based policies) because, while SCPs are not granting permissions, they could potentially restrict actions. Therefore, ensuring that the SCP in Account A (and Account B) does not block the necessary sts:AssumeRole action is important. B (resource-based policies) isn't relevant for the cross-account role assumption in this context."
      },
      {
        "date": "2024-08-11T02:19:00.000Z",
        "voteCount": 1,
        "content": "A: SCPs (Service Control Policies) are used to set permission boundaries at the organizational or account level. SCPs can restrict or allow certain actions, but they do not grant permissions directly. An SCP in Account A would typically not be responsible for directly allowing a user to assume a role in Account B, though it could block the action if not configured properly."
      },
      {
        "date": "2024-08-11T02:19:00.000Z",
        "voteCount": 1,
        "content": "B: Resource-based policies are policies attached directly to AWS resources (like S3 buckets or IAM roles). However, in this scenario, resource-based policies are less relevant because the focus is on role assumption, which is governed by identity policies and trust policies rather than resource-based policies."
      },
      {
        "date": "2024-07-05T00:38:00.000Z",
        "voteCount": 1,
        "content": "E Trust policy in B\nD Identity-based policy on the ROLE in Account B to allow the action (I think typo in question)\nC Configure the identity-based policy on the user in Account A to allow the action.\n\nJust try it in AWS env."
      },
      {
        "date": "2024-06-11T17:09:00.000Z",
        "voteCount": 2,
        "content": "you generally do not need to modify the Service Control Policies (SCPs) to allow one account's IAM users to assume roles in another account, as long as the SCPs do not explicitly deny the required actions (like sts:AssumeRole)."
      },
      {
        "date": "2024-06-01T14:38:00.000Z",
        "voteCount": 2,
        "content": "BCE - SCP is not required here &amp; used for deny not for allow"
      },
      {
        "date": "2024-05-23T07:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is BCE.\nSCPs are not used for ALLOW actions but for DENY actions at Org level."
      },
      {
        "date": "2024-05-13T06:54:00.000Z",
        "voteCount": 2,
        "content": "The key point here is \"The company's current security configuration for the account architecture includes SCPs,\" so if SCPs are in place, the SCP in the account A has to be configured to allow the action."
      },
      {
        "date": "2024-05-03T02:57:00.000Z",
        "voteCount": 2,
        "content": "ACE for me"
      },
      {
        "date": "2024-04-14T08:13:00.000Z",
        "voteCount": 2,
        "content": "B, C, E are correct answers."
      },
      {
        "date": "2024-03-29T09:24:00.000Z",
        "voteCount": 2,
        "content": "A: By default, an account is created and added to an AWS Organization inherits a \"FullAWSAccess\" policy, we don't have to \"allow\" the action"
      },
      {
        "date": "2024-03-28T14:28:00.000Z",
        "voteCount": 3,
        "content": "Options B, D, and F are not directly relevant to enabling cross-account role assumption in this context:\n\nB. Resource-based policies are not typically configured on IAM users but on resources like S3 buckets or KMS keys.\nD. The identity-based policy on a user in Account B is irrelevant since the action is being initiated by a user in Account A.\nF. Session policies are used to pass permissions when you create a session for a role or federated user. The GetSessionToken API operation is used with IAM users to create a session with MFA, not for assuming roles across accounts.\nTherefore, the correct combination of steps is A, C, and E."
      },
      {
        "date": "2024-03-14T05:11:00.000Z",
        "voteCount": 3,
        "content": "A: if \"allow\" is taken to mean \"not deny\"\nB: Resource policies have nothing to do with this\nC: required\nD: The user is in account A, not in account B, so this is out\nE: required\nF: Not how things are done when assuming roles"
      },
      {
        "date": "2024-02-17T04:20:00.000Z",
        "voteCount": 3,
        "content": "ACE is correct answer.\nF is wrong, you cannot use GetSessionToken to configure session policy. \nYou can pass a single inline session policy programmatically by using the policy parameter with the AssumeRole, AssumeRoleWithSAML, AssumeRoleWithWebIdentity, and GetFederationToken API operations."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 310,
    "url": "https://www.examtopics.com/discussions/amazon/view/124809-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to use Amazon S3 to back up its on-premises file storage solution. The company\u2019s on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-30T14:36:00.000Z",
        "voteCount": 3,
        "content": "Glacier Deep Archive is more cost-effective than Standard-IA, so A and C are out.\nDecision between B and D: The solution requires support for NFS -&gt; File Gateway instead of Volume Gateway --&gt; D it is."
      },
      {
        "date": "2023-11-18T22:38:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2023-11-17T05:40:00.000Z",
        "voteCount": 2,
        "content": "D, with Depp Archive, Retrieval time within 12 hours"
      },
      {
        "date": "2023-11-02T21:48:00.000Z",
        "voteCount": 3,
        "content": "File Gateway\nGlacier"
      },
      {
        "date": "2023-10-30T06:27:00.000Z",
        "voteCount": 2,
        "content": "D - File Gateway supports NFS and Deep Glacier is the cheapest storage option in S3"
      },
      {
        "date": "2023-10-28T05:19:00.000Z",
        "voteCount": 3,
        "content": "Answer: D\n\nusing AWS Storage file is appropriate and straight to S3 Glacier Deep Archive is most cost efficient as the company is willing to wait a few days for  the retrieval of those files in case of DR"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 311,
    "url": "https://www.examtopics.com/discussions/amazon/view/124810-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instances experience a continuous and stable load. The Lambda functions experience a varied and unpredictable load. The application includes a caching layer that uses an Amazon MemoryDB for Redis cluster.<br><br>A solutions architect must recommend a solution to minimize the company's overall monthly costs.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a Compute Savings Plan to cover the EC2 instances. Purchase Lambda reserved concurrency to cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodes. Purchase Lambda reserved concurrency to cover the expected Lambda usage."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-01T02:55:00.000Z",
        "voteCount": 9,
        "content": "EC2 - Saving Plan, MemoryDB - Reserved Node, Lambda - Compute Saving Plan"
      },
      {
        "date": "2023-10-29T17:07:00.000Z",
        "voteCount": 6,
        "content": "Answer is A, it saves the most cost saving option.\nB and D are out as reserved concurrency doesn\u2019t help for cost saving. Compared between A&amp;C, A is more cost effective solution, additionally compute saving plan doesn\u2019t cover costs for elastic cache node."
      },
      {
        "date": "2023-12-03T03:38:00.000Z",
        "voteCount": 1,
        "content": "A makes sense. Reserved concurrency for Lambda doesn't address cost savings nor varied load. And compute plans don't cover MemoryDB. Reserved nodes should work."
      },
      {
        "date": "2023-11-25T02:08:00.000Z",
        "voteCount": 3,
        "content": "Compute Saving Plans don't cover MemoryDB"
      },
      {
        "date": "2023-11-20T07:37:00.000Z",
        "voteCount": 2,
        "content": "We don\u2019t know the expected load of Lambda so B and D out (it says expected Lambda usage) and A it\u2019s more cost effective than C"
      },
      {
        "date": "2023-11-18T22:43:00.000Z",
        "voteCount": 1,
        "content": "A is most cost effective."
      },
      {
        "date": "2023-10-29T07:22:00.000Z",
        "voteCount": 3,
        "content": "Reserved concurrency for lambda wont reduce costs, and lambda will benefit from compute savings plan https://aws.amazon.com/about-aws/whats-new/2020/02/aws-lambda-participates-in-compute-savings-plans/"
      },
      {
        "date": "2023-10-28T19:49:00.000Z",
        "voteCount": 5,
        "content": "ChatGPT"
      },
      {
        "date": "2023-10-28T05:27:00.000Z",
        "voteCount": 3,
        "content": "EC2 - Saving Plan, MemoryDB - Reserved Node, Lambda - reserved concurrency"
      },
      {
        "date": "2023-11-01T02:55:00.000Z",
        "voteCount": 2,
        "content": "Change this to A as it is correct that Lambda Reserved Concurrency does not help in saving costs."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 312,
    "url": "https://www.examtopics.com/discussions/amazon/view/124812-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is launching a new online game on Amazon EC2 instances. The game must be available globally. The company plans to run the game in three AWS Regions us-east-1, eu-west-1, and ap-southeast-1. The game's leaderboards, player inventory and event status must be available across Regions.<br><br>A solutions architect must design a solution that will give any Region the ability to scale to handle the load of all Regions. Additionally, users must automatically connect to the Region that provides the least latency.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EC2 Spot Fleet. Attach the Spot Fleet to a Network Load Balancer (NLB) in each Region. Create an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53 latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an Amazon RDS for MySQL DB instance in each Region. Set up a read replica in the other Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group for the EC2 instances Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximity routing and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2 instances in each Region. Set up replication between the database EC2 instances in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Global View. Deploy the EC2 instances to each Region. Attach the instances to a Network Load Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic on each DNS server to redirect the user to the Region that provides the lowest latency. Save the game metadata to an Amazon Aurora global database."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-16T13:23:00.000Z",
        "voteCount": 3,
        "content": "Answer: C\nkeywords \"latency-based routing \" and \"DynamoDB global table.\""
      },
      {
        "date": "2023-11-18T22:50:00.000Z",
        "voteCount": 3,
        "content": "Option C that uses DynamoDB wins"
      },
      {
        "date": "2023-11-17T05:47:00.000Z",
        "voteCount": 4,
        "content": "C, Latency &gt; Geoproximity."
      },
      {
        "date": "2023-11-01T16:06:00.000Z",
        "voteCount": 2,
        "content": "easy C"
      },
      {
        "date": "2023-10-29T18:18:00.000Z",
        "voteCount": 2,
        "content": "Should be C"
      },
      {
        "date": "2023-10-29T07:26:00.000Z",
        "voteCount": 1,
        "content": "C 100%"
      },
      {
        "date": "2023-10-28T19:52:00.000Z",
        "voteCount": 1,
        "content": "Latency Routing"
      },
      {
        "date": "2023-10-28T05:35:00.000Z",
        "voteCount": 2,
        "content": "Autoscaling and NLB for Load Distribution, Latency Routing  for Least Latency and DynamoDB Global Table for replication across regin."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 313,
    "url": "https://www.examtopics.com/discussions/amazon/view/124813-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor and protect traffic that leaves the company's AWS environments. The company wants to deploy this appliance into a shared services VPC and route all outbound internet-bound traffic through the appliances.<br><br>A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single AWS Region. The company has set up routing from the shared services VPC to other VPCs.<br><br>Which steps should the solutions architect recommend to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy two firewall appliances into the shared services VPC, each in a separate Availability Zone.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Gateway Load Balancer in the shared services VPCreate a new target group, and attach it to the new Gateway Load Balancer Add each of the firewall appliance instances to the target group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy two firewall appliances into the shared services VPC, each in the same Availability Zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "ACD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ABD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-17T10:41:00.000Z",
        "voteCount": 14,
        "content": "Need (A) two firewalls spread over two availability zones for HA and balanced by an NLB, then (C) a Gateway Load Balancer to interface to the virtual 3rd party network firewalls through the NLB, then (F) a Gateway Load Balancer EndPoint in the Consumer VPC with routes taking the traffic to the shared GLB + Firewalls\n\nA simple diagram is given here so you don't forget if you are visual like me :)\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/getting-started.html"
      },
      {
        "date": "2023-12-21T11:08:00.000Z",
        "voteCount": 2,
        "content": "apologies, I meant balanced by the GLB (A)"
      },
      {
        "date": "2023-10-31T00:04:00.000Z",
        "voteCount": 6,
        "content": "ACF\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/create-gateway-load-balancer-endpoint-service.html"
      },
      {
        "date": "2024-07-13T05:43:00.000Z",
        "voteCount": 1,
        "content": "A, C, F for sure.\nUsing Gateway Load Balancer for distributing traffic across multiple virtual appliances"
      },
      {
        "date": "2024-05-13T17:31:00.000Z",
        "voteCount": 1,
        "content": "healthy virtual appliances means gateway load balancer."
      },
      {
        "date": "2023-11-26T06:18:00.000Z",
        "voteCount": 1,
        "content": "Obviously A over E.  GWLB's don't make good load balancers.  Avoid C and F.  Need NLB to minimize the failover time between the (2) 3rd party FW's."
      },
      {
        "date": "2023-11-26T12:25:00.000Z",
        "voteCount": 2,
        "content": "Well, I read a bit further on Gateway Load Balancers and a 3rd party firewall is the perfect scenario to use a GwLB.   So, looks like the correct answers are ACF."
      },
      {
        "date": "2023-11-03T07:27:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-gateway-load-balancer-supported-architecture-patterns/"
      },
      {
        "date": "2023-10-28T05:47:00.000Z",
        "voteCount": 2,
        "content": "A: Use 2 firewall Apliances  for each AZ\nC: Use GWLB for 3rd party appliances routing traffic\nD: nables the routing of outbound internet-bound traffic through the firewall appliances."
      },
      {
        "date": "2023-10-29T07:36:00.000Z",
        "voteCount": 1,
        "content": "Why not gw endpoint?"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 314,
    "url": "https://www.examtopics.com/discussions/amazon/view/124998-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs on two servers behind a load balancer. The application requires a license file that is associated with the MAC address of the server's network adapter It takes the software vendor 12 hours to send new license files. The application also uses configuration files with a static IP address to access a database server, host names are not supported.<br><br>Given these requirements, which combination of steps should be taken to implement highly available architecture for the application servers in AWS? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pool of ENIs. Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pool of ENIs. Request license files from the vendor for the pool, store the license files on an Amazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bootstrap automation script to request a new license file from the vendor .When the response is received, apply the license file to an Amazon EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit an Amazon EC2 instance to include the database server IP address in the configuration files and re-create the AMI to use for all future EC2 stances."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-01T03:03:00.000Z",
        "voteCount": 6,
        "content": ".Option A covers the licensing aspect, and option D addresses the configuration file requirements."
      },
      {
        "date": "2024-05-13T17:51:00.000Z",
        "voteCount": 1,
        "content": "AD are the right answer"
      },
      {
        "date": "2023-12-23T02:58:00.000Z",
        "voteCount": 3,
        "content": "AD AS\nhttps://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/"
      },
      {
        "date": "2023-11-18T23:07:00.000Z",
        "voteCount": 3,
        "content": "A &amp; D are right options"
      },
      {
        "date": "2023-10-31T00:06:00.000Z",
        "voteCount": 3,
        "content": "AD\nBootstrap scripts"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 315,
    "url": "https://www.examtopics.com/discussions/amazon/view/124888-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and is accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to the API Gateway API.<br><br>In the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already deployed an API Gateway API and Lambda functions in the new Region.<br><br>A solutions architect must design a solution that minimizes latency for users who download reports.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a cross-Region read replica for the RDS database in the new Region Change the Route 53 record to latency-based routing to connect to the API Gateway API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-31T15:41:00.000Z",
        "voteCount": 11,
        "content": "minimizes latency for users who download reports."
      },
      {
        "date": "2023-10-31T00:09:00.000Z",
        "voteCount": 5,
        "content": "C\nQuestion specifies minimal latency for end users, latency based is more appropriate than geo based routing"
      },
      {
        "date": "2024-05-13T17:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2024-03-06T08:17:00.000Z",
        "voteCount": 1,
        "content": "minimizes latency"
      },
      {
        "date": "2023-11-25T02:48:00.000Z",
        "voteCount": 3,
        "content": "C - minimizes latency for users who download reports"
      },
      {
        "date": "2023-11-18T23:17:00.000Z",
        "voteCount": 2,
        "content": "As latency is key Option C"
      },
      {
        "date": "2023-11-03T07:59:00.000Z",
        "voteCount": 3,
        "content": "The concern is the latency not the region itself."
      },
      {
        "date": "2023-10-29T08:04:00.000Z",
        "voteCount": 1,
        "content": "Geo routing ftw"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 316,
    "url": "https://www.examtopics.com/discussions/amazon/view/124899-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software company needs to create short-lived test environments to test pull requests as part of its development process. Each test environment consists of a single Amazon EC2 instance that is in an Auto Scaling group.<br><br>The test environments must be able to communicate with a central server to report test results. The central server is located in an on-premises data center. A solutions architect must implement a solution so that the company can create and delete test environments without any manual intervention. The company has created a transit gateway with a VPN attachment to the on-premises network.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that contains a transit gateway attachment and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each test environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new OU in AWS Organizations for testing. Create an AWS CioudFormation template that contains a VPC, necessary networking resources, a transit gateway attachment, and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets for deployments into each account under the testing OU. Create a new account for each test environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the test environment EC2 instances into Docker images. Use AWS CloudFormation to configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gateway attachment, and create related routing configurations. Use Kubernetes to manage the deployment and lifecycle of the test environments."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-07T15:57:00.000Z",
        "voteCount": 7,
        "content": "This question is very vague. It doesn't say whether each test env is an a separate VPC or a separate account. Not sure why Transit gateway is used here."
      },
      {
        "date": "2023-11-01T16:26:00.000Z",
        "voteCount": 5,
        "content": "A: no need for new VPC for each test\nB: sounds ok \nC: creating new OU for each test? -&gt; out\nD: too complex since need to containerize from code in EC2 instance"
      },
      {
        "date": "2024-05-09T05:41:00.000Z",
        "voteCount": 2,
        "content": "I'm going with A.\nThere is no needed to have the Transit Gateway without more VPCs for each account.\nIt's also a best practices to have separate environments to isolate test."
      },
      {
        "date": "2024-08-14T23:35:00.000Z",
        "voteCount": 1,
        "content": "jusy b"
      },
      {
        "date": "2023-11-18T23:27:00.000Z",
        "voteCount": 3,
        "content": "Choice is between A &amp; B. Given there are no other requirements for using stacksets B is most simple."
      },
      {
        "date": "2024-04-14T12:34:00.000Z",
        "voteCount": 2,
        "content": "What's the purpose of the transit gateway in this case and what you'll do if you want to delete a single test environment? I lean more to A."
      },
      {
        "date": "2024-01-14T00:27:00.000Z",
        "voteCount": 1,
        "content": "I am following your answer. :)"
      },
      {
        "date": "2023-11-18T01:36:00.000Z",
        "voteCount": 1,
        "content": "B is the best answer"
      },
      {
        "date": "2023-11-03T08:10:00.000Z",
        "voteCount": 1,
        "content": "LEAST operational overhead -&gt; B \nno need to over-complicate it"
      },
      {
        "date": "2023-10-29T11:04:00.000Z",
        "voteCount": 1,
        "content": "Not sure abot the benefits od using a stack set in this case, going with B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 317,
    "url": "https://www.examtopics.com/discussions/amazon/view/124901-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional API endpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API, stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customer managed key in AWS Key Management Service (AWS KMS). The company has deployed its own API into a single AWS Region.<br><br>A solutions architect needs to change the API components of the company\u2019s API to ensure that the components can run across multiple Regions in an active-active configuration.<br><br>Which combination of changes will meet this requirement with the LEAST operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multiRegion key. Use the multi-Region key in other Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-30T19:44:00.000Z",
        "voteCount": 1,
        "content": "Why C? Does the new KMS key not need to create a new encrypted secret?"
      },
      {
        "date": "2024-01-09T08:45:00.000Z",
        "voteCount": 2,
        "content": "A, B , C are the right options."
      },
      {
        "date": "2024-01-04T11:19:00.000Z",
        "voteCount": 2,
        "content": "ABC:\n\nA : deploy the API to other region includes deploy the lambda functions too. so F is not needed."
      },
      {
        "date": "2024-03-22T14:53:00.000Z",
        "voteCount": 1,
        "content": "that's a big assumption"
      },
      {
        "date": "2023-12-23T03:34:00.000Z",
        "voteCount": 1,
        "content": "ABC ANShttps://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/"
      },
      {
        "date": "2023-12-19T17:54:00.000Z",
        "voteCount": 1,
        "content": "The diagram mentioned here supports F\nhttps://docs.aws.amazon.com/architecture-diagrams/latest/multi-region-api-gateway-with-cloudfront/multi-region-api-gateway-with-cloudfront.html"
      },
      {
        "date": "2023-12-20T08:35:00.000Z",
        "voteCount": 1,
        "content": "it is mention here about AWS Lambda @Edge not simple lambda"
      },
      {
        "date": "2024-01-09T08:30:00.000Z",
        "voteCount": 1,
        "content": "This diagram has cloudfront. Option F does not include Cloudfront. So F is not correct."
      },
      {
        "date": "2023-11-29T06:38:00.000Z",
        "voteCount": 3,
        "content": "ABC is the answer"
      },
      {
        "date": "2023-11-28T06:58:00.000Z",
        "voteCount": 3,
        "content": "Cannot convert single region KMS to multi-region. ABC is the answer"
      },
      {
        "date": "2023-11-28T06:57:00.000Z",
        "voteCount": 3,
        "content": "Cannot convert single region KMS to multi-region. ABC is the answer"
      },
      {
        "date": "2023-11-18T23:47:00.000Z",
        "voteCount": 2,
        "content": "B, C and D is right answer. In an Active Active setup with Regional API Gateway endpoint Lambda must be deployed in each Region. https://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/"
      },
      {
        "date": "2023-10-31T00:15:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-create.html\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/create-manage-multi-region-secrets.html"
      },
      {
        "date": "2023-10-29T11:11:00.000Z",
        "voteCount": 1,
        "content": "ABC, others make no sense"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 318,
    "url": "https://www.examtopics.com/discussions/amazon/view/124924-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture.<br><br>Which solution should provide the HIGHEST level of reliability?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon Neptune",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer Store sessions in Amazon Kinesis Data Firehose.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-30T00:35:00.000Z",
        "voteCount": 5,
        "content": "Amazon Aurora provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services.\nRedis supports replication: https://aws.amazon.com/elasticache/redis-vs-memcached/. When adding both solutions B seems the correct answer"
      },
      {
        "date": "2024-01-09T08:51:00.000Z",
        "voteCount": 1,
        "content": "Option B is an obvious answer."
      },
      {
        "date": "2023-11-28T07:01:00.000Z",
        "voteCount": 3,
        "content": "Neptune, MariaDB &amp; Kinesis Firehose out!\nB is the right answer, very reliable with Aurora"
      },
      {
        "date": "2023-11-01T16:32:00.000Z",
        "voteCount": 3,
        "content": "A, C: store sessions in Neptune or Kinesis Firehose? -&gt; out\nD: migrate MySQL to MariaDB instance out of the blue? -&gt; out\nB is valid with classic architecture."
      },
      {
        "date": "2023-10-31T00:17:00.000Z",
        "voteCount": 2,
        "content": "Highest availability is the key"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 319,
    "url": "https://www.examtopics.com/discussions/amazon/view/124902-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company\u2019s solutions architect needs to provide secure Remote Desktop connectivity to users for Amazon EC2 Windows instances that are hosted in a VPC. The solution must integrate centralized user management with the company's on-premises Active Directory. Connectivity to the VPC is through the internet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in the VPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the target instances through RDP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a VPN between the on-premises environment and the target VPEnsure that the target instances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDP access through the VPN. Connect from the company\u2019s network to the target instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS by using an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use the Remote Desktop Gateway to access the target instances through RDP."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T07:22:00.000Z",
        "voteCount": 17,
        "content": "I think this question is not really about Active Directory or AD Connector.\nA secure VPN connection is all you need in this question.\nThe company has hardware can be used to establish an AWS S2S connection. \nIn order to have a secure connection, the first thing you have to do is to implement a VPN connection between on-premise and target VPC.\nSo C is the answer."
      },
      {
        "date": "2023-11-13T20:24:00.000Z",
        "voteCount": 12,
        "content": "You cannot join an EC2 to On-prem AD just over the VPN. You should be having an AD connector for the same. \nhttps://aws.amazon.com/blogs/security/how-to-connect-your-on-premises-active-directory-to-aws-using-ad-connector/"
      },
      {
        "date": "2024-01-02T22:28:00.000Z",
        "voteCount": 5,
        "content": "Can you provide the link saying why EC2 cannot join an onprem AD over VPN? As long as the network connectivity is created, I don't see a problem for an EC2 instance to join an on-prem domain."
      },
      {
        "date": "2024-01-14T23:47:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/tw/blogs/networking-and-content-delivery/integrating-your-directory-services-dns-resolution-with-amazon-route-53-resolvers/\n\nYou should config DHCP and DNS"
      },
      {
        "date": "2024-03-25T11:39:00.000Z",
        "voteCount": 2,
        "content": "The article is about \"Integrating your Directory Service\u2019s DNS resolution with Amazon Route 53 Resolvers\". It doesn't mean an EC2 cannot join an onprem AD. If AWS says you can't use onprem AD even the network is connected, that is really a terrible design. I don't think AWS can design it that way."
      },
      {
        "date": "2024-03-25T11:41:00.000Z",
        "voteCount": 3,
        "content": "AWS might recommend the consumers to use Active directory connect, but cannot deny using on-prem ADDS directly. And as long as the network is connected, all you need is to create a custom DHCP option set pointing to that ADDS."
      },
      {
        "date": "2024-10-15T06:44:00.000Z",
        "voteCount": 1,
        "content": "I do not see any reasons why C would not work. And it is simpler than B."
      },
      {
        "date": "2024-05-30T09:02:00.000Z",
        "voteCount": 2,
        "content": "C is the cheapest option"
      },
      {
        "date": "2024-05-09T06:28:00.000Z",
        "voteCount": 3,
        "content": "For me it's C.\nNo need to Managed AD Connector. We have already a VPN, so we can leverage to spend less."
      },
      {
        "date": "2024-05-03T03:10:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2024-04-14T13:15:00.000Z",
        "voteCount": 2,
        "content": "Ans C.\nIf the VPC and the on-prem network are connected, there is no need for AD Connector, it works like any other interconnected networks. The EC2s must have DNS resolution, usually those will be the AD domain controllers (which in this case are located on prem)."
      },
      {
        "date": "2024-03-15T21:27:00.000Z",
        "voteCount": 4,
        "content": "It is B and not C. You need to AD connector to connect to on-premises AD. Did not find any article that suggests you can connect to on-premises AD over VPN without using AD connector or Active directory trust."
      },
      {
        "date": "2024-03-24T03:30:00.000Z",
        "voteCount": 2,
        "content": "If you just change your DHCP on AWS and put the domain IP from your on-premise AD, yes you can, but I think AWS expects that you use SSM for that, so B is the answer, but again, you can definitely connect your all environment EC2 to your On-Premise AD with just VPN"
      },
      {
        "date": "2024-02-17T06:09:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2024-02-17T04:56:00.000Z",
        "voteCount": 3,
        "content": "C is the answer. most cost-effective."
      },
      {
        "date": "2024-02-12T10:54:00.000Z",
        "voteCount": 3,
        "content": "It is C"
      },
      {
        "date": "2024-02-09T15:23:00.000Z",
        "voteCount": 2,
        "content": "B is the answer. C would be the cheapest option, BUT it say's they currently access over the internet. This means that they don't have DNS appliances setup. Those are not included in the answer and they also cost money, making B the only real option here."
      },
      {
        "date": "2024-01-14T17:04:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\nKeyword \"AWS IAM Identity Center (AWS Single Sign-On) \""
      },
      {
        "date": "2024-01-09T09:34:00.000Z",
        "voteCount": 6,
        "content": "C is the cheapest option. D is possible but there are hidden cost like Windows server licensing cost for each subnet + Secrets Manager cost."
      },
      {
        "date": "2024-01-14T00:43:00.000Z",
        "voteCount": 1,
        "content": "I am following your answer. Windows connect question is really hard for me. I have no experience."
      },
      {
        "date": "2023-11-20T02:10:00.000Z",
        "voteCount": 3,
        "content": "B seems cheaper than D"
      },
      {
        "date": "2023-11-07T11:12:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/mt/console-based-access-to-windows-instances-using-aws-systems-manager-fleet-manager/"
      },
      {
        "date": "2023-11-04T07:39:00.000Z",
        "voteCount": 1,
        "content": "D is the cheapest option: \n| A | AWS Directory Service for Microsoft Active Directory: $0.90 per directory per month + EC2 instance: $0.006 per hour |\n| B | AWS IAM Identity Center: $0.25 per user per month + AWS Directory Service for Microsoft Active Directory AD Connector: $0.25 per directory per month + AWS Systems Manager: $0.033 per hour per instance |\n| C | VPN connection: Varies depending on the provider and the type of VPN connection + Target instances: $0.006 per hour per instance |\n| D | AWS Directory Service for Microsoft Active Directory: $0.90 per directory per month + Remote Desktop Gateway Quick Start: No additional cost |"
      },
      {
        "date": "2023-11-12T10:18:00.000Z",
        "voteCount": 1,
        "content": "AWS Directory Service for Microsoft Active Directory in Ireland costs about 92 $ per month, not 0.90"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 320,
    "url": "https://www.examtopics.com/discussions/amazon/view/124903-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes that were created in an AWS account were not encrypted. A solutions architect must implement a solution to encrypt all new EBS volumes at rest.<br><br>Which solution will meet this requirement with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Audit Manager with data encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on EBS encryption by default in all AWS Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-01T16:36:00.000Z",
        "voteCount": 5,
        "content": "\"must implement a solution to encrypt all NEWWW EBS volumes at rest.\""
      },
      {
        "date": "2024-07-12T02:27:00.000Z",
        "voteCount": 1,
        "content": "there is no direct way to encrypt existing unencrypted EBS volumes or snapshots.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html"
      },
      {
        "date": "2024-06-29T08:26:00.000Z",
        "voteCount": 1,
        "content": "I am not picking an answer, I just wanted to point out that EBS encryption is regions specific. option D says : Turn on EBS encryption by default in all AWS Regions. there is no such feature. Option D still appears to be the best answer"
      },
      {
        "date": "2024-01-12T14:16:00.000Z",
        "voteCount": 4,
        "content": "Answer: D\nEncryption of Amazon Elastic Block Store (Amazon EBS) volumes is important to an organization's data protection strategy. It is an important step in establishing a well-architected environment. Although there is no direct way to encrypt existing unencrypted EBS volumes or snapshots, you can encrypt them by creating a new volume or snapshot. \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html"
      },
      {
        "date": "2024-01-09T09:36:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-01T03:29:00.000Z",
        "voteCount": 2,
        "content": "The keyword is all NEW EBS volumes.\nSo by make EBS Encryption default, it means all new EBS will be encrypted without additional configuration."
      },
      {
        "date": "2023-10-31T00:29:00.000Z",
        "voteCount": 3,
        "content": "Least effort option"
      },
      {
        "date": "2023-10-30T00:07:00.000Z",
        "voteCount": 3,
        "content": "The question states: ' A solutions architect must implement a solution to encrypt all new EBS volumes at rest'\nreference: https://repost.aws/knowledge-center/ebs-automatic-encryption"
      },
      {
        "date": "2023-10-29T11:21:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 321,
    "url": "https://www.examtopics.com/discussions/amazon/view/124904-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A research company is running daily simulations in the AWS Cloud to meet high demand. The simulations run on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an EC2 instance through SSH.<br><br>Company policy states that no EC2 instance can use the same SSH key and that all connections must be logged in AWS CloudTrail.<br><br>How can a solutions architect meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch new EC2 instances, and generate an individual SSH key for each instance. Store the SSH key in AWS Secrets Manager. Create a new IAM policy, and attach it to the engineers\u2019 IAM role with an Allow statement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Systems Manager document to run commands on EC2 instances to set a new unique SSH key. Create a new IAM policy, and attach it to the engineers\u2019 IAM role with an Allow statement to run Systems Manager documents. Instruct the engineers to run the document to set an SSH key and to connect through any SSH client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers\u2019 IAM role with an Allow statement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Secrets Manager to store the EC2 SSH key. Create a new AWS Lambda function to create a new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2 instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-29T11:28:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-eic.html"
      },
      {
        "date": "2023-11-01T03:56:00.000Z",
        "voteCount": 7,
        "content": "Answer C is correct with the following reasons:\nThe keywords: \"no EC2 instance can use the same SSH key\" AND \" all connections must be logged in AWS CloudTrail.\"\n1. EC2 Instance connect using temporary ssh key, one-time SSH keys each time the user connects\n2. User connections via EC2 Instance Connect are logged to AWS CloudTrail"
      },
      {
        "date": "2024-05-05T04:36:00.000Z",
        "voteCount": 2,
        "content": "D\nWhy not D. In C with instance connect, there are 100s of instances and key would be created for each instance manually would take lot of time"
      },
      {
        "date": "2024-04-24T04:44:00.000Z",
        "voteCount": 1,
        "content": "Option C -   https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-configure-IAM-role.html"
      },
      {
        "date": "2024-04-23T09:45:00.000Z",
        "voteCount": 1,
        "content": "Option C -   https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-configure-IAM-role.html"
      },
      {
        "date": "2024-04-09T06:18:00.000Z",
        "voteCount": 1,
        "content": "can some one justify why cant use AWS system manager (Session manager) option  B ??"
      },
      {
        "date": "2024-06-22T16:38:00.000Z",
        "voteCount": 1,
        "content": "No CloudTrail logging for AWS system manager documents."
      },
      {
        "date": "2024-01-09T09:52:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 322,
    "url": "https://www.examtopics.com/discussions/amazon/view/124867-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backend service applications run in an on-premises data center. The data center has an AWS Direct Connect connection into AWS. The applications that run in the VPC need to resolve DNS requests to an on-premises Active Directory domain that runs in the data center.<br><br>Which solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-11T18:38:00.000Z",
        "voteCount": 2,
        "content": "Option C: Amazon Route 53 Resolver with Conditional Forwarding Rules\n\nLeast Administrative Overhead: This option leverages AWS-managed services to handle DNS resolution without the need to manage additional infrastructure or complicated configurations.\nRoute 53 Resolver Endpoints: Create inbound and outbound endpoints to handle DNS queries between AWS and the on-premises environment.\n        Inbound Endpoints: Allow on-premises systems to resolve DNS names hosted in AWS.\n        Outbound Endpoints: Forward DNS queries from AWS to on-premises DNS servers.\nConditional Forwarding Rules: Set up rules to forward specific domain queries (like your Active Directory domain) to the on-premises DNS servers. This ensures seamless DNS resolution for the applications in the VPC.\n\nB: Private hosted zones are intended for DNS records within AWS. \nA &amp; D: too much overhead"
      },
      {
        "date": "2024-05-13T18:23:00.000Z",
        "voteCount": 1,
        "content": "Why not B?"
      },
      {
        "date": "2024-01-09T09:57:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-11-28T07:21:00.000Z",
        "voteCount": 2,
        "content": "Answer is C, least admin overhead using Route 53 resolver with conditional forwarding"
      },
      {
        "date": "2023-11-25T10:40:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-11-01T04:22:00.000Z",
        "voteCount": 4,
        "content": "Option C: Amazon Route 53 Resolver &gt; Conditional Forwarding\nLower Maintenance than Option A which using EC2."
      },
      {
        "date": "2023-10-30T00:23:00.000Z",
        "voteCount": 3,
        "content": "To forward DNS queries from your VPCs to your network, you create an outbound endpoint. \nreference: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-forwarding-outbound-queries.html"
      },
      {
        "date": "2023-10-29T19:05:00.000Z",
        "voteCount": 1,
        "content": "I vote for C"
      },
      {
        "date": "2023-10-28T19:57:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/security/how-to-set-up-dns-resolution-between-on-premises-networks-and-aws-using-aws-directory-service-and-amazon-route-53/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 323,
    "url": "https://www.examtopics.com/discussions/amazon/view/126753-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company processes environmental data. The company has set up sensors to provide a continuous stream of data from different areas in a city. The data is available in JSON format.<br><br>The company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be sent in real time.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to send the data to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to send the data to Amazon DynamoDB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose to send the data to Amazon Keyspaces (for Apache Cassandra)."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T07:24:00.000Z",
        "voteCount": 7,
        "content": "Kinesis Data streams is real-time. Firehose is near real-time. DynamDB is not a relational DB and does not enforce fixed schemas on its tables. Answer is B"
      },
      {
        "date": "2024-07-04T20:22:00.000Z",
        "voteCount": 1,
        "content": "By using Amazon Kinesis Data Firehose to send the data to Amazon Keyspaces, the company can efficiently stream real-time data and store it in a schema-less database, meeting the requirement for flexibility and real-time processing.\n\nOption B is not correct:\nWhile Amazon Kinesis Data Streams can handle real-time data, it does not directly integrate with Amazon DynamoDB. Additional steps are needed to process and insert the data into DynamoDB. Additionally, DynamoDB, though flexible, typically benefits from having a defined schema for efficient access patterns."
      },
      {
        "date": "2024-08-16T18:45:00.000Z",
        "voteCount": 1,
        "content": "just B"
      },
      {
        "date": "2024-06-02T08:06:00.000Z",
        "voteCount": 1,
        "content": "B, Firehose+ DynamoDB \nhttps://aws.amazon.com/blogs/database/working-with-json-data-in-amazon-dynamodb/"
      },
      {
        "date": "2024-06-01T15:26:00.000Z",
        "voteCount": 1,
        "content": "Option D - By using Amazon Kinesis Data Firehose to send the environmental data to Amazon Keyspaces (for Apache Cassandra), you can leverage a fully managed streaming data ingestion service and a schema-flexible NoSQL database, meeting the requirements for real-time processing and storage of data without a fixed schema."
      },
      {
        "date": "2024-01-12T13:28:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\nOption B leverages the strengths of both Kinesis Data Streams and DynamoDB to provide a scalable and real-time solution for ingesting and storing JSON-format data without fixed schemas.\nOption A:  Kinesis Data Firehose: While suitable for real-time data delivery, it has a limited set of destinations, not including DynamoDB."
      },
      {
        "date": "2024-01-09T09:58:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-11-25T10:37:00.000Z",
        "voteCount": 2,
        "content": "Correct B"
      },
      {
        "date": "2023-11-25T03:38:00.000Z",
        "voteCount": 2,
        "content": "Load json format to DynamoDB"
      },
      {
        "date": "2023-11-22T06:58:00.000Z",
        "voteCount": 3,
        "content": "Correct is B\nAmazon DynamoDB: DynamoDB is a NoSQL database service provided by AWS that does not require fixed schemas"
      },
      {
        "date": "2023-11-21T23:13:00.000Z",
        "voteCount": 1,
        "content": "B is right option. D is not correct because Firehose can not write to Keyspaces."
      },
      {
        "date": "2023-11-21T12:25:00.000Z",
        "voteCount": 1,
        "content": "Correct is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 324,
    "url": "https://www.examtopics.com/discussions/amazon/view/126754-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T09:44:00.000Z",
        "voteCount": 17,
        "content": "The database must be able to scale based on demand, so Provisioned IOPS volume is out because they will be throttled. A and D are out.\nEC2 hosted in a private subnet without an internet connection, have to use VPC Endpoint, for DynamoDB, it must be Gateway VPC endpoint.\nB is the answer."
      },
      {
        "date": "2023-11-21T23:19:00.000Z",
        "voteCount": 6,
        "content": "D is right option. Instance endpoint is for connecting specific instance (primary or replica) and not recommended."
      },
      {
        "date": "2024-01-14T01:04:00.000Z",
        "voteCount": 2,
        "content": "This time you are wrong. A and D option use provisioned IOPS  which is not scalable.\nBetween B and C. DynamoDB only works with gateway endpoint. Answer is B."
      },
      {
        "date": "2024-06-22T18:13:00.000Z",
        "voteCount": 1,
        "content": "It does not say you need automated scaling. You can manually scale DynanoDB with provisioned IOPS."
      },
      {
        "date": "2024-10-10T15:31:00.000Z",
        "voteCount": 1,
        "content": "The key here is Can you use Amazon Dynamodb to replace a MongoDB used as a key-value database and the answer is YES!\n\nAmazon DynamoDB supports interface VPC endpoints (AWS PrivateLink). This allows you to securely connect to DynamoDB from your VPC without the need for an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection."
      },
      {
        "date": "2024-09-24T16:39:00.000Z",
        "voteCount": 1,
        "content": "D. This is the correct answer. Amazon DocumentDB (with MongoDB compatibility) meets the requirements:\n\n1. It can be hosted in a private subnet without an internet connection, as required by the technical guidelines.\n2. Connectivity between the application and the database can be encrypted, as stated in the requirements.\n3. Amazon DocumentDB can scale based on demand, which is another requirement mentioned in the question.\n4. The use of the cluster endpoint to connect to Amazon DocumentDB is the appropriate approach, as it provides a single, highly available endpoint for the database cluster.\n\nTherefore, option D is the solution that best meets the given requirements."
      },
      {
        "date": "2024-09-10T10:51:00.000Z",
        "voteCount": 3,
        "content": "DynamoDB isn't compatible withMongo"
      },
      {
        "date": "2024-08-11T06:01:00.000Z",
        "voteCount": 1,
        "content": "EC2 has no such concept called `cluster endpoint`. has to be B"
      },
      {
        "date": "2024-06-01T15:28:00.000Z",
        "voteCount": 1,
        "content": "Correct ans: D"
      },
      {
        "date": "2024-05-25T10:13:00.000Z",
        "voteCount": 2,
        "content": "D :Compatibility: Amazon DocumentDB, which is compatible with MongoDB, is an ideal choice. This ensures that the application can be migrated with minimal changes.\nScalability:  can automatically scale the storage and supports read scaling by adding more replicas. This meets the requirement for the database to scale based on demand.\nEncryption: DocumentDB supports encryption at rest and in transit, ensuring that all data connectivity is encrypted as per the company's guidelines.\nPrivate Connectivity: Amazon DocumentDB can be accessed within a VPC using a cluster endpoint, and it does not require internet connectivity, making it suitable for private subnet deployments.\n\nOption B: DynamoDB is a managed NoSQL database service that could meet the key-value requirement and scalability. However, it is not MongoDB-compatible, which means significant changes to the application code might be required"
      },
      {
        "date": "2024-07-16T23:40:00.000Z",
        "voteCount": 1,
        "content": "You don't need MongoDB compatibility as it is used as key-value, not as a document db"
      },
      {
        "date": "2024-05-25T10:11:00.000Z",
        "voteCount": 1,
        "content": "D. Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB."
      },
      {
        "date": "2024-04-08T04:28:00.000Z",
        "voteCount": 2,
        "content": "I guess the key par here is key-value . That kind of confirms that we can use DynamoDB here and hence B looks more promisin now.\n\nD seems good but Provisioned IOPS is a red flag regarding scaling"
      },
      {
        "date": "2024-03-29T10:48:00.000Z",
        "voteCount": 1,
        "content": "DocumentDB is not DynamoDB.\nGateway Endpoint does not support DocumentDB."
      },
      {
        "date": "2024-04-01T08:58:00.000Z",
        "voteCount": 1,
        "content": "My bad, B is using DynamoDB, so it is B"
      },
      {
        "date": "2024-03-20T07:40:00.000Z",
        "voteCount": 2,
        "content": "B: If MongoDB is used as a key-value store, then a gateway endpoint is the way to connect to DynamoDB, which is a straight-up key-value store."
      },
      {
        "date": "2024-02-17T15:12:00.000Z",
        "voteCount": 3,
        "content": "B b/c needs to scale based on demand and Gateway VPC endpoint with DynamoDB goes together like peanut butter and jelly"
      },
      {
        "date": "2024-02-03T01:54:00.000Z",
        "voteCount": 3,
        "content": "D is the right option.\n- It's legacy application, so re-factoring to dynamodb hardly possible.\n- D is scalable and compatible, cluster endpoint is right choise.\n-  Provisioned IOPS volumes are for he application, not for database, so database is still scalable."
      },
      {
        "date": "2024-02-04T00:32:00.000Z",
        "voteCount": 1,
        "content": "How are IOPS volumes not for the database? The sentence is: \"Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes\", which means that the DB is provisioned for the application, but it's still DB with IOPS volumes."
      },
      {
        "date": "2024-01-12T12:32:00.000Z",
        "voteCount": 5,
        "content": "Answer: D\nB and C are ruled out since they are using DynamoDB which is a NoSQL database service, and may not be a direct replacement for MongoDB if the application specifically requires MongoDB compatibility when you have Document DB.\nSo the answer should be either A or D. Why D? because Amazon DocumentDB provides a cluster endpoint that can be used for connecting to the cluster. This endpoint is accessible from within your Virtual Private Cloud (VPC) but doesn't require internet access. It aligns with the guideline of hosting instances in private subnets."
      },
      {
        "date": "2024-01-08T02:18:00.000Z",
        "voteCount": 2,
        "content": "Sorry. Answer is B. Gateway endpoint use private internet."
      },
      {
        "date": "2024-01-04T19:27:00.000Z",
        "voteCount": 2,
        "content": "C. Because gateway endpoint use public internet."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 325,
    "url": "https://www.examtopics.com/discussions/amazon/view/126816-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company\u2019s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.<br><br>A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility).<br><br>Which strategy should the solutions architect choose to perform this migration?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T10:05:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-08T04:27:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html"
      },
      {
        "date": "2023-11-28T07:33:00.000Z",
        "voteCount": 2,
        "content": "B is straightforward. Use DMS to migrate to a Mongo DB Compatible Document DB instance on AWS. Correct!"
      },
      {
        "date": "2023-11-25T03:46:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-11-22T07:27:00.000Z",
        "voteCount": 2,
        "content": "Correct is B"
      },
      {
        "date": "2023-11-21T23:23:00.000Z",
        "voteCount": 3,
        "content": "B is right option."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 326,
    "url": "https://www.examtopics.com/discussions/amazon/view/126818-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is rearchitecting its applications to run on AWS. The company\u2019s infrastructure includes multiple Amazon EC2 instances. The company's development team needs different levels of access. The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. The company also wants to implement enhanced security processes such as multi-factor authentication (MFA). The company wants to use managed AWS services wherever possible.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-04T06:42:00.000Z",
        "voteCount": 9,
        "content": "B is correct. The question mention \"Windows EC2\", no \"Windows user desktops\". Maybe the Windows EC2 can be Windows Servers."
      },
      {
        "date": "2023-12-19T18:35:00.000Z",
        "voteCount": 9,
        "content": "I support B as well per this link where EC2 is recommended:\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/directory_administration.html"
      },
      {
        "date": "2024-07-26T19:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct, it is application infrastructure, not for desktop."
      },
      {
        "date": "2024-07-14T08:12:00.000Z",
        "voteCount": 1,
        "content": "I will go for A based on this \"RADIUS MFA is applicable only to authenticate access to the AWS Management Console, or to Amazon Enterprise applications and services such as WorkSpaces, Amazon QuickSight, or Amazon Chime. It does not provide MFA to Windows workloads running on EC2 instances, or for signing into an EC2 instance\" https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_mfa.html"
      },
      {
        "date": "2024-06-12T09:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-06-11T18:58:00.000Z",
        "voteCount": 3,
        "content": "Technically, you can use AWS Workspace for domain security configuration tasks. So A is correct"
      },
      {
        "date": "2024-06-01T15:34:00.000Z",
        "voteCount": 1,
        "content": "A is right answer"
      },
      {
        "date": "2024-05-25T12:00:00.000Z",
        "voteCount": 1,
        "content": "A is right answer as the Amazon WorkSpaces provides a managed desktop-as-a-service solution that allows you to access a Windows desktop environment in the AWS Cloud"
      },
      {
        "date": "2024-05-25T10:18:00.000Z",
        "voteCount": 1,
        "content": "A. Amazon WorkSpaces is more secure and managed,"
      },
      {
        "date": "2024-05-04T16:39:00.000Z",
        "voteCount": 1,
        "content": "You can managed AD Admin tasks from Workspace. The requirement is to use AWS Managed Services where possible. So answer is A - nothing you can manage AD wise on EC2 that you can't do on the Windows Workspace"
      },
      {
        "date": "2024-04-15T01:11:00.000Z",
        "voteCount": 1,
        "content": "A - correct."
      },
      {
        "date": "2024-03-22T06:26:00.000Z",
        "voteCount": 5,
        "content": "Option A -  Three requirements,  1. join AD domain, 2. enable MFA, 3. Use AWS managed service.  Nothing about cost or any additional requirements.  Option A checks all the boxes from the article information -  https://aws.amazon.com/blogs/security/how-to-enable-multi-factor-authentication-for-amazon-workspaces-and-amazon-quicksight-by-using-microsoft-ad-and-on-premises-credentials/"
      },
      {
        "date": "2024-03-09T10:02:00.000Z",
        "voteCount": 1,
        "content": "Because managed services."
      },
      {
        "date": "2024-02-17T15:18:00.000Z",
        "voteCount": 3,
        "content": "I would choose A over B because of the last requirement: \u201cThe company wants to use managed AWS services wherever possible.\u201d"
      },
      {
        "date": "2024-02-09T15:39:00.000Z",
        "voteCount": 3,
        "content": "\"The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS\". Workspaces are automatically domain joined. EC2 aren't going to be automatically domain joined without some extra steps. I feel like that's what they're getting at here..."
      },
      {
        "date": "2024-02-04T00:42:00.000Z",
        "voteCount": 3,
        "content": "A seems better, as it uses managed Workspaces, which we can apply different security controls to despite what some people here say https://docs.aws.amazon.com/whitepapers/latest/best-practices-deploying-amazon-workspaces/security.html"
      },
      {
        "date": "2024-02-04T00:51:00.000Z",
        "voteCount": 2,
        "content": "Additionally, you can apply Group Policies to Windows Workspaces, which is a domain security task, though there are some limitations https://docs.aws.amazon.com/workspaces/latest/adminguide/group_policy.html"
      },
      {
        "date": "2024-01-09T10:21:00.000Z",
        "voteCount": 1,
        "content": "Option B. Workspace Windows servers can not be used for Domain Security tasks."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 327,
    "url": "https://www.examtopics.com/discussions/amazon/view/126755-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its on-premises application to AWS. The database for the application stores structured product data and temporary user session data. The company needs to decouple the product data from the user session data. The company also needs to implement replication in another AWS Region for disaster recovery.<br><br>Which solution will meet these requirements with the HIGHEST performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS DB instance with separate schemas to host the product data and the user session data. Configure a read replica for the DB instance in another Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for Memcached to host the user session data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Amazon DynamoDB global tables. Use one global table to host the product data. Use the other global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create an Amazon DynamoDB global table to host the user session data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-30T09:52:00.000Z",
        "voteCount": 15,
        "content": "C - DynamoDB is for structured, semi-structured and unstructured data. So it can also hold the product data. Indeed many e-commerce shops use DynamoDB  to save the product catalogue. There is nothing in the questin that would exclude DynamoDB for the product data. C has caching with DAX so it definitely has a higher performance than D which does not have caching and even no read replica in the same region."
      },
      {
        "date": "2024-05-24T14:48:00.000Z",
        "voteCount": 1,
        "content": "\"C\" should be wrong. \"B\" should be correct.\n\n\"A traditional relational database management system (RDBMS) stores data in a normalized relational structure.\n[...]\nAs a NON-relational database service, DynamoDB offers many advantages over traditional relational database management systems.\"\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-relational-modeling.html"
      },
      {
        "date": "2024-08-16T18:53:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-02-12T12:54:00.000Z",
        "voteCount": 5,
        "content": "Structured Data = RDS"
      },
      {
        "date": "2024-07-31T08:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is D, Elasticache Global data Store only supports Redis"
      },
      {
        "date": "2024-07-14T11:57:00.000Z",
        "voteCount": 1,
        "content": "I think D. ElastiCache for Memcached and DAX are good for read-heavy so they can improve performance, but not a good  choice for read and write, and also Memcached \u2013 no replication"
      },
      {
        "date": "2024-06-11T19:08:00.000Z",
        "voteCount": 1,
        "content": "Querying structured data on DynamoDB does not provide as good performance as RDS"
      },
      {
        "date": "2024-05-25T12:10:00.000Z",
        "voteCount": 1,
        "content": "C - To meet Disaster Recovery requirements &amp; get high performance with DAX"
      },
      {
        "date": "2024-05-25T10:21:00.000Z",
        "voteCount": 1,
        "content": "D. Structure Data so Amazon RDS and Amazon DynamoDB is well-suited for handling session data and can provide low-latency access.\nNot C because Dynamo is not for structured data"
      },
      {
        "date": "2024-05-02T23:10:00.000Z",
        "voteCount": 1,
        "content": "I did not like \"Create 2 DynamoDB global tables\", but as soon as the question is asking for the HIGHEST performance it's poining to DAX. C is the correct answer."
      },
      {
        "date": "2024-04-15T01:29:00.000Z",
        "voteCount": 1,
        "content": "Ans B.\n\"ElastiCache and RDS provide applications a combination of the top-tier speed of an in-memory cache with the reliability of a relational database. When provisioning an Aurora or RDS database using the RDS console, you now have the option to create and attach an associated ElastiCache cluster at the same time.\"\n\nhttps://aws.amazon.com/about-aws/whats-new/2023/04/amazon-elasticache-cache-rds-databases-console/"
      },
      {
        "date": "2024-04-07T07:25:00.000Z",
        "voteCount": 1,
        "content": "B - In general, SQL databases are better suited for traditional, structured data, while NoSQL databases are better suited for handling large volumes of unstructured or semi-structured data."
      },
      {
        "date": "2024-04-23T11:00:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-03-15T22:32:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-02-23T10:26:00.000Z",
        "voteCount": 4,
        "content": "The database for the application stores structured product data and temporary user session data, therefore. option D"
      },
      {
        "date": "2024-02-03T02:25:00.000Z",
        "voteCount": 2,
        "content": "The HIGHEST performance is C. Structured data does not means \"SQL\". Dynamodb can handle structured data with no issues."
      },
      {
        "date": "2024-01-19T17:43:00.000Z",
        "voteCount": 2,
        "content": "B talks about Global Datastore for memcached while memcached doesnt support global datastore, hence B is ruled out and D is the answer."
      },
      {
        "date": "2024-01-15T00:18:00.000Z",
        "voteCount": 1,
        "content": "C - DynamoDB with DAX is for structured data and the highest performance."
      },
      {
        "date": "2024-01-12T12:09:00.000Z",
        "voteCount": 2,
        "content": "Answer: D\nB also works but you will endup doing lot of custom stuff to deploy multiple ElastiCache for Memcached clusters in different Availability Zones. Pay attention to the question which says \"The company also needs to implement replication in another AWS Region for disaster recovery\""
      },
      {
        "date": "2024-01-09T10:33:00.000Z",
        "voteCount": 3,
        "content": "Choice is between B and D. Application information about how it is deployed in second region is missing for this question. B does not address cross region replication requirement for user sessions and assuming that is needed, option D is the right answer."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 328,
    "url": "https://www.examtopics.com/discussions/amazon/view/126923-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company orchestrates a multi-account structure on AWS by using AWS Control Tower. The company is using AWS Organizations, AWS Config, and AWS Trusted Advisor. The company has a specific OU for development accounts that developers use to experiment on AWS. The company has hundreds of developers, and each developer has an individual development account.<br><br>The company wants to optimize costs in these development accounts. Amazon EC2 instances and Amazon RDS instances in these accounts must be burstable. The company wants to disallow the use of other services that are not relevant.<br><br>What should a solutions architect recommend to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom SCP in AWS Organizations to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the SCP to the development OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom detective control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom preventive control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule in the AWS Control Tower account. Configure the AWS Config rule to allow the deployment of only burstable instances and to disallow services that are not relevant. Deploy the AWS Config rule to the development OU by using AWS CloudFormation StackSets."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-08T07:03:00.000Z",
        "voteCount": 7,
        "content": "I don't think it's appropriate to make SCP changes from Organization to an OU managed by Control Tower, as it will cause drift.\nThe recommended method is to set it as Preventive.\n\nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html\nhttps://docs.aws.amazon.com/controltower/latest/userguide/governance-drift.html"
      },
      {
        "date": "2024-06-22T17:50:00.000Z",
        "voteCount": 5,
        "content": "Cannot be A. SCP will create drift and SCPs are used for denying any specific action, not allow as stated in option A."
      },
      {
        "date": "2024-10-09T16:03:00.000Z",
        "voteCount": 2,
        "content": "Preventive guardrails deployed by AWS Control Tower are implemented via service control policies (SCPs). \nhttps://docs.aws.amazon.com/wellarchitected/latest/management-and-governance-guide/controls.html"
      },
      {
        "date": "2024-10-01T01:04:00.000Z",
        "voteCount": 2,
        "content": "I go with C, because of https://docs.aws.amazon.com/controltower/latest/userguide/governance-drift.html#drift-scp-attached-ou"
      },
      {
        "date": "2024-07-17T00:05:00.000Z",
        "voteCount": 1,
        "content": "SCP and \"allow\" are always incompatible"
      },
      {
        "date": "2024-05-25T10:26:00.000Z",
        "voteCount": 1,
        "content": "A -because SCPs are a more straightforward and integrated solution within AWS Organizations for this purpose than preventive controls in Control Tower"
      },
      {
        "date": "2024-08-16T18:56:00.000Z",
        "voteCount": 1,
        "content": "just c"
      },
      {
        "date": "2024-04-27T18:59:00.000Z",
        "voteCount": 4,
        "content": "Applying the custom SCP to the development OU will enforce the restrictions on all the accounts within that OU, effectively limiting the developers to using only the allowed resources and services.\n\nAWS Control Tower guardrails (options B and C) are not the ideal solution in this case because they are primarily used for governance and compliance purposes, rather than granular service-level restrictions."
      },
      {
        "date": "2024-04-15T02:22:00.000Z",
        "voteCount": 1,
        "content": "C - correct."
      },
      {
        "date": "2024-03-22T07:33:00.000Z",
        "voteCount": 2,
        "content": "Option A -  The preventive controls are implemented using Service Control Policies (SCPs), which are part of AWS Organizations\n\nRead \" Implementation of control behavior\" section  \nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html"
      },
      {
        "date": "2024-03-15T22:18:00.000Z",
        "voteCount": 2,
        "content": "Anwer is A.  \"Custom SCP\"\nDrift is caused if you edit the existing SCP. \n\nDon't use AWS Organizations to update service control policies (SCPs) attached to an OU that is registered with AWS Control Tower. Doing so could result in the controls entering an unknown state, which will require you to repair your landing zone or re-register your OU in AWS Control Tower. Instead, you can create new SCPs and attach those to the OUs rather than editing the SCPs that AWS Control Tower has created.\nhttps://docs.aws.amazon.com/controltower/latest/userguide/orgs-guidance.html"
      },
      {
        "date": "2024-03-09T10:11:00.000Z",
        "voteCount": 2,
        "content": "Custom preventive guardrails in CT can't do this. The correct answer is A."
      },
      {
        "date": "2024-02-16T12:48:00.000Z",
        "voteCount": 3,
        "content": "Answer : C\nbecause A said the SCP will apply to \" AWS Organizations\" not the OU."
      },
      {
        "date": "2024-02-03T02:35:00.000Z",
        "voteCount": 2,
        "content": "AWS Control Tower offers an abstracted, automated, and prescriptive experience on top of AWS Organizations. It automatically sets up AWS Organizations as the underlying AWS service to organize accounts and implement preventive controls using service control policies (SCPs). \nUsing AWS Organizations, you can further create and attach custom SCPs that centrally control the use of AWS services and resources across multiple AWS accounts.\n\nhttps://aws.amazon.com/controltower/faqs/"
      },
      {
        "date": "2024-02-17T07:55:00.000Z",
        "voteCount": 1,
        "content": "A is right: https://docs.aws.amazon.com/controltower/latest/userguide/orgs-guidance.html\nDon't use AWS Organizations to update service control policies (SCPs) attached to an OU that is registered with AWS Control Tower. Doing so could result in the controls entering an unknown state, which will require you to repair your landing zone or re-register your OU in AWS Control Tower. Instead, you can create new SCPs and attach those to the OUs rather than editing the SCPs that AWS Control Tower has created."
      },
      {
        "date": "2024-01-28T20:09:00.000Z",
        "voteCount": 3,
        "content": "Answer C:\nI know its usually safe to choose the SCP answer, but according to the docs that would create drift with Control Tower and need to be remediated.\n\nhttps://docs.aws.amazon.com/controltower/latest/userguide/drift.html#scp-invariance-scans"
      },
      {
        "date": "2024-01-12T11:49:00.000Z",
        "voteCount": 4,
        "content": "Answer C: \nAWS Control tower already using and preventive control (guardrail) is the key"
      },
      {
        "date": "2024-01-09T10:39:00.000Z",
        "voteCount": 4,
        "content": "C is the best option. A is possible but given that customer is  using Control Tower it option A will cause a drift in landing zone."
      },
      {
        "date": "2023-12-23T08:03:00.000Z",
        "voteCount": 1,
        "content": "a ans \nHere's why this solution is optimal and why the other options are not as suitable:\n\n1. Enforcement:\n\nSCPs (Service Control Policies) are the most effective way to centrally enforce service and instance restrictions across multiple accounts within an OU.\nDetective controls (guardrails) in Control Tower only detect and report violations, not prevent them.\nAWS Config rules are for configuration compliance, not access control.\n2. Granular Control:\n\nSCPs allow fine-grained control over specific services and instance types, enabling the specific allowance of burstable instances and restriction of other services.\n3. Ease of Management:\n\nSCPs are managed centrally within AWS Organizations, making it efficient to apply and update policies across multiple accounts.\n4. Alignment with Control Tower:\n\nSCPs integrate seamlessly with AWS Control Tower, ensuring consistent governance within the multi-account environment."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 329,
    "url": "https://www.examtopics.com/discussions/amazon/view/126757-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours.<br><br>The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version, the company deletes the CloudFormation stack of the old version.<br><br>The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architect needs to resolve this issue without making major changes to the application's architecture.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket's resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudF ormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket\u2019s resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T23:02:00.000Z",
        "voteCount": 16,
        "content": "It should be A,  becase with DeletionPolicy you can only keep or delete bucket, but bucket can't be deleted if it is not empty. So better way in that case - to create a lambda function as a custom resource, that will clean bucket before deletion.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\nhttps://awstut.com/en/2022/05/08/create-and-delete-s3-object-by-cfn-custom-resource-en/"
      },
      {
        "date": "2024-02-17T15:29:00.000Z",
        "voteCount": 5,
        "content": "A because anyone who goes by the name of HunkyBunky must know what they are talking about"
      },
      {
        "date": "2024-03-13T03:40:00.000Z",
        "voteCount": 1,
        "content": "100% HunkyBunky explanation. You cannot just delete a non-empty bucket"
      },
      {
        "date": "2024-01-09T10:53:00.000Z",
        "voteCount": 2,
        "content": "Option A. Option C can be a good option but application itself deletes the objects after 24 hours so it will affect and will requires changes to application that is clearly stated in question as No."
      },
      {
        "date": "2023-12-04T22:08:00.000Z",
        "voteCount": 1,
        "content": "Answer: A, I agree with @HunkyBunky's reasoning"
      },
      {
        "date": "2023-11-28T08:03:00.000Z",
        "voteCount": 3,
        "content": "Answer is A. S3 buckets can't be deleted if they are not empty. Create a Lambda function to empty the bucket so bucket can be deleted."
      },
      {
        "date": "2023-11-25T05:14:00.000Z",
        "voteCount": 2,
        "content": "Same as HunkyBunky comment"
      },
      {
        "date": "2023-11-21T13:09:00.000Z",
        "voteCount": 2,
        "content": "For sure D"
      },
      {
        "date": "2023-11-25T10:07:00.000Z",
        "voteCount": 1,
        "content": "Change to A. Its better option than D"
      },
      {
        "date": "2024-02-20T04:11:00.000Z",
        "voteCount": 3,
        "content": "D is not an option at all it will keep the S3 regardless empty or not and only delete the stack"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 330,
    "url": "https://www.examtopics.com/discussions/amazon/view/126758-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic.<br><br>The load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game\u2019s varying load and provide low-latency data access. The API model should not be changed.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-04T07:08:00.000Z",
        "voteCount": 8,
        "content": "C is correct. For Elastic Architecture the best option is API GW + Lambda + DynamoDB"
      },
      {
        "date": "2024-08-16T02:33:00.000Z",
        "voteCount": 1,
        "content": "Respuesta c"
      },
      {
        "date": "2024-01-09T11:24:00.000Z",
        "voteCount": 2,
        "content": "option C"
      },
      {
        "date": "2023-11-28T08:11:00.000Z",
        "voteCount": 2,
        "content": "C is correct. API Gateway, Lambda &amp; DynamoDB for session data"
      },
      {
        "date": "2023-11-25T09:42:00.000Z",
        "voteCount": 3,
        "content": "C is the right Answer: APIGW is the ideal choice for exposing the REST API because it can handle varying loads efficiently and scale automatically. API Gateway also integrates seamlessly with AWS Lambda, which is used for the business logic in this solution. This setup allows for easy management and can handle peaks in traffic without manual intervention."
      },
      {
        "date": "2023-11-22T07:52:00.000Z",
        "voteCount": 2,
        "content": "C is answer"
      },
      {
        "date": "2023-11-21T13:14:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 331,
    "url": "https://www.examtopics.com/discussions/amazon/view/126759-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system.<br><br>The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system.<br><br>What is the MOST operationally efficient way to replicate the images?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using a public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-10T00:12:00.000Z",
        "voteCount": 2,
        "content": "see on-premises as source and AWS EFS as target"
      },
      {
        "date": "2024-03-21T00:06:00.000Z",
        "voteCount": 1,
        "content": "B: no efs connection"
      },
      {
        "date": "2024-08-16T19:01:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2024-01-11T12:20:00.000Z",
        "voteCount": 4,
        "content": "Answer: D\nOption C is also correct but why you need s3 when DataSync moves data directly from the on-premises NFS to EFS, eliminating intermediate storage and transfer steps, reducing latency and potential bottlenecks."
      },
      {
        "date": "2024-01-09T11:37:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-12-13T06:14:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/datasync/latest/userguide/datasync-in-vpc.html"
      },
      {
        "date": "2023-11-28T18:10:00.000Z",
        "voteCount": 1,
        "content": "Everybody sure Answer is D?? So: \nAmazon Elastic File System (Amazon EFS) does not offer AWS PrivateLink support directly."
      },
      {
        "date": "2023-11-29T05:14:00.000Z",
        "voteCount": 1,
        "content": "why not ? See https://docs.aws.amazon.com/efs/latest/ug/efs-vpc-endpoints.html\nSeems to be supported"
      },
      {
        "date": "2023-12-21T21:12:00.000Z",
        "voteCount": 1,
        "content": "but that is not for EFS APIs, not for data flow."
      },
      {
        "date": "2023-12-21T21:14:00.000Z",
        "voteCount": 1,
        "content": "Also, EFS is accessed over a mount point which can be either an IP or DNS name. Both in private network. so, DX connection is good enough for it. PrivateLink in the answer is meaningless in this case"
      },
      {
        "date": "2023-11-28T08:33:00.000Z",
        "voteCount": 1,
        "content": "Answer - D. Leveraging AWS PrivateLink with a private VIF ensures a private and secure connection between the on-premises environment and the Amazon EFS file system. This eliminates the need for public internet access."
      },
      {
        "date": "2023-11-25T05:39:00.000Z",
        "voteCount": 2,
        "content": "D is most likely"
      },
      {
        "date": "2023-11-21T13:25:00.000Z",
        "voteCount": 3,
        "content": "D for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 332,
    "url": "https://www.examtopics.com/discussions/amazon/view/126760-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company recently migrated a web application from an on-premises data center to the AWS Cloud. The web application infrastructure consists of an Amazon CloudFront distribution that routes to an Application Load Balancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recent security audit revealed that the web application is accessible by using both CloudFront and ALB endpoints. However, the company requires that the web application must be accessible only by using the CloudFront endpoint.<br><br>Which solution will meet this requirement with the LEAST amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new security group and attach it to the CloudFront distribution. Update the ALB security group ingress to allow access only from the CloudFront security group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a com.amazonaws.region.elasticloadbalancing VPC interface endpoint for Elastic Load Balancing. Update the ALB scheme from internet-facing to internal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtract CloudFront IPs from the AWS provided ip-ranges.json document. Update ALB security group ingress to allow access only from CloudFront IPs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-12T04:47:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2024-03-11T11:52:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/limit-access-to-your-origins-using-the-aws-managed-prefix-list-for-amazon-cloudfront/"
      },
      {
        "date": "2024-01-30T04:39:00.000Z",
        "voteCount": 1,
        "content": "B, but this is why security architects &gt; solution architects. Any cloudfront distribution, belonging to any account in any org will still have direct access the origin."
      },
      {
        "date": "2024-01-09T11:47:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-13T06:24:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/about-aws/whats-new/2022/02/amazon-cloudfront-managed-prefix-list/?nc1=h_ls"
      },
      {
        "date": "2023-11-28T08:38:00.000Z",
        "voteCount": 4,
        "content": "Allow ingress access to ALB SG only from CloudFront prefix list. Answer - B"
      },
      {
        "date": "2023-11-25T05:55:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-11-22T23:37:00.000Z",
        "voteCount": 4,
        "content": "Definitely - B, becase you can't assign securityGroup on Cloudfront. Also, security group can have only 60 rules, so you can't add ALL CloudFront IPs  into it, so prefix list"
      },
      {
        "date": "2023-11-21T13:30:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 333,
    "url": "https://www.examtopics.com/discussions/amazon/view/126927-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a community forum site using an Application Load Balancer (ALB) and a Docker application hosted in an Amazon ECS cluster. The site data is stored in Amazon RDS for MySQL and the container image is stored in ECR. The company needs to provide their customers with a disaster recovery SLA with an RTO of no more than 24 hours and RPO of no more than 8 hours.<br><br>Which of the following solutions is the MOST cost-effective way to meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to deploy identical ALB, EC2, ECS and RDS resources in two regions. Schedule RDS snapshots every 8 hours. Use RDS multi-region replication to update the secondary region's copy of the database. In the event of a failure, restore from the latest snapshot, and use an Amazon Route 53 DNS failover policy to automatically redirect customers to the ALB in the secondary region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the Docker image in ECR in two regions. Schedule RDS snapshots every 8 hours with snapshots copied to the secondary region. In the event of a failure, use AWS CloudFormation to deploy the ALB, EC2, ECS and RDS resources in the secondary region, restore from the latest snapshot, and update the DNS record to point to the ALB in the secondary region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to deploy identical ALB, EC2, ECS, and RDS resources in a secondary region. Schedule hourly RDS MySQL backups to Amazon S3 and use cross-region replication to replicate data to a bucket in the secondary region. In the event of a failure, import the latest Docker image to Amazon ECR in the secondary region, deploy to the EC2 instance, restore the latest MySQL backup, and update the DNS record to point to the ALB in the secondary region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a pilot light environment in a secondary region with an ALB and a minimal resource EC2 deployment for Docker in an AWS Auto Scaling group with a scaling policy to increase instance size and number of nodes. Create a cross-region read replica of the RDS data. In the event of a failure, promote the replica to primary, and update the DNS record to point to the ALB in the secondary region."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-26T05:00:00.000Z",
        "voteCount": 9,
        "content": "With an RTO of 24 hours, using the 'Cold' DR solution option B is the cheapest.  Option D is a partial on DR solution which I would think would be more expensive in the long run then the 2nd ECR container in another region."
      },
      {
        "date": "2023-11-28T08:42:00.000Z",
        "voteCount": 6,
        "content": "Since RTO is 24 hours, no need to have all resources provisioned already on site 2. Take RDS Snapshots every 8 hrs to satisfy RPO. Deploy the environment using CF incase of DR and restore RDS snapshots. Answer - B"
      },
      {
        "date": "2023-11-28T08:42:00.000Z",
        "voteCount": 1,
        "content": "Also update DNS records to point to DR region"
      },
      {
        "date": "2024-01-09T11:53:00.000Z",
        "voteCount": 1,
        "content": "Option B is lowest cost."
      },
      {
        "date": "2023-11-25T11:14:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2023-11-25T09:51:00.000Z",
        "voteCount": 4,
        "content": "B. ECR and RDS Snapshots in Two Regions: Storing Docker images in ECR in two regions and copying RDS snapshots to the secondary region is a good strategy. In case of failure, CloudFormation deploys necessary resources in the secondary region, and the DNS is updated. This option is more cost-effective than A, as it doesn't require maintaining a full duplicate environment or multi-region replication constantly."
      },
      {
        "date": "2023-11-25T08:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-11-25T06:06:00.000Z",
        "voteCount": 2,
        "content": "B is the most cost-effective"
      },
      {
        "date": "2023-11-22T08:26:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 334,
    "url": "https://www.examtopics.com/discussions/amazon/view/126762-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety of regulatory standards for different projects. The company needs a multi-account environment.<br><br>A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistent baseline of management and security, but it must allow flexibility for different compliance requirements within various AWS accounts. The solution also needs to integrate with the existing on-premises Active Directory Federation Services (AD FS) server.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Create a single SCP for least privilege access across all accounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with conformance packs for all accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Create SCPs for least privilege access. Create an OU structure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with aggregators and conformance packs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure an IAM identity provider for federation with the on-premises AD FS server."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-11T20:12:00.000Z",
        "voteCount": 1,
        "content": "LEAST operational overhead: Control Tower\nIntergate with existing AD:  IAM Identity Center"
      },
      {
        "date": "2024-05-13T04:49:00.000Z",
        "voteCount": 1,
        "content": "Very poorly worded question. One must CONFIGURE (external) identity provider... but what does it mean:\n(B) \"connect IAM IC to on-prem ADFS\" or\n(C) \"configure an IAM identity provider\"!?!?\nWe have to guess what's the author wanted to say :("
      },
      {
        "date": "2024-02-27T21:23:00.000Z",
        "voteCount": 2,
        "content": "Key point is \"Flexibility\" nd least operational overhead,So I'll go with Opt B"
      },
      {
        "date": "2024-02-17T15:47:00.000Z",
        "voteCount": 2,
        "content": "B. because: \n\n(1) \u201cLeast amount of operational overhead\u201drequirement is met with Control Tower. Control Tower automates the creation of a well-architected, multi-account environment using best-practice blueprints, and \n\n(2) IAM Identity Center is the recommended approach for workforce authentication and authorization"
      },
      {
        "date": "2024-01-11T11:59:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\nA. Manual setup: Requires more manual configuration and maintenance, increasing operational overhead.\nC. Central logging and Config setup: While valuable, these components add complexity and management overhead. Control Tower can automate their setup and management.\nD. IAM identity provider: Doesn't leverage Control Tower's automation and centralized management features, leading to more manual effort."
      },
      {
        "date": "2024-01-09T11:56:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-11T02:34:00.000Z",
        "voteCount": 2,
        "content": "B is better over D as it mentions OU."
      },
      {
        "date": "2023-11-25T06:11:00.000Z",
        "voteCount": 2,
        "content": "B over D, should add OU"
      },
      {
        "date": "2023-11-22T23:47:00.000Z",
        "voteCount": 2,
        "content": "B or C, but B - provides LEAST amount of operational overhead"
      },
      {
        "date": "2023-11-21T13:38:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 335,
    "url": "https://www.examtopics.com/discussions/amazon/view/126811-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An online magazine will launch its latest edition this month. This edition will be the first to be distributed globally. The magazine's dynamic website currently uses an Application Load Balancer in front of the web tier, a fleet of Amazon EC2 instances for web and application servers, and Amazon Aurora MySQL. Portions of the website include static content and almost all traffic is read-only.<br><br>The magazine is expecting a significant spike in internet traffic when the new edition is launched. Optimal performance is a top priority for the week following the launch.<br><br>Which combination of steps should a solutions architect take to reduce system response times for a global audience? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse logical cross-Region replication to replicate the Aurora MySQL database to a secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-Region replication mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the web and application tiers are each in Auto Scaling groups. Introduce an AWS Direct Connect connection. Deploy the web and application tiers in Regions across the world.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three of the application tiers \u2013 web, application, and database \u2013 are in private subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T09:24:00.000Z",
        "voteCount": 5,
        "content": "Aurora Global databases, S3 cross region replication with Route 53, Cloud Front.\nAnswer - D&amp;E"
      },
      {
        "date": "2023-11-28T09:24:00.000Z",
        "voteCount": 1,
        "content": "Also Auto Scaling for Web &amp; Appln tiers"
      },
      {
        "date": "2024-03-20T07:57:00.000Z",
        "voteCount": 1,
        "content": "D and E."
      },
      {
        "date": "2024-02-17T15:49:00.000Z",
        "voteCount": 2,
        "content": "D and E for sure"
      },
      {
        "date": "2024-01-09T12:00:00.000Z",
        "voteCount": 1,
        "content": "Option D and E"
      },
      {
        "date": "2023-11-25T06:23:00.000Z",
        "voteCount": 2,
        "content": "E for sure, D should be additional"
      },
      {
        "date": "2023-11-21T23:06:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 336,
    "url": "https://www.examtopics.com/discussions/amazon/view/126812-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An online gaming company needs to optimize the cost of its workloads on AWS. The company uses a dedicated account to host the production environment for its online gaming application and an analytics application.<br><br>Amazon EC2 instances host the gaming application and must always be available. The EC2 instances run all year. The analytics application uses data that is stored in Amazon S3. The analytics application can be interrupted and resumed without issue.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase an EC2 Instance Savings Plan for the online gaming application instances. Use On-Demand Instances for the analytics application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spot Instances for the online gaming application and the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse On-Demand Instances for the online gaming application. Use Spot Instances for the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T15:51:00.000Z",
        "voteCount": 2,
        "content": "B all the way"
      },
      {
        "date": "2024-01-09T12:03:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-11-29T20:29:00.000Z",
        "voteCount": 4,
        "content": "B is correct, Spot instances uses for the analytical application can be interrupted and resumed at any time."
      },
      {
        "date": "2023-11-28T09:23:00.000Z",
        "voteCount": 4,
        "content": "B... use spot instances for Analytics appln and Instance savings for Gaming"
      },
      {
        "date": "2023-11-25T06:24:00.000Z",
        "voteCount": 3,
        "content": "B no doubt"
      },
      {
        "date": "2023-11-21T23:07:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 337,
    "url": "https://www.examtopics.com/discussions/amazon/view/126764-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs applications in hundreds of production AWS accounts. The company uses AWS Organizations with all features enabled and has a centralized backup operation that uses AWS Backup.<br><br>The company is concerned about ransomware attacks. To address this concern, the company has created a new policy that all backups must be resilient to breaches of privileged-user credentials in any production account.<br><br>Which combination of steps will meet this new requirement? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement cross-account backup with AWS Backup vaults in designated non-production accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an SCP that restricts the modification of AWS Backup vaults.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement AWS Backup Vault Lock in compliance mode.<br>C. Implement least privilege access for the IAM service role that is assigned to AWS Backup.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "ACD",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T13:59:00.000Z",
        "voteCount": 8,
        "content": "ACE for sure\n\nA. Implement cross-account backup with AWS Backup vaults in designated non-production accounts. This will allow the company to securely copy their backups to other accounts that are part of their organization for operational or security reasons1.\nC. Implement AWS Backup Vault Lock in compliance mode. This will provide an additional layer of protection and immutability to the backup vaults, preventing any user (including the root user) or AWS from deleting or modifying the backups until the retention period is complete2.\nE. Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier. This will help the company to avoid accidental or malicious deletion of backups by enforcing a minimum retention period and moving the backups to a lower-cost storage tier2."
      },
      {
        "date": "2024-04-15T06:32:00.000Z",
        "voteCount": 1,
        "content": "A, C1, D you mean."
      },
      {
        "date": "2023-12-08T01:08:00.000Z",
        "voteCount": 4,
        "content": "ACD you mean?"
      },
      {
        "date": "2023-12-09T02:31:00.000Z",
        "voteCount": 8,
        "content": "The solution is A, B and C1.\nWe need to create a Cross Account Backup -&gt; Put it in a Backup Account -&gt; Control modification to the backup account with SCP.\n\nA. Implement cross-account backup with AWS Backup vaults in designated non-production accounts.\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/manage-cross-account.html\n\nB. Add an SCP that restricts the modification of AWS Backup vaults.\nhttps://aws.amazon.com/blogs/storage/managing-access-to-backups-using-service-control-policies-with-aws-backup/\n\nC1. Implement AWS Backup Vault Lock in compliance mode.\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html"
      },
      {
        "date": "2024-07-09T23:48:00.000Z",
        "voteCount": 1,
        "content": "A C(C1) D are correct in questions."
      },
      {
        "date": "2024-07-09T23:46:00.000Z",
        "voteCount": 1,
        "content": "ACD are correct, that is A, C1 and D in question."
      },
      {
        "date": "2024-06-26T00:24:00.000Z",
        "voteCount": 1,
        "content": "Should be BCD. https://aws.amazon.com/blogs/storage/managing-access-to-backups-using-service-control-policies-with-aws-backup/ \n\nCross-Account is not feasible. Hundreds of accounts."
      },
      {
        "date": "2024-06-11T20:30:00.000Z",
        "voteCount": 2,
        "content": "A, C1, D\nB is incorrect: concern of compromised credentials:  SCPs could potentially be modified by a user with sufficient privileges in the organization\u2019s master account.\nC2: good for ensuring backup availability but does not directly address resilience against breaches of privileged-user credentials.\nE: provide similar benefits to using AWS Backup Vault Lock but is more complex to manage. AWS Backup Vault Lock is specifically designed for backup resilience and is more straightforward to implement within AWS Backup's framework."
      },
      {
        "date": "2024-05-30T05:22:00.000Z",
        "voteCount": 2,
        "content": "A, B, C for me."
      },
      {
        "date": "2024-05-25T14:51:00.000Z",
        "voteCount": 1,
        "content": "ABC1 is the answer"
      },
      {
        "date": "2024-05-25T10:58:00.000Z",
        "voteCount": 1,
        "content": "A. Implement cross-account backup with AWS Backup vaults in designated non-production accounts.\nC. Implement AWS Backup Vault Lock in compliance mode.\nE. Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled."
      },
      {
        "date": "2024-05-03T03:19:00.000Z",
        "voteCount": 2,
        "content": "ABC For me"
      },
      {
        "date": "2024-03-06T07:36:00.000Z",
        "voteCount": 6,
        "content": "ABC is definitely the answer.\n\nD. Configuring backup frequency does not do anything to prevent breaches\nE. AWS backup does not currently support S3 as a storage location for backups. You can use AWS backup to make a backup of S3 buckets but cannot use it to store backups."
      },
      {
        "date": "2024-02-12T13:02:00.000Z",
        "voteCount": 3,
        "content": "ACD for sure"
      },
      {
        "date": "2024-02-04T01:40:00.000Z",
        "voteCount": 4,
        "content": "ABC seems more reasonable over D(E) - as others mentioned, configuring backup doesn't protect from compromised creds attack.\n\nModerator, please fix the answer letters order"
      },
      {
        "date": "2024-01-15T01:13:00.000Z",
        "voteCount": 4,
        "content": "ABC1 for sure"
      },
      {
        "date": "2024-01-11T11:40:00.000Z",
        "voteCount": 4,
        "content": "Answer : ACC ( ACD).. there is typo in question second  C should be D, D should be E, E should be F.. saying that the other options\nB. SCP restricting vault modification: Offers a good layer of protection, but doesn't directly address the concern of compromised credentials in production accounts.\nE. Cold Tier backups: Ensures backup accessibility in case of attacks, but doesn't specifically protect against compromised credentials.\nF. S3 Object Lock: Provides immutability within the non-production account, but if that account is breached, backups could still be compromised."
      },
      {
        "date": "2024-01-09T12:09:00.000Z",
        "voteCount": 3,
        "content": "A, C, D"
      },
      {
        "date": "2024-01-04T19:57:00.000Z",
        "voteCount": 4,
        "content": "ABC are obvious correct. The question is why the rest of the answers are wrong.\nC. Implement least privilege access for the IAM service role that is assigned to AWS Backup.\nThe question is looking for solution that survive privilege access breach. No matter how least privilege is granted, there must be other privilege users which can get more privileges.\n.\nD. Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier.\nLifecycle doesn't prevent the backups to be deleted\n.\nE. Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled.\nAWS backup doesn't support S3 as the storage."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 338,
    "url": "https://www.examtopics.com/discussions/amazon/view/126765-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central logging account. The collected logs must remain in the AWS Region of creation. The central logging account will then process the logs, normalize the logs into standard output format, and stream the output logs to a security tool for more processing.<br><br>A solutions architect must design a solution that can handle a large volume of logging data that needs to be ingested. Less logging will occur outside normal business hours than during normal business hours. The logging solution must scale with the anticipated load. The solutions architect has decided to use an AWS Control Tower design to handle the multi-account logging process.<br><br>Which combination of steps should the solutions architect take to meet the requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination Amazon Kinesis data stream in the central logging account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T09:33:00.000Z",
        "voteCount": 7,
        "content": "Cloud Watch logs  -&gt; Kinesis Data Streams -&gt; Lambda - &gt; Security Tool\nACE"
      },
      {
        "date": "2024-09-28T07:42:00.000Z",
        "voteCount": 1,
        "content": "BDE\nWhy would you write to the member account? makes no sense. \nWhy would you use kinesis it says nothing about real time. BD"
      },
      {
        "date": "2024-01-09T12:14:00.000Z",
        "voteCount": 1,
        "content": "A, C and E"
      },
      {
        "date": "2023-12-31T01:51:00.000Z",
        "voteCount": 3,
        "content": "A vs B: Kinesis data stream is a possible destination of CloudWatch Logs subscriptions, SQS isn't --&gt; A\nC vs. D: As we had to choose Kinesis only C makes sense.\nE vs. F: Difference is that E runs the Lambda function in the central logging account while F runs the Lambda function in the member accounts. So clearly E, as we have streamed the logs to the central accounts Kinesis, which easily can use Lambda for the final processing etc."
      },
      {
        "date": "2023-12-20T13:44:00.000Z",
        "voteCount": 1,
        "content": "ACE Kinesis for sure"
      },
      {
        "date": "2023-11-25T06:34:00.000Z",
        "voteCount": 4,
        "content": "I vote for ACE"
      },
      {
        "date": "2023-11-23T00:04:00.000Z",
        "voteCount": 4,
        "content": "Definitely - ACE"
      },
      {
        "date": "2023-11-21T14:03:00.000Z",
        "voteCount": 4,
        "content": "ACE for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 339,
    "url": "https://www.examtopics.com/discussions/amazon/view/126766-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL Server database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes.<br><br>The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services.<br><br>Which combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VM Import/Export to import the application server VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the VM images to an AWS Snowball Edge Storage Optimized device.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-06T15:34:00.000Z",
        "voteCount": 8,
        "content": "AD, RDS Database has max size of less than 500TB, cannot use RDS!"
      },
      {
        "date": "2024-03-02T09:53:00.000Z",
        "voteCount": 4,
        "content": "It does not actually say that the DB itself is 500TB, but that its the total size of storage for both VMs. I really do not like this question. The information provided leaves a lot of room for assumptions."
      },
      {
        "date": "2023-12-07T09:26:00.000Z",
        "voteCount": 1,
        "content": "Where did you get that? 16TB\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html#Concepts.Storage.GeneralSSD"
      },
      {
        "date": "2023-12-28T12:29:00.000Z",
        "voteCount": 1,
        "content": "on the page you mentioned\n\nVolume size\t\n100 GiB\u201364 TiB (16 TiB on RDS for SQL Server)\n\n20 GiB\u201364 TiB (16 TiB on RDS for SQL Server)\n\n20 GiB\u201364 TiB (16 TiB on RDS for SQL Server)"
      },
      {
        "date": "2023-11-28T08:46:00.000Z",
        "voteCount": 7,
        "content": "Should be DE \"LEAST amount of downtime\""
      },
      {
        "date": "2024-05-03T03:21:00.000Z",
        "voteCount": 1,
        "content": "DE for me"
      },
      {
        "date": "2024-04-15T08:18:00.000Z",
        "voteCount": 2,
        "content": "D and E - \"LEAST amount of downtime\"."
      },
      {
        "date": "2024-04-05T23:43:00.000Z",
        "voteCount": 3,
        "content": "3 limit here:\nRDS volume - 16TB\nDMS - 30TB\nEBS - 64TB\nnone of them matching the 500TB of size.\nso only possible here:\nwrite forgot the size limit but made the question only focus on comparision between DX and Snowball.\nOr, the 500TB size of file is no db or not a single file which can be split to different volumes. And in either case above, DE would be the answer that author is looking for, Simple as Do you know what DMS is."
      },
      {
        "date": "2024-03-25T06:31:00.000Z",
        "voteCount": 1,
        "content": "Option ACE.  You need to create a transit gateway,  set up at routing table for communication route rules and finally, create a transit gateway attachment to a VPN .\n\nOption E - https://docs.aws.amazon.com/vpc/latest/tgw/tgw-vpn-attachments.html\nOption A&amp;C -  https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-isolated.html"
      },
      {
        "date": "2024-03-10T19:57:00.000Z",
        "voteCount": 2,
        "content": "Option D &amp; E"
      },
      {
        "date": "2024-03-09T10:41:00.000Z",
        "voteCount": 1,
        "content": "There is no requirement to migrate to RDS, hence VMs only."
      },
      {
        "date": "2024-03-21T13:49:00.000Z",
        "voteCount": 2,
        "content": "Change of mind: DE."
      },
      {
        "date": "2024-02-27T09:57:00.000Z",
        "voteCount": 3,
        "content": "D, E SMS and DMS"
      },
      {
        "date": "2024-02-27T10:00:00.000Z",
        "voteCount": 1,
        "content": "Changing to AD\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Limits.html"
      },
      {
        "date": "2024-03-02T10:09:00.000Z",
        "voteCount": 1,
        "content": "That is temporary storage as a staging area until it replicates to the target server. You can replicate more than 30TB to a target VM using DMS.\n\n\"The 30,000-GB quota for storage applies to all your AWS DMS replication instances in a given AWS Region. This storage is used to cache changes if a target can't keep up with a source, and for storing log information.\""
      },
      {
        "date": "2024-02-17T15:57:00.000Z",
        "voteCount": 5,
        "content": "I vote DE"
      },
      {
        "date": "2024-02-09T16:29:00.000Z",
        "voteCount": 3,
        "content": "A is wrong. For least downtime you will migrate a Database with DMS. \nD &amp; E is the correct answer."
      },
      {
        "date": "2024-02-03T04:11:00.000Z",
        "voteCount": 1,
        "content": "https://www.amazonaws.cn/en/server-migration-service/faqs/\nQ: What is the difference between EC2 VM Import and Amazon Server Migration Service?\nAmazon Server Migration Service is a significant enhancement of EC2 VM Import. The Amazon Server Migration Service provides automated, live incremental server replication and Amazon Web Services Console support. For customers using EC2 VM Import for migration, we recommend using Amazon Server Migration Service."
      },
      {
        "date": "2024-01-10T16:17:00.000Z",
        "voteCount": 1,
        "content": "Answer: DE\nBoth AWS SMS and AWS DMS offer continuous replication, allowing the application and database to be kept in sync with their AWS counterparts during the migration process. This enables a switchover with minimal downtime."
      },
      {
        "date": "2024-01-09T12:21:00.000Z",
        "voteCount": 3,
        "content": "Migrate application using VM Export/Import \nAs DB is MS SQL running on VM, use DMS to Migrate to RDS."
      },
      {
        "date": "2024-01-04T08:46:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/compute/learn-about-hourly-replication-in-server-migration-service-and-the-ability-to-migrate-large-data-volumes/"
      },
      {
        "date": "2024-01-03T06:14:00.000Z",
        "voteCount": 2,
        "content": "The maximum storage size for SQL Server DB instances is the following:\n    General Purpose (SSD) storage \u2013 16 TiB for all editions\n    Provisioned IOPS storage \u2013 16 TiB for all editions\n    Magnetic storage \u2013 1 TiB for all editions\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.FeatureSupport.Limits"
      },
      {
        "date": "2023-12-28T04:24:00.000Z",
        "voteCount": 2,
        "content": "d,e ans https://aws.amazon.com/blogs/publicsector/how-migrate-on-premises-workloads-aws-application-migration-service/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 340,
    "url": "https://www.examtopics.com/discussions/amazon/view/126768-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs.<br><br>Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure attachments to all VPCs and VPNs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup transit gateway route tables. Associate the VPCs and VPNs with the route tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC peering between the VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure attachments between the VPCs and VPNs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup route tables on the VPCs and VPNs."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 21,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T19:55:00.000Z",
        "voteCount": 14,
        "content": "As transit gateway follows a hub and spoke model connecting all VPCs and VPNs to it makes more sense. Moreover, between VPCs and VPNs is invalid."
      },
      {
        "date": "2023-11-23T02:57:00.000Z",
        "voteCount": 10,
        "content": "I guess ACE. The company wants to control which  VPC will communicate with other VPC, that means that we don't need to setup attachment for all VPCs"
      },
      {
        "date": "2023-11-25T08:31:00.000Z",
        "voteCount": 3,
        "content": "Option E proposes configuring attachments between the VPCs and VPNs. This option is necessary to connect the VPCs and VPNs to the transit gateway."
      },
      {
        "date": "2024-10-09T18:59:00.000Z",
        "voteCount": 1,
        "content": "For those who think that, in relation to the requirement \"The company wants to control which VPCs can communicate with other VPCs\", option E would be correct, in fact this will be possible through letter C, therefore the answer is A, B, C."
      },
      {
        "date": "2024-07-23T08:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct instea of E because all VPCs and VPN attach to  Transit-GW"
      },
      {
        "date": "2024-07-05T22:54:00.000Z",
        "voteCount": 1,
        "content": "The question and opitons include (or lack) some typo errors.\n\nE should be \"Configure 'transit gateway' attachments between the VPCs and VPNs.\"\n\nThen, I think ABE is correct, not ABC.\n\nThe company wants to control \"which VPCs can communicate with other VPCs.\" It doesn't say \"all VPCs and VPNs.\"."
      },
      {
        "date": "2024-07-05T22:55:00.000Z",
        "voteCount": 1,
        "content": "Sorry I think ACE is correct, not ABC."
      },
      {
        "date": "2024-05-03T03:22:00.000Z",
        "voteCount": 3,
        "content": "ABC for me"
      },
      {
        "date": "2024-03-19T20:37:00.000Z",
        "voteCount": 3,
        "content": "We don't need \"all\""
      },
      {
        "date": "2024-03-06T07:48:00.000Z",
        "voteCount": 6,
        "content": "E. You don't configure attachments between VPCs and VPNs, you configure attachments to both VPCs and VPN from the transit gateway, thus B."
      },
      {
        "date": "2024-02-12T13:13:00.000Z",
        "voteCount": 1,
        "content": "It is ACE"
      },
      {
        "date": "2024-01-15T01:41:00.000Z",
        "voteCount": 4,
        "content": "I go ABC"
      },
      {
        "date": "2024-01-10T16:05:00.000Z",
        "voteCount": 1,
        "content": "My Answer \"ACE\" Why B is correct? The question asks \"The company wants to control which VPCs can communicate with other VPCs\" Saying that Option B is \"Involves attaching every single VPC and VPN within the organization directly to the Transit Gateway\" where as Option C focuses on \"establishing attachments only between the VPCs that need to communicate with each other and the VPN gateway\" \nCan one explain why B is correct?"
      },
      {
        "date": "2024-01-10T16:06:00.000Z",
        "voteCount": 1,
        "content": "Typo... I mean Option E\nOption E... focuses on \"establishing attachments only between the VPCs that need to communicate with each other and the VPN gateway\"\nCan anyone explain why B is correct?"
      },
      {
        "date": "2024-01-09T12:35:00.000Z",
        "voteCount": 6,
        "content": "Option A, B, C. Option E looks feasible instead of B but that is not a requirement as company only wants to control VPC to VPC communication."
      },
      {
        "date": "2023-12-09T03:13:00.000Z",
        "voteCount": 5,
        "content": "ABC - we need to read the answers as a combination of steps."
      },
      {
        "date": "2023-12-19T07:10:00.000Z",
        "voteCount": 1,
        "content": "One issue though that in order to control which VPC talks to which one, we need to setup route tables on each VPC (E) and not on the transit VPC (C) as that need to be light.  So I am thinking that the choice should be ABE and not ABC.  \n\nThe specific use case is not mentioned here but this link should give an idea of how route tables need to be configured. https://docs.aws.amazon.com/vpc/latest/tgw/TGW_Scenarios.html"
      },
      {
        "date": "2023-12-19T07:14:00.000Z",
        "voteCount": 1,
        "content": "This article suggests the use of NACL to control inter-vpc traffic but that option is not available in the question (although there is another question that brings it up)\n\nhttps://intuitive.cloud/blog/securing-multi-vpc-connectivity-with-aws-transit-gateway-#:~:text=Use%20security%20groups%20and%20NACLs,connected%20to%20the%20Transit%20Gateway."
      },
      {
        "date": "2023-12-05T20:35:00.000Z",
        "voteCount": 5,
        "content": "Answer - ABC"
      },
      {
        "date": "2023-11-28T09:59:00.000Z",
        "voteCount": 3,
        "content": "ACE. Option B mentions attaching 'all' VPCs, might not suggest control of what VPCs the company wants to include communcation"
      },
      {
        "date": "2023-12-05T20:34:00.000Z",
        "voteCount": 2,
        "content": "I stand corrected! Answer should be ABC.\nB- Configure attachments to all VPCs and VPNs. This is the TGW attachments to all VPCs and VPNs. \nE - Configure attachments between the VPCs and VPNs - WRONG!!"
      },
      {
        "date": "2023-11-26T10:44:00.000Z",
        "voteCount": 5,
        "content": "i'd go for abc as well."
      },
      {
        "date": "2023-11-25T06:40:00.000Z",
        "voteCount": 3,
        "content": "I guess ACE"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 341,
    "url": "https://www.examtopics.com/discussions/amazon/view/126769-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to optimize the cost of its application on AWS. The application uses AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) containers that run on AWS Fargate. The application is write-heavy and stores data in an Amazon Aurora MySQL database.<br><br>The load on the application is not consistent. The application experiences long periods of no usage, followed by sudden and significant increases and decreases in traffic. The database runs on a memory optimized DB instance that cannot handle the load.<br><br>A solutions architect must design a solution that can scale to handle the changes in traffic.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd additional read replicas to the database. Purchase Instance Savings Plans and RDS Reserved Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Aurora DB cluster that has multiple writer instances. Purchase Instance Savings Plans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Aurora global database. Purchase Compute Savings Plans and RDS Reserved instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Aurora Serverless v1. Purchase Compute Savings Plans.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T06:50:00.000Z",
        "voteCount": 5,
        "content": "Aurora Serverless v1 provides a relatively simple, cost-effective option for infrequent, intermittent, or unpredictable workloads"
      },
      {
        "date": "2023-12-11T03:12:00.000Z",
        "voteCount": 5,
        "content": "Aurora Serverless designed to be handling heavy and unpredictable load while Aurora global table  is more on low-lantency connection"
      },
      {
        "date": "2024-08-21T23:22:00.000Z",
        "voteCount": 1,
        "content": "I guess this is a very old question; as of today, Aurora Serverless v1 is going EOL by December, 2024. So, invalid question. But if we still had to make a decision with D, it will be B."
      },
      {
        "date": "2024-01-09T12:39:00.000Z",
        "voteCount": 2,
        "content": "Option D. This question looks incomplete as it does not give options for cost savings opportunity for application layer."
      },
      {
        "date": "2023-12-05T20:37:00.000Z",
        "voteCount": 2,
        "content": "Answer D\nAurora Serverless can scale better to handle heavy loads"
      },
      {
        "date": "2023-11-30T20:36:00.000Z",
        "voteCount": 2,
        "content": "Per scenario, the application is write intensive and the load varies due to burst. Aurora Serverless with compute saving plans is the correct answer"
      },
      {
        "date": "2023-11-21T14:12:00.000Z",
        "voteCount": 3,
        "content": "D for sure"
      },
      {
        "date": "2023-11-25T08:21:00.000Z",
        "voteCount": 1,
        "content": "Change to C. Its most cost effective"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 342,
    "url": "https://www.examtopics.com/discussions/amazon/view/126868-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instances behind an Application Load Balancer (ALB).<br>Application data is stored in a MySQL database that runs on an additional EC2 instance. The application's use of the database is read-heavy.<br><br>The application loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. The static content is updated frequently and must be copied to each EBS volume.<br><br>The load on the application changes throughout the day. During peak hours, the application cannot handle all the incoming requests. Trace data shows that the database cannot handle the read load during peak hours.<br><br>Which solution will improve the reliability of the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to a set of AWS Lambda functions. Set the Lambda functions as targets for the ALB. Create a new single EBS volume for the static content. Configure the Lambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to a set of AWS Step Functions state machines. Set the state machines as targets for the ALCreate an Amazon Elastic File System (Amazon EFS) file system for the static content. Configure the state machines to read from the EFS file system. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new single EBS volume for the static content. Mount the new EBS volume on the ECS cluster. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to each container. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T07:02:00.000Z",
        "voteCount": 5,
        "content": "Amazon Aurora MySQL Serverless v2 with a reader DB instance will provide heavy-read"
      },
      {
        "date": "2023-12-28T19:58:00.000Z",
        "voteCount": 3,
        "content": "The question is not clear on the nature of application and assumes that the application is Linux based, all option would be incorrect if this was a windows app. Given the information D is the best bet."
      },
      {
        "date": "2023-12-04T08:11:00.000Z",
        "voteCount": 2,
        "content": "Answer: D"
      },
      {
        "date": "2023-11-28T10:19:00.000Z",
        "voteCount": 4,
        "content": "D is good. Not sure if Static content on EBS will have an issue when DB is multi AZ as EBS cannot span AZs."
      },
      {
        "date": "2023-11-28T10:20:00.000Z",
        "voteCount": 1,
        "content": "\"Not sure if Static content on EBS will have an issue when DB is multi AZ as EBS cannot span AZs\" - This is for Option C"
      },
      {
        "date": "2024-08-18T18:47:00.000Z",
        "voteCount": 1,
        "content": "just D"
      },
      {
        "date": "2023-11-25T08:10:00.000Z",
        "voteCount": 4,
        "content": "D is better than C."
      },
      {
        "date": "2023-11-22T03:09:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2024-08-21T23:18:00.000Z",
        "voteCount": 1,
        "content": "Stop saying \"for sure\""
      },
      {
        "date": "2024-01-19T08:57:00.000Z",
        "voteCount": 10,
        "content": "You always say \"for sure\". Again you're wrong, that's for sure."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 343,
    "url": "https://www.examtopics.com/discussions/amazon/view/126998-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The solutions architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.<br><br>How can the solutions architect design the API Gateway access control and perform request inspections?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the API Gateway resource, set CORS to enabled and only return the company's domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T13:45:00.000Z",
        "voteCount": 5,
        "content": "Answer: A \nKeyword \"X-ray\" AWS X-Ray is used to trace and analyze user requests to API Gateway, providing an end-to-end view of each request and helping analyze latency. This meets the requirement for creating service maps and analyzing request latency."
      },
      {
        "date": "2024-05-13T19:36:00.000Z",
        "voteCount": 1,
        "content": "Why not C ?"
      },
      {
        "date": "2024-10-09T19:19:00.000Z",
        "voteCount": 1,
        "content": "Because a Lambda custom authorizer can validate tokens or credentials but is more complex than necessary when AWS_IAM authorization is already suitable.\nAWS_IAM can directly control access based on IAM roles and policies, making it simpler and more secure for restricting access.\nThe question specifies using IAM permissions for access control, making AWS_IAM a better fit, so the correct answer is A."
      },
      {
        "date": "2024-03-25T07:04:00.000Z",
        "voteCount": 1,
        "content": "Option A -  https://aws.amazon.com/blogs/aws/apigateway-xray/"
      },
      {
        "date": "2024-01-13T20:33:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-xray.html"
      },
      {
        "date": "2023-12-09T03:30:00.000Z",
        "voteCount": 2,
        "content": "A - See: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html#api-gateway-who-can-invoke-an-api-method-using-iam-policies"
      },
      {
        "date": "2023-12-04T09:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-03T05:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct,  use Ian and role for authentication and x-ray for tracing and analyzing"
      },
      {
        "date": "2023-11-25T07:03:00.000Z",
        "voteCount": 3,
        "content": "A - Use X-ray"
      },
      {
        "date": "2023-11-23T03:44:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 344,
    "url": "https://www.examtopics.com/discussions/amazon/view/126870-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts. As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.<br><br>How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to run in a non-production environment before approving the change for production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions and generate notifications on errors. Deploy to a test environment and run a manual test plan before approving the change for production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T16:10:00.000Z",
        "voteCount": 10,
        "content": "Use Code Build to run unit/automated testing. Code Deploy for blue/green deployments"
      },
      {
        "date": "2024-01-09T13:35:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\nKey word \" blue/green deployment\" and not D.. coz manual testing."
      },
      {
        "date": "2024-01-09T12:49:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-21T09:26:00.000Z",
        "voteCount": 1,
        "content": "Agree B is the best here"
      },
      {
        "date": "2023-11-24T06:16:00.000Z",
        "voteCount": 3,
        "content": "BBBBBdaswsfasdfasdfasdfasdf"
      },
      {
        "date": "2023-11-22T03:22:00.000Z",
        "voteCount": 4,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 345,
    "url": "https://www.examtopics.com/discussions/amazon/view/126813-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recovery capabilities in an active-passive configuration with the us-west-1 Region.<br><br>Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALDeploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T23:10:00.000Z",
        "voteCount": 1,
        "content": "ALB can't span VPCs in different regions, the key statement in B is \"Deploy the same solution in us-west-1\""
      },
      {
        "date": "2024-04-11T23:48:00.000Z",
        "voteCount": 2,
        "content": "B. Peering is not needed here as we dont need syncing of environments"
      },
      {
        "date": "2024-03-25T07:13:00.000Z",
        "voteCount": 2,
        "content": "Option B - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html"
      },
      {
        "date": "2024-01-09T13:30:00.000Z",
        "voteCount": 1,
        "content": "My Answer is also B but i feel D is also same as B.. Only difference is the words.\nB \" Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions\"\nD \"Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions\"\nCan someone clarify???"
      },
      {
        "date": "2024-03-12T12:22:00.000Z",
        "voteCount": 2,
        "content": "D missing keyword Failover"
      },
      {
        "date": "2024-01-09T12:54:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-21T09:37:00.000Z",
        "voteCount": 2,
        "content": "B is correct Route 53 failover policy. A and C wrong - ALB can't span VPC which are in different regions. ALB is region specific service"
      },
      {
        "date": "2023-11-28T16:14:00.000Z",
        "voteCount": 4,
        "content": "Answer B. ALB + Autoscaling of EC2 instances on both regions. Route53 with Failover routing policy."
      },
      {
        "date": "2023-11-25T07:11:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2023-11-22T03:28:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2023-11-21T23:19:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 346,
    "url": "https://www.examtopics.com/discussions/amazon/view/126814-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a legacy application that runs on multiple NET Framework components. The components share the same Microsoft SQL Server database and communicate with each other asynchronously by using Microsoft Message Queueing (MSMQ).<br><br>The company is starting a migration to containerized .NET Core components and wants to refactor the application to run on AWS. The .NET Core components require complex orchestration. The company must have full control over networking and host configuration. The application's database model is strongly relational.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the INET Core components on AWS App Runner. Host the database on Amazon RDS for SQL Server. Use Amazon EventBiridge for asynchronous messaging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the AWS Fargate launch type. Host the database on Amazon DynamoDUse Amazon Simple Notification Service (Amazon SNS) for asynchronous messaging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the .NET Core components on AWS Elastic Beanstalk. Host the database on Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) for asynchronous messaging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T12:53:00.000Z",
        "voteCount": 7,
        "content": "Option D seems to be the best fit:\n\nAmazon ECS with EC2 offers the needed control and orchestration capabilities.\nAmazon Aurora MySQL Serverless v2 can support the relational database model, though it requires adapting from Microsoft SQL Server to MySQL.\nAmazon SQS aligns well with the need for asynchronous messaging and can be a suitable replacement for MSMQ."
      },
      {
        "date": "2024-04-06T00:27:00.000Z",
        "voteCount": 2,
        "content": "A - eventbridge for replacing MSMQ\uff1f\nB - dynamodb is not relational\nC - Amazon Aurora PostgreSQL serverless v2 is not existing"
      },
      {
        "date": "2024-03-06T07:54:00.000Z",
        "voteCount": 2,
        "content": "SQS is perfect solution for queue solution replacement."
      },
      {
        "date": "2024-01-09T12:59:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-12-29T21:42:00.000Z",
        "voteCount": 2,
        "content": "D :- Containerization and Orchestration:\n\nAmazon ECS is a fully managed container orchestration service that can seamlessly manage containerized .NET Core components.\nThe EC2 launch type provides full control over the underlying EC2 instances, enabling customization of networking and host configuration as needed.\n2. Relational Database:\n\nAmazon RDS for SQL Server is a managed relational database service that natively supports SQL Server, aligning perfectly with the application's strongly relational database model.\n3. Asynchronous Messaging:\nD is ans\nAmazon SQS offers a reliable and scalable managed message queueing service that mirrors the functionality of MSMQ, ensuring smooth integration with the existing application architecture"
      },
      {
        "date": "2023-12-21T09:43:00.000Z",
        "voteCount": 1,
        "content": "A Moving from SQL Server to RDS is the easiest. RDS allows network control customisation https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-setup-sqlserver.html#custom-setup-sqlserver.iam-vpc also App Runner is good for .Net code https://docs.aws.amazon.com/apprunner/latest/dg/service-source-code-net6.html"
      },
      {
        "date": "2023-11-28T16:22:00.000Z",
        "voteCount": 3,
        "content": "Option D. Since the company wants control over host networking, EC2 is the best choice compared to Fargate or Beanstalk. Aurora MySQL is relational."
      },
      {
        "date": "2023-11-25T07:13:00.000Z",
        "voteCount": 4,
        "content": "D - DB is strongly relational"
      },
      {
        "date": "2023-11-22T13:24:00.000Z",
        "voteCount": 3,
        "content": "Because the DB is strongly relational"
      },
      {
        "date": "2023-11-22T03:33:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-11-25T02:19:00.000Z",
        "voteCount": 1,
        "content": "Yes; go with D"
      },
      {
        "date": "2023-11-21T23:21:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 347,
    "url": "https://www.examtopics.com/discussions/amazon/view/126815-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.<br><br>What should the solutions architect do to troubleshoot this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a spread placement group. Set a minimum of eight instances for each Availability Zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop and start all the instances in the placement group. Try the launch again.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new placement group. Merge the new placement group with the original placement group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the additional instances as Dedicated Hosts in the placement groups."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T20:12:00.000Z",
        "voteCount": 14,
        "content": "Should be B\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nIf you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error.\n\nIf you stop an instance in a placement group and then start it again, it still runs in the placement group. However, the start fails if there isn't enough capacity for the instance.\n\nIf you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Starting the instances may migrate them to hardware that has capacity for all of the requested instances."
      },
      {
        "date": "2023-11-25T13:02:00.000Z",
        "voteCount": 2,
        "content": "I don't know about this -- you would stop all the instances handling a production load? That would immediately induce downtime"
      },
      {
        "date": "2023-12-02T06:58:00.000Z",
        "voteCount": 2,
        "content": "You're right. Straight from the documentation. Thank you for researching this one."
      },
      {
        "date": "2024-03-25T07:26:00.000Z",
        "voteCount": 1,
        "content": "Option B -  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity \nOR \nhttps://repost.aws/knowledge-center/ec2-insufficient-capacity-errors"
      },
      {
        "date": "2024-01-09T13:01:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-12-21T09:47:00.000Z",
        "voteCount": 1,
        "content": "B https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#concepts-placement-groups:~:text=stop%20and%20start%20all%20of%20the%20instances%20in%20the%20placement%20group%2C%20and%20try%20the%20launch%20again"
      },
      {
        "date": "2023-12-04T07:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#concepts-placement-groups"
      },
      {
        "date": "2023-11-29T05:57:00.000Z",
        "voteCount": 3,
        "content": "I agree with answer B despite the fact that you will have to incur downtime and (obviously) will discuss that before executing the stop and start. This question does not particular state that there must be no downtime. So my advice would be taking appropriate actions and stop/start placement group instead of add Dedicated Host."
      },
      {
        "date": "2023-11-28T16:28:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/89258-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-27T03:10:00.000Z",
        "voteCount": 3,
        "content": "George88's article specifically states B as the answer.  However, I agree with Heatblur's reply that in a prod load in real life this is unacceptable unless your app is resilent and can afford a handful of servers being rebooted."
      },
      {
        "date": "2023-11-25T13:08:00.000Z",
        "voteCount": 2,
        "content": "Using Dedicated Hosts (D) can be a solution if the capacity issue is persistent and critical, and if the cost and complexity of managing Dedicated Hosts are justifiable.\n\nA: Spread Placement might help but doesn't directly address the capacity issue.\n\nB: All the instances are handling traffic -- stopping them surely won't help.\n\nC: You can't merge placement groups."
      },
      {
        "date": "2023-11-25T07:21:00.000Z",
        "voteCount": 2,
        "content": "Vote B"
      },
      {
        "date": "2023-11-22T03:36:00.000Z",
        "voteCount": 1,
        "content": "A forsure"
      },
      {
        "date": "2023-11-25T02:09:00.000Z",
        "voteCount": 1,
        "content": "Go with B"
      },
      {
        "date": "2023-11-21T23:23:00.000Z",
        "voteCount": 3,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 348,
    "url": "https://www.examtopics.com/discussions/amazon/view/126817-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. The instances have remained the same for several years.<br><br>The company's business has grown rapidly in the past few months. In response, the company\u2019s operations team has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policy requires a monthly installation of security updates on all operating systems that are running.<br><br>The most recent security update required a reboot. As a result, the Auto Scaling group terminated the instances and replaced them with new, unpatched instances.<br><br>Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-30T10:14:00.000Z",
        "voteCount": 8,
        "content": "Terrible, terrible question. All answers are technically wrong. But the answer they want is A &amp; D.\n\nC &amp; E - There is nothing in the question to suggest any requirement that warrants the introduction of a load balancer of any kind. Is there any inbound traffic? Maybe, maybe not. Even if the traffic is inbound, what if they've implemented DNS-round-robin \"load balancing\" directly to the EC2 public/private IPs (ie no need for ELB)?\n\nThere's also nothing to suggest that the \"traffic\" is consistently the same 24x7, which means they may want the ASG to periodically scale-in and scale-out instances dynamically e.g. in response to EC2 CPU usage. Enabling termination protection will also prevent the ASG from replacing genuinely unhealthy instances, defeating the purpose of having an ASG in the first place.\n\nSo that leaves us with A, B &amp; D."
      },
      {
        "date": "2024-01-30T10:14:00.000Z",
        "voteCount": 2,
        "content": "But why is ASG terminates those instances?\n\nWhat's happening is that ansible/puppet/chef/whatever IaC processes are causing  OS updates to be applied long after the default 300s health check grace period ends, which means new kernel, new glibc, etc packages are installed, requiring a reboot for the change to take effect. During these reboots, EC2 ASG thinks the instances are unhealthy (EC2 ping health checks will fail) and replaces them with new instances instantiated from an old unpatched AMI.\n\nIf you still have lingering doubts about eliminating C and E, then consider the fact that deploying an ELB and turning on ELB health checks in the ASG wont make a difference. A rebooting instance will still get terminated by ASG because EC2 + ELB health checks will fail during the reboot. The instances will probably die faster.\n\nSo the problem isn't the reboot. The problem is ASG killing rebooting servers and replacing them with unpatched servers."
      },
      {
        "date": "2024-01-30T10:15:00.000Z",
        "voteCount": 3,
        "content": "The simplest solution would be to just increase the health check grace period to something large, like 1 hour, and make sure IaC patches &amp; reboots new instances within the grace period. That will buy you a month before the next senseless EC2 massacre. But nothing resembling that option is being offered here.\n\nThe next simplest option is to protect individual EC2 instances from scale-in while they're being rebooted. But nothing resembling that option is being offered here either.\n\nSo we're left with somehow updating the kernel/glibc/etc that's baked into the AMI itself, thus altogether avoiding the need for new instances to reboot in the first place (let's just ignore livepatch methods for the moment).\n\nYes, we all know that launch configs can't technically be updated in place (and AMIs can't be \"patched\" either), but if we eliminate D for that reason then we're left A &amp; B, neither of which mention new AMIs or launch configs at all."
      },
      {
        "date": "2024-01-30T10:15:00.000Z",
        "voteCount": 3,
        "content": "Can we eliminate B? Yes. I can safely assume the intention of B is to create a new ASG with the same old launch config + existing AMI. The behaviour of new ASG will match the old ASG. Any instance rebooted after the health check grace period ends will get terminated, even during a \"maintenance window\" (which is not a thing).\n\nOption A wants to modify the termination policy of the existing ASG to \"Oldest launch configuration\". That's unnecessary but harmless. The default termination policy will do this anyway, and AZ re-balancing always takes precedence even when using a non-default termination policy.\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html\n\n\"Amazon EC2 Auto Scaling always balances instances across Availability Zones first, regardless of which termination policy is used\""
      },
      {
        "date": "2024-01-30T10:16:00.000Z",
        "voteCount": 4,
        "content": "So where does that leave us?\n\nA - does nothing meaningful at all, but at least it's harmless.\nB - working instances will all die on reboot during the \"maintenance window\" (all at the same time? lol) \nC - working instances will die faster when rebooted\nD - perfect, except it technically isn't possible to \"update\" launch configs or \"patch\" AMIs in place. Bummer.\nE - broken instances will never get replaced, defeating the purpose of ASGs.\n\nI think it's safe to conclude the author of this question was just really sloppy with how they worded option D.\n\nTo avoid a re-occurrence of this issue, I am compelled by common sense to adopt a more relaxed interpretation of D. If I infer that the intent of D is New AMI + New launch config + Invoke ASG refresh, then I don't actually need to do anything else. D will be enough to prevent re-occurrence. But I have to pair it with a second option. So I'll pair it with A, which sounds good but actually does nothing and is harmless."
      },
      {
        "date": "2024-01-30T10:17:00.000Z",
        "voteCount": 2,
        "content": "Answer: A + D.\n\nTerrible, terrible, terrible question."
      },
      {
        "date": "2023-12-21T09:53:00.000Z",
        "voteCount": 6,
        "content": "A and C. D is wrong launch config can't be updated https://docs.aws.amazon.com/autoscaling/ec2/userguide/change-launch-config.html"
      },
      {
        "date": "2024-08-11T22:32:00.000Z",
        "voteCount": 1,
        "content": "D should get the job done.\nThe rest are redundant. But C seems to be not hurting to do anyway"
      },
      {
        "date": "2024-07-06T06:32:00.000Z",
        "voteCount": 1,
        "content": "While options A and D are considered the most suitable (and safest) answers to the question's requirements, they may not completely solve the problem. For example, if there are 10 instances in an Auto Scaling group (No.1, No.2, No.3 ... No.10), and two of them are unpatched, the Oldest Launch setting would prioritize replacing the older unpatched instances. However, there's a possibility that only No.10 might be scaled in, leaving No.9 still unpatched and active in the group."
      },
      {
        "date": "2024-05-03T03:28:00.000Z",
        "voteCount": 1,
        "content": "CD i think"
      },
      {
        "date": "2024-04-15T14:37:00.000Z",
        "voteCount": 1,
        "content": "A and C."
      },
      {
        "date": "2024-03-09T12:38:00.000Z",
        "voteCount": 2,
        "content": "Sometimes I really hate the AWS exam writers. This questions is on a level to which even they shouldn't plumb. \n\nAll of the alternatives are wrong in some way. So you have to guess. Whoever wrote this should be fired.\n\nD, since it addresses the AMI (though \"update\" is not what you do with an AMI). And then A, for the reasons LazyAutonomy gives.\n\nBut wow do I sometimes hate the exam writers. It's one thing to force us to focus on minute details; it's quite another to subject us to their own sloppiness."
      },
      {
        "date": "2024-02-17T16:48:00.000Z",
        "voteCount": 1,
        "content": "Answer:  A, C\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/automation-tutorial-update-patch-windows-ami-autoscaling.html"
      },
      {
        "date": "2024-01-09T14:11:00.000Z",
        "voteCount": 2,
        "content": "Option C and D"
      },
      {
        "date": "2024-01-05T14:01:00.000Z",
        "voteCount": 1,
        "content": "A and C does not prevent EC2 to be terminated by security patch. B is very burdened(create new group each time?).\nD is poorly worded as it says 'update configuration'.\nBut I will go with D E."
      },
      {
        "date": "2024-01-05T14:12:00.000Z",
        "voteCount": 1,
        "content": "Sorry. E does not prevent terminating from patch.Let me go with C D."
      },
      {
        "date": "2023-12-29T19:19:00.000Z",
        "voteCount": 3,
        "content": "CD \nAnswer is worded a bit poorly but this is correct."
      },
      {
        "date": "2023-12-13T23:01:00.000Z",
        "voteCount": 2,
        "content": "D is out\nAC then"
      },
      {
        "date": "2023-12-13T22:56:00.000Z",
        "voteCount": 2,
        "content": "Option D is out because it says to \"update launch configuration\"\nAWS Auto Scaling launch configurations cannot be updated directly. Once a launch configuration is created, it cannot be modified; instead, a new one must be created to reflect any changes"
      },
      {
        "date": "2023-12-13T22:49:00.000Z",
        "voteCount": 4,
        "content": "The answer is A and D."
      },
      {
        "date": "2023-12-04T05:44:00.000Z",
        "voteCount": 2,
        "content": "Answer: CD"
      },
      {
        "date": "2023-12-03T07:51:00.000Z",
        "voteCount": 4,
        "content": "https://www.examtopics.com/discussions/amazon/view/68855-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-21T23:24:00.000Z",
        "voteCount": 3,
        "content": "Answer: C D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 349,
    "url": "https://www.examtopics.com/discussions/amazon/view/126819-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machine learning (ML) models. The SageMaker instances are deployed in a VPC that does not have access to or from the internet. Datasets for ML model training are stored in an Amazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.<br><br>Occasionally, the data scientists require access to the Python Package Index (PyPI) repository to update Python packages that they use as part of their workflow. A solutions architect must provide access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository for each package that the data scientists need to access. Configure code synchronization between the PyPI repository and the CodeCommit repository. Create a VPC endpoint for CodeCommit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a NAT gateway in the VPC. Configure VPC routes to allow access to the internet with a network ACL that allows access to only the PyPI repository endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a NAT instance in the VPConfigure VPC routes to allow access to the internet. Configure SageMaker notebook instance firewall rules that allow access to only the PyPI repository endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeArtifact domain and repository. Add an external connection for public:pypi to the CodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPC endpoint for CodeArtifact.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T07:35:00.000Z",
        "voteCount": 8,
        "content": "CodeArtifact allows you to store artifacts using popular package managers and build tools like Maven, Gradle, npm, Yarn, Twine, pip, NuGet, and SwiftPM"
      },
      {
        "date": "2024-04-27T19:16:00.000Z",
        "voteCount": 1,
        "content": "Why not C , can use NAT gateway and Sagemaker instance notebook rules as there were not asking for cost-effective"
      },
      {
        "date": "2024-08-18T01:12:00.000Z",
        "voteCount": 1,
        "content": "Requirement is to isolate Sagemaker instances from the Internet"
      },
      {
        "date": "2024-01-09T14:15:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-08T09:14:00.000Z",
        "voteCount": 2,
        "content": "Answer : D\nNot option C\nBy using CodeArtifact, you can effectively meet the requirements of providing access to PyPI while maintaining isolation, security, and cost-efficiency for the SageMaker instances. NAT are additional costs... which you can avoid"
      },
      {
        "date": "2023-12-31T02:58:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/machine-learning/private-package-installation-in-amazon-sagemaker-running-in-internet-free-mode/"
      },
      {
        "date": "2023-11-30T08:27:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nIt can't be A -- CodeCommit is primarily a source control service and does not directly synchronize with external repositories like PyPI. This option requires significant overhead in maintaining the sync."
      },
      {
        "date": "2023-11-22T03:55:00.000Z",
        "voteCount": 2,
        "content": "D for sure"
      },
      {
        "date": "2023-11-21T23:28:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 350,
    "url": "https://www.examtopics.com/discussions/amazon/view/127000-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect works for a government agency that has strict disaster recovery requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additional AWS Regions. The agency also is required to maintain the lowest possible operational overhead.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge to schedule an AWS Lambda function to copy the EBS snapshots to the additional Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region Replication to copy the EBS snapshots to the additional Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to the additional Regions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T08:30:00.000Z",
        "voteCount": 8,
        "content": "The best answer is A: configuring Amazon Data Lifecycle Manager to automate the copying of EBS snapshots to additional regions, is the most suitable solution. It meets the requirement of minimal operational overhead while ensuring that snapshots are stored in multiple regions for disaster recovery. This approach is straightforward and leverages AWS's native capabilities for snapshot management.\n\nCan't be C...EBS snapshots are not stored in S3 in a direct manner that would allow the use of S3 Cross-Region Replication. This option seems to misunderstand the nature of EBS snapshots and S3 integration."
      },
      {
        "date": "2024-04-15T15:15:00.000Z",
        "voteCount": 1,
        "content": "Indeed, you can do a cross-Region snapshot copy with AWS Backup, but ans \u201cC\u201d states to do a local-Region copy, then from the S3 console to run a S3 cross-Region replication, which is NOT possible. See the text and link below.\n\u201cEBS snapshots are stored in Amazon S3, in S3 buckets that you CAN\u2019T access directly. You can create and manage your snapshots using the Amazon EC2 console or the Amazon EC2 API. You can't access your snapshots using the Amazon S3 console or the Amazon S3 API.\u201d\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-snapshots.html\n\nSo, correct ans is \u201cA\u201d."
      },
      {
        "date": "2024-01-30T04:24:00.000Z",
        "voteCount": 1,
        "content": "EBS snapshot are stored in S3."
      },
      {
        "date": "2024-07-16T08:56:00.000Z",
        "voteCount": 1,
        "content": "CRR is a feature in AWS S3 that automatically replicates data from one S3 bucket in one AWS region to another S3 bucket in a different region.\nthe requirements here to send backup to two regions"
      },
      {
        "date": "2024-07-12T05:35:00.000Z",
        "voteCount": 1,
        "content": "AWS Backup does not store snapshots in Amazon S3: AWS Backup stores the EBS snapshots in the same AWS Region where the EBS volumes reside. It does not store the snapshots in Amazon S3 buckets."
      },
      {
        "date": "2024-04-15T15:17:00.000Z",
        "voteCount": 1,
        "content": "\"A\" - correct."
      },
      {
        "date": "2024-03-01T14:20:00.000Z",
        "voteCount": 3,
        "content": "\"You can now copy snapshots across regions using Data Lifecycle Manager (DLM). You can enable policies which, along with create, can now also copy snapshots to one or more AWS region(s). Copies can be scheduled for up to three regions from a single policy and retention periods are set for each region separately.\" \nhttps://aws.amazon.com/about-aws/whats-new/2019/12/amazon-data-lifecycle-manager-enables-automation-snapshot-copy-via-policies/"
      },
      {
        "date": "2024-02-25T10:46:00.000Z",
        "voteCount": 1,
        "content": "A (Amazon Data Lifecycle Manager) could work, but it's more suitable for lifecycle management tasks such as creating, retaining, and deleting EBS snapshots based on defined policies. It doesn't inherently handle cross-region replication."
      },
      {
        "date": "2024-01-09T14:36:00.000Z",
        "voteCount": 2,
        "content": "Option A. EBS Data Lifecycle manager supports automated cross region snapshot.\nhttps://aws.amazon.com/about-aws/whats-new/2019/12/amazon-data-lifecycle-manager-enables-automation-snapshot-copy-via-policies/"
      },
      {
        "date": "2024-01-03T10:52:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\nIt help's - Create disaster recovery backup policies that back up data to isolated Regions or accounts."
      },
      {
        "date": "2023-12-28T10:01:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-ami-policy.html"
      },
      {
        "date": "2023-12-28T09:58:00.000Z",
        "voteCount": 1,
        "content": "C ans \na IS not correct \nSnapshots must be archived in the same Region in which they were created. If you enabled cross-Region copy and snapshot archiving, Amazon Data Lifecycle Manager does not archive the snapshot copy."
      },
      {
        "date": "2023-12-21T10:01:00.000Z",
        "voteCount": 1,
        "content": "C Amazon data lifecycle manager can't copy snapshots. AWS backup has cross-Region copy feature https://aws.amazon.com/getting-started/hands-on/amazon-ebs-backup-and-restore-using-aws-backup/#:~:text=same%20AWS%20Region%20(-,however%2C%20see%20step%203.2%20for%20information%20on%20cross%2DRegion%20copy,-).%20This%20tutorial%20uses"
      },
      {
        "date": "2023-12-28T06:48:00.000Z",
        "voteCount": 1,
        "content": "Since 2019, DLM can copy to other regions. See https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-data-lifecycle-manager-enables-automation-snapshot-copy-via-policies/ I'm pretty sure the answer is A"
      },
      {
        "date": "2023-11-30T03:49:00.000Z",
        "voteCount": 1,
        "content": "For me A would be the solution. \nC will imply copying the ebs snapshots to s3, why not using directly the AWS Backup cross-region backup copy feature?"
      },
      {
        "date": "2023-11-29T06:10:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-data-lifecycle-manager-now-automates-copying-ebs-snapshots-across-accounts/\n\nfully automated and no overhead.  Answer A"
      },
      {
        "date": "2023-11-26T12:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-11-24T23:52:00.000Z",
        "voteCount": 1,
        "content": "should be C"
      },
      {
        "date": "2023-11-23T04:18:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. Therefore, option C is the most efficient and cost-effective solution that aligns with the agency's strict disaster recovery requirements while minimizing operational complexity."
      },
      {
        "date": "2023-11-25T07:37:00.000Z",
        "voteCount": 1,
        "content": "How AWS Backup create Snaphot?"
      },
      {
        "date": "2023-11-28T19:28:00.000Z",
        "voteCount": 1,
        "content": "yes. i'm researching and saw that: https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/new-ebs-volume-backups.html#amazon-dlm"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 351,
    "url": "https://www.examtopics.com/discussions/amazon/view/126820-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small EC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company's organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project's account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T21:33:00.000Z",
        "voteCount": 14,
        "content": "Should be D.\nQuestion says \"The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT\"\nYou need organisation for SCP."
      },
      {
        "date": "2024-05-31T17:20:00.000Z",
        "voteCount": 1,
        "content": "only possible way"
      },
      {
        "date": "2024-05-05T08:46:00.000Z",
        "voteCount": 1,
        "content": "C\nWhy not C as developers has to select only T3.small. why can't we purchase RI with only T3.small"
      },
      {
        "date": "2024-02-25T11:41:00.000Z",
        "voteCount": 2,
        "content": "option D is the only answer. the scenario clearly stated the IT team in this project cannot be part of the organization."
      },
      {
        "date": "2024-01-09T16:50:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-08T08:45:00.000Z",
        "voteCount": 1,
        "content": "Answer D:\nOption B: An SCP can manage IAM permissions across an organization, but the project account isn't part of the organization."
      },
      {
        "date": "2023-12-09T08:11:00.000Z",
        "voteCount": 1,
        "content": "SCP can be applied only to those users and roles which are managed by accounts that are part of any organization\nSee: https://digitalcloud.training/aws-scp-mastering-aws-service-control-policies/#:~:text=SCP%20can%20be%20applied%20only,including%20the%20account's%20root%20user."
      },
      {
        "date": "2023-12-03T11:20:00.000Z",
        "voteCount": 1,
        "content": "D meets the needs with an IAM-based access control policy specific to the standalone project account and its developers' roles/groups."
      },
      {
        "date": "2023-11-22T11:04:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_ec2.html#example-ec2-1"
      },
      {
        "date": "2023-11-21T23:29:00.000Z",
        "voteCount": 4,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 352,
    "url": "https://www.examtopics.com/discussions/amazon/view/126904-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A scientific company needs to process text and image data from an Amazon S3 bucket. The data is collected from several radar stations during a live, time-critical phase of a deep space mission. The radar stations upload the data to the source S3 bucket. The data is prefixed by radar station identification number.<br><br>The company created a destination S3 bucket in a second account. Data must be copied from the source S3 bucket to the destination S3 bucket to meet a compliance objective. This replication occurs through the use of an S3 replication rule to cover all objects in the source S3 bucket.<br><br>One specific radar station is identified as having the most accurate data. Data replication at this radar station must be monitored for completion within 30 minutes after the radar station uploads the objects to the source S3 bucket.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to the destination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure that itis in the TRANSFERRING status. Create an Amazon EventBridge rule to initiate an alert if this status changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the second account, create another S3 bucket to receive data from the radar station with the most accurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the other radar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station with the most accurate data to use the new endpoint. Monitor the S3 destination bucket's TotalRequestLatency metric. Create an Amazon EventBridge rule to initiate an alert if this status changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of the radar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T04:50:00.000Z",
        "voteCount": 8,
        "content": "D for sure"
      },
      {
        "date": "2024-01-09T16:55:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-08T08:38:00.000Z",
        "voteCount": 4,
        "content": "Answer: D \nNot C....\nFeature    |S3 RTC                       |S3 Transfer Acceleration\n\nPurpose    |Faster replication           |Faster uploads/downloads\nScope      |Replication across buckets   |Individual file transfers\nPerformance|SLA for 15-minute replication|Up to 50-500% speed improvement\nCost       |Additional charge            |Additional charge"
      },
      {
        "date": "2023-12-21T10:07:00.000Z",
        "voteCount": 2,
        "content": "D https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-time-control.html"
      },
      {
        "date": "2023-11-30T03:54:00.000Z",
        "voteCount": 2,
        "content": "C would also be ok, but its additional overhead of configuring the additional bucket and modifying the sensor to send data to it, so my option is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 353,
    "url": "https://www.examtopics.com/discussions/amazon/view/126822-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration.<br><br>Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Application Discovery Service\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS SMS",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS X-Ray",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Cloud Adoption Readiness Tool (CART)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Inspector",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Migration Hub\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "ABF",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ABD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T23:30:00.000Z",
        "voteCount": 9,
        "content": "Answer: ADF"
      },
      {
        "date": "2024-09-28T12:43:00.000Z",
        "voteCount": 1,
        "content": "ADF is correct"
      },
      {
        "date": "2024-04-25T04:10:00.000Z",
        "voteCount": 1,
        "content": "Option ADF not B -  As of March 31, 2022, Amazon Web Services will discontinue Server Migration Service (Amazon Web Services SMS). Going forward, we recommend Amazon Web Services Application Migration Service (Amazon Web Services MGN)\n\nAWS CART - https://aws.amazon.com/blogs/publicsector/get-migration-ready-aws-cloud-adoption-readiness-tool/"
      },
      {
        "date": "2024-01-09T16:58:00.000Z",
        "voteCount": 2,
        "content": "A, D, F"
      },
      {
        "date": "2024-01-08T08:25:00.000Z",
        "voteCount": 3,
        "content": "Answer: ADF sure... \nB. AWS SMS was discontinued on March 31, 2022.AWS recommends using alternative solutions for server migration, such as: AWS Application Migration Service (AMS),AWS Database Migration Service (DMS), AWS Transfer Family\nhttps://awscli.amazonaws.com/v2/documentation/api/2.4.19/reference/sms/index.html"
      },
      {
        "date": "2024-01-06T00:02:00.000Z",
        "voteCount": 1,
        "content": "D. CART is just some of questionarier tool to assess and find any gap of people, organization, process between on-premise and AWS Cloud for migration. It is not directly needed or aimed for system."
      },
      {
        "date": "2024-01-05T09:39:00.000Z",
        "voteCount": 1,
        "content": "A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration.\n\nA. AWS Application Discovery Service (ok): gathers information about your source servers to support the migration planning.\nB. AWS SMS (?): Isn't it AWS Application Migration Service?\nC. AWS X-Ray (ok): AWS X-Ray provides a complete view of requests as they travel through your application\nD. AWS (CART) (out): Strategic planning tool, focused on Public Sector\nE. Amazon Inspector (out): Doesn't work on premises\nF. AWS Migration Hub (ok): Migration Hub monitors the status of your migrations in all AWS Regions\n\nADS helps understand machines, X-ray maps relationships in an undocumented env, and Hub tracks migration data."
      },
      {
        "date": "2023-12-21T10:11:00.000Z",
        "voteCount": 2,
        "content": "Agree ADF"
      },
      {
        "date": "2023-12-05T02:33:00.000Z",
        "voteCount": 1,
        "content": "ADF -- D and not B because one requirement is \"estimate the cloud resource costs after the migration\""
      },
      {
        "date": "2023-12-04T05:20:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer is ABF - SMS Server Migration Service seems to be more essential than CART. Servers migrations are mention which SMS is great for. CART should be used before migration when you're just assessing an organization's readiness for cloud adoption"
      },
      {
        "date": "2023-12-21T23:11:00.000Z",
        "voteCount": 1,
        "content": "The question is about before migration"
      },
      {
        "date": "2023-11-29T22:03:00.000Z",
        "voteCount": 2,
        "content": "ADF for sure"
      },
      {
        "date": "2023-11-28T19:02:00.000Z",
        "voteCount": 2,
        "content": "ADF correct"
      },
      {
        "date": "2023-11-22T04:54:00.000Z",
        "voteCount": 2,
        "content": "ABD for sure"
      },
      {
        "date": "2023-11-25T01:28:00.000Z",
        "voteCount": 2,
        "content": "Yes. ADF is the correct one"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 354,
    "url": "https://www.examtopics.com/discussions/amazon/view/126823-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.<br><br>The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T00:02:00.000Z",
        "voteCount": 5,
        "content": "A ans \nBest practices\n\nIf your resources span multiple Availability Zones (AZ) , then create one NAT gateway per AZ. This helps to avoid a single point of failure and zone data transfer charges.\nData that's transferred between Amazon EC2 and Elastic Network Interfaces in the same AZ is free. However, data that's transferred to and from Amazon EC2 and Elastic Network Interfaces across multiple AZs in the same AWS Region is charged. The charges depend on the data transfer rates for the Region.\nhttps://repost.aws/knowledge-center/nat-gateway-vpc-private-subnet"
      },
      {
        "date": "2024-01-09T17:02:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-30T04:01:00.000Z",
        "voteCount": 3,
        "content": "A.\nall the other options make no sense in this scenario"
      },
      {
        "date": "2023-11-28T19:08:00.000Z",
        "voteCount": 2,
        "content": "A...other answers don't make sense"
      },
      {
        "date": "2023-11-22T05:02:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-11-21T23:30:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 355,
    "url": "https://www.examtopics.com/discussions/amazon/view/126824-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate its on-premises transaction-processing application to AWS. The application runs inside Docker containers that are hosted on VMs in the company's data center. The Docker containers have shared storage where the application records transaction data.<br><br>The transactions are time sensitive. The volume of transactions inside the application is unpredictable. The company must implement a low-latency storage solution that will automatically scale throughput to meet increased demand. The company cannot develop the application further and cannot continue to administer the Docker hosting environment.<br><br>How should the company migrate the application to AWS to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the containers that run the application to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon S3 to store the transaction data that the containers share.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic File System (Amazon EFS) file system. Create a Fargate task definition. Add a volume to the task definition to point to the EFS file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic Block Store (Amazon EBS) volume. Create a Fargate task definition. Attach the EBS volume to each running task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch Amazon EC2 instances. Install Docker on the EC2 instances. Migrate the containers to the EC2 instances. Create an Amazon Elastic File System (Amazon EFS) file system. Add a mount point to the EC2 instances for the EFS file system."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T23:26:00.000Z",
        "voteCount": 6,
        "content": "To mount an Amazon EFS file system on a Fargate task or container, you must first create a task definition. Then, make that task definition available to the containers in your task across all Availability Zones in your AWS Region. Then, your Fargate tasks use Amazon EFS to automatically mount the file system to the tasks that you specify in your task definition.\nhttps://repost.aws/knowledge-center/ecs-fargate-mount-efs-containers-tasks"
      },
      {
        "date": "2024-08-12T03:34:00.000Z",
        "voteCount": 1,
        "content": "I was thinking between B and C.\n\nBut according to AWS doc: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html\n\nAmazon EBS volumes provide cost-effective, durable, high-performance block storage for data-intensive containerized workloads.\n\nAmazon EFS volumes support concurrency and are useful for containerized applications that scale horizontally and need storage functionalities like low latency, high throughput, and read-after-write consistency. \n\nSo should be B"
      },
      {
        "date": "2024-01-09T17:07:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-21T10:18:00.000Z",
        "voteCount": 1,
        "content": "answer B"
      },
      {
        "date": "2023-12-04T04:43:00.000Z",
        "voteCount": 1,
        "content": "Answer: B Fargate+EFS"
      },
      {
        "date": "2023-12-03T21:23:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer for the given scenario"
      },
      {
        "date": "2023-11-22T07:10:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2023-11-21T23:31:00.000Z",
        "voteCount": 3,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 356,
    "url": "https://www.examtopics.com/discussions/amazon/view/126825-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications.<br><br>The company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company\u2019s current environment and develop a migration plan.<br><br>Which solution will provide the solutions architect with the required information to develop the migration plan?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Migration Hub import tool to load the details of the company\u2019s on-premises environment. Generate a report by using Migration Hub Strategy Recommendations."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T09:22:00.000Z",
        "voteCount": 7,
        "content": "Always remember. If you want to find data for migration that is related to\n1. Network, system performance, running process, etc\n2. The current on-prem load that you need to find has physical servers in it.\nAlways use an Application discovery agent.\nso A and C are out (since they use agentless discovery which is only used for on-prem VMs)\nBetween B and D: D is wrong the question itself mentions we are not aware of the current load so import data is not possible.\n\nCorrect ans is B"
      },
      {
        "date": "2024-02-27T23:13:00.000Z",
        "voteCount": 1,
        "content": "Why not D?\nAWS Migration Hub (Migration Hub) import allows you to import details of your on-premises environment directly into Migration Hub without using the Application Discovery Service Agentless Collector (Agentless Collector) or AWS Application Discovery Agent (Discovery Agent)\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-import.html"
      },
      {
        "date": "2024-03-13T11:58:00.000Z",
        "voteCount": 3,
        "content": "coz the company don't have a detailed list of servers to be imported"
      },
      {
        "date": "2024-01-09T17:33:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-09T17:10:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-09T08:39:00.000Z",
        "voteCount": 4,
        "content": "The Discovery Agent captures system configuration, system performance, running processes, and details of the network connections between systems.\n\nThe Agentless Collector is only installed as an OVA on the VMware vCenter so it doesn't apply to all servers.\n\nhttps://aws.amazon.com/application-discovery/faqs/"
      },
      {
        "date": "2023-12-04T04:38:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2023-11-28T19:46:00.000Z",
        "voteCount": 2,
        "content": "Answer B. Application Discovery service agent installed on all servers and VMs to gather information."
      },
      {
        "date": "2023-11-22T07:13:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2023-11-21T23:33:00.000Z",
        "voteCount": 3,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 357,
    "url": "https://www.examtopics.com/discussions/amazon/view/126920-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally.<br><br>For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization's management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T04:43:00.000Z",
        "voteCount": 9,
        "content": "i thinks C is correct answer"
      },
      {
        "date": "2024-03-20T08:32:00.000Z",
        "voteCount": 6,
        "content": "C is correct. Do not get fooled by the phrase \"deploy the trail for all accounts\" to think that a trail is created in each account \u2013 it means that the new organisational-level trail is _configured_ to capture data for all accounts."
      },
      {
        "date": "2024-09-29T09:46:00.000Z",
        "voteCount": 1,
        "content": "I will go with D as the correct answer because C has versioning turned on which is not necessary in this case. You can configure a trail to use Amazon SNS topic and be notifies when cloud trail publishes new log files to the Amazon S3 bucket."
      },
      {
        "date": "2024-01-09T17:42:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-08T11:59:00.000Z",
        "voteCount": 3,
        "content": "A: Should always create new bucket for cloudtrail\nB: When you create an organization trail, a trail with the name that you give it is created in every AWS account that belongs to your organization.\nC: Correct\nD: For several reasons, use SNS only to notify admin, not to use email as a external mgmt system"
      },
      {
        "date": "2023-12-29T23:02:00.000Z",
        "voteCount": 1,
        "content": "D ans :- https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html"
      },
      {
        "date": "2023-12-04T04:36:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2023-11-26T05:08:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-11-26T00:47:00.000Z",
        "voteCount": 3,
        "content": "i thinks C is correct answer"
      },
      {
        "date": "2023-11-22T07:19:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-11-25T01:14:00.000Z",
        "voteCount": 3,
        "content": "Yes, C is the correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 358,
    "url": "https://www.examtopics.com/discussions/amazon/view/127001-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions.<br><br>The company requires the lowest possible networking latency to achieve maximum performance.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch memory optimized EC2 instances in a partition placement group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch compute optimized EC2 instances in a partition placement group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch memory optimized EC2 instances in a cluster placement group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch compute optimized EC2 instances in a spread placement group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T19:53:00.000Z",
        "voteCount": 6,
        "content": "Answer C. Memory optimized in Cluster placement group for low latency replication between worker nodes."
      },
      {
        "date": "2023-11-23T04:45:00.000Z",
        "voteCount": 5,
        "content": "Option C."
      },
      {
        "date": "2024-08-12T03:51:00.000Z",
        "voteCount": 3,
        "content": "God I wish all SCP questions are like this.\nEasy to read.\nEasy to answer.\nEasy to move on to next question without spending extra time to read through all comments, ask google/chatGPT and read AWS doc to make sure the community vote is correct"
      },
      {
        "date": "2024-01-09T17:44:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-12-04T12:17:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer for sure"
      },
      {
        "date": "2023-12-04T04:29:00.000Z",
        "voteCount": 1,
        "content": "Answer: C, I guess memory optimized is the obvious way to go and \nCluster placement group provides the lowest possible networking latency"
      },
      {
        "date": "2023-11-25T01:10:00.000Z",
        "voteCount": 3,
        "content": "C is ok"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 359,
    "url": "https://www.examtopics.com/discussions/amazon/view/126921-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company maintains information on premises in approximately 1 million.csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud.<br><br>Backups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.<br><br>Which solution will meet the backup requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T00:47:00.000Z",
        "voteCount": 13,
        "content": "Because of:  The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories."
      },
      {
        "date": "2023-12-01T11:06:00.000Z",
        "voteCount": 2,
        "content": "The only problem with C is that a data sync is not a backup. If you delete a file, the sync will delete the file on AWS, but with backups you can restore it from yesterday's backup. So I think it's B."
      },
      {
        "date": "2024-01-07T11:13:00.000Z",
        "voteCount": 2,
        "content": "I agree AWS DataSync is not a dedicated backup solution but it can be used for data replication that serves as a backup, it's essential to understand its limitations and distinctions compared to a comprehensive backup service:\nWhen to Use DataSync for Backup-Like Purposes:\n\nInitial Data Transfer: It's efficient for bulk migration of large datasets to AWS storage services.\nIncremental Updates: It excels at replicating ongoing changes to keep a copy of data in AWS, serving as a near-real-time backup.\nCost-Effective Replication: It's often more cost-effective than traditional backup tools for ongoing data replication, especially for large datasets."
      },
      {
        "date": "2023-12-09T08:52:00.000Z",
        "voteCount": 6,
        "content": "For me there are two cues: \n1- \"custom filters\" which are available in Datasync\n2- AWS Backup does not back up to S3, rather to a Storage Vault."
      },
      {
        "date": "2024-03-25T08:47:00.000Z",
        "voteCount": 1,
        "content": "Option C: How To \n https://docs.aws.amazon.com/datasync/latest/userguide/create-s3-location.html"
      },
      {
        "date": "2024-01-09T17:57:00.000Z",
        "voteCount": 2,
        "content": "Option C - Due to filtering requirement."
      },
      {
        "date": "2024-01-07T11:10:00.000Z",
        "voteCount": 1,
        "content": "Answer: C\nOption B: AWS Backup offers centralized backup management, but it might not support custom filtering for specific files or directories as granularly as DataSync."
      },
      {
        "date": "2023-12-21T10:35:00.000Z",
        "voteCount": 1,
        "content": "B AWS Backup can do backup from on-premise (https://aws.amazon.com/backup/faqs/ Can I use AWS Backup to back up on-premises data?)"
      },
      {
        "date": "2023-12-25T09:40:00.000Z",
        "voteCount": 1,
        "content": "based on the FQA,  AWS Backup can only back up on-premises \"Storage Gateway\" volumes and \"VMware virtual machines\"."
      },
      {
        "date": "2023-12-14T14:22:00.000Z",
        "voteCount": 3,
        "content": "B correct. Because Datasync is not for backup"
      },
      {
        "date": "2023-12-04T12:29:00.000Z",
        "voteCount": 2,
        "content": "as to option B, AWS Backup doesn't natively support direct backups of on-premises data into Amazon S3."
      },
      {
        "date": "2023-12-04T04:24:00.000Z",
        "voteCount": 2,
        "content": "Answer: C"
      },
      {
        "date": "2023-11-28T20:07:00.000Z",
        "voteCount": 2,
        "content": "Answer C - with Datasync custom filters can be created to select what data needs to be backed up / replicated."
      },
      {
        "date": "2023-11-22T07:31:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2024-01-20T05:54:00.000Z",
        "voteCount": 4,
        "content": "For sure you ate wrong."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 360,
    "url": "https://www.examtopics.com/discussions/amazon/view/126826-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial services company has an asset management product that thousands of customers use around the world. The customers provide feedback about the product through surveys. The company is building a new analytical solution that runs on Amazon EMR to analyze the data from these surveys. The following user personas need to access the analytical solution to perform different actions:<br><br>\u2022\tAdministrator: Provisions the EMR cluster for the analytics team based on the team\u2019s requirements<br>\u2022\tData engineer: Runs ETL scripts to process, transform, and enrich the datasets<br>\u2022\tData analyst: Runs SQL and Hive queries on the data<br><br>A solutions architect must ensure that all the user personas have least privilege access to only the resources that they need. The user personas must be able to launch only applications that are approved and authorized. The solution also must ensure tagging for all resources that the user personas create.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform. Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup Kerberos-based authentication for EMR clusters upon launch. Specify a Kerberos security configuration along with cluster-specific Kerberos options.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the EMR cluster by using AWS CloudFormation, Attach resource-based policies to the EMR cluster during cluster creation. Create an AWS. Config rule to check for noncompliant clusters and noncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate the noncompliant resources."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T18:10:00.000Z",
        "voteCount": 5,
        "content": "Option C. Option A does not provide control over deployment of resources and configurations."
      },
      {
        "date": "2024-10-14T00:25:00.000Z",
        "voteCount": 1,
        "content": "Why not Option A is because it involves creating IAM roles and attaching identity-based policies, which is solid. But it relies on AWS Config rules to ensure compliance, which adds an extra layer of management and potential lag in remediation. \n\nAWS Service Catalog, on the other hand, simplifies control over EMR versions, configurations, and permissions, and it also enforces resource tagging directly during deployment, making it more efficient and streamlined for managing access and compliance."
      },
      {
        "date": "2024-07-13T06:17:00.000Z",
        "voteCount": 1,
        "content": "C, for sure.\nAWS Service Catalog ensures that all resources created are compliant with the organization's policies, including mandatory tagging."
      },
      {
        "date": "2024-01-09T14:09:00.000Z",
        "voteCount": 3,
        "content": "C because tagging ensured by Service Catalogue."
      },
      {
        "date": "2024-01-07T11:04:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C\nOption A: While IAM roles and identity-based policies offer user-level control, they lack the functionality for managing EMR deployment options and configurations centrally."
      },
      {
        "date": "2023-12-14T14:32:00.000Z",
        "voteCount": 4,
        "content": "keyword here are: \"...only applications that are approved and authorized...\"\nOnly C provides this"
      },
      {
        "date": "2023-12-09T08:58:00.000Z",
        "voteCount": 4,
        "content": "A - IAM Roles define actions   Service Catalogue is about resources (EMR)"
      },
      {
        "date": "2023-12-19T10:19:00.000Z",
        "voteCount": 5,
        "content": "it seems that I was wrong and C is the approach as per: https://aws.amazon.com/blogs/big-data/build-a-self-service-environment-for-each-line-of-business-using-amazon-emr-and-aws-service-catalog/"
      },
      {
        "date": "2023-12-06T03:54:00.000Z",
        "voteCount": 1,
        "content": "Please vote your answers rather than just commenting. It skews the vote % for someone who doesnt read all the comments."
      },
      {
        "date": "2023-12-04T02:08:00.000Z",
        "voteCount": 2,
        "content": "It seems that AWS is upselling AWS Service Catalog here with this question. Some key parts in this question:\n1. Least privilige access\n2. launch only approved and authorized applications\n3. ensure tagging."
      },
      {
        "date": "2023-12-04T02:08:00.000Z",
        "voteCount": 2,
        "content": "due to point 3, all options with AWS config rule are out since it only measures if you are compliant, so that means tagging is not ensured upfront. A and D are out!\nB doenst fullfill the requirement for tagging and even more, is kerberos really helpfull here?"
      },
      {
        "date": "2023-12-04T02:08:00.000Z",
        "voteCount": 4,
        "content": "Leaves only C, \nquote from https://aws.amazon.com/servicecatalog/\nCreate, organize, and govern a curated catalog of AWS resources that can be shared at the permissions level so you can quickly provision approved cloud resources without needing direct access to the underlying AWS services. -&gt; meets only allowed and authorized application launch. \nAutoTag fulfills the requirement to tag resources with creator -&gt; aws:servicecatalog:provisioningPrincipalArn - The ARN of the provisioning principal (user) who created the provisioned product.\n\nthis can only be AWS Server Catalog.\n\nand please stop seeding GPT answers! do your own research."
      },
      {
        "date": "2023-12-01T16:15:00.000Z",
        "voteCount": 3,
        "content": "Answer A - \nThe answers from Chat GPT are inaccurate and untrustable."
      },
      {
        "date": "2023-11-28T20:15:00.000Z",
        "voteCount": 3,
        "content": "From GPT: AWS Service Catalog allows you to control and manage access to resources by defining portfolios and products with specific permissions. Allows you to create portfolios with approved and authorized applications, ensuring that only the specified applications are launched. AWS Service Catalog can enforce tagging on provisioned resources, ensuring that all resources created by the user personas are appropriately tagged."
      },
      {
        "date": "2023-11-25T14:20:00.000Z",
        "voteCount": 3,
        "content": "C is correct: AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. This is ideal for controlling which Amazon EMR versions and cluster configurations are available to users. Specific cluster configurations and permissions can be set for each user persona, ensuring they have only the access they need. This meets the least privilege principle. The Service Catalog can be configured to allow users to launch only certain applications, ensuring adherence to company policies on approved and authorized software. It also supports resource tagging."
      },
      {
        "date": "2023-11-25T00:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct\n\nAws:\nTo ensure that all user personas have least privilege access to only the resources they need, can launch only approved and authorized applications, and ensure tagging for all resources that the user personas create, a solutions architect can consider the following steps:\n1. IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform.\n2.Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources."
      },
      {
        "date": "2023-11-21T23:37:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 361,
    "url": "https://www.examtopics.com/discussions/amazon/view/126827-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink. The service consists of proprietary software that runs on three Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in private subnets in multiple Availability Zones in the eu-west-2 Region. All the company's customers are in eu-west-2.<br><br>However, the company now acquires a new customer in the us-east-1 Region. The company creates a new VPC and new subnets in us-east-1. The company establishes inter-Region VPC peering between the VPCs in the two Regions.<br><br>The company wants to give the new customer access to the SaaS service, but the company does not want to immediately deploy new EC2 resources in us-east-1.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2. Grant specific AWS accounts access to connect to the SaaS service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an NLB in us-east-1. Create an IP target group that uses the IP addresses of the company's instances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. Create an NLB in us-east-1. Associate the NLB that is in us-east-1 with an ALB target group that uses the ALB that is in eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2. In us-east-1, create an NLB and an instance target group that includes the shared EC2 instances from eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T07:45:00.000Z",
        "voteCount": 14,
        "content": "A\n\nExplanation:\n* Configuring a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2 will allow the new customer to access the SaaS service without deploying new EC2 resources in us-east-1 1.\n* Granting specific AWS accounts access to connect to the SaaS service will ensure that only authorized users can access the service 1."
      },
      {
        "date": "2024-01-07T04:01:00.000Z",
        "voteCount": 2,
        "content": "Answer is A because ... VPC peering between the VPCs in the two Regions already done &amp;  company does not want to immediately deploy new EC2 resources in us-east-1, later on company will change the architecture"
      },
      {
        "date": "2023-12-01T06:00:00.000Z",
        "voteCount": 4,
        "content": "Network Load Balancers now support connections from clients to IP-based targets in peered VPCs across different AWS Regions. Previously, access to Network Load Balancers from an inter-region peered VPC was not possible. With this launch, you can now have clients access Network Load Balancers over an inter-region peered VPC. Network Load Balancers can also load balance to IP-based targets that are deployed in an inter-region peered VPC. This support on Network Load Balancers is available in all AWS Regions. \n\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/network-load-balancer-now-supports-inter-region-vpc-peering/\n\nNLB support client from different region, I think A is correct."
      },
      {
        "date": "2023-11-30T11:49:00.000Z",
        "voteCount": 11,
        "content": "The best option among these is B. While it introduces some complexity, it's the most viable solution that aligns with AWS capabilities and the company's requirements. Creating an NLB in us-east-1 and targeting the IP addresses of the existing instances in eu-west-2 is a feasible approach. This setup allows the company to use their existing infrastructure in eu-west-2 while providing access to the customer in us-east-1 through the PrivateLink endpoint service in us-east-1. This avoids the immediate need to deploy new EC2 resources in the us-east-1 region.\n\nIt can't be A because AWS PrivateLink endpoint services cannot span regions. They are region-specific, so an endpoint service in us-east-1 cannot directly use an NLB located in eu-west-2."
      },
      {
        "date": "2024-03-08T09:23:00.000Z",
        "voteCount": 3,
        "content": "I was unable to find documentation saying that an AWS PrivateLink endpoint requires the NLB to be in the same region but if you go to the console for instance here:\n\nhttps://eu-west-1.console.aws.amazon.com/vpcconsole/home?region=eu-west-1#CreateVpcEndpointServiceConfiguration:\n\ntry to create an endpoint service and you don't have a NLB there the console explicitly states:\n\n\"No Network Load Balancers or Gateway Load Balancers available in this Region.\" so for me A in invalid"
      },
      {
        "date": "2024-04-04T07:22:00.000Z",
        "voteCount": 4,
        "content": "Wrong on part where private link support for inter region vpc peering .\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-privatelink-now-supports-access-over-inter-region-vpc-peering/"
      },
      {
        "date": "2023-12-09T09:03:00.000Z",
        "voteCount": 2,
        "content": "But the company has establishing Inter-Region VPC Peering so the endpoint would work"
      },
      {
        "date": "2024-10-10T16:29:00.000Z",
        "voteCount": 1,
        "content": "Inter-Region endpoint services\n\"Service providers can leverage a Network Load Balancer in a remote Region and create an IP target group that uses the IPs of their instance fleet in the remote Region hosting the service.\"\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/use-case-examples.html#:~:text=Inter%2DRegion%20access%20to%20endpoint%20services,-As%20customers%20expand&amp;text=Inter%2DRegion%20VPC%20peering%20traffic%20is%20transported%20over%20Amazon's%20network,costs%20between%20the%20two%20Regions."
      },
      {
        "date": "2024-08-21T12:27:00.000Z",
        "voteCount": 1,
        "content": "A is wrong, In this scenario, the existing NLB is located in the eu-west-2 Region, while the new customer is in the us-east-1 Region. PrivateLink does not support cross-Region connectivity directly. Therefore, you cannot create a PrivateLink endpoint service in us-east-1 and associate it with the NLB in eu-west-2.\n\nTo provide access to the SaaS service for the new customer in us-east-1, you need to create a load balancer (in this case, an NLB) in the us-east-1 Region and then configure a PrivateLink endpoint service in us-east-1 that uses that NLB. This NLB can then forward traffic to the instances in eu-west-2 over the inter-Region VPC peering connection, as described in the correct solution (option B)."
      },
      {
        "date": "2024-07-14T01:31:00.000Z",
        "voteCount": 2,
        "content": "Option A is not possible because a PrivateLink endpoint service in us-east-1 cannot directly use an NLB in another Region (eu-west-2)."
      },
      {
        "date": "2024-05-07T08:22:00.000Z",
        "voteCount": 1,
        "content": "a because of this https://aws.amazon.com/about-aws/whats-new/2018/10/aws-privatelink-now-supports-access-over-inter-region-vpc-peering/"
      },
      {
        "date": "2024-07-27T03:38:00.000Z",
        "voteCount": 1,
        "content": "it is accessing private endpoint from remote region, it is not possible to configure private endpoint to a nlb in the remote region."
      },
      {
        "date": "2024-08-12T04:19:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\n\nIn this case, the remote EU region is accessing US region, becuase the EU region is the SaaS, the US region is \"customer\""
      },
      {
        "date": "2024-05-03T03:41:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2024-04-25T05:59:00.000Z",
        "voteCount": 3,
        "content": "Option A :  you don't need to create a new NLB in the us-east-1.  Read  the link below for Inter-Region access to endpoint service .\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/use-case-examples.html#inter-region-endpoint-services"
      },
      {
        "date": "2024-06-23T10:10:00.000Z",
        "voteCount": 1,
        "content": "This article requires new NLB in new region which uses the instances in old region."
      },
      {
        "date": "2024-04-16T13:40:00.000Z",
        "voteCount": 2,
        "content": "A - correct."
      },
      {
        "date": "2024-04-14T22:54:00.000Z",
        "voteCount": 1,
        "content": "A. A looks to be right answer"
      },
      {
        "date": "2024-03-29T23:37:00.000Z",
        "voteCount": 2,
        "content": "AWS PrivateLink now supports access over Inter-Region VPC Peering since 2018.\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-privatelink-now-supports-access-over-inter-region-vpc-peering/"
      },
      {
        "date": "2024-03-29T03:58:00.000Z",
        "voteCount": 2,
        "content": "This is the use case: https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/use-case-examples.html#inter-region-endpoint-services"
      },
      {
        "date": "2024-03-16T07:25:00.000Z",
        "voteCount": 3,
        "content": "It is A.\nFor all those saying can not access PrivateLink endpoint service across region.\n \n\"This release makes it possible for customers to privately connect to a service even if the service endpoint resides in a different AWS Region.\"\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-privatelink-now-supports-access-over-inter-region-vpc-peering/"
      },
      {
        "date": "2024-03-02T12:01:00.000Z",
        "voteCount": 1,
        "content": "When you create  PrivateLink endpoint service in us-east-1  you also need a NLB to handle traffic flow between target NLB . So A doesn't seem to be a complete answer"
      },
      {
        "date": "2024-02-24T19:04:00.000Z",
        "voteCount": 2,
        "content": "Private link endpoint service can only use the NLB in the same region. So A is wrong."
      },
      {
        "date": "2024-02-18T06:44:00.000Z",
        "voteCount": 1,
        "content": "A:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-share-your-services.html"
      },
      {
        "date": "2024-02-17T09:28:00.000Z",
        "voteCount": 3,
        "content": "A: AWS PrivateLink endpoints can now be accessed across both intra- and inter-region VPC peering connections.\nhttps://aws.amazon.com/about-aws/whats-new/2019/03/aws-privatelink-now-supports-access-over-vpc-peering/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 362,
    "url": "https://www.examtopics.com/discussions/amazon/view/126828-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new 3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T23:40:00.000Z",
        "voteCount": 10,
        "content": "Answer: C"
      },
      {
        "date": "2024-05-31T18:33:00.000Z",
        "voteCount": 2,
        "content": "This use-case is really too rare to learn about on your own"
      },
      {
        "date": "2024-03-25T09:16:00.000Z",
        "voteCount": 4,
        "content": "Option C: Not A because the requirement is asking for  \"Least Operation Overhead\" w/ S3 Storage Lens has a default dashboard.  If you include QucikSight you are adding additional operational overhead, now you have to build your dashboard. \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_default_dashboard"
      },
      {
        "date": "2024-01-09T18:39:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-07T10:18:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nStorage Lens is a built-in S3 feature that automatically collects and aggregates storage metrics, eliminating the need for custom development or infrastructure management.\nOption A: While Storage Lens supports multiple dashboards, creating and aggregating regional dashboards in QuickSight adds complexity and maintenance overhead.\nOption B: Involves custom Lambda development, data storage in S3, Athena queries, and QuickSight integration, increasing operational complexity and costs.\nOption D: Requires EventBridge rule configuration, Lambda function development, DynamoDB table management, and QuickSight integration, adding significant overhead."
      },
      {
        "date": "2023-12-22T00:06:00.000Z",
        "voteCount": 4,
        "content": "C\nI was leaning towards A but it says in each region so that is wrong since Storage Lens gives you a view of all the regions. Someone has chosen B which is wrong b/c it has operational overhead."
      },
      {
        "date": "2023-12-13T15:45:00.000Z",
        "voteCount": 1,
        "content": "I doubt B as the question is asking for LEAST operational choice instead of Best choice. The lambda function needs developer to write code."
      },
      {
        "date": "2023-12-06T04:06:00.000Z",
        "voteCount": 1,
        "content": "Answer C.\nStorage Lens metrics include % of encrypted objects"
      },
      {
        "date": "2023-12-04T04:01:00.000Z",
        "voteCount": 3,
        "content": "Answer: C, S3 Storage Lens default=free metrics which offers encryption tracking. It's easy to set up and least overhead. https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_metrics_glossary.html"
      },
      {
        "date": "2023-11-30T11:53:00.000Z",
        "voteCount": 1,
        "content": "C is the best answer -- it's the most straightforward and involves the least operational overhead. It directly addresses the need to monitor S3 buckets and track encryption status without the need for additional setup or custom integrations. While it may not offer the same level of customization as some of the other options, it should suffice for most internal compliance requirements and is the most efficient choice in terms of minimizing operational complexity."
      },
      {
        "date": "2023-11-28T14:17:00.000Z",
        "voteCount": 1,
        "content": "Given the scenario specifics, it's the only option that answers the need to aggregate data from two regions in a dashboard for compliance teams."
      },
      {
        "date": "2023-11-29T11:22:00.000Z",
        "voteCount": 1,
        "content": "On second thought, I'm switching to option B. It appears to be the lightest between the candidates."
      },
      {
        "date": "2023-11-25T00:22:00.000Z",
        "voteCount": 2,
        "content": "B is ok.\nTo monitor a growing number of Amazon S3 buckets across two AWS Regions and track the percentage of objects that are encrypted in Amazon S3 with the least operational overhead, a solutions architect can consider the following steps:\nDeploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3.\nUse Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 363,
    "url": "https://www.examtopics.com/discussions/amazon/view/126829-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company\u2019s CISO has asked a solutions architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its application can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.<br><br>The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.<br><br>What CI/CD configuration meets all of the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T11:58:00.000Z",
        "voteCount": 6,
        "content": "B is the best choice.  Using a B/G approach aligns with the requirements for quick patch deployments and minimal downtime. In the event of an issue, the company can quickly revert to the previous version, meeting the need for a fast rollback process. This method offers a balance of speed, reliability, and safety for critical updates."
      },
      {
        "date": "2024-08-23T07:27:00.000Z",
        "voteCount": 1,
        "content": "Manual code rollback in CodeDeploy: \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#:~:text=a%20deployment%20group.-,Manual%20rollbacks,gotten%20into%20an%20unknown%20state."
      },
      {
        "date": "2024-01-09T18:45:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-13T15:47:00.000Z",
        "voteCount": 2,
        "content": "AWS like green/blue deployment for new code &amp; roll back scenario"
      },
      {
        "date": "2023-12-04T03:47:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2023-12-01T16:18:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2023-11-28T20:49:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2023-11-22T07:54:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2023-11-21T23:41:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 364,
    "url": "https://www.examtopics.com/discussions/amazon/view/126830-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is managing many AWS accounts by using an organization in AWS Organizations. Different business units in the company run applications on Amazon EC2 instances. All the EC2 instances must have a BusinessUnit tag so that the company can track the cost for each business unit.<br><br>A recent audit revealed that some instances were missing this tag. The company manually added the missing tag to the instances.<br><br>What should a solutions architect do to enforce the tagging requirement in the future?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned off. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the root of the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned on. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the organization's management account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP and attach the SCP to the root of the organization. Include the following statement in the SCP:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image8.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP and attach the SCP to the organization\u2019s management account. Include the following statement in the SCP:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image9.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "\u30a6",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-10T07:26:00.000Z",
        "voteCount": 13,
        "content": "Answer is C.  To those that are getting confused between a Management Account vs Root of the Organisation here is my two pennies: \n\nManagement Account is where you create accounts, management payments, create organisation, etc. \n\nRoot of Organisation is where you apply the policies\n\nSee: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html"
      },
      {
        "date": "2024-02-19T10:37:00.000Z",
        "voteCount": 3,
        "content": "You apply SCP in root account and tag policy in management account, but I think crucial issue here is to \"enforce the tagging requirement in the future\", only SCP can do that.\n\nhttps://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/ \n\"SCPs can be used along-side tag policies to ensure that the tags are applied at the resource creation time and remain attached to the resource.\"\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html \n\"When you sign in to the organization's management account, you use Organizations to enable the tag policies feature. [...] in the organization's management account. Then you can create tag policies and attach them to the organization entities to put those tagging rules in effect. \""
      },
      {
        "date": "2024-01-04T07:52:00.000Z",
        "voteCount": 6,
        "content": "From repost:\n * Use tag policies to prevent tagging on existing resources\n * Use SCPs to prevent tagging for creating new resources\nhttps://repost.aws/knowledge-center/organizations-scp-tag-policies\n\nWhat should a solutions architect do to enforce the tagging requirement in the future?\nYou can use SCPs to prevent the creation of new AWS resources that aren't tagged for your Organization\u2019s tagging restriction guidelines. To make sure that the AWS resources are created only if a certain tag is present, use the example SCP policy to require a tag on specified created resources: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html#example-require-tag-on-create"
      },
      {
        "date": "2024-01-04T07:53:00.000Z",
        "voteCount": 1,
        "content": "Looks like I can't post json code here, so follow the last link to find the policy"
      },
      {
        "date": "2024-07-07T05:29:00.000Z",
        "voteCount": 1,
        "content": "Option A and B is incorrect:\n\nTag policies with capitalization control provide the following regulation:\nFor example, if the \"BusinessUnit\" tag requires case sensitivity, creating resources with tags like \"BusineSSUnit\" or \"businessunit\" will fail, while creating resources with the \"Business\" tag will be allowed.\nCase sensitivity enforces rules within the same string, but does not fulfill the requirements of this question."
      },
      {
        "date": "2024-07-07T05:02:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A, rather than B.\n\nC: While this SCP would prevent instances from being created without the tag, it's a more restrictive approach than using tag policies. SCPs are better suited for broad permission management rather than enforcing tagging."
      },
      {
        "date": "2024-05-12T02:38:00.000Z",
        "voteCount": 2,
        "content": "For me it's C.\nHere we have to note that when the AWS Organization Units are mentioned, for the most we need to use SCP or RAM at the exams. Just little tips.\nA part of this, C seems most correct answer in my point of view :)"
      },
      {
        "date": "2024-04-14T23:22:00.000Z",
        "voteCount": 1,
        "content": "C. \u201ctrue\u201d: This means that the condition will evaluate to true (and thus the policy statement will be in effect) if the Project tag is not present in the request.\n\ncondition states that the policy statement is in effect when the Project tag is not included in the request. If the Project tag is present, the condition will evaluate to false"
      },
      {
        "date": "2024-03-29T23:44:00.000Z",
        "voteCount": 1,
        "content": "Tag policies take control of auto-tagging but do not \"enforce\" the tagging requirement."
      },
      {
        "date": "2024-03-25T10:49:00.000Z",
        "voteCount": 1,
        "content": "Option C -  SCP for tagging resources  \n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html#example-require-tag-on-create"
      },
      {
        "date": "2024-03-14T13:26:00.000Z",
        "voteCount": 1,
        "content": "C\nDid a recent project which is similar to this question.\nB D out since they apply to management account which is wrong.\nFor C, SCP will deny the resource creation, if it is missing the tag\nFor A, tagging policy will deny tag creation if the tag key is not matching the name\nFor this question asked, it is C\nIf question is asking that resource must be have tag key ABC=***, and can't not have tag key CBA=*** then A would be the answer.\nFor a real world restriction, you may have both A and C setup"
      },
      {
        "date": "2024-01-09T18:54:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-03T10:37:00.000Z",
        "voteCount": 1,
        "content": "After you create a tagging policy, you can put your tagging rules into effect. To do this, attach the policy to the organization root, organizational units (OUs), AWS Accounts within the organization, or a combination of organization entities.\n\nhttps://docs.aws.amazon.com/pt_br/organizations/latest/userguide/orgs_manage_policies_tag-policies-create.html\n\nOption B asks to attach the management account, but the question informs you that you have several accounts.\n\nThat's why I'll go with \"C\""
      },
      {
        "date": "2023-12-30T21:08:00.000Z",
        "voteCount": 1,
        "content": "The answer is c. Tag policies control the key and value when a tag is applied, but they cannot prevent the application of tags themselves."
      },
      {
        "date": "2023-12-30T08:26:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
      },
      {
        "date": "2023-12-30T04:57:00.000Z",
        "voteCount": 1,
        "content": "ANs :c \nhttps://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
      },
      {
        "date": "2023-12-29T03:12:00.000Z",
        "voteCount": 1,
        "content": "Implement a tag policy that specifically requires the BusinessUnit tag on EC2 instances. This policy can be enforced across the organization, ensuring that all EC2 instances carry the mandatory tag. Compliance with tag key capitalization can be turned off to allow flexibility in how the tag key is formatted. Once the policy is created, it should be attached to the root of the organization, which ensures that it is applied across all accounts within the organization."
      },
      {
        "date": "2023-12-28T22:47:00.000Z",
        "voteCount": 1,
        "content": "Use AWS Organizations to manage tag policies. When you sign in to the organization's management account, you use Organizations to enable the tag policies feature. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html\nhttps://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
      },
      {
        "date": "2024-01-20T10:12:00.000Z",
        "voteCount": 1,
        "content": "Tag Policy only enforces the accepted value of a tag, and not its presence. Therefore, users (with appropriate IAM permissions) would still be able to create untagged resources. To restrict the creation of an AWS resource without the appropriate tags, we will utilize SCPs to set guardrails around resource creation requests."
      },
      {
        "date": "2023-12-14T01:01:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 365,
    "url": "https://www.examtopics.com/discussions/amazon/view/126831-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.<br><br>A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing VPC, and associate a custom IPV6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPV6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-24T21:26:00.000Z",
        "voteCount": 10,
        "content": "Answer: C\nhttps://repost.aws/knowledge-center/configure-private-ipv6-subnet"
      },
      {
        "date": "2023-11-21T23:42:00.000Z",
        "voteCount": 6,
        "content": "Answer: C"
      },
      {
        "date": "2024-01-09T19:07:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-12-22T09:57:00.000Z",
        "voteCount": 3,
        "content": "C https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html"
      },
      {
        "date": "2023-12-13T15:59:00.000Z",
        "voteCount": 6,
        "content": "IPV6 can only be used by Engress only gateway"
      },
      {
        "date": "2023-12-10T07:28:00.000Z",
        "voteCount": 2,
        "content": "IP6 --&gt; Egress GW"
      },
      {
        "date": "2023-12-04T03:25:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2023-11-28T21:11:00.000Z",
        "voteCount": 6,
        "content": "Answer C. No NAT gateway for IPv6 subnets. Only Egress-only Internet gateway to allow only outbound traffic from private subnets."
      },
      {
        "date": "2023-11-22T08:48:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\n\nExplanation:\n* Updating the existing VPC and associating an Amazon-provided IPv6 CIDR block with the VPC and all subnets will enable the EC2 instances to use IPv6 \n* Updating the VPC route tables for all private subnets and adding a route for ::/0 to the NAT gateway will ensure that the EC2 instances that are in private subnets are not accessible from the public internet"
      },
      {
        "date": "2023-12-21T19:04:00.000Z",
        "voteCount": 2,
        "content": "NAT gateway does not support IPv6.\nYou should use egress-only internet gateway in-place of NAT gateway for IPv6.\nhttps://repost.aws/knowledge-center/configure-private-ipv6-subnet"
      },
      {
        "date": "2024-01-07T08:59:00.000Z",
        "voteCount": 1,
        "content": "My Answer is C because of ease and cost effective... NAT gateway do support IPv6 indirectly which is NAT64 and DNS64 provide a workaround for IPv6-to-IPv4 communication\nhttps://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-nat64-dns64.html"
      },
      {
        "date": "2024-01-20T10:26:00.000Z",
        "voteCount": 3,
        "content": "Be careful ! This guy gives wrong answers on purpose..."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 366,
    "url": "https://www.examtopics.com/discussions/amazon/view/126832-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using Amazon API Gateway to deploy a private REST API that will provide access to sensitive data. The API must be accessible only from an application that is deployed in a VPC. The company deploys the API successfully. However, the API is not accessible from an Amazon EC2 instance that is deployed in the VPC.<br><br>Which solution will provide connectivity between the EC2 instance and the API?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows apigateway:* actions. Disable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC. Use the VPC endpoint's DNS name to access the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows the execute-api:Invoke action. Enable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC endpoint. Use the API endpoint\u2019s DNS names to access the API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) and a VPC link. Configure private integration between API Gateway and the NLB. Use the API endpoint\u2019s DNS names to access the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) and a VPC Link. Configure private integration between API Gateway and the ALB. Use the ALB endpoint\u2019s DNS name to access the API."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T23:43:00.000Z",
        "voteCount": 6,
        "content": "Answer: B"
      },
      {
        "date": "2024-04-14T23:38:00.000Z",
        "voteCount": 2,
        "content": "C. Why not C here ?"
      },
      {
        "date": "2024-01-09T19:14:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-10T07:30:00.000Z",
        "voteCount": 3,
        "content": "Answer B. Enable Private naming for VPC Endpoint"
      },
      {
        "date": "2023-12-06T04:19:00.000Z",
        "voteCount": 3,
        "content": "Answer B. Enable Private naming for VPC Endpoint"
      },
      {
        "date": "2023-12-05T03:38:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 367,
    "url": "https://www.examtopics.com/discussions/amazon/view/126833-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large payroll company recently merged with a small staffing company. The unified company now has multiple business units, each with its own existing AWS account.<br><br>A solutions architect must ensure that the company can centrally manage the billing and access policies for all the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to all member accounts of the company from a centralized management account.<br><br>What should the solutions architect do next to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T14:52:00.000Z",
        "voteCount": 8,
        "content": "C is the Answer:\nThis setup enables centralized management of member accounts from the management account. Administrators in the management account can assume the OrganizationAccountAccessRole in member accounts to perform necessary actions, aligning with AWS best practices for Organizations. It simplifies the management and auditing of various accounts and ensures a standardized role exists across all accounts for consistent access control."
      },
      {
        "date": "2023-12-22T10:05:00.000Z",
        "voteCount": 5,
        "content": "C https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role"
      },
      {
        "date": "2024-01-06T15:48:00.000Z",
        "voteCount": 2,
        "content": "Thank you!"
      },
      {
        "date": "2024-03-09T22:50:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-30T06:43:00.000Z",
        "voteCount": 1,
        "content": "Is it possible C ?  Role in the each member account and management account just grant assume the role. How to implement it? @@"
      },
      {
        "date": "2023-12-10T07:35:00.000Z",
        "voteCount": 3,
        "content": "See: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html"
      },
      {
        "date": "2023-12-04T03:01:00.000Z",
        "voteCount": 2,
        "content": "Answer: C"
      },
      {
        "date": "2023-11-28T21:19:00.000Z",
        "voteCount": 3,
        "content": "OrganizationAccountAccessRole is created in the member accounts and this role can be assumed by IAM users in the Management account to perform any actions in member accounts. Answer C."
      },
      {
        "date": "2023-11-24T23:05:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nhttps://fullbacksystems.com/aws_organizations/"
      },
      {
        "date": "2023-11-24T08:43:00.000Z",
        "voteCount": 2,
        "content": "Answer D. Be is not correct\n\nTo centrally manage the billing and access policies for all the AWS accounts of a company that has multiple business units, each with its own existing AWS account, the following steps can be taken:\n1.Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.\n2.Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account"
      },
      {
        "date": "2023-11-22T08:58:00.000Z",
        "voteCount": 2,
        "content": "Option B is the correct solution because it creates the OrganizationAccountAccessPolicy IAM policy in each member account and connects the member accounts to the management account by using cross-account access. This will ensure that the company can centrally manage the billing and access policies for all the AWS accounts."
      },
      {
        "date": "2023-11-21T23:43:00.000Z",
        "voteCount": 3,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 368,
    "url": "https://www.examtopics.com/discussions/amazon/view/126834-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.<br><br>What changes to the current architecture will reduce operational overhead and support the product release?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-04T02:57:00.000Z",
        "voteCount": 7,
        "content": "Option D with Fargate can potentially provide a more serverless-like experience, emphasizing ease of use and reduced operational responsibilities"
      },
      {
        "date": "2024-06-03T08:02:00.000Z",
        "voteCount": 1,
        "content": "It's not specified if there are in pipe a refactoring o rearchitecting and how many time you have to rearchitect before going to prod. I will go with B because D should increment the delay to go in prod; instead with B it's immediate the production deploy."
      },
      {
        "date": "2024-03-09T22:55:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2024-01-29T17:56:00.000Z",
        "voteCount": 1,
        "content": "Why do you need fargate when you are hosting on kubernetes"
      },
      {
        "date": "2024-01-29T18:01:00.000Z",
        "voteCount": 3,
        "content": "Modified my answer to D. Fargate  will handle unexpected load."
      },
      {
        "date": "2024-01-10T19:38:00.000Z",
        "voteCount": 2,
        "content": "cloudfront for static content. \nThen aws kubernetes over kubernetes on ec2."
      },
      {
        "date": "2023-12-22T10:11:00.000Z",
        "voteCount": 1,
        "content": "My answer is C. I don't agree with D \"significant increase of orders \" means more data, read replicas will not resolve this"
      },
      {
        "date": "2024-01-04T10:58:00.000Z",
        "voteCount": 1,
        "content": "On C, the number of EC2 instances is fixed, so can't provide elasticity beyond this limit. Could be another history if ASG was mentioned."
      },
      {
        "date": "2023-11-28T21:24:00.000Z",
        "voteCount": 3,
        "content": "Answer - D"
      },
      {
        "date": "2023-11-22T09:02:00.000Z",
        "voteCount": 3,
        "content": "D for sure"
      },
      {
        "date": "2023-11-21T23:44:00.000Z",
        "voteCount": 3,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 369,
    "url": "https://www.examtopics.com/discussions/amazon/view/126835-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to access files in their Windows home directories. Recently, there has been a large growth in the number of employees who work remotely. As a result, bandwidth usage for connections into the data center has begun to reach 100% during business hours.<br><br>The company must design a solution on AWS that will support the growth of the company's remote workforce, reduce the bandwidth usage for connections into the data center, and reduce operational overhead.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the home directories to Amazon FSx for Windows File Server.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the home directories to Amazon FSx for Lustre.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate remote users to AWS Client VPN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Direct Connect connection from the on-premises data center to AWS."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-10T06:53:00.000Z",
        "voteCount": 1,
        "content": "Option B and D"
      },
      {
        "date": "2023-12-22T10:16:00.000Z",
        "voteCount": 1,
        "content": "BC For Migrating existing file storage to FSx for Windows File Server is needed Direct Connect  https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-fsx.html"
      },
      {
        "date": "2023-12-27T08:36:00.000Z",
        "voteCount": 2,
        "content": "I mean BE"
      },
      {
        "date": "2023-12-10T07:43:00.000Z",
        "voteCount": 2,
        "content": "Agreed: Answer: B &amp; D"
      },
      {
        "date": "2023-12-09T07:05:00.000Z",
        "voteCount": 1,
        "content": "Why not direct connect ? the question did not mention about cost but rather it mentions \"reduce the bandwidth usage for connections into the data center\" . any thoughts ?"
      },
      {
        "date": "2023-12-22T02:37:00.000Z",
        "voteCount": 7,
        "content": "Key is \"... a large growth in the number of employees who work remotely.\" These users are connecting from home to their data centre over VPN. They now need to be diverted to AWS. It therefore makes sense VPN Client here, not DX"
      },
      {
        "date": "2024-01-05T13:56:00.000Z",
        "voteCount": 2,
        "content": "Agree...My answer is B&amp;D\nAWS Client VPN allows remote users to securely connect to AWS resources, including Amazon FSx for Windows File Server, without the need for a VPN connection to the on-premises data center. Migrating remote users to AWS Client VPN can help reduce bandwidth usage for connections into the on-premises data center, as users will access resources directly from AWS. This approach is more scalable and can be managed with less operational overhead compared to maintaining a VPN infrastructure in the on-premises data center."
      },
      {
        "date": "2023-12-04T02:40:00.000Z",
        "voteCount": 2,
        "content": "Answer: B &amp; D"
      },
      {
        "date": "2023-11-28T21:26:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D are correct"
      },
      {
        "date": "2023-11-22T09:06:00.000Z",
        "voteCount": 2,
        "content": "BD for sure"
      },
      {
        "date": "2023-11-21T23:45:00.000Z",
        "voteCount": 2,
        "content": "Answer: BD"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 370,
    "url": "https://www.examtopics.com/discussions/amazon/view/126836-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.<br><br>A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-03T21:56:00.000Z",
        "voteCount": 6,
        "content": "A: strongly recommended controls - detects whether the Amazon EBS volumes attached to an Amazon EC2 instance are encrypted\nC: Best way to encrypt an unencrypted volume"
      },
      {
        "date": "2023-12-09T08:29:00.000Z",
        "voteCount": 5,
        "content": "the appropriate guardrail is: A\nStrongly recommended guardrail: Detect Whether Encryption is Enabled for Amazon EBS Volumes Attached to Amazon EC2 Instances.\n\nThis guardrail continuously monitors your environment and detects any EC2 instances with unencrypted EBS volumes attached"
      },
      {
        "date": "2024-07-09T01:31:00.000Z",
        "voteCount": 2,
        "content": "A and C are correct according to \n\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption"
      },
      {
        "date": "2024-02-01T22:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption"
      },
      {
        "date": "2024-01-10T07:06:00.000Z",
        "voteCount": 1,
        "content": "Option A &amp; C"
      },
      {
        "date": "2023-12-10T07:45:00.000Z",
        "voteCount": 1,
        "content": "Answer A+C"
      },
      {
        "date": "2023-12-06T04:47:00.000Z",
        "voteCount": 2,
        "content": "Answer AC"
      },
      {
        "date": "2023-11-29T21:50:00.000Z",
        "voteCount": 3,
        "content": "AC for sure. Unencrypted EBS detection is part of strongly recommended guardrails, and you cannot encrypt a volume or snapshot in place. You need to create a new encrypted volume from an unencrypted snapshot, and attach it to the instance."
      },
      {
        "date": "2023-11-28T21:32:00.000Z",
        "voteCount": 3,
        "content": "\"and ensure that unencrypted volumes will be detected automatically in the future. \" - to automatically detect unencrypted volumes, we need CloudTrail and Eventbridge to detect and encrypt unencrypted volumes automatically."
      },
      {
        "date": "2023-12-06T04:47:00.000Z",
        "voteCount": 2,
        "content": "Changing to A&amp;C."
      },
      {
        "date": "2023-11-28T21:04:00.000Z",
        "voteCount": 2,
        "content": "\"...centrally manage multiple AWS accounts with a focus on compliance and security\", and \"...ensure that unencrypted volumes will be detected automatically...\""
      },
      {
        "date": "2023-11-22T09:14:00.000Z",
        "voteCount": 2,
        "content": "BD for sure"
      },
      {
        "date": "2023-11-24T08:23:00.000Z",
        "voteCount": 1,
        "content": "Change to BE\n\nCreating an organization in AWS Organizations, setting up AWS Control Tower, and turning on the mandatory controls (guardrails) (Option D) is not required since the strongly recommended controls (guardrails) are sufficient"
      },
      {
        "date": "2023-11-21T23:46:00.000Z",
        "voteCount": 4,
        "content": "Answer: A C"
      },
      {
        "date": "2023-11-24T08:19:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html\nCreating a snapshot of each unencrypted volume, creating a new encrypted volume from the unencrypted snapshot, detaching the existing volume, and replacing it with the encrypted volume (Option C) is not required since the volumes can be encrypted in place"
      },
      {
        "date": "2023-11-25T15:03:00.000Z",
        "voteCount": 1,
        "content": "The volumes can not be encrypted in place -- see the steps (copy/pasted from the link you shared):\n1. AWS Config detects an unencrypted EBS volume.\n2. An administrator uses AWS Config to send a remediation command to Systems Manager.\n3. The Systems Manager automation takes a snapshot of the unencrypted EBS volume.\n4. The Systems Manager automation uses AWS KMS to create an encrypted copy of the snapshot.\n5. The Systems Manager automation does the following: Stops the affected EC2 instance if it is running. Attaches the new, encrypted copy of the volume to the EC2 instance. Returns the EC2 instance to its original state.\n\nAlso, under the Limitations section: \"When you remediate existing, unencrypted EBS volumes, ensure that the EC2 instance is not in use. This automation shuts down the instance in order to detach the unencrypted volume and attach the encrypted one. There is downtime while the remediation is in progress.\""
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 371,
    "url": "https://www.examtopics.com/discussions/amazon/view/126928-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.<br><br>The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (ldP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALSpecify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the directory as a new IAM identity provider (ldP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the ldP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (ldP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T03:04:00.000Z",
        "voteCount": 10,
        "content": "D is complete nonsense. Don't know why so many people are voting for it. \n\n\"Configure a role policy that allows access to the ALB\" - Come on, guys. ALB is accessed via http or https. You can restrict access via security groups not roles. Also cognito is mentioned in D but cognito is not connected to to the SAML provider. So B is the correct answer."
      },
      {
        "date": "2023-12-19T12:55:00.000Z",
        "voteCount": 8,
        "content": "There are two options either via Cognito or Auth0 and then attach an IDP to one of them. \n\nSee: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\n\nhttps://aws.amazon.com/blogs/aws/built-in-authentication-in-alb/"
      },
      {
        "date": "2024-09-11T18:32:00.000Z",
        "voteCount": 1,
        "content": "Option B focuses more on user identity management for web and mobile applications, providing rich user management features and flexible authentication processes. If the company's main requirement is to manage user identities and data for applications, then option B may be more appropriate.\nOption D focuses more on providing single sign on access to AWS accounts and applications for organizational employees, as well as integration with external identity providers through SAML. If the company wishes to integrate its existing identity management system (such as Microsoft Active Directory) with AWS accounts and applications, and wants employees to easily access these resources, then option D may be more appropriate."
      },
      {
        "date": "2024-08-12T16:50:00.000Z",
        "voteCount": 2,
        "content": "IAM Identity Center is primarily designed for single sign-on (SSO) access to AWS accounts, applications, and services that are integrated with AWS. It provides centralized identity management for users accessing these resources.\nIn contrast, the requirement here is for web application authentication directly tied to an intranet web application hosted on EC2 instances, not for general access to AWS resources."
      },
      {
        "date": "2024-07-09T01:19:00.000Z",
        "voteCount": 1,
        "content": "ALB Authenticate users through corporate identities, using SAML, OpenID Connect (OIDC), or OAuth, through the user pools supported by Amazon Cognito.\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html"
      },
      {
        "date": "2024-06-01T18:36:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-05-03T04:18:00.000Z",
        "voteCount": 2,
        "content": "B vote"
      },
      {
        "date": "2024-05-03T04:18:00.000Z",
        "voteCount": 2,
        "content": "B vote"
      },
      {
        "date": "2024-05-03T04:17:00.000Z",
        "voteCount": 1,
        "content": "I vote for B"
      },
      {
        "date": "2024-04-25T06:53:00.000Z",
        "voteCount": 1,
        "content": "Option D:  Per AWS doc \"  An Amazon Cognito user pool is a user directory for web and mobile app authentication and authorization. \" .  The question states \"  The company hosts an intranet web application\".  So, you can't select Cognito\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html"
      },
      {
        "date": "2024-04-01T10:23:00.000Z",
        "voteCount": 5,
        "content": "A: The Active Directory directory does not use OIDC. \nB: Make sense. \nC: Cannot add the directory as a new IAM IdP. \nD: Why \"authenticate-cognito action\""
      },
      {
        "date": "2024-03-20T08:51:00.000Z",
        "voteCount": 5,
        "content": "A: Doesn't support OIDC directly.\nB: ALBs can interface directly to Cognito. The correct answer.\nC: Rubbish, as IAM doesn't directly interface to any AD.\nD: Mixes things up royally."
      },
      {
        "date": "2024-03-17T08:25:00.000Z",
        "voteCount": 1,
        "content": "Attach the new role to all groups ???"
      },
      {
        "date": "2024-03-09T23:24:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2024-01-30T22:59:00.000Z",
        "voteCount": 2,
        "content": "refer to below. \n\n46\n\nI am on the Amazon Cognito team.\n\nAmazon Cognito is our identity management solution for developers building B2C or B2B apps for their customers, which makes it a customer-targeted IAM and user directory solution.\n\nAWS SSO is focused on SSO for employees accessing AWS and business apps, initially with Microsoft AD as the underlying employee directory.\n\nWe plan to integrate Cognito User Pools and AWS SSO as part of our roadmap."
      },
      {
        "date": "2024-01-30T22:48:00.000Z",
        "voteCount": 4,
        "content": "They have already AD so we have to use SSO."
      },
      {
        "date": "2023-12-25T04:20:00.000Z",
        "voteCount": 7,
        "content": "If the question were an internet web application I would go with B but as the question says it is an intranet application and internal database I would go with D, I don't think Cognito is the best answer."
      },
      {
        "date": "2024-01-04T09:39:00.000Z",
        "voteCount": 4,
        "content": "The scenario says it in this part \"The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory\". For this reason, Cognito is the best option"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 372,
    "url": "https://www.examtopics.com/discussions/amazon/view/126932-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a website that serves many visitors. The company deploys a backend service for the website in a primary AWS Region and a disaster recovery (DR) Region.<br><br>A single Amazon CloudFront distribution is deployed for the website. The company creates an Amazon Route 53 record set with health checks and a failover routing policy for the primary Region\u2019s backend service. The company configures the Route 53 record set as an origin for the CloudFront distribution. The company configures another record set that points to the backend service's endpoint in the DR Region as a secondary failover record type. The TTL for both record sets is 60 seconds.<br><br>Currently, failover takes more than 1 minute. A solutions architect must design a solution that will provide the fastest failover time.<br><br>Which solution will achieve this goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the TTL to 4 second for the existing Route 53 record sets that are used for the backend service in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-10T08:44:00.000Z",
        "voteCount": 9,
        "content": "In summary, CloudFront Origin Failover fails over immediately when it detects a failure from the origin. However, it may also introduce latency as it tries to forward every request to the primary origin first.\n\nRoute53 DNS Failover offers more stability, but it requires more time to detect failure from the origin. However, you can combine both solutions to increase availability without affecting performance. See: https://aws.amazon.com/blogs/networking-and-content-delivery/improve-web-application-availability-with-cloudfront-and-route53-hybrid-origin-failover/#:~:text=In%20summary%2C%20CloudFront%20Origin%20Failover,detect%20failure%20from%20the%20origin."
      },
      {
        "date": "2023-11-29T07:21:00.000Z",
        "voteCount": 5,
        "content": "Answer - D. Create a Cloud origin group with both Primary and DR origin and configure Origin failover in the Cache behavior. \nReducing TTL might impact performance as all or most of the requests will be authoritative and place heavy load on DNS."
      },
      {
        "date": "2024-01-10T08:15:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-12-22T10:29:00.000Z",
        "voteCount": 1,
        "content": "D see:https://aws.amazon.com/blogs/networking-and-content-delivery/improve-web-application-availability-with-cloudfront-and-route53-hybrid-origin-failover/"
      },
      {
        "date": "2023-11-22T09:28:00.000Z",
        "voteCount": 4,
        "content": "D for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 373,
    "url": "https://www.examtopics.com/discussions/amazon/view/126933-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also they would like to administer multiple accounts together as a single unit.<br><br>What combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Deny list strategy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Access Advisor in AWS IAM to determine services recently used\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the AWS Trusted Advisor report to determine services recently used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the default FullAWSAccess SCP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine organizational units (OUs) and place the member accounts in the OUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the default DenyAWSAccess SCP."
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T14:51:00.000Z",
        "voteCount": 8,
        "content": "ABE is the answer:\nA: This approach involves explicitly denying access to specific AWS services that the company wants to restrict. It allows all other services to be accessible, which aligns with the company's requirement to allow services that are currently in use. \n\nB: AWS IAM Access Advisor shows the service permissions granted to a user and when those services were last accessed. This information is valuable to understand which AWS services are actively used and which are not, helping to make informed decisions about which services to restrict.\n\nE: Organizational Units allow for grouping AWS accounts that have similar needs or requirements. This structure enables the solutions architect to apply policies at the OU level, making it easier to manage permissions and restrictions across multiple accounts."
      },
      {
        "date": "2023-11-30T14:52:00.000Z",
        "voteCount": 4,
        "content": "Also: it shouldn't be D because the FullAWSAccess SCP allows all actions on all resources in the account. Removing it without a carefully crafted replacement policy can lead to unintended access restrictions."
      },
      {
        "date": "2024-01-05T11:59:00.000Z",
        "voteCount": 1,
        "content": "No...explicitly deny access/explicit Deny Statements to specific actions or resources, effectively override FullAWSAccess"
      },
      {
        "date": "2024-01-05T12:00:00.000Z",
        "voteCount": 1,
        "content": "Saying the above statement my answer is E B A in the order."
      },
      {
        "date": "2024-01-05T12:05:00.000Z",
        "voteCount": 1,
        "content": "Order of Evaluation\n-------------------\nExplicit deny statements in IAM policies or SCPs take precedence over everything else.\nIf no explicit denies exist, AWS evaluates policies in this order: Service-Linked Roles &gt; Resource-Based Policies &gt; IAM Policies (including FullAWSAccess) &gt; SCPs &gt; Conditional Access Policies"
      },
      {
        "date": "2024-01-05T12:10:00.000Z",
        "voteCount": 1,
        "content": "I mean YES... throwing some light on the permissions evaluation.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html"
      },
      {
        "date": "2024-04-18T02:46:00.000Z",
        "voteCount": 2,
        "content": "Isn't this called IAM Access Analyzer instead of Advisor?"
      },
      {
        "date": "2024-01-21T11:14:00.000Z",
        "voteCount": 1,
        "content": "With a deny list strategy a default SCP allows all services and deny lists must be implemented for any specific services that must be restricted."
      },
      {
        "date": "2024-01-10T08:25:00.000Z",
        "voteCount": 1,
        "content": "A, B and E"
      },
      {
        "date": "2023-12-10T08:49:00.000Z",
        "voteCount": 2,
        "content": "Agreed E+B+A in that order :)"
      },
      {
        "date": "2023-11-29T08:42:00.000Z",
        "voteCount": 1,
        "content": "manage as single unit -&gt;OU's is out of scope (answer e)\n\ndeny some of the AWS services -&gt; remove the default FullAWSAcces \nallow current in use services -&gt; access advisor to determine recently used services\nUse deny list strategy to allow only services that are required\n\nleaves only valid answer: ABD"
      },
      {
        "date": "2023-12-04T02:06:00.000Z",
        "voteCount": 3,
        "content": "I have to rectify one answer, \nYou can use organizational units (OUs) to group accounts together to administer as a single unit. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\nSo E is correct, D is incorrect\n\nAnswer must be ABE"
      },
      {
        "date": "2023-11-29T07:30:00.000Z",
        "voteCount": 1,
        "content": "To administer multiple accounts together as a single unit - Create OU's with member accounts\nRemove blanket Allow on OUs - Remove the default FullAWSAccess  SCP from OU's\nReview Access Advisor to view which services have been in use or accessed by users / roles\n\nAnswer BDE"
      },
      {
        "date": "2023-11-29T07:31:00.000Z",
        "voteCount": 2,
        "content": "There is no DenyAWSAccess SCP created by default on OUs during creation."
      },
      {
        "date": "2023-12-06T05:01:00.000Z",
        "voteCount": 2,
        "content": "correction - ABE\nD is wrong, removal of FullAccessSCP without replacing it with a custom SCP is not correct.\nA is correct, using a Deny list to restrict access to specific services"
      },
      {
        "date": "2023-11-22T09:33:00.000Z",
        "voteCount": 1,
        "content": "ABE for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 374,
    "url": "https://www.examtopics.com/discussions/amazon/view/126837-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.<br><br>The company needs a scaling solution to maximize availability during the sale events.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Mulli-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T14:56:00.000Z",
        "voteCount": 8,
        "content": "D is the best answer.\n\nIt leverages scheduled scaling for EC2 instances, which is ideal for handling predictable, high-traffic event peaks. Amazon Aurora PostgreSQL is a high-performance database solution that provides the reliability needed for such critical operations. The use of a larger Aurora Replica during the event and scaling down afterward allows for efficient resource utilization, aligning the database capacity with the fluctuating demand.\n\nWhile it introduces some complexity in terms of manual replica management, this approach offers a good balance between performance, reliability, and cost-effectiveness, making it well-suited for the described scenario."
      },
      {
        "date": "2023-11-21T23:50:00.000Z",
        "voteCount": 5,
        "content": "Answer: D"
      },
      {
        "date": "2024-06-29T04:33:00.000Z",
        "voteCount": 1,
        "content": "D for sure."
      },
      {
        "date": "2024-02-10T20:27:00.000Z",
        "voteCount": 2,
        "content": "key point is  Create an Amazon EventBridge - Monitor Application Auto Scaling events with Amazon EventBridge \nAmazon EventBridge, formerly called CloudWatch Events, helps you monitor events that are specific to Application Auto Scaling and initiate target actions that use other AWS services. Events from AWS services are delivered to EventBridge in near real time.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/monitoring-eventbridge.html"
      },
      {
        "date": "2024-01-10T16:21:00.000Z",
        "voteCount": 4,
        "content": "Bad question design. \nAurora support auto scaling, so the answer should have Aurora autoscaling. But the predictive scaling for ASG in A and C is obviously wrong. And B is using Lambda function to fail over while Aurora already has this feature. Which leaves D the only possible answer. \nWho the hell designed this stupid answers."
      },
      {
        "date": "2024-01-15T20:25:00.000Z",
        "voteCount": 3,
        "content": "Aurora auto scaling requires some time to adjust, and cannot handle sudden spikes in traffic.\nAuto scaling is more suitable for gradually increasing traffic."
      },
      {
        "date": "2024-01-10T08:30:00.000Z",
        "voteCount": 1,
        "content": "B or D are the possible choices. D is better choice as it uses Aurora engine that has better availability and scaling performance."
      },
      {
        "date": "2023-12-03T20:56:00.000Z",
        "voteCount": 2,
        "content": "leverages scheduled scaling and Aurora PostgreSQL is high-performance database"
      },
      {
        "date": "2023-11-22T09:40:00.000Z",
        "voteCount": 4,
        "content": "Answer D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 375,
    "url": "https://www.examtopics.com/discussions/amazon/view/126838-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution.<br><br>The company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuring configuration of the replication servers, select the option to use private IP addresses for data replication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuring configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance\u2019s private IP address matches the source server's private IP address."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "DEF",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BDE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T15:30:00.000Z",
        "voteCount": 8,
        "content": "ADE\n\nOption D: Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.\nOption E: During configuration of the replication servers, select the option to use private IP addresses for data replication.\nOption A: could be considered if the private subnets are used without the NAT gateways, ensuring internal-only network access"
      },
      {
        "date": "2024-01-03T11:25:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/drs/latest/userguide/quick-start-guide-gs.html\n\n(E) Data routing and throttling controls how data flows from the external server to the replication servers. If you choose not to use a private IP, your replication servers will be automatically assigned a public IP and data will flow over the public internet. Check \"Use private IP for data replication\".\n\n(F) On Default DRS launch settings, check \"Copy private IP\". This way all other servers can transparently reach the recovered server.\n\n(D) Architects could use VPN or AWS DC, but \"...The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.\", preferably use AWS Direct Connect."
      },
      {
        "date": "2024-10-14T09:24:00.000Z",
        "voteCount": 1,
        "content": "Why it cannot be the following:\n\n\u2022\tA. Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway. - NAT Gateway not necessary\n\u2022\tB. Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway. - IGW not required\n\u2022\tC. Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network. - You need bandwidth so that teh solution does not impact other applications"
      },
      {
        "date": "2024-09-27T10:38:00.000Z",
        "voteCount": 2,
        "content": "Direct Connect is an overkill for such a solution. You cant set it all up just to do DR."
      },
      {
        "date": "2024-09-05T23:40:00.000Z",
        "voteCount": 1,
        "content": "Regarding Option A, I'm not sure why there should be at least 2 subnets in the VPC. When configuring the Elastic Disaster Recovery, you only need to choose 1 subnet as target area. Besides, NAT is not needed here. \nFor Option F, you can choose \"Copy private IP\" to match source server's IP address, but this is not a must, it is an optional choice, you don't need to choose it to meet the question's requirement.\nI'm really confused"
      },
      {
        "date": "2024-08-20T12:52:00.000Z",
        "voteCount": 1,
        "content": "Those who picked A, why would you need the NAT gateways?!"
      },
      {
        "date": "2024-08-12T18:57:00.000Z",
        "voteCount": 1,
        "content": "I am super confused about A\n\nA says Virtual Private Gateway, which is for Site-to-Site VPNs. Why do we need this ???"
      },
      {
        "date": "2024-07-09T00:58:00.000Z",
        "voteCount": 1,
        "content": "replication traffic does not travel through the public internet. --&gt; Not A \nmust not be accessible from the internet --&gt; Not B\nThe company does not want this solution to consume all available network bandwidth --&gt; not C, it requires D as dedicated network\nE and F during the Disaster Recovery step 3 and 4 as described as link below,\nhttps://docs.aws.amazon.com/drs/latest/userguide/quick-start-guide-gs.html"
      },
      {
        "date": "2024-01-31T02:21:00.000Z",
        "voteCount": 3,
        "content": "We don't need to connect internet, why we need NAT gateway in A?"
      },
      {
        "date": "2024-04-13T03:55:00.000Z",
        "voteCount": 2,
        "content": "the question says not accessible from internet \nNAT gateway is for inbound to internet and not internet -&gt; inbound"
      },
      {
        "date": "2024-02-14T12:32:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/drs/latest/userguide/Network-Requirements.html\nThere are two ways to establish direct connectivity to the Internet for the VPC of the staging area, as described in the VPC FAQ\n1. Public IP address + Internet gateway\n2. Private IP address + NAT instance"
      },
      {
        "date": "2024-02-14T12:35:00.000Z",
        "voteCount": 1,
        "content": "Thats the only info I found, however this doesn't exactly answer your question."
      },
      {
        "date": "2024-01-24T04:10:00.000Z",
        "voteCount": 4,
        "content": "How about A,C,E?\nA. Create an intranet application and other application in a private subnet.\nIntranet applications connect to a private gateway(one).\nOther applications connect to the NAT gateway(one).\nEliminates traffic interference.\nC. Site-to-Site VPN connect to private gateway.\nE. Replicates private IP."
      },
      {
        "date": "2024-01-26T06:55:00.000Z",
        "voteCount": 1,
        "content": "Can not backup for other application through Site-to-Site VPN.\nIt is correct Option D. 'Direct Connect gateway'\nA, D, E"
      },
      {
        "date": "2024-01-24T04:41:00.000Z",
        "voteCount": 1,
        "content": "Can other applications communicate with the Internet through the NAT gateway?"
      },
      {
        "date": "2024-01-10T08:37:00.000Z",
        "voteCount": 2,
        "content": "A, D and E"
      },
      {
        "date": "2023-12-22T10:39:00.000Z",
        "voteCount": 1,
        "content": "Answer ADE"
      },
      {
        "date": "2023-12-06T05:31:00.000Z",
        "voteCount": 2,
        "content": "Answer ADE"
      },
      {
        "date": "2023-12-03T20:45:00.000Z",
        "voteCount": 4,
        "content": "DX is needed as it Provides a dedicated, private network connection that can be managed to avoid consuming all available network bandwidth"
      },
      {
        "date": "2023-12-01T03:41:00.000Z",
        "voteCount": 1,
        "content": "Not Option - A, I don't see the point of creating NAT gateways."
      },
      {
        "date": "2023-12-03T01:11:00.000Z",
        "voteCount": 1,
        "content": "mb, answer should A,D,E"
      },
      {
        "date": "2023-11-29T08:59:00.000Z",
        "voteCount": 1,
        "content": "Answer - ACE \nVPC with 2 private subnets and 2 NAT gateways for application and replication traffic which has to be private\nSite to Site VPN - for secure connection between Onprem and Customer VPC so both replication and application traffic does not flow over public internet\nChoosing private IP address for replication."
      },
      {
        "date": "2023-12-06T05:31:00.000Z",
        "voteCount": 1,
        "content": "Correction - ADE\nDirect Connect needed for this solution. VPN is not needed"
      },
      {
        "date": "2023-11-29T09:00:00.000Z",
        "voteCount": 1,
        "content": "Direct connect not needed as there is no ask for a dedicated connection  or high speed."
      },
      {
        "date": "2023-11-30T15:00:00.000Z",
        "voteCount": 3,
        "content": "Question states: \"The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.\"\n\nUsage of a VPN relies on the companies bandwidth and could very easily consume most of it. They'd need a dedicated connection (aka Direct Connect) to meet this requirement."
      },
      {
        "date": "2023-11-27T02:08:00.000Z",
        "voteCount": 1,
        "content": "I guess ADE"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 376,
    "url": "https://www.examtopics.com/discussions/amazon/view/126839-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that provides image storage services wants to deploy a customer-facing solution to AWS. Millions of individual customers will use the solution. The solution will receive batches of large image files, resize the files, and store the files in an Amazon S3 bucket for up to 6 months.<br><br>The solution must handle significant variance in demand. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Step Functions to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge to process the S3 event that occurs when a user uploads an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Event Notifications to invoke an AWS Lambda function when a user stores an image. Use the Lambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "\u30a4",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T15:23:00.000Z",
        "voteCount": 14,
        "content": "Considering the requirements, Option B (Amazon EventBridge with AWS Lambda and S3 Lifecycle Expiration Policy) seems to be the most cost-effective and appropriate solution. It combines the scalability and flexibility of AWS Lambda for image processing with the straightforward event handling of Amazon EventBridge, and appropriately manages the image lifecycle with an S3 expiration policy. While Option C is also a strong contender, the misalignment of the lifecycle policy with the requirement makes Option B a better fit. Option A might be more suitable for complex workflows but is likely not needed for this scenario, and Option D includes unnecessary long-term archival steps."
      },
      {
        "date": "2024-08-12T20:49:00.000Z",
        "voteCount": 2,
        "content": "How do you rerun for failure with option B?\n\nSQS can handle \"rerun\", hence D"
      },
      {
        "date": "2023-12-22T10:44:00.000Z",
        "voteCount": 11,
        "content": "B is for sure\nA no because Step Function is not in list of s3 event destinations https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html\nC and D has option for storing data longer than 6 months which is not required"
      },
      {
        "date": "2024-10-14T09:35:00.000Z",
        "voteCount": 1,
        "content": "Yes it is ....\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html"
      },
      {
        "date": "2024-09-24T23:21:00.000Z",
        "voteCount": 2,
        "content": "\"ability to rerun processing jobs in the event of failure\""
      },
      {
        "date": "2024-08-19T21:42:00.000Z",
        "voteCount": 3,
        "content": "Let\u2019s break down the question into some decisive pieces:\n1.Millions of customers will use solution \uf0e0 this to me has to be a robust queuing solution (like SQS, not eventbridge, not step-function)\n2.Store the files in an Amazon S3 bucket for up to 6 months\uf0e0 This doesn\u2019t talk about deleting the files. It says store in \u201can\u201d S3 bucket for 6-months, which means it can definitely go to another \u201cMOST cost-effective\u201d bucket, i.e. Glacier Deep Archive\n3.Solution must handle significant variance in demand \uf0e0 \u201csignificant\u201d variance can be interpreted as infrequent usage.\n4.Solution must also be reliable /ability to rerun processing in the event of failure \u2013 Only SQS can achieve this.\nMy verdict: Answer = D"
      },
      {
        "date": "2024-08-17T22:59:00.000Z",
        "voteCount": 2,
        "content": "1. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.\n     Use SQS , option A / B / C are not include AWS SQS.\n2. The cost-effective solution , option D contains S3 Glacier Deep Archive to reduce S3 storage costs"
      },
      {
        "date": "2024-08-14T02:43:00.000Z",
        "voteCount": 1,
        "content": "I go for B but this question is completely wrong. First of all, if you modify in the same bucket you are going to have pontential infinite loop... which means 3 answers are out. But why would you save things in glacier when they can be deleted? About ReRun, in EventBridge you can replay..."
      },
      {
        "date": "2024-08-13T08:20:00.000Z",
        "voteCount": 1,
        "content": "Everybody is focused on choosing the MOST cost-effective option, but there's also this requirement:\n\"The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.\"\n\nwhich I believe it can be achieved only by option D"
      },
      {
        "date": "2024-07-07T19:28:00.000Z",
        "voteCount": 1,
        "content": "I think this question is warding wrong. If we look at the requirement \"store the files in an Amazon S3 bucket for up to 6 months.\" and decide that objects can be deleted after 6 months, C and D are excluded.\nBut is that true?\nWould AWS create a problem involving such an elementary mistake?"
      },
      {
        "date": "2024-07-02T18:59:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2024-06-27T18:23:00.000Z",
        "voteCount": 3,
        "content": "Vote D because the requirement \"rerun processing jobs in the event of failure.\"  Glacier Deep archive is also really cost-effective"
      },
      {
        "date": "2024-05-26T12:07:00.000Z",
        "voteCount": 1,
        "content": "Option D is right answer as it gets the batch files with significant variance in demand"
      },
      {
        "date": "2024-05-19T22:55:00.000Z",
        "voteCount": 2,
        "content": "Going for A as it's the only option that achieve reprocessing, B could be a good answer but it doesn\u00b4t allow any reporcessing."
      },
      {
        "date": "2024-05-13T17:19:00.000Z",
        "voteCount": 2,
        "content": "a: step function for 'the ability to rerun processing jobs in the event of failure'.\nc,d out for keeping the files after 6 months.\nb: not able to rerun failures.."
      },
      {
        "date": "2024-04-20T08:53:00.000Z",
        "voteCount": 3,
        "content": "Correct: A\nAn S3 event is created by eventbridge and can trigger a step function.\nImage processing cannot be done a lambda function alone. Step function is preferred for image processing. You also need the ability to rerun the jobs.\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-data-processing.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html"
      },
      {
        "date": "2024-05-06T13:12:00.000Z",
        "voteCount": 2,
        "content": "That is wrong lambda functions alone can be used to resize images, there is no need for step functions, there is even an article on this from 6 YEARS AGO on how to resize images just with lambda https://aws.amazon.com/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/"
      },
      {
        "date": "2024-04-07T04:42:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.\nA is wrong - AWS Step Function cannot directly be invoked by S3 Event Notification."
      },
      {
        "date": "2024-04-04T06:44:00.000Z",
        "voteCount": 1,
        "content": "B is the best answer (cheapest option) compared to A as step functions requires event notification (for trigger) which is typically done using event bridge ."
      },
      {
        "date": "2024-03-20T09:04:00.000Z",
        "voteCount": 3,
        "content": "Another poorly worded question. First of all, C and D can be eliminated since they keep files after the 6 month period. Then, both A and B are valid. Both will use a Lambda to do the work, but the state changes in the Step Function will incur a very slight cost. Personally, I'd choose A to get control of retries, etc, but the MOST cost-effective alternative is B."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 377,
    "url": "https://www.examtopics.com/discussions/amazon/view/126935-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an organization in AWS Organizations that includes a separate AWS account for each of the company\u2019s departments. Application teams from different departments develop and deploy solutions independently.<br><br>The company wants to reduce compute costs and manage costs appropriately across departments. The company also wants to improve visibility into billing for individual departments. The company does not want to lose operational flexibility when the company selects compute resources.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Budgets for each department. Use Tag Editor to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Budgets for each department. Use SCPs to apply tags to appropriate resources. Purchase Compute Savings Plans."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T15:41:00.000Z",
        "voteCount": 9,
        "content": "C appears to be the most suitable solution. The combination of consolidated billing, a comprehensive tagging strategy using Tag Editor, and the purchase of Compute Savings Plans provides a balanced approach. This solution offers a centralized view and management of costs, ensures accurate cost allocation through tagging, and maintains flexibility in compute resource selection with the Compute Savings Plans. The Compute Savings Plans are particularly beneficial as they provide savings not only on EC2 instances but also on AWS Fargate and AWS Lambda, offering a broader range of applicability than EC2 Instance Savings Plans."
      },
      {
        "date": "2024-01-10T10:36:00.000Z",
        "voteCount": 2,
        "content": "Option C."
      },
      {
        "date": "2024-01-05T11:01:00.000Z",
        "voteCount": 1,
        "content": "Answer: C\nOption A: Lacks consolidated billing, limiting cost visibility and potential discounts.\nOption B: SCPs are primarily for compliance enforcement, not tag application.\nOption D: Misses consolidated billing's benefits for cost visibility and management."
      },
      {
        "date": "2023-11-29T10:04:00.000Z",
        "voteCount": 2,
        "content": "Answer C. Compute Savings plan. Tagging resources in each account using Tag editor &amp; Consolidated Billing to view billing across the accounts."
      },
      {
        "date": "2023-11-27T00:20:00.000Z",
        "voteCount": 4,
        "content": "Answer: C\nBecause for apply Tags to already created resources - you need to use Tag editor."
      },
      {
        "date": "2023-11-27T00:22:00.000Z",
        "voteCount": 2,
        "content": "Compute Savings Plans - cover Amazon EC2, AWS Lambda, and AWS Fargate usage = operational flexibility"
      },
      {
        "date": "2023-11-25T17:53:00.000Z",
        "voteCount": 4,
        "content": "Answer: C\nCompute Savings Plans covers more resources than EC2 Instance Savings Plans.\nYou use Tag Editor to apply tags, not SCPs."
      },
      {
        "date": "2023-11-22T09:54:00.000Z",
        "voteCount": 2,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 378,
    "url": "https://www.examtopics.com/discussions/amazon/view/126840-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.<br><br>What can a solutions architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user 3: PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T15:29:00.000Z",
        "voteCount": 5,
        "content": "Considering the primary concern of improving upload performance for large files while maintaining secure access for authenticated users, Option C (Enable S3 Transfer Acceleration and use it in the presigned URL) is the most suitable solution. It directly addresses the issue of slow uploads for large objects by leveraging CloudFront\u2019s edge locations for accelerated data transfer to S3, and it works seamlessly with the existing mechanism of generating presigned URLs for authenticated users."
      },
      {
        "date": "2024-01-15T22:19:00.000Z",
        "voteCount": 5,
        "content": "A is wrong.\nThe limit of API Gateway payload is 10MB"
      },
      {
        "date": "2024-08-12T21:00:00.000Z",
        "voteCount": 1,
        "content": "C\n\"Most users are reporting slow upload times for objects larger than 100 MB.\"\nStraight to S3 Transfer Acceleration."
      },
      {
        "date": "2024-02-03T16:20:00.000Z",
        "voteCount": 1,
        "content": "Presigned URLs still ensure that only authenticated users can upload content, as the generation of a presigned URL requires valid AWS credentials. The URL is temporary and grants the bearer permission to perform the action defined in the URL, in this case, a PUT operation to upload an object"
      },
      {
        "date": "2024-02-03T16:21:00.000Z",
        "voteCount": 1,
        "content": "Sorry I mean = C"
      },
      {
        "date": "2024-01-10T10:45:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-04T05:28:00.000Z",
        "voteCount": 5,
        "content": "https://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/\n\n(C)"
      },
      {
        "date": "2023-12-31T07:00:00.000Z",
        "voteCount": 1,
        "content": "C has the most votes currently. How does C ensure that only authenticated users are allowed to post content?"
      },
      {
        "date": "2024-01-04T05:33:00.000Z",
        "voteCount": 2,
        "content": "S3TA supports presigned URL. The only problem the architect mest solve is the slow upload. Multipart upload can overcome TCP speed limitations and S3TA reduces latency.\n\nSee the link in my vote"
      },
      {
        "date": "2023-12-22T10:51:00.000Z",
        "voteCount": 1,
        "content": "C is the easiest"
      },
      {
        "date": "2023-12-10T10:12:00.000Z",
        "voteCount": 4,
        "content": "Answer is A to secure the API.  \nhttps://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/#:~:text=Adding%20authentication%20to%20the%20upload%20process&amp;text=You%20can%20restrict%20access%20to,as%20Amazon%20Cognito%20or%20Auth0."
      },
      {
        "date": "2023-12-06T05:54:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-11-21T23:53:00.000Z",
        "voteCount": 2,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 379,
    "url": "https://www.examtopics.com/discussions/amazon/view/126841-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A large company is migrating its entire IT portfolio to AWS. Each business unit in the company has a standalone AWS account that supports both development and test environments. New accounts to support production workloads will be needed soon.<br><br>The finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs.<br><br>The security team requires a centralized mechanism to control IAM usage in all the company\u2019s accounts.<br><br>What combination of the following options meets the company\u2019s needs with the LEAST effort? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a collection of parameterized AWS CloudFormation templates defining common IAM permissions that are launched into each account. Require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequire each business unit to use its own AWS accounts. Tag each AWS account appropriately and enable Cost Explorer to administer chargebacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsolidate all of the company's AWS accounts into a single AWS account. Use tags for billing purposes and the IAM\u2019s Access Advisor feature to enforce the least privilege model."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-25T08:23:00.000Z",
        "voteCount": 2,
        "content": "Option BD not C: The management account has the responsibilities of a payer account and is responsible for paying all charges that are accrued by the member accounts. You can't change an organization's management account.\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html"
      },
      {
        "date": "2024-03-26T06:11:00.000Z",
        "voteCount": 3,
        "content": "Option BD - You need to use Service Control Policies (SCP) for the Security Team requirements. \n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2024-03-02T11:27:00.000Z",
        "voteCount": 2,
        "content": "C is wrong: since it didn't mention Organization at all. \nWe can get each group's cost by utilizing OU."
      },
      {
        "date": "2024-03-01T15:19:00.000Z",
        "voteCount": 4,
        "content": "options B and C offers a balance between centralized management, cost visibility, and minimal disruption during the migration process. The company can leverage AWS Organizations to establish a central structure and implement security controls later, while maintaining separate accounts for business units with tagging and Cost Explorer to ensure cost allocation. i maybe wrong, but these are my picks"
      },
      {
        "date": "2024-06-02T13:16:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2024-02-28T08:31:00.000Z",
        "voteCount": 1,
        "content": "BC \nWe need AWS Organization and we need tagging for cost allocation.\nThose are the only answers viable."
      },
      {
        "date": "2024-02-25T20:07:00.000Z",
        "voteCount": 2,
        "content": "\u201cCentralized method for payment\u201d maps to AWS organization. So B is one of the answer.\n\u201cmaintain visibility into each group's spending to allocate costs\u201d means all resources need to be tagged for Cost Explorer to provide visibility into each group\u2019s spending. So, C is one of the answer\nI don\u2019t think D is a good answer, coz SCP is not a good way for IAM permission control. The usual way is to create different roles and allow different users/groups to assume different roles.\nA is wrong because there isn\u2019t so called common IAM permissions; and least privilege model is a best practice rather than a detailed template, so there is nothing to enforce.\nE Consolidating accounts into one single account is obviously not a good solution."
      },
      {
        "date": "2024-01-23T12:53:00.000Z",
        "voteCount": 2,
        "content": "Why not B,C?  It looks good to me."
      },
      {
        "date": "2024-01-10T10:48:00.000Z",
        "voteCount": 1,
        "content": "Option B and D"
      },
      {
        "date": "2023-12-22T10:53:00.000Z",
        "voteCount": 1,
        "content": "Also vote for B and D"
      },
      {
        "date": "2023-11-29T11:52:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D - Create Organizations in AWS Organizations from a chosen payer account and invite all member accounts and create new accounts as a part of the Organizations. Enable All features and create appropriate SCPs for services access control."
      },
      {
        "date": "2023-11-22T15:35:00.000Z",
        "voteCount": 4,
        "content": "Options B and D offers a centralized, efficient, and scalable solution that meets both the finance department's and the security team's requirements."
      },
      {
        "date": "2023-11-22T09:59:00.000Z",
        "voteCount": 2,
        "content": "BD for sure"
      },
      {
        "date": "2023-11-21T23:53:00.000Z",
        "voteCount": 2,
        "content": "Answer: B D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 380,
    "url": "https://www.examtopics.com/discussions/amazon/view/126842-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a solution that analyzes weather data from thousands of weather stations. The weather stations send the data over an Amazon API Gateway REST API that has an AWS Lambda function integration. The Lambda function calls a third-party service for data pre-processing. The third-party service gets overloaded and fails the pre-processing, causing a loss of data.<br><br>A solutions architect must improve the resiliency of the solution. The solutions architect must ensure that no data is lost and that data can be processed later if failures occur.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue as the dead-letter queue for the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Amazon Simple Queue Service (Amazon SQS) queues: a primary queue and a secondary queue. Configure the secondary queue as the dead-letter queue for the primary queue. Update the API to use a new integration to the primary queue. Configure the Lambda function as the invocation target for the primary queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Amazon EventBridge event buses: a primary event bus and a secondary event bus. Update the API to use a new integration to the primary event bus. Configure an EventBridge rule to react to all events on the primary event bus. Specify the Lambda function as the target of the rule. Configure the secondary event bus as the failure destination for the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Amazon EventBridge event bus. Configure the event bus as the failure destination for the Lambda function."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T15:48:00.000Z",
        "voteCount": 7,
        "content": "B is the best solution. It uses two Amazon SQS queues to ensure that incoming data is not lost and can be processed later in case of failures. The primary queue acts as the initial landing point for data from the API Gateway, and the secondary queue serves as a dead-letter queue, capturing data that could not be processed due to third-party service failures or other issues. This setup maintains data integrity and allows for later processing, effectively improving the solution's resiliency."
      },
      {
        "date": "2024-01-10T10:54:00.000Z",
        "voteCount": 2,
        "content": "Option B is most suitable. Eventbridge can not be target for API Gateway"
      },
      {
        "date": "2024-01-10T11:04:00.000Z",
        "voteCount": 2,
        "content": "API gateway will need to do http post to post an event to Eventbridge bus and a single eventbus has throttle limits on events/sec. SQS will be a better and more scalable in this case."
      },
      {
        "date": "2023-11-29T11:56:00.000Z",
        "voteCount": 3,
        "content": "Answer - B. Create 2 SQS queues, one for tasks and second as DLQ. Create Lambda as target invocation."
      },
      {
        "date": "2023-11-22T10:02:00.000Z",
        "voteCount": 4,
        "content": "B for sure"
      },
      {
        "date": "2023-11-21T23:54:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 381,
    "url": "https://www.examtopics.com/discussions/amazon/view/126843-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.<br><br>Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.<br><br>Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray."
    ],
    "answer": "ABD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABD",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T15:42:00.000Z",
        "voteCount": 6,
        "content": "Publishing slow query and error logs to CloudWatch Logs will allow for better analysis of database performance issues. It helps in identifying slow-running queries that might be contributing to the application's performance problems.\n\nIntegrating AWS X-Ray SDK into the application will enable tracing of incoming HTTP requests on the EC2 instances. Tracing SQL queries with the X-Ray SDK for Java will provide insights into how database queries are impacting application performance.\nX-Ray can give a detailed analysis of both service-level and database-level operations, which is essential for diagnosing performance bottlenecks.\n\nIntegrating AWS X-Ray SDK into the application will enable tracing of incoming HTTP requests on the EC2 instances. Tracing SQL queries with the X-Ray SDK for Java will provide insights into how database queries are impacting application performance.\nX-Ray can give a detailed analysis of both service-level and database-level operations, which is essential for diagnosing performance bottlenecks."
      },
      {
        "date": "2024-01-10T11:10:00.000Z",
        "voteCount": 2,
        "content": "A, B and D"
      },
      {
        "date": "2023-12-22T10:58:00.000Z",
        "voteCount": 1,
        "content": "Answer ABD"
      },
      {
        "date": "2023-12-10T11:18:00.000Z",
        "voteCount": 2,
        "content": "ABD - Effectively we need to collect logs (from DB, Instance) and Trace the Request &lt;-&gt; Response from the calls using XRay  to understand what is happening.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.Concepts.MySQL.html#USER_LogAccess.MySQLDB.PublishAuroraMySQLtoCloudWatchLogs\nhttps://aws.amazon.com/blogs/mt/simplifying-apache-server-logs-with-amazon-cloudwatch-logs-insights/\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-messagehandler.html\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-sqlclients.html"
      },
      {
        "date": "2023-12-03T04:49:00.000Z",
        "voteCount": 1,
        "content": "Answer: ABD"
      },
      {
        "date": "2023-11-28T05:18:00.000Z",
        "voteCount": 1,
        "content": "Answer ABD"
      },
      {
        "date": "2023-11-22T10:15:00.000Z",
        "voteCount": 2,
        "content": "Answer ABD"
      },
      {
        "date": "2023-11-22T00:00:00.000Z",
        "voteCount": 2,
        "content": "Answer: A B D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 382,
    "url": "https://www.examtopics.com/discussions/amazon/view/126844-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage. The backend services run on a pair of Amazon EC2 instances behind an Application Load Balancer with Amazon DynamoDB as the datastore. Application read and write traffic is slow during peak seasons.<br><br>Which option provides a scalable application architecture to handle peak seasons with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the backend services to AWS Lambda. Increase the read and write capacity of DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the backend services to AWS Lambda. Configure DynamoDB to use global tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling groups for the backend services. Use DynamoDB auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling groups for the backend services. Use Amazon Simple Queue Service (Amazon SQS) and an AWS Lambda function to write to DynamoDB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-28T14:32:00.000Z",
        "voteCount": 1,
        "content": "Is the correct answer.\nNormally B would right but it says the Least development effort, which would be required in B to re write the app for Lambda,\n\nTherefor configuring scaling groups, allows to scale to handle the peak season traffic"
      },
      {
        "date": "2024-01-10T11:13:00.000Z",
        "voteCount": 3,
        "content": "Option C"
      },
      {
        "date": "2023-12-22T10:59:00.000Z",
        "voteCount": 1,
        "content": "C is the best"
      },
      {
        "date": "2023-12-03T04:41:00.000Z",
        "voteCount": 4,
        "content": "C has the least development effort"
      },
      {
        "date": "2023-11-29T16:27:00.000Z",
        "voteCount": 3,
        "content": "Answer C. Autoscaling"
      },
      {
        "date": "2023-11-28T05:19:00.000Z",
        "voteCount": 2,
        "content": "LEAST development effort -&gt; Answer C"
      },
      {
        "date": "2023-11-22T15:44:00.000Z",
        "voteCount": 4,
        "content": "Auto scaling"
      },
      {
        "date": "2023-11-22T10:19:00.000Z",
        "voteCount": 4,
        "content": "C for sure"
      },
      {
        "date": "2023-11-22T00:00:00.000Z",
        "voteCount": 4,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 383,
    "url": "https://www.examtopics.com/discussions/amazon/view/126845-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. The company would also like to monitor network connections to map communications between servers.<br><br>Which would enable the collection of this data MOST cost effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T22:45:00.000Z",
        "voteCount": 9,
        "content": "To get details on network connections, you would need a agent-based discovery. AWS documentation does mention it.\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html"
      },
      {
        "date": "2023-11-22T00:02:00.000Z",
        "voteCount": 9,
        "content": "Answer: A"
      },
      {
        "date": "2024-01-10T11:15:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-02T07:19:00.000Z",
        "voteCount": 6,
        "content": "Agentless doesn't support to collect running process data and network inbound/outbound connections information.\n\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html#compare-tools"
      },
      {
        "date": "2023-12-03T04:36:00.000Z",
        "voteCount": 1,
        "content": "Network connections requires a discovery agent.\nhttps://tutorialsdojo.com/aws-application-discovery-service/"
      },
      {
        "date": "2023-11-29T17:15:00.000Z",
        "voteCount": 2,
        "content": "According to this link, VM utilization metrics are picked up by Agentless and not Agent-based. - https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html#Database%20Discovery . \n\nAt the same time, network connections are pickedup by Agent-based and not Agentless. \nWas pretty confident about A, but now i see A doesn't suffice all criteria nor does C."
      },
      {
        "date": "2023-11-26T08:31:00.000Z",
        "voteCount": 6,
        "content": "agentless discovery supports VMWare only. The question didn't mention VMWare."
      },
      {
        "date": "2023-11-25T15:55:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer...you must use the agent to pick up on network connections and processes running on the VMs. Agentless will not read those details."
      },
      {
        "date": "2023-11-22T15:47:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html"
      },
      {
        "date": "2023-11-22T10:22:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 384,
    "url": "https://www.examtopics.com/discussions/amazon/view/126846-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company provides a software as a service (SaaS) application that runs in the AWS Cloud. The application runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in an Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.<br><br>The company is deploying the application into additional Regions. The company must provide static IP addresses for the application to customers so that the customers can add the IP addresses to allow lists. The solution must automatically route customers to the Region that is geographically closest to them.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution. Create a CloudFront origin group. Add the NLB for each additional Region to the origin group. Provide customers with the IP address ranges of the distribution\u2019s edge locations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Global Accelerator standard accelerator. Create a standard accelerator endpoint for the NLB in each additional Region. Provide customers with the Global Accelerator IP address.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution. Create a custom origin for the NLB in each additional Region. Provide customers with the IP address ranges of the distribution\u2019s edge locations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Global Accelerator custom routing accelerator. Create a listener for the custom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-10T11:29:00.000Z",
        "voteCount": 12,
        "content": "Answer is B not D.  CloudFront does not work with NLB nor does it accept a fixed IP address\n\nA Standard accelerators automatically route traffic to a healthy endpoint that is nearest to your user. Since they're designed to load balance traffic, you can't deterministically route multiple users to a specific EC2 destination behind your accelerator. Custom routing accelerators allows you to do just that. \n\nAnother difference is that standard routing accelerators support Network Load Balancers, Application Load Balancers, EC2 instances, and Elastic IPs as endpoints. Custom routing accelerators support only VPC subnet endpoints, each containing one or more EC2 instances that are running your application.\n\nhttps://aws.amazon.com/global-accelerator/faqs/#:~:text=A%3A%20Standard%20accelerators%20automatically%20route,you%20to%20do%20just%20that."
      },
      {
        "date": "2024-03-23T04:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-02-12T09:55:00.000Z",
        "voteCount": 1,
        "content": "D -  With a custom routing accelerator, Global Accelerator does not route traffic based on the geoproximity or health of the endpoint.\nA and C - Though it may work but the IP address list keeps on changing and we can use this only in internal AWS implementations where we have access to the prefix list of Cloudfront IPs\n\nB is the correct answer"
      },
      {
        "date": "2024-01-10T11:24:00.000Z",
        "voteCount": 2,
        "content": "Option B is most appropriate here because requirement is to route the customer to closest region and not to specific EC2 instance. Option D provides custom routing that is not required in this case."
      },
      {
        "date": "2023-12-03T04:25:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-global-accelerator-custom-routing-accelerators/"
      },
      {
        "date": "2023-11-29T17:35:00.000Z",
        "voteCount": 3,
        "content": "Answer B. \nFor standard accelerators, the endpoints are Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses.\nFor custom routing accelerators, the endpoints are virtual private cloud (VPC) subnets with one or more EC2 instances."
      },
      {
        "date": "2023-11-22T15:50:00.000Z",
        "voteCount": 1,
        "content": "standard"
      },
      {
        "date": "2023-11-22T10:27:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-11-22T00:03:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 385,
    "url": "https://www.examtopics.com/discussions/amazon/view/126937-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running multiple workloads in the AWS Cloud. The company has separate units for software development. The company uses AWS Organizations and federation with SAML to give permissions to developers to manage resources in their AWS accounts. The development units each deploy their production workloads into a common production account.<br><br>Recently, an incident occurred in the production account in which members of a development unit terminated an EC2 instance that belonged to a different development unit. A solutions architect must create a solution that prevents a similar incident from happening in the future. The solution also must allow developers the possibility to manage the instances used for their workloads.<br><br>Which strategy will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate OUs in AWS Organizations for each development unit. Assign the created OUs to the company AWS accounts. Create separate SCP with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag that matches the development unit name. Assign the SCP to the corresponding OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Update the IAM policy for the developers\u2019 assumed IAM role with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Create an SCP with an allow action and a StringEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. Assign the SCP to the root OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate IAM policies for each development unit. For every IAM policy, add an allow action and a StringEquals condition for the DevelopmentUnit resource tag and the development unit name. During SAML federation, use AWS Security Token Service (AWS STS) to assign the IAM policy and match the development unit name to the assumed IAM role."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T13:25:00.000Z",
        "voteCount": 6,
        "content": "Answer: B\nOption A: While OUs and SCPs can provide access control, they are more suitable for broader permission boundaries and might not offer the same granularity as STS session tags and IAM policies."
      },
      {
        "date": "2024-01-10T11:32:00.000Z",
        "voteCount": 4,
        "content": "Option A will not work for common Production Account."
      },
      {
        "date": "2023-12-06T06:10:00.000Z",
        "voteCount": 3,
        "content": "Answer B. \nA won't work as developer units needs to deploy resources in the common Production account"
      },
      {
        "date": "2023-12-03T03:46:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      },
      {
        "date": "2023-12-01T22:54:00.000Z",
        "voteCount": 2,
        "content": "A won't work for the common account which everybody needs access to. B is the way to go."
      },
      {
        "date": "2023-11-30T19:37:00.000Z",
        "voteCount": 4,
        "content": "B is the best answer. This approach involves tagging federated identity sessions with a DevelopmentUnit attribute and then using IAM policies to deny actions if the DevelopmentUnit tag of the resource does not match the aws:PrincipalTag/DevelopmentUnit. This method directly ties permissions to the federated identity, allowing for finer-grained access control that aligns with your requirements."
      },
      {
        "date": "2023-11-28T11:01:00.000Z",
        "voteCount": 2,
        "content": "Should be B"
      },
      {
        "date": "2023-11-26T22:25:00.000Z",
        "voteCount": 2,
        "content": "Should be B - https://www.examtopics.com/discussions/amazon/view/60000-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-22T10:31:00.000Z",
        "voteCount": 3,
        "content": "A for sure"
      },
      {
        "date": "2024-02-14T12:38:00.000Z",
        "voteCount": 4,
        "content": "For sure not, this doesn't address the problem where developers need to deploy in common Production Account."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 386,
    "url": "https://www.examtopics.com/discussions/amazon/view/126939-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An enterprise company is building an infrastructure services platform for its users. The company has the following requirements:<br><br>\u2022\tProvide least privilege access to users when launching AWS infrastructure so users cannot provision unapproved services.<br>\u2022\tUse a central account to manage the creation of infrastructure services.<br>\u2022\tProvide the ability to distribute infrastructure services to multiple accounts in AWS Organizations.<br>\u2022\tProvide the ability to enforce tags on any infrastructure that is started by users.<br><br>Which combination of actions using AWS services will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop infrastructure services using AWS CloudFormation templates. Add the templates to a central Amazon S3 bucket and add the IAM roles or users that require access to the S3 bucket policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop infrastructure services using AWS CloudFormation templates. Upload each template as an AWS Service Catalog product to portfolios created in a central AWS account. Share these portfolios with the Organizations structure created for the company.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow user IAM roles to have AWSCloudFormationFullAccess and AmazonS3ReadOnlyAccess permissions. Add an Organizations SCP at the AWS account root user level to deny all services except AWS CloudFormation and Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow user IAM roles to have ServiceCatalogEndUserAccess permissions only. Use an automation script to import the central portfolios to local AWS accounts, copy the TagOption, assign users access, and apply launch constraints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Service Catalog TagOption Library to maintain a list of tags required by the company. Apply the TagOption to AWS Service Catalog products or portfolios.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CloudFormation Resource Tags property to enforce the application of tags to any CloudFormation templates that will be created for users."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-26T08:46:00.000Z",
        "voteCount": 7,
        "content": "BDE - refer Service Catalog"
      },
      {
        "date": "2024-08-12T22:38:00.000Z",
        "voteCount": 1,
        "content": "Love this kinda question. \n\nBasically, there is no spaghetti between choices. If you know Service Catalogue is the choice, then you can immediately choose all 3 correct answers without hesitation"
      },
      {
        "date": "2024-03-04T17:46:00.000Z",
        "voteCount": 1,
        "content": "Approved == Service Catalog"
      },
      {
        "date": "2024-01-10T11:38:00.000Z",
        "voteCount": 1,
        "content": "Option B, D, E"
      },
      {
        "date": "2024-01-04T13:14:00.000Z",
        "voteCount": 3,
        "content": "Answer: B\nB. Develop infrastructure using CloudFormation and AWS Service Catalog\nD. Use Service Catalog EndUserAccess and automation\nE. Use Service Catalog TagOption Library and apply to products/portfolios:"
      },
      {
        "date": "2024-01-04T13:14:00.000Z",
        "voteCount": 1,
        "content": "I mean Answer: BDE"
      },
      {
        "date": "2023-12-14T10:41:00.000Z",
        "voteCount": 1,
        "content": "+1 for BDE"
      },
      {
        "date": "2023-11-26T22:19:00.000Z",
        "voteCount": 4,
        "content": "I guess that the right answer should be BDE, because we uses Service catalog, so all other options should to refer on it."
      },
      {
        "date": "2023-11-22T10:35:00.000Z",
        "voteCount": 1,
        "content": "Answer BCE"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 387,
    "url": "https://www.examtopics.com/discussions/amazon/view/126940-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company deploys a new web application. As part of the setup, the company configures AWS WAF to log to Amazon S3 through Amazon Kinesis Data Firehose. The company develops an Amazon Athena query that runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs is constant. However, over time, the same query is taking more time to run.<br><br>A solutions architect needs to design a solution to prevent the query time from continuing to increase. The solution must minimize operational overhead.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that consolidates each day's AWS WAF logs into one log file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-11T00:24:00.000Z",
        "voteCount": 3,
        "content": "D ans :https://repost.aws/knowledge-center/athena-queries-long-processing-time"
      },
      {
        "date": "2024-01-10T11:40:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-04T13:05:00.000Z",
        "voteCount": 2,
        "content": "Answer: D\nPartitioning is a powerful technique for optimizing query performance and cost in Athena, especially for large, growing datasets.Firehose and Athena seamlessly support partitioning, making it easy to implement."
      },
      {
        "date": "2023-12-14T10:50:00.000Z",
        "voteCount": 1,
        "content": "D. The user can split various logs into daily partitions. As daily volume is constant, the time to process will not increase over time."
      },
      {
        "date": "2023-12-14T03:57:00.000Z",
        "voteCount": 1,
        "content": "D is simple and easy to do"
      },
      {
        "date": "2023-12-10T05:35:00.000Z",
        "voteCount": 1,
        "content": "D, is correct.  It looks like option a is viable as well."
      },
      {
        "date": "2023-11-27T05:45:00.000Z",
        "voteCount": 3,
        "content": "Answer: D\nhttps://aws.amazon.com/blogs/big-data/kinesis-data-firehose-now-supports-dynamic-partitioning-to-amazon-s3/"
      },
      {
        "date": "2023-11-22T10:41:00.000Z",
        "voteCount": 3,
        "content": "D is ok"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 388,
    "url": "https://www.examtopics.com/discussions/amazon/view/126847-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed to access the application. The company needs the ability to log the access requests that have been blocked. The solution should require the least possible maintenance.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IPSet containing a list of IP ranges that belong to the specified country. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet. Associate the rule with the web ACL. Associate the web ACL with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Shield to block any requests that do not originate from the specified country. Associate AWS Shield with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a security group rule that allows ports 80 and 443 from IP ranges that belong to the specified country. Associate the security group with the ALB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T12:59:00.000Z",
        "voteCount": 6,
        "content": "Answer: B\nAWS WAF supports geo-matching rules, allowing you to easily block requests based on country of origin. This eliminates the need to manually manage IP ranges.\nOption C - Shield primarily defends against DDoS attacks and does not offer granular geo-blocking capabilities."
      },
      {
        "date": "2024-01-10T11:42:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-03T02:36:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2023-11-28T05:41:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-11-23T05:17:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html"
      },
      {
        "date": "2023-11-22T10:46:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-11-22T00:04:00.000Z",
        "voteCount": 2,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 389,
    "url": "https://www.examtopics.com/discussions/amazon/view/126954-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating an application from on-premises infrastructure to the AWS Cloud. During migration design meetings, the company expressed concerns about the availability and recovery options for its legacy Windows file server. The file server contains sensitive business-critical data that cannot be recreated in the event of data corruption or data loss. According to compliance requirements, the data must not travel across the public internet. The company wants to move to AWS managed services where possible.<br><br>The company decides to store the data in an Amazon FSx for Windows File Server file system. A solutions architect must design a solution that copies the data to another AWS Region for disaster recovery (DR) purposes.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination Amazon S3 bucket in the DR Region. Establish connectivity between the FSx for Windows File Server file system in the primary Region and the S3 bucket in the DR Region by using Amazon FSx File Gateway. Configure the S3 bucket as a continuous backup source in FSx File Gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC the primary Region and the VPC in the DR Region by using AWS Site-to-Site VPN. Configure AWS DataSync to communicate by using VPN endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using VPC peering. Configure AWS DataSync to communicate by using interface VPC endpoints with AWS PrivateLink.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using AWS Transit Gateway in each Region. Use AWS Transfer Family to copy files between the FSx for Windows File Server file system in the primary Region and the FSx for Windows File Server file system in the DR Region over the private AWS backbone network."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T17:19:00.000Z",
        "voteCount": 6,
        "content": "A - for S3, data will traverse via internet\nB- Site to Site VPN is not required for 2 VPC within AWS\nC -VPC Peering is the best option for connecting 2 VPCs in different regions\nD - Transit Gateway not required as the connection is only between 2 VPC, Peering is more cost effective."
      },
      {
        "date": "2024-09-27T11:28:00.000Z",
        "voteCount": 1,
        "content": "C is very wrong"
      },
      {
        "date": "2024-03-02T08:33:00.000Z",
        "voteCount": 1,
        "content": "Option A doesn't indicate what kind of connection will be created between the DR region. Option C is correct."
      },
      {
        "date": "2024-02-04T21:52:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2024-01-31T04:27:00.000Z",
        "voteCount": 4,
        "content": "Another terrible, terrible question. NONE of the answers meet all the requirements.\n\n- The data cannot be recreated in the event of data corruption or data loss.\n- The data must not travel across the public internet.\n\nA - doesn't specify how it avoids traversing the internet (so you can safely assume it traverses the internet), but at least it's an actual \"backup\" that allows the business to recover corrupted or deleted files. \nB, C, D - file corruption or accidental deletion will propagate to the DR site, no previous versions.\n\nIn the exam, if I get this question and I'm feeling really confident with all my other answers, I'll pick A to intentionally get this question \"wrong\" and hopefully get it flagged as a crap question. But the answer they're looking for is C.\n\nhttps://search.brave.com/search?q=statistical+analysis+discrimination+index"
      },
      {
        "date": "2024-10-14T17:17:00.000Z",
        "voteCount": 1,
        "content": "This might be required in this age of AI!"
      },
      {
        "date": "2024-08-20T08:06:00.000Z",
        "voteCount": 1,
        "content": "If it wasn't for exam time, I'd rant in the comment section about it too."
      },
      {
        "date": "2024-01-10T11:47:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-04T12:51:00.000Z",
        "voteCount": 3,
        "content": "Option C is correct - keyword - \"VPC endpoints with AWS PrivateLink\" offer a powerful way to keep data within the AWS network and avoid exposure to the public internet."
      },
      {
        "date": "2024-01-02T11:52:00.000Z",
        "voteCount": 4,
        "content": "Connect FSx VPCs using VPC peering. Allow DataSync client to communicate with server using PrivateLink\nhttps://aws.amazon.com/blogs/storage/how-to-replicate-amazon-fsx-file-server-data-across-aws-regions/"
      },
      {
        "date": "2023-12-22T12:38:00.000Z",
        "voteCount": 2,
        "content": "C see https://aws.amazon.com/blogs/storage/how-to-replicate-amazon-fsx-file-server-data-across-aws-regions/"
      },
      {
        "date": "2023-11-29T18:14:00.000Z",
        "voteCount": 1,
        "content": "Answer C. FSx fs on Region B and configure VPC Peering. Access using VPC Interface endpoints so data stays private."
      },
      {
        "date": "2023-11-29T07:19:00.000Z",
        "voteCount": 1,
        "content": "Option A feels more typical DR with S3 continuous backup and less complexity than option C"
      },
      {
        "date": "2023-11-30T04:52:00.000Z",
        "voteCount": 1,
        "content": "A is out since compagny requires data to travel not over the internet. no endpoints are defined so S3 is not targeted over AWS backbone network but over internet."
      },
      {
        "date": "2023-11-26T08:52:00.000Z",
        "voteCount": 2,
        "content": "vote C"
      },
      {
        "date": "2023-11-22T13:02:00.000Z",
        "voteCount": 3,
        "content": "C is ok"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 390,
    "url": "https://www.examtopics.com/discussions/amazon/view/126955-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region.<br><br>Which solution will meet these business requirements at the LOWEST cost?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T07:54:00.000Z",
        "voteCount": 1,
        "content": "It's B. In A, the RTO won't be met using snapshots, in C, well, it works but it's expensive. D doesn't even fulfill the regional requirement."
      },
      {
        "date": "2024-01-31T06:50:00.000Z",
        "voteCount": 1,
        "content": "I choose B. \nC : Aurora DB automaticly support sync. We don't use DMS."
      },
      {
        "date": "2024-01-10T11:49:00.000Z",
        "voteCount": 2,
        "content": "Option B is most cost effective"
      },
      {
        "date": "2024-01-04T12:44:00.000Z",
        "voteCount": 2,
        "content": "B for sure...  Cross-Region read replicas continuously replicate data from the primary RDS instance to the secondary Region, providing a near-real-time RPO of less than 5 minutes. Failover to the replica can typically be achieved within minutes, meeting the RTO requirement. \nOption D doesn't provide cross-Region failover, which is a key requirement in this scenario."
      },
      {
        "date": "2023-12-10T06:35:00.000Z",
        "voteCount": 2,
        "content": "Option C is not cost effective as per requirement"
      },
      {
        "date": "2023-12-02T03:44:00.000Z",
        "voteCount": 2,
        "content": "B for sure, C is way too expensive even though it's a correct solution"
      },
      {
        "date": "2023-11-29T18:19:00.000Z",
        "voteCount": 1,
        "content": "Answer B."
      },
      {
        "date": "2023-11-22T23:28:00.000Z",
        "voteCount": 4,
        "content": "Answer: B"
      },
      {
        "date": "2023-11-22T13:06:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2023-11-23T15:41:00.000Z",
        "voteCount": 1,
        "content": "Sorry, correct D.\n\ndeploying an Amazon RDS instance with a read replica in the same Region and promoting the read replica to become the primary in the event of a failure, the company can meet the business requirements of an RPO of less than 5 minutes and an RTO of less than 10 minutes for the application that will store approximately 10 TB of data and provide the ability to fail over to a secondary Region at the lowest cost."
      },
      {
        "date": "2023-11-26T11:49:00.000Z",
        "voteCount": 4,
        "content": "Deploying a read replica in the same region as their existing DB will not provide any failover to a secondary region. They must use a cross region replica to achieve this."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 391,
    "url": "https://www.examtopics.com/discussions/amazon/view/126848-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the management account to create a new member account with finance1@example.com as the email address.<br><br>What should the solutions architect do to create IAM users in the new member account?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSign in to the AWS Management Console with AWS account root user credentials by using the 64-character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to the AWS Management Console sign-in page. Choose \u201cSign in using root account credentials.\u201d Sign in in by using the email address finance 1@example.com and the management account's root password. Set up the IAM users as required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T12:24:00.000Z",
        "voteCount": 7,
        "content": "Option B correct:\nKey word - \"OrganizationAccountAccessRole\", By assuming the OrganizationAccountAccessRole, you gain temporary, controlled access to the member account without sharing root credentials or creating separate IAM users for cross-account access. This enhances security and reduces administrative overhead."
      },
      {
        "date": "2024-01-10T11:53:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-02T04:55:00.000Z",
        "voteCount": 1,
        "content": "b ISANS\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html"
      },
      {
        "date": "2023-12-20T07:05:00.000Z",
        "voteCount": 1,
        "content": "D is my answer.  Those who chose B are correct about the process and the role that is created when you setup the account.   But the user (Support1) that has management account access to setup a new account in the organisation automatically becomes part of the administrators in the new account that gets created and therefore will be able to access the new account with his/her credentials by specifying the new account.   \n\nThe root user with the 64 character password is also a valid approach but it is not a recommended one by AWS."
      },
      {
        "date": "2024-01-31T04:47:00.000Z",
        "voteCount": 3,
        "content": "This is an incorrect understanding.\n\n\"But the user (Support1) ... automatically becomes part of the administrators in the new account that gets created\" - yes, by virtue of the cross-account OrganizationAccountAccessRole role ONLY. No IAM users are ever automatically created anywhere, ever, never ever, never ever ever. Never! :)"
      },
      {
        "date": "2023-12-14T13:06:00.000Z",
        "voteCount": 2,
        "content": "B as most secure way"
      },
      {
        "date": "2023-12-13T10:00:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html"
      },
      {
        "date": "2023-12-03T02:20:00.000Z",
        "voteCount": 3,
        "content": "Answer: B"
      },
      {
        "date": "2023-11-30T04:57:00.000Z",
        "voteCount": 3,
        "content": "quote out of article posted by thala:\n\n\"When you create a member account, AWS Organizations automatically creates an AWS Identity and Management (IAM) role called OrganizationAccountAccessRole in the account. This role has full administrative permissions in the member account.\"\n\nB is only valid answer, assume the role and perform administrative actions"
      },
      {
        "date": "2023-11-22T16:18:00.000Z",
        "voteCount": 4,
        "content": "https://repost.aws/knowledge-center/organizations-member-account-access"
      },
      {
        "date": "2023-11-22T13:13:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-11-22T00:05:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 392,
    "url": "https://www.examtopics.com/discussions/amazon/view/126849-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.<br><br>Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.<br><br>The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.<br><br>Which strategy meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T12:12:00.000Z",
        "voteCount": 9,
        "content": "Option A is correct\nWhile A and B do the job but the question says \"minimizing the increase in costs associated with the solution\".. I'll go with A coz Edge-optimized endpoints cache responses at edge locations closer to users, significantly reducing the number of requests reaching the database and Lambda functions.. While Option B -- While ElastiCache for Redis a good caching solution, it adds complexity and cost compared to edge caching."
      },
      {
        "date": "2024-01-10T11:59:00.000Z",
        "voteCount": 9,
        "content": "Option A because B is more expensive than A"
      },
      {
        "date": "2024-10-10T14:42:00.000Z",
        "voteCount": 1,
        "content": "For those who are saying that option A is the correct one, why should API Gateway Regional Endpoint be converted to Edge-Optimized Endpoint, if caching can be enabled on either? Also, Between the two options, Option B tends to have a lower cost increase in the long term, especially because ElastiCache allows for more direct control over costs and can be adjusted to meet demand. Option A may result in significant data transfer costs, as switching to an Edge-Optimized endpoint involves CloudFront usage costs, which can increase rapidly as traffic grows. Therefore, if the goal is to minimize the increase in costs, Option B (using ElastiCache for Redis) is likely the best choice.,"
      },
      {
        "date": "2024-08-20T07:09:00.000Z",
        "voteCount": 3,
        "content": "Well, I'm convinced with B too, but apparently Neal Davis chose A in one of his exams so.."
      },
      {
        "date": "2024-05-20T01:49:00.000Z",
        "voteCount": 2,
        "content": "Minimizing is not the most cheaper."
      },
      {
        "date": "2024-05-14T01:50:00.000Z",
        "voteCount": 7,
        "content": "For me it's B. We are using a REGIONAL (Not Global) API Endpoint.\nTake in mind that the errors are in Database layer, so it's not a problem to APIs, but with Redis Cache for sure we solve it. For me it's the best choice."
      },
      {
        "date": "2024-04-01T18:29:00.000Z",
        "voteCount": 3,
        "content": "A doesn't resolve the problems. It is B."
      },
      {
        "date": "2024-03-21T16:21:00.000Z",
        "voteCount": 3,
        "content": "B is expensive"
      },
      {
        "date": "2024-01-02T05:08:00.000Z",
        "voteCount": 1,
        "content": "kEY WORK MOBILE APP  b IS ANY\nhttps://aws.amazon.com/elasticache/redis/"
      },
      {
        "date": "2023-12-30T02:40:00.000Z",
        "voteCount": 4,
        "content": "API Gateway can take care of caching and it should be the cheaper solution compared to ElastiCache for Redis. That why I go with A."
      },
      {
        "date": "2023-12-27T09:48:00.000Z",
        "voteCount": 6,
        "content": "The main option is \"clients are making multiple HTTP GET requests for the same queries in a short period of time.\" Enable Cache from APIGW https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\n\nOption B is workable solution but will add more cost"
      },
      {
        "date": "2023-12-20T09:08:00.000Z",
        "voteCount": 4,
        "content": "Its B \nFor those choosing A, a change between regional and edge API is not required but API caching is.  The problem is that A doesn't explain \"how\" which is explained in B."
      },
      {
        "date": "2023-12-14T15:36:00.000Z",
        "voteCount": 4,
        "content": "Should be B  :\"Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time\" . Same query with same result should be cached."
      },
      {
        "date": "2024-05-11T06:37:00.000Z",
        "voteCount": 1,
        "content": "It's what is doing the answer A as well. Where's the problem?"
      },
      {
        "date": "2023-12-11T21:03:00.000Z",
        "voteCount": 4,
        "content": "Option A suggest Converting the API Gateway endpoint and enabling caching is not as effective for this scenario because edge-optimized endpoints are primarily for global distribution. This application is regional"
      },
      {
        "date": "2023-12-11T21:31:00.000Z",
        "voteCount": 3,
        "content": "The problem being solved in this scenario is not a latency related, but catching, therefore, i am sticking with pick of B over A"
      },
      {
        "date": "2023-12-02T03:55:00.000Z",
        "voteCount": 4,
        "content": "I'd say B, solution A can reduce latency using Edge API, moreover the caching part is too vague, what does it mean enable caching in the production? It means exactly solution B."
      },
      {
        "date": "2023-11-30T05:01:00.000Z",
        "voteCount": 2,
        "content": "multiple HTTP GET requests for the same queries -&gt; leaning towards caching.\nAlso remark that company requires minimize costs, redis is out since you will have to spend money while there is need for it during low loads. A is the best solution here."
      },
      {
        "date": "2023-11-29T21:06:00.000Z",
        "voteCount": 2,
        "content": "B is right answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 393,
    "url": "https://www.examtopics.com/discussions/amazon/view/126850-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.<br><br>The database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-29T19:32:00.000Z",
        "voteCount": 5,
        "content": "Answer B - Company has created a DB schema on AWS. So next logical step is to use DMS for DB migration over the Private VIF. VPC Endpoint is also used for DMS."
      },
      {
        "date": "2024-01-10T12:01:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-12-14T15:40:00.000Z",
        "voteCount": 2,
        "content": "Should be B\nAll other options are Loading data into S3 then copy again to DB . Way to slow"
      },
      {
        "date": "2023-12-14T13:10:00.000Z",
        "voteCount": 2,
        "content": "B: Definitly DMS"
      },
      {
        "date": "2023-11-28T04:32:00.000Z",
        "voteCount": 3,
        "content": "database migration AND least possible downtime? AWS DMS"
      },
      {
        "date": "2023-11-22T21:20:00.000Z",
        "voteCount": 2,
        "content": "B. Use o AWS Database Migration Service (AWS DMS) para migrar os dados para a AWS. Crie uma inst\u00e2ncia de replica\u00e7\u00e3o DMS em uma sub-rede privada. Crie endpoints VPC para AWS DMS. Configure uma tarefa DMS para copiar dados do banco de dados local para a inst\u00e2ncia de banco de dados usando carga total mais captura de dados de altera\u00e7\u00e3o (CDC). Use a chave padr\u00e3o do AWS Key Management Service (AWS KMS) para criptografia em repouso. Use TLS para criptografia em tr\u00e2nsito."
      },
      {
        "date": "2023-11-22T16:24:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/89247-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-22T13:33:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-11-22T00:07:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 394,
    "url": "https://www.examtopics.com/discussions/amazon/view/126958-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "Accompany is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.<br><br>All of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.<br><br>Which storage solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "\u30a4",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-03T01:59:00.000Z",
        "voteCount": 17,
        "content": "- General purpose performance mode (default)\nIdeal for latency-sensitive use cases.\n- Max I/O mode\nCan scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations."
      },
      {
        "date": "2024-04-18T12:23:00.000Z",
        "voteCount": 1,
        "content": "It's \"B\", not \"D\".\n\n\"For workloads that require high throughput and IOPS, use Regional file systems configured with the General Purpose performance mode and Elastic throughput.\nNote: To achieve the maximum 250,000 read IOPS for frequently accessed data, the file system must use Elastic throughput.\nNote: Elastic throughput is available only for file systems that use the General Purpose performance mode.\nMax I/O mode is not supported for One Zone file systems or file systems that use Elastic throughput.\"\n-\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html"
      },
      {
        "date": "2024-05-11T08:07:00.000Z",
        "voteCount": 2,
        "content": "Yes, that's the reason the description is saying that the EC2 are spread in multiple AZs. It's option \"D\" the correct one"
      },
      {
        "date": "2024-03-19T15:54:00.000Z",
        "voteCount": 8,
        "content": "D\nIn contrast, Max I/O file systems are suitable for workloads such as data analytics, media processing, and machine learning. These workloads need to perform parallel operations from hundreds or even thousands of containers and require the highest possible aggregate throughput and IOPS\n2 keywords matching the question, Throughput and Data analytic\nhttps://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/storage-efs.html"
      },
      {
        "date": "2024-05-03T04:24:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2024-04-18T12:25:00.000Z",
        "voteCount": 1,
        "content": "B - correct."
      },
      {
        "date": "2024-03-21T19:03:00.000Z",
        "voteCount": 2,
        "content": "for analytics workload"
      },
      {
        "date": "2024-03-16T20:47:00.000Z",
        "voteCount": 3,
        "content": "D looks correct"
      },
      {
        "date": "2024-03-10T10:40:00.000Z",
        "voteCount": 3,
        "content": "Option D because of High Throughput requirement"
      },
      {
        "date": "2024-03-08T13:56:00.000Z",
        "voteCount": 2,
        "content": "from https://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes: \n\n\"Due to the higher per-operation latencies with Max I/O, we recommend using General Purpose performance mode for all file systems.\""
      },
      {
        "date": "2024-02-23T23:51:00.000Z",
        "voteCount": 3,
        "content": "Performance\n......'' In contrast, Max I/O file systems are suitable for workloads such as data analytics, media processing, and machine learning. \".........\nref:\nhttps://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/storage-efs.html"
      },
      {
        "date": "2024-02-18T11:50:00.000Z",
        "voteCount": 1,
        "content": "IOPS is something different than throughput"
      },
      {
        "date": "2024-01-27T09:40:00.000Z",
        "voteCount": 2,
        "content": "IOPS is not throughput...  General Purpose performance mode has a higher throughput"
      },
      {
        "date": "2024-01-16T00:06:00.000Z",
        "voteCount": 2,
        "content": "\"Max I/O performance mode has higher per-operation latencies than General Purpose performance mode. For faster performance, we recommend always using General Purpose performance mode\"\n\nNo performance requirement but high I/O in the question."
      },
      {
        "date": "2024-01-10T12:05:00.000Z",
        "voteCount": 3,
        "content": "Option D as Maximum Throughput is primary requirement here."
      },
      {
        "date": "2024-01-06T08:55:00.000Z",
        "voteCount": 1,
        "content": "B seems more appropriate."
      },
      {
        "date": "2024-01-03T14:48:00.000Z",
        "voteCount": 1,
        "content": "think Both B and D are correct.. But I'll go with B coz the hierarchy is general purpose then Max I/O..\n- A. Storage Gateway: While providing NFS access to S3, it's not optimized for high throughput or real-time access like EFS, making it less suitable for big data analytics workloads.\n- C. EBS Volume: EBS volumes can only be attached to a single EC2 instance at a time, limiting their use for shared storage across multiple instances.\n- D. EFS Max I/O: While offering the highest throughput, it's more expensive than General Purpose mode and might not be necessary for all big data workloads."
      },
      {
        "date": "2024-01-02T05:28:00.000Z",
        "voteCount": 1,
        "content": "D ans\nWhat latency, throughput, and IOPS performance can I expect for my Amazon EFS file system?\nThe expected performance for your Amazon EFS file system depends on its specific configuration (for instance, storage class and thoroughput mode) and the specific file system operation type (read or write). Please see the File System Performance documentation for more information on expected latency , maximum throughput, and maximum IOPS performance for Amazon EFS file systems."
      },
      {
        "date": "2024-01-01T00:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes\nImportant\nDue to the higher per-operation latencies with Max I/O, we recommend using General Purpose performance mode for all file systems."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 395,
    "url": "https://www.examtopics.com/discussions/amazon/view/126851-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.<br><br>The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.<br><br>A solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.<br><br>What should the solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T04:39:00.000Z",
        "voteCount": 10,
        "content": "One thing we can learn here is if you see \"aurora serverless VERSION 1\" -&gt; migrate away from this"
      },
      {
        "date": "2023-11-26T12:20:00.000Z",
        "voteCount": 7,
        "content": "D is the answer. \n\nConvert the Aurora Serverless v1 database to a standard Aurora MySQL global database extending across the source and target regions, launch the solution in the target region, and configure the two regional solutions to work in an active-passive configuration. This approach provides the necessary speed for recovery and data replication to meet the strict RTO and RPO.\n\nAurora Serverless v1 doesn't support read replicas, cross region replicas, or global databases.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations"
      },
      {
        "date": "2024-09-27T12:06:00.000Z",
        "voteCount": 1,
        "content": "B is indeed a better choice here because it focuses on using a global database with automated deployment, which is critical for achieving both the RTO and RPO requirements efficiently."
      },
      {
        "date": "2024-01-10T12:11:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-11-28T11:24:00.000Z",
        "voteCount": 4,
        "content": "D will provide \"RTO of 5 minutes and an RPO of 1 minute\""
      },
      {
        "date": "2023-11-22T21:13:00.000Z",
        "voteCount": 2,
        "content": "D. Altere o banco de dados Aurora Serverless v1 para um banco de dados global Aurora MySQL padr\u00e3o que se estende pela regi\u00e3o de origem e pela regi\u00e3o de destino. Inicie a solu\u00e7\u00e3o na regi\u00e3o de destino. Configure as duas solu\u00e7\u00f5es regionais para funcionarem em uma configura\u00e7\u00e3o ativa-passiva."
      },
      {
        "date": "2023-11-22T16:35:00.000Z",
        "voteCount": 4,
        "content": "Option D (Change to Aurora MySQL Global Database and Launch Solution in Target Region with Active-Passive Configuration) is the most suitable solution. It addresses both the database replication and application layer readiness in the target region, meeting the specified RTO and RPO requirements."
      },
      {
        "date": "2023-11-23T15:13:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect because changing the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region and launching the solution in the target Region does not meet the RTO and RPO requirements."
      },
      {
        "date": "2023-11-22T13:42:00.000Z",
        "voteCount": 2,
        "content": "A for sure"
      },
      {
        "date": "2023-11-23T15:12:00.000Z",
        "voteCount": 1,
        "content": "To design a disaster recovery (DR) strategy that can recover the solution in another AWS Region with an RTO of 5 minutes and an RPO of 1 minute, the best solution would be to create a read replica of the Aurora Serverless v1 database in the target Region. Then, use AWS SAM to create a runbook to deploy the solution to the target Region. Finally, promote the read replica to primary in case of disaster."
      },
      {
        "date": "2023-11-29T19:52:00.000Z",
        "voteCount": 2,
        "content": "Aurora Serverless v1 db does not support replicas."
      },
      {
        "date": "2023-11-22T00:12:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 396,
    "url": "https://www.examtopics.com/discussions/amazon/view/126852-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.<br><br>Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.<br><br>During content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the EC2 instances before the content updates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution\u2019s DNS alias. Manually scale up EC2 instances before the content updates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the application before the content updates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution's DNS alias. Manually scale up EC2 instances before the content updates."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T20:25:00.000Z",
        "voteCount": 29,
        "content": "The length of these questions should be a crime...."
      },
      {
        "date": "2024-08-13T05:30:00.000Z",
        "voteCount": 1,
        "content": "Couldn't agree more.\n\nThey can just ask us, hey for DynamoDB's cache, should you use DAX or ElasticCache or Memcached? Is CloudFront Distribution designed for fixing sudden traffic spike?\n\nThen we can just say: 1. DAX; 2. no. :D"
      },
      {
        "date": "2024-01-03T14:18:00.000Z",
        "voteCount": 8,
        "content": "Option A correct... other options\nB. ElastiCache for Redis: While a good caching solution, DAX is specifically optimized for DynamoDB, making it a better choice in this context.\nC. ElastiCache for Memcached: Memcached is not as feature-rich as Redis and lacks DAX's DynamoDB integration.\nD. CloudFront: While useful for content delivery, it's not the primary solution for handling database load and scaling EC2 instances."
      },
      {
        "date": "2024-08-20T06:26:00.000Z",
        "voteCount": 1,
        "content": "That felt like reading a eulogy with a gun to my face."
      },
      {
        "date": "2024-07-14T02:42:00.000Z",
        "voteCount": 1,
        "content": "A, for sure.\nNo need for CF in the case of content updates"
      },
      {
        "date": "2024-03-20T09:42:00.000Z",
        "voteCount": 1,
        "content": "A: Correct. Utilizes DAX for DynamoDB caching, Auto Scaling for EC2, and ALB for traffic distribution; aligns with best practices.\n\nB Incorrect. CloudFront is not optimal for dynamic content load handling; manual scaling is less efficient than scheduled scaling."
      },
      {
        "date": "2024-01-10T12:20:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-29T20:03:00.000Z",
        "voteCount": 3,
        "content": "Answer - A. use DAX, in memory cache of DynamoDB.\nB is wrong - manually scale up &amp; Autoscaling group as origin for the CF distro"
      },
      {
        "date": "2023-11-26T09:44:00.000Z",
        "voteCount": 3,
        "content": "A - Update issue no need CloudFront here"
      },
      {
        "date": "2023-11-22T20:30:00.000Z",
        "voteCount": 3,
        "content": "A. Configure o DynamoDB Accelerator (DAX) como cache na mem\u00f3ria. Atualize o aplicativo para usar o DAX. Crie um grupo do Auto Scaling para as inst\u00e2ncias do EC2. Crie um balanceador de carga de aplicativo (ALB). Defina o grupo do Auto Scaling como destino para o ALB. Atualize o registro do Route 53 para usar uma pol\u00edtica de roteamento simples que tenha como alvo o alias DNS do ALB. Configure o escalonamento programado para as inst\u00e2ncias do EC2 antes das atualiza\u00e7\u00f5es de conte\u00fado."
      },
      {
        "date": "2023-11-22T16:44:00.000Z",
        "voteCount": 3,
        "content": "https://www.examtopics.com/discussions/amazon/view/70883-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-22T13:54:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-23T15:05:00.000Z",
        "voteCount": 1,
        "content": "Yes, A is correct"
      },
      {
        "date": "2023-11-22T00:16:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 397,
    "url": "https://www.examtopics.com/discussions/amazon/view/126853-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to store and process image data that will be uploaded from mobile devices using a custom mobile app. Usage peaks between 8 AM and 5 PM on weekdays, with thousands of uploads per minute. The app is rarely used at any other time. A user is notified when image processing is complete.<br><br>Which combination of actions should a solutions architect take to ensure image processing can scale to handle the load? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon MQ queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Lambda function to perform image processing when a message is available in the queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an S3 Batch Operations job to perform image processing when a message is available in the queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend a push notification to the mobile app by using Amazon Simple Email Service (Amazon SES) when processing is complete."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-29T20:15:00.000Z",
        "voteCount": 5,
        "content": "BCE - SQS +  Lambda + SNS"
      },
      {
        "date": "2024-03-29T12:35:00.000Z",
        "voteCount": 1,
        "content": "100% vote on SNS over SES, when I see this question.\n\u201cA user is notified when image processing is complete\u201d, that means the user needs to subscribe the SNS.\nThen, there are two ways to achieve this: Create different SNS for each user, or create different subscription for each user on the same SNS and apply filter policy. Apparently, the latter one is better, but it still need a heavy administration overhead which can\u2019t not be completed manually. Then, another automation piece will be required to maintain the subscription list. Which is not mentioned in any of the answers. \nDoes that sound a good design? \nI go with SES, cause it will be much easier to design the solution. I understand SES is not usually for push notification, but I hate complex solutions."
      },
      {
        "date": "2024-08-20T06:12:00.000Z",
        "voteCount": 1,
        "content": "Don't complicate it for yourself. SES isn't for push notifications."
      },
      {
        "date": "2024-01-10T12:23:00.000Z",
        "voteCount": 1,
        "content": "Option B, C, E"
      },
      {
        "date": "2024-01-03T13:01:00.000Z",
        "voteCount": 3,
        "content": "BCE... other options incorrect\nA. Amazon MQ: While viable for durable messaging, it's less scalable and cost-effective compared to SQS for this use case.\nD. S3 Batch Operations: Designed for batch processing of large datasets, not real-time processing of individual image uploads.\nF. Amazon SES: Primarily for email delivery, not push notifications to mobile apps."
      },
      {
        "date": "2023-12-22T12:53:00.000Z",
        "voteCount": 2,
        "content": "agree BCE"
      },
      {
        "date": "2023-11-28T04:44:00.000Z",
        "voteCount": 3,
        "content": "BCE answer"
      },
      {
        "date": "2023-11-26T09:48:00.000Z",
        "voteCount": 3,
        "content": "BCE answer"
      },
      {
        "date": "2023-11-22T16:46:00.000Z",
        "voteCount": 3,
        "content": "ditto S3"
      },
      {
        "date": "2023-11-22T13:58:00.000Z",
        "voteCount": 2,
        "content": "BCE Answers"
      },
      {
        "date": "2023-11-22T00:17:00.000Z",
        "voteCount": 1,
        "content": "Answer: B C E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 398,
    "url": "https://www.examtopics.com/discussions/amazon/view/126854-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is building an application on AWS. The application sends logs to an Amazon OpenSearch Service cluster for analysis. All data must be stored within a VPC.<br><br>Some of the company\u2019s developers work from home. Other developers work from three different company office locations. The developers need to access OpenSearch Service to analyze and visualize logs directly from their local development machines.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway, and connect it to the VPC. Create an AWS Site-to-Site VPN. Create an attachment to the transit gateway. Instruct the developers to connect by using an OpenVPN client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway, and connect it to the VPOrder an AWS Direct Connect connection. Set up a public VIF on the Direct Connect connection. Associate the public VIF with the transit gateway. Instruct the developers to connect to the Direct Connect connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and configure a bastion host in a public subnet of the VPC. Configure the bastion host security group to allow SSH access from the company CIDR ranges. Instruct the developers to connect by using SSH."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T12:52:00.000Z",
        "voteCount": 5,
        "content": "A correct: Best choice to use Client VPN\nB. Site-to-Site VPN: Designed for connecting entire networks, not individual devices, and requires VPN hardware/software at each office location.\nC. Direct Connect: Primarily for high-bandwidth, low-latency connections between on-premises networks and AWS, not individual developer access.\nD. Bastion Host: While providing access, it introduces a potential security risk by exposing a public-facing host and requires developers to learn SSH."
      },
      {
        "date": "2024-01-10T12:25:00.000Z",
        "voteCount": 3,
        "content": "Option A"
      },
      {
        "date": "2023-12-14T13:19:00.000Z",
        "voteCount": 3,
        "content": "A because work from home"
      },
      {
        "date": "2023-11-30T05:16:00.000Z",
        "voteCount": 3,
        "content": "Site-to-Site and Direct Connect eliminates the developers from home to acces VPC -&gt; B and C out \nD states compagny CIDR range, so also developers at home are excluded -&gt; D out\nA is only valid option. Each developer needs to access environment using point to site construction."
      },
      {
        "date": "2023-11-29T20:16:00.000Z",
        "voteCount": 1,
        "content": "Answer A - Client VPN endpoint"
      },
      {
        "date": "2023-11-23T08:51:00.000Z",
        "voteCount": 3,
        "content": "1. https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/cvpn-working-endpoints.html\n2. https://docs.aws.amazon.com/vpn/latest/clientvpn-user/self-service-portal.html"
      },
      {
        "date": "2023-11-22T16:49:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/69499-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-22T00:18:00.000Z",
        "voteCount": 3,
        "content": "Answer: A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 399,
    "url": "https://www.examtopics.com/discussions/amazon/view/126855-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company\u2019s security policy states that privileges and network permissions must be configured according to best practice, using least privilege.<br><br>A solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.<br><br>What steps are required after the deployment to meet the requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate tasks using the bridge network mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate tasks using the awsvpc network mode.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply security groups to the tasks, and use IAM roles for tasks to access other resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T01:36:00.000Z",
        "voteCount": 4,
        "content": "BE , you can apply security group to the task using vpcmode, because in vpcmode the task will use ENI within the VPC and the ENI can use security groups"
      },
      {
        "date": "2024-03-21T16:43:00.000Z",
        "voteCount": 1,
        "content": "BE for sure"
      },
      {
        "date": "2024-01-10T12:28:00.000Z",
        "voteCount": 1,
        "content": "Option B and E"
      },
      {
        "date": "2023-11-28T04:48:00.000Z",
        "voteCount": 2,
        "content": "BE, easy"
      },
      {
        "date": "2023-11-22T20:23:00.000Z",
        "voteCount": 2,
        "content": "B. Crie tarefas usando o modo de rede awsvpc.\nE. Aplique grupos de seguran\u00e7a \u00e0s tarefas e use fun\u00e7\u00f5es do IAM para tarefas para acessar outros recursos."
      },
      {
        "date": "2023-11-22T20:22:00.000Z",
        "voteCount": 2,
        "content": "B. Crie tarefas usando o modo de rede awsvpc.\nE. Aplique grupos de seguran\u00e7a \u00e0s tarefas e use fun\u00e7\u00f5es do IAM para tarefas para acessar outros recursos."
      },
      {
        "date": "2023-11-22T16:58:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/5362-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-22T14:02:00.000Z",
        "voteCount": 1,
        "content": "BE for sure"
      },
      {
        "date": "2023-11-22T00:18:00.000Z",
        "voteCount": 1,
        "content": "Answer: B E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 400,
    "url": "https://www.examtopics.com/discussions/amazon/view/126856-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a serverless application that consists of several AWS Lambda functions and Amazon DynamoDB tables. The company has created new functionality that requires the Lambda functions to access an Amazon Neptune DB cluster. The Neptune DB cluster is located in three subnets in a VPC.<br><br>Which of the possible solutions will allow the Lambda functions to access the Neptune DB cluster and DynamoDB tables? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three public subnets in the Neptune VPC, and route traffic through an internet gateway. Host the Lambda functions in the three new public subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the Lambda functions outside the VPUpdate the Neptune security group to allow access from the IP ranges of the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the Lambda functions outside the VPC. Create a VPC endpoint for the Neptune database, and have the Lambda functions access Neptune over the VPC endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-26T20:51:00.000Z",
        "voteCount": 9,
        "content": "B and E is the answer. Was really torn about option D....\n\nD involves hosting Lambda functions outside the VPC and creating a VPC endpoint for the Neptune database. The key issue here is that while AWS supports VPC endpoints for several services, as of my last update in April 2023, Amazon Neptune does not support VPC endpoints. Without VPC endpoint support for Neptune, Lambda functions outside the VPC cannot access the Neptune DB cluster in this way.\n\nSo it must be B and E !"
      },
      {
        "date": "2024-10-14T18:16:00.000Z",
        "voteCount": 1,
        "content": "For B: \nWhy do we need to  route internet traffic through a NAT gateway??"
      },
      {
        "date": "2024-03-20T09:55:00.000Z",
        "voteCount": 4,
        "content": "The only thing to remember with this question is that the two alternatives are SEPARATE. They are complete on their own and are not in conjunction."
      },
      {
        "date": "2024-03-19T22:09:00.000Z",
        "voteCount": 1,
        "content": "For B how will the Lambda access DynamoDB from a Private subnet and without an IGW? Should be A."
      },
      {
        "date": "2024-03-19T17:18:00.000Z",
        "voteCount": 2,
        "content": "till March 2024 \n\"its endpoints are only accessible within that VPC\"\nhttps://docs.aws.amazon.com/neptune/latest/userguide/security-vpc.html\nso any answer outside the VPC is wrong\napparently you won't choose A to have it public"
      },
      {
        "date": "2024-03-08T09:16:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      },
      {
        "date": "2023-12-11T02:00:00.000Z",
        "voteCount": 4,
        "content": "Amazon Neptune only allows connections from clients located in the same VPC as the Neptune cluster. So we have to use a load balancer or proxy inside the vpc to give us access.  The following Github article show architectural designs that outline the approach.\n\nhttps://aws-samples.github.io/aws-dbs-refarch-graph/src/connecting-using-a-load-balancer/#:~:text=your%20Neptune%20cluster.-,Amazon%20Neptune%20only%20allows%20connections%20from%20clients%20located%20in%20the,via%20an%20Application%20Load%20Balancer."
      },
      {
        "date": "2023-11-23T08:34:00.000Z",
        "voteCount": 2,
        "content": "B. Create three private subnets in the Neptune VPC, route internet traffic through a NAT gateway, and host the Lambda functions in the new private subnets.\nE. Create three private subnets in the Neptune VPC, host the Lambda functions in these subnets, and create a VPC endpoint for DynamoDB."
      },
      {
        "date": "2023-11-22T20:17:00.000Z",
        "voteCount": 1,
        "content": "op\u00e7\u00f5es B e E s\u00e3o as mais vi\u00e1veis"
      },
      {
        "date": "2023-11-22T20:16:00.000Z",
        "voteCount": 2,
        "content": "Portanto, as op\u00e7\u00f5es B e E s\u00e3o as mais vi\u00e1veis para permitir que as fun\u00e7\u00f5es Lambda acessem tanto o cluster de banco de dados Amazon Neptune quanto as tabelas do Amazon DynamoDB."
      },
      {
        "date": "2023-11-22T17:07:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/81635-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2023-11-22T14:11:00.000Z",
        "voteCount": 1,
        "content": "Answer DE"
      },
      {
        "date": "2023-11-23T14:54:00.000Z",
        "voteCount": 1,
        "content": "It's true BE"
      },
      {
        "date": "2023-11-22T00:19:00.000Z",
        "voteCount": 1,
        "content": "Answer: B E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 401,
    "url": "https://www.examtopics.com/discussions/amazon/view/126960-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to design a disaster recovery (DR) solution for an application that runs in the company\u2019s data center. The application writes to an SMB file share and creates a copy on a second file share. Both file shares are in the data center. The application uses two types of files: metadata files and image files.<br><br>The company wants to store the copy on AWS. The company needs the ability to use SMB to access the data from either the data center or AWS if a disaster occurs. The copy of the data is rarely accessed but must be available within 5 minutes.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS Outposts with Amazon S3 storage. Configure a Windows Amazon EC2 instance on Outposts as a file server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon FSx File Gateway. Configure an Amazon FSx for Windows File Server Multi-AZ file system that uses SSD storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and to use S3 Glacier Deep Archive for the image files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-20T11:19:00.000Z",
        "voteCount": 9,
        "content": "Answer is D = S3 File Gateway.\nFor those that have chosen B, they are right of course as FSx File Gateway will work as well.   But if you read the requirements (The company wants to store the copy on AWS. The company needs the ability to use SMB to access the data from either the data center or AWS if a disaster occurs. The copy of the data is rarely accessed but must be available within 5 minutes.)  it is only about storing the data with rare access.  So why would you choose option B that has a Multi-AZ + SSD over a cheaper option for DR?"
      },
      {
        "date": "2023-12-20T11:23:00.000Z",
        "voteCount": 1,
        "content": "and A is just silly"
      },
      {
        "date": "2023-12-20T11:22:00.000Z",
        "voteCount": 2,
        "content": "and of course C would be wrong because for glacier, it would take more than 5 minutes to get the files out"
      },
      {
        "date": "2024-02-04T23:01:00.000Z",
        "voteCount": 7,
        "content": "Vote for D:\n1. Amazon S3 File Gateway is suitable for SMB file share https://docs.aws.amazon.com/filegateway/latest/files3/CreatingAnSMBFileShare.html\n2. S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. https://aws.amazon.com/s3/storage-classes/"
      },
      {
        "date": "2024-05-03T04:28:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2024-03-08T09:29:00.000Z",
        "voteCount": 2,
        "content": "Option B is right."
      },
      {
        "date": "2024-03-10T22:52:00.000Z",
        "voteCount": 4,
        "content": "I wouldnt waste tons of money to host data that a rarely accessed on FSx. S3 IA will do just fine."
      },
      {
        "date": "2024-01-28T21:32:00.000Z",
        "voteCount": 3,
        "content": "Amazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3 cloud storage. Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises data-intensive Amazon EC2-based applications that need file protocol access to S3 object storage.\n\nhttps://aws.amazon.com/storagegateway/file/s3/"
      },
      {
        "date": "2024-01-22T06:33:00.000Z",
        "voteCount": 1,
        "content": "https://www.amazonaws.cn/en/storagegateway/faqs/\n\nWith S3 File Gateway, your configured S3 buckets will be available as Network File System (NFS) mount points or Server Message Block (SMB) file shares. Your applications read and write files and directories over NFS or SMB, interfacing to the gateway as a file server. In turn, the gateway translates these file operations into object requests on your S3 buckets."
      },
      {
        "date": "2024-01-11T12:12:00.000Z",
        "voteCount": 3,
        "content": "Option B. - Requirement states that company needs SMB protocol access to it in case of Disaster in AWS, this is only possible with Fsx Filegateway."
      },
      {
        "date": "2023-12-28T16:05:00.000Z",
        "voteCount": 1,
        "content": "While S3 File Gateway options (C and D) are cost-effective for long-term storage, they introduce retrieval delays that don't meet the 5-minute availability requirement."
      },
      {
        "date": "2023-12-16T06:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is B. Although S3 filegateway supports both NFS and SMB, D cannot be the right answer since question does not mention it to be cost efficient."
      },
      {
        "date": "2023-11-29T20:39:00.000Z",
        "voteCount": 3,
        "content": "Answer D - User S3 file GW with S3 Infreq Access for metadata and image files"
      },
      {
        "date": "2023-11-23T08:36:00.000Z",
        "voteCount": 3,
        "content": "Amazon S3 File Gateway supports SMB and can be used to store and retrieve files in Amazon S3 using file-based interfaces. Using S3 Standard-Infrequent Access for both metadata and image files ensures that the data is available within the required 5 minutes while optimizing costs for infrequently accessed data."
      },
      {
        "date": "2023-11-22T20:00:00.000Z",
        "voteCount": 4,
        "content": "D. Implantar um gateway de arquivos Amazon S3. Configure o S3 File Gateway para usar o Amazon S3 Standard-Infrequent Access (S3 Standard-IA) para os arquivos de metadados e arquivos de imagem."
      },
      {
        "date": "2023-11-23T14:43:00.000Z",
        "voteCount": 4,
        "content": "D is incorrect because deploying an Amazon S3 File Gateway and configuring the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files does not provide the ability to use SMB to access the data from either the data center or AWS if a disaster occurs."
      },
      {
        "date": "2023-11-26T22:35:00.000Z",
        "voteCount": 1,
        "content": "GPT ? lol"
      },
      {
        "date": "2024-01-03T11:16:00.000Z",
        "voteCount": 1,
        "content": "D is correct --- option B is incorrect, FSx File Gateway with Multi-AZ SSD: Offers high performance but is more expensive for infrequently accessed data."
      },
      {
        "date": "2023-11-22T14:14:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 402,
    "url": "https://www.examtopics.com/discussions/amazon/view/126961-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is creating a solution that can move 400 employees into a remote working environment in the event of an unexpected disaster. The user desktops have a mix of Windows and Linux operating systems. Multiple types of software, such as web browsers and mail clients, are installed on each desktop.<br><br>A solutions architect needs to implement a solution that can be integrated with the company\u2019s on-premises Active Directory to allow employees to use their existing identity credentials. The solution must provide multifactor authentication (MFA) and must replicate the user experience from the existing desktops.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Activate MFA for Amazon WorkSpaces by using the AWS Management Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon AppStream 2.0 as an application streaming service. Configure Desktop View for the employees. Set up a VPN connection to the on-premises network. Set up Active Directory Federation Services (AD FS) on premises. Connect the VPC network to AD FS through the VPN connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Configure a RADIUS server for MFA.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon AppStream 2.0 as an application streaming service. Set up Active Directory Federation Services on premises. Configure MFA to grant users access on AppStream 2.0."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-02T05:12:00.000Z",
        "voteCount": 16,
        "content": "C is the only way to implement MFA. \"To enable MFA for AWS services such as Amazon WorkSpaces and QuickSight, a key requirement is an MFA solution that is a Remote Authentication Dial-In User Service (RADIUS) server or a plugin to a RADIUS server already implemented in your on-premises infrastructure. \"  https://aws.amazon.com/it/blogs/security/how-to-enable-multi-factor-authentication-for-amazon-workspaces-and-amazon-quicksight-by-using-microsoft-ad-and-on-premises-credentials/"
      },
      {
        "date": "2024-02-08T00:02:00.000Z",
        "voteCount": 7,
        "content": "C. is the answer, but really none of the answers are right. The real flaw here is that they're using an AD connector as a backup. They should be using a managed AD or have an EC2 AD server. If there's an actual disaster, relying on a VPN and a server that might be unreachable well architected."
      },
      {
        "date": "2024-05-05T17:28:00.000Z",
        "voteCount": 2,
        "content": "So true!  I was about to write the exact same thing... Disaster can often equals no more Datacenter so no AD to \"connect\" to for the AD connector."
      },
      {
        "date": "2024-01-15T23:01:00.000Z",
        "voteCount": 1,
        "content": "Answer C. \nhttps://aws.amazon.com/workspaces/\n\"maximize user experience\" is the keyword to decide Option C."
      },
      {
        "date": "2024-01-11T12:19:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-12-27T13:43:00.000Z",
        "voteCount": 1,
        "content": "A and C are out because these options require implementing a RADIUS server on-premise. \nSo, B or D. \nI would prefer B because it is a more secure solution, but since there is no mention of traffic security, I choose D.\nUsing SAML2 you can set MFA for users. \nhttps://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers-further-info.html"
      },
      {
        "date": "2023-11-30T12:56:00.000Z",
        "voteCount": 2,
        "content": "you enable MFA through RADIUS not AWS Console. so A is out.\nthere is no AppStream Linux so B and D are out."
      },
      {
        "date": "2024-01-14T19:31:00.000Z",
        "voteCount": 1,
        "content": "Amazon AppStream 2.0 Introduces Linux Application Streaming\nhttps://aws.amazon.com/about-aws/whats-new/2021/11/amazon-appstream-2-0-linux-application-streaming/"
      },
      {
        "date": "2023-11-27T15:50:00.000Z",
        "voteCount": 2,
        "content": "To enable MFA for AWS services such as Amazon WorkSpaces and QuickSight, a key requirement is an MFA solution that is RADIUS"
      },
      {
        "date": "2023-11-27T06:57:00.000Z",
        "voteCount": 1,
        "content": "why answer is D:  https://aws.amazon.com/appstream2/?p=pm&amp;c=euc&amp;pd=appstream2&amp;z=4"
      },
      {
        "date": "2023-11-26T10:19:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/security/how-to-enable-multi-factor-authentication-for-amazon-workspaces-and-amazon-quicksight-by-using-microsoft-ad-and-on-premises-credentials/"
      },
      {
        "date": "2023-11-22T19:54:00.000Z",
        "voteCount": 2,
        "content": "C. Use Amazon WorkSpaces para o servi\u00e7o de desktop em nuvem. Configure uma conex\u00e3o VPN com a rede local. Crie um conector AD e conecte-se ao Active Directory local. Configure um servidor RADIUS para MFA."
      },
      {
        "date": "2023-11-23T14:39:00.000Z",
        "voteCount": 1,
        "content": "incorrect because it requires you to configure a RADIUS server for MFA, which is not required for this solution"
      },
      {
        "date": "2023-11-22T14:17:00.000Z",
        "voteCount": 3,
        "content": "A for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 403,
    "url": "https://www.examtopics.com/discussions/amazon/view/126962-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has deployed an Amazon Connect contact center. Contact center agents are reporting large numbers of computer-generated calls. The company is concerned about the cost and productivity effects of these calls. The company wants a solution that will allow agents to flag the call as spam and automatically block the numbers from going to an agent in the future.<br><br>What is the MOST operationally efficient solution to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustomize the Contact Control Panel (CCP) by adding a flag call button that will invoke an AWS Lambda function that calls the UpdateContactAttributes API. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the updated attribute and to use a Lambda function to read and write to the DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Contact Lens for Amazon Connect rule that will look for spam calls. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the rule and to invoke an AWS Lambda function to read and write to the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon DynamoDB table to store the spam numbers. Create a quick connect that the agents can transfer the spam call to from the Contact Control Panel (CCP). Modify the quick connect contact flow to invoke an AWS Lambda function to write to the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the initial contact flow to ask for caller input. If the agent does not receive input, the agent should mark the caller as spam. Use an Amazon DynamoDB table to store the spam numbers. Use an AWS Lambda function to read and write to the DynamoDB table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-14T02:56:00.000Z",
        "voteCount": 2,
        "content": "D, operationally efficient also.\nhttps://aws.amazon.com/blogs/contact-center/deter-spam-callers-using-amazon-connect/"
      },
      {
        "date": "2024-02-01T04:13:00.000Z",
        "voteCount": 2,
        "content": "why not C ?"
      },
      {
        "date": "2024-01-15T22:30:00.000Z",
        "voteCount": 4,
        "content": "Sorry. It should be Answer A as per AWS URL. \nhttps://repost.aws/knowledge-center/connect-deny-list-numbers"
      },
      {
        "date": "2024-01-15T22:08:00.000Z",
        "voteCount": 1,
        "content": "Surely Answer C. \nhttps://repost.aws/knowledge-center/connect-deny-list-numbers"
      },
      {
        "date": "2023-11-30T04:03:00.000Z",
        "voteCount": 3,
        "content": "Answer A. Create a Lambda function to store spam /denied numbers in the DynamDB table. Create a second Lambda function to check the table against any incoming number and take appropriate action. \nhttps://repost.aws/knowledge-center/connect-deny-list-numbers"
      },
      {
        "date": "2023-11-26T21:07:00.000Z",
        "voteCount": 2,
        "content": "A is the most operationally efficient solution. It directly empowers agents to flag spam calls with minimal disruption, automates the blocking process via contact flows, and effectively utilizes AWS Lambda and DynamoDB for real-time processing and storage. This approach is both agent-friendly and technically robust, aligning well with the requirements."
      },
      {
        "date": "2023-11-22T19:38:00.000Z",
        "voteCount": 1,
        "content": "C. Use uma tabela do Amazon DynamoDB para armazenar os n\u00fameros de spam. Crie uma conex\u00e3o r\u00e1pida para a qual os agentes possam transferir a chamada de spam a partir do Painel de controle de contato (CCP). Modifique o fluxo de contato de conex\u00e3o r\u00e1pida para invocar uma fun\u00e7\u00e3o do AWS Lambda para gravar na tabela do DynamoDB."
      },
      {
        "date": "2023-11-22T17:23:00.000Z",
        "voteCount": 4,
        "content": "The most operationally efficient solution to allow agents to flag calls as spam and automatically block these numbers from reaching agents in the future in an Amazon Connect contact center involves a combination of Amazon Connect's features, AWS Lambda, and Amazon DynamoDB. Let's evaluate the options:\n\nA. Customize CCP and Use Lambda with DynamoDB:\n\nCustomizing the Contact Control Panel (CCP) by adding a 'flag call' button allows agents to easily mark calls as spam.\nThe button can invoke an AWS Lambda function, which calls the UpdateContactAttributes API to flag the call.\nUsing an Amazon DynamoDB table to store spam numbers is an effective way to maintain a blocklist.\nModifying contact flows to check for the spam attribute and interact with the DynamoDB table via Lambda ensures that future calls from these numbers are blocked.\nThis solution provides a seamless experience for agents and integrates efficiently with Amazon Connect and AWS services."
      },
      {
        "date": "2023-11-22T14:23:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 404,
    "url": "https://www.examtopics.com/discussions/amazon/view/126857-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has mounted sensors to collect information about environmental parameters such as humidity and light throughout all the company's factories. The company needs to stream and analyze the data in the AWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations team must receive a notification immediately.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream the data to an Amazon Kinesis Data Firehose delivery stream. Use AWS Step Functions to consume and analyze the data in the Kinesis Data Firehose delivery stream. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. Set up a trigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream the data to an Amazon Kinesis Data Analytics application. Use an automatically scaled and containerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-08T10:21:00.000Z",
        "voteCount": 1,
        "content": "Option C."
      },
      {
        "date": "2024-01-12T10:31:00.000Z",
        "voteCount": 1,
        "content": "Near real time analysis needs a long running function, while Lambda can only run about 15mins. So, none of the Lamda function answers should be in the picture.\nIOT streaming can be done by Kinesis solution or MSK. \nB: Since this is a continuously running analysis, trigger is not required.\nD: The answer doesn\u2019t mention what solution is used to stream data to Kinesis Data Analytics. And Kinesis Data Analytics itself is a real time analytics tool, which means the ECS is not required.\nNone of B and D is flawless. I vote B because B has less flaws."
      },
      {
        "date": "2024-03-20T17:26:00.000Z",
        "voteCount": 4,
        "content": "B is wrong when you see it use SES for notification"
      },
      {
        "date": "2024-01-11T12:30:00.000Z",
        "voteCount": 2,
        "content": "Option C. D is possible but requirement does not state the notification over e-mail."
      },
      {
        "date": "2023-11-30T04:06:00.000Z",
        "voteCount": 4,
        "content": "Answer C. Use Kinesis Data streams to ingest data streams in real-time and use a AWS Lambda function to analyze data. Use SNS to send notifications to the factory Operations team."
      },
      {
        "date": "2023-11-26T10:25:00.000Z",
        "voteCount": 2,
        "content": "C is answer"
      },
      {
        "date": "2023-11-22T19:34:00.000Z",
        "voteCount": 1,
        "content": "C. Transmita os dados para um fluxo de dados do Amazon Kinesis. Crie uma fun\u00e7\u00e3o AWS Lambda para consumir o fluxo de dados do Kinesis e analisar os dados. Use o Amazon Simple Notification Service (Amazon SNS) para notificar a equipe de opera\u00e7\u00f5es."
      },
      {
        "date": "2023-11-22T17:26:00.000Z",
        "voteCount": 3,
        "content": "Streaming data to an Amazon Kinesis data stream and using an AWS Lambda function for consuming and analyzing the data in real-time is a robust solution.\nAWS Lambda can process the data stream efficiently and trigger immediate actions.\nUsing Amazon SNS for notifications ensures quick and effective communication with the operations team.\nThis solution is likely to provide the real-time analysis and immediate notification required."
      },
      {
        "date": "2023-11-22T14:25:00.000Z",
        "voteCount": 1,
        "content": "Answer c"
      },
      {
        "date": "2023-11-22T00:27:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 405,
    "url": "https://www.examtopics.com/discussions/amazon/view/126963-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for a workload. The company expects the cluster to support an unpredictable number of stateless pods. Many of the pods will be created during a short time period as the workload automatically scales the number of replicas that the workload uses.<br><br>Which solution will MAXIMIZE node resilience?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the workload node groups. Use a smaller number of node groups and larger instances in the node groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the workload to use topology spread constraints that are based on Availability Zone.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-22T17:29:00.000Z",
        "voteCount": 11,
        "content": "Use Topology Spread Constraints Based on Availability Zone"
      },
      {
        "date": "2023-11-23T14:28:00.000Z",
        "voteCount": 5,
        "content": "To maximize node resilience for an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is expected to support an unpredictable number of stateless pods, the best solution would be to configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned."
      },
      {
        "date": "2023-11-24T00:22:00.000Z",
        "voteCount": 9,
        "content": "I guess D, because question requres to MAXIMIZE NODE resilience. Node not workload, so we need to spread nodes across AZs."
      },
      {
        "date": "2024-03-08T10:25:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-11T12:34:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2023-12-12T11:20:00.000Z",
        "voteCount": 5,
        "content": "\"To achieve high availability, customers deploy Amazon EKS worker nodes (Amazon EC2 instances) across multiple distinct AZs. To complement this approach, we recommend customers to implement Kubernetes primitives, such as pod topology spread constraints to achieve pod-level high availability as well as efficient resource utilization.\"\n\nhttps://aws.amazon.com/blogs/containers/getting-visibility-into-your-amazon-eks-cross-az-pod-to-pod-network-bytes/"
      },
      {
        "date": "2023-11-30T04:16:00.000Z",
        "voteCount": 2,
        "content": "Answer D. \nFrom GPT - This approach ensures that the stateless pods are distributed across different Availability Zones, maximizing node resilience. If a failure occurs in one Availability Zone, the impact on the workload is minimized because other pods are spread across different zones. Makes sense for Node Resilience!"
      },
      {
        "date": "2023-11-26T21:13:00.000Z",
        "voteCount": 3,
        "content": "D is the answer. Configuring the workload to use topology spread constraints based on Availability Zone \u2014 is the best solution to maximize node resilience. This approach enhances the stability and availability of the EKS cluster by ensuring that the workload is evenly spread across different Availability Zones, thereby mitigating the risks associated with zone-specific failures or performance issues.\n\nRemember, it's asking about Node Resilience, not Pod Resilience"
      },
      {
        "date": "2023-11-22T19:32:00.000Z",
        "voteCount": 2,
        "content": "D. Configure a carga de trabalho para usar restri\u00e7\u00f5es de propaga\u00e7\u00e3o de topologia baseadas na zona de disponibilidade."
      },
      {
        "date": "2023-11-23T14:27:00.000Z",
        "voteCount": 1,
        "content": "To maximize node resilience for an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is expected to support an unpredictable number of stateless pods, the best solution would be to configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned."
      },
      {
        "date": "2023-11-22T14:28:00.000Z",
        "voteCount": 3,
        "content": "C for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 406,
    "url": "https://www.examtopics.com/discussions/amazon/view/126859-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to implement a disaster recovery (DR) plan for a web application. The application runs in a single AWS Region.<br><br>The application uses microservices that run in containers. The containers are hosted on AWS Fargate in Amazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MySQL DB instance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarm invokes an Amazon EventBridge rule if the application experiences a failure.<br><br>A solutions architect must design a DR solution to provide application recovery to a separate Region. The solution must minimize the time that is necessary to recover from a failure.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a second ECS cluster and ECS service on Fargate in the separate Region. Create an AWS Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that creates a second ECS cluster and ECS service in the separate Region. Configure the Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a second ECS cluster and ECS service on Fargate in the separate Region. Create a cross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambda function to promote the read replica to the primary database. Configure the Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a second ECS cluster and ECS service on Fargate in the separate Region. Take a snapshot of the RDS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWS Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T04:23:00.000Z",
        "voteCount": 5,
        "content": "Answer C. Configure RDS read-replica instead of Snapshots. Invoke Lambda function to promote read-replica to primary and update Route53 to point to secondary region incase of DR"
      },
      {
        "date": "2024-07-11T21:32:00.000Z",
        "voteCount": 1,
        "content": "C, for sure.\nIt's not straight forward to Convert the RDS MySQL snapshot to an Amazon DynamoDB global table."
      },
      {
        "date": "2024-03-08T10:28:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-11T12:41:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-12-13T19:04:00.000Z",
        "voteCount": 2,
        "content": "C. read replica"
      },
      {
        "date": "2023-11-28T04:12:00.000Z",
        "voteCount": 2,
        "content": "Answer c"
      },
      {
        "date": "2023-11-26T10:30:00.000Z",
        "voteCount": 2,
        "content": "The solution must minimize the time that is necessary to recover from a failure"
      },
      {
        "date": "2023-11-22T17:33:00.000Z",
        "voteCount": 1,
        "content": "Second ECS Cluster and RDS Read Replica with Lambda"
      },
      {
        "date": "2023-11-22T14:32:00.000Z",
        "voteCount": 1,
        "content": "Answer c"
      },
      {
        "date": "2023-11-22T00:29:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 407,
    "url": "https://www.examtopics.com/discussions/amazon/view/126860-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has AWS accounts that are in an organization in AWS Organizations. The company wants to track Amazon EC2 usage as a metric. The company\u2019s architecture team must receive a daily alert if the EC2 usage is more than 10% higher the average EC2 usage from the last 30 days.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Budgets in the organization's management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Cost Anomaly Detection in the organization's management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Trusted Advisor in the organization's management account. Configure a cost optimization advisory alert to notify the architecture team if the EC2 usage is 10% more than the reported average usage for the last 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Detective in the organization's management account. Configure an EC2 usage anomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T04:29:00.000Z",
        "voteCount": 14,
        "content": "Answer A. C cannot be correct because Cost Anomaly detection is for a surprise cost exceeds. A is a perfect use case for this scenario."
      },
      {
        "date": "2023-11-29T12:49:00.000Z",
        "voteCount": 9,
        "content": "\"A\" describe perfectly the process to create this kind of control. Besides Cost Anomaly is very focused on \"Cost\", while the question ask to control the \"usage\" (ex:hours), not exactly $ cost. I suggest doing a demo. \"A\" for sure"
      },
      {
        "date": "2024-10-15T16:00:00.000Z",
        "voteCount": 1,
        "content": "AWS Budgets can be used to set custom budget based on your expected usage and notify you when a threshold is exceeded. AWS Cost Anomaly Detection uses advanced machine learning (ML) technologies to identify anomalous spend and root causes."
      },
      {
        "date": "2024-10-11T10:35:00.000Z",
        "voteCount": 2,
        "content": "For those who think the correct answer is B: \nhttps://docs.aws.amazon.com/cost-management/latest/userguide/getting-started-ad.html\n\"For Threshold, enter a number to configure the anomalies that you want to generate alerts for. There are two types of thresholds: absolute and percentage. Absolute thresholds trigger alerts when an anomaly's total COST impact exceeds your chosen threshold. Percentage thresholds trigger alerts when an anomaly's total impact percentage exceeds your chosen threshold. Total impact percentage is the percentage difference between the total expected SPEND and total actual SPEND.\"\"\nAWS COST Anomaly Detection primarily focuses on COST anomalies rather than specific usage metrics."
      },
      {
        "date": "2024-08-16T10:04:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\nAWS Cost Anomaly Detection: This service can monitor your AWS usage and costs, identifying anomalies and deviations from normal usage patterns. By setting up a monitor for Amazon EC2 usage, you can detect if the usage is significantly higher than usual, such as exceeding 10% of the average usage over the past 30 days.\n\nMonitor Type: Choosing \"AWS Service\" as the monitor type allows you to focus specifically on EC2 usage.\n\nAlert Subscription: You can configure alerts to notify the architecture team when the detected usage anomaly exceeds the threshold, such as a 10% increase over the historical average."
      },
      {
        "date": "2024-08-13T18:01:00.000Z",
        "voteCount": 1,
        "content": "It has to be A.\nI have tried to do it in AWS Budget Console. Here's a step by step breakdown of what I have done:\n\nStep 1: Click on the \"Create budget\" button and choose the \"Usage budget\" type\nStep 2: Set the Usage type groups as `EC2 Running hours`,  Set budget amount's baseline timerange as `Last 30 days` with a `daily` period\nStep 3: Configure alerts with 110% of budgeted amount"
      },
      {
        "date": "2024-08-13T18:06:00.000Z",
        "voteCount": 2,
        "content": "When setting the time range, even if the label says `Last 30 days`, but if you hover on it, it expands and says `last-30 day average`\n\nSo really now AWS Budget can help us collect daily average using a 30-day sliding window. You can use this as baseline, and use 110% of baseline value to trigger the alert"
      },
      {
        "date": "2024-07-04T23:07:00.000Z",
        "voteCount": 1,
        "content": "A, for sure.\nService Monitor tracks spend across all deployed services, but not for a specific service (like ec2)"
      },
      {
        "date": "2024-06-01T07:54:00.000Z",
        "voteCount": 1,
        "content": "It should be D as AWS Cost Anomaly Detection is a service that monitors your cost and usage data to detect anomalies based on machine learning models. It can identify unusual spending patterns and notify you when anomalies are detected based on historical usage patterns"
      },
      {
        "date": "2024-06-01T07:54:00.000Z",
        "voteCount": 1,
        "content": "I mean B"
      },
      {
        "date": "2024-03-26T11:00:00.000Z",
        "voteCount": 5,
        "content": "Option A - Maybe I am the problem here, I don't why people are selecting option \"B\", when the the first line in AWS Cost Management documentation Under AWS Budget states -  \"You can use AWS Budgets to track and take action on your AWS costs and usage. You can use AWS Budgets to monitor your aggregate utilization and coverage metrics for your Reserved Instances (RIs) or Savings Plans.\" \nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\nAWS Blog - https://aws.amazon.com/blogs/mt/manage-cost-overruns-part-1/"
      },
      {
        "date": "2024-03-21T03:24:00.000Z",
        "voteCount": 1,
        "content": "It's B. Cost Anomaly detection _can_ do this kind of thing. AWS Budgets is for overall costs and is a less sharp tool here."
      },
      {
        "date": "2024-01-11T13:05:00.000Z",
        "voteCount": 5,
        "content": "Option A - Cost Anomaly detection does not allow to filter based on EC2 type only."
      },
      {
        "date": "2024-01-09T09:27:00.000Z",
        "voteCount": 1,
        "content": "A:\n\nCost dedection is for cost, not for EC2 metrix"
      },
      {
        "date": "2023-12-14T19:56:00.000Z",
        "voteCount": 2,
        "content": "This questions is weird. The best soltuion should be  AWS CloudWatch. No such answer to choose !"
      },
      {
        "date": "2023-11-29T12:39:00.000Z",
        "voteCount": 3,
        "content": "I recommend doing a demo.\nFor sure it is A. It describe perfectly the process."
      },
      {
        "date": "2024-01-01T16:01:00.000Z",
        "voteCount": 2,
        "content": "steps to set up an AWS Budget to track EC2 usage and receive an alert if it's more than 10% higher than the average usage from the last 30 days:\n\nGo to the AWS Management Console |Open the \"Budgets\" service |Create a New Budget:|Choose \"Cost budget\" as the budget type.|Choose the time period for the budget (e.g., Monthly).|Set the start and end dates for the budget.\nConfigure Cost and Usage Details:|Choose the \"Cost and usage\" option.|Specify the \"Service\" as \"Amazon EC2\" to focus on EC2 costs.|Choose the \"Usage type\" as \"Usage Quantity.\"|Set Budgeted Amount:|Set the budgeted amount to be 110% of the average EC2 usage from the last 30 days. \nConfigure Alerts:|Enable the alert threshold.|Set the alert threshold to be \"Actual &gt; Forecasted\" and \"More than 0%\" to be alerted when the actual usage exceeds the forecast."
      },
      {
        "date": "2024-08-13T17:49:00.000Z",
        "voteCount": 1,
        "content": "&gt;|Choose \"Cost budget\" as the budget type.\n\nIt should be \"Usage budget\""
      },
      {
        "date": "2024-08-13T17:49:00.000Z",
        "voteCount": 1,
        "content": "In AWS Budget Console:\nUsage budget\nMonitor your usage of one or more specified usage types or usage type groups and receive alerts when your user-defined thresholds are met. Using usage budgets, the budgeted amount represents your expected usage. For example, you can use a usage budget to monitor the usage of certain services such as Amazon EC2 and Amazon S3."
      },
      {
        "date": "2023-11-26T21:23:00.000Z",
        "voteCount": 2,
        "content": "B is the answer, AWS Cost Anomaly Detection is specifically designed to monitor AWS service usage, identify anomalies based on historical patterns, and can be configured to send alerts when the usage exceeds a certain threshold compared to the average of the last 30 days. This aligns well with the requirement to receive daily alerts if EC2 usage is more than 10% higher than the average usage from the past 30 days."
      },
      {
        "date": "2023-12-18T12:31:00.000Z",
        "voteCount": 1,
        "content": "You're right on most, but on this one, it is A."
      },
      {
        "date": "2023-11-25T22:50:00.000Z",
        "voteCount": 3,
        "content": "Answer: A\nhttps://aws.amazon.com/blogs/aws-cloud-financial-management/launch-daily-cost-and-usage-budgets/"
      },
      {
        "date": "2023-11-22T19:20:00.000Z",
        "voteCount": 3,
        "content": "B. Configure o AWS Cost Anomaly Detection na conta de gerenciamento da organiza\u00e7\u00e3o. Configure um tipo de monitor de servi\u00e7o AWS. Aplique um filtro do Amazon EC2. Configure uma assinatura de alerta para notificar a equipe de arquitetura se o uso for 10% maior que o uso m\u00e9dio dos \u00faltimos 30 dias."
      },
      {
        "date": "2023-11-23T14:16:00.000Z",
        "voteCount": 4,
        "content": "Option B is incorrect because AWS Cost Anomaly Detection is not designed to track EC2 usage as a metric. It is used to detect anomalies in your AWS costs and usage patterns."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 408,
    "url": "https://www.examtopics.com/discussions/amazon/view/126964-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The company\u2019s CIO has asked a solutions architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.<br><br>Which of the following is the MOST reliable approach to meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReceive the orders in an Amazon EC2-hosted database and use EC2 instances to process them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReceive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReceive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReceive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-29T00:05:00.000Z",
        "voteCount": 6,
        "content": "Loosely coupled = SQS - Lambda is also the simplest to use"
      },
      {
        "date": "2024-10-14T11:50:00.000Z",
        "voteCount": 1,
        "content": "Yip that key words \"loosely coupled\" has been repeated several times during the trainings"
      },
      {
        "date": "2024-01-11T13:11:00.000Z",
        "voteCount": 5,
        "content": "option B"
      },
      {
        "date": "2024-03-08T11:03:00.000Z",
        "voteCount": 3,
        "content": "Option B"
      },
      {
        "date": "2024-02-10T09:39:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\nOrder processing is a multi-step cycle not a two step one. Stepfunction and ECS is the most reliable way to go."
      },
      {
        "date": "2024-03-16T21:24:00.000Z",
        "voteCount": 1,
        "content": "what about loosely coupled? SQS required for it."
      },
      {
        "date": "2024-01-15T20:25:00.000Z",
        "voteCount": 3,
        "content": "Option B"
      },
      {
        "date": "2024-01-01T15:38:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: B -- SQS for sure coz you can't take a chance of loosing data."
      },
      {
        "date": "2023-12-12T12:40:00.000Z",
        "voteCount": 3,
        "content": "\". The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table.\" We can't assume, without further information, that it's a multistep action. For now, it just processes one order and send info to Dynamo.\n\nLooks reasonable to use SQS+Lambda for a loosely coupled solution\nB"
      },
      {
        "date": "2023-12-11T02:55:00.000Z",
        "voteCount": 2,
        "content": "Here is an example of a Step Function for a simple order flow.  You can see how many lambda functions will be necessary that can't be replaced by a single SQS and Lambda\n\nhttps://dev.to/aws-builders/aws-step-functions-simple-order-flow-6gn"
      },
      {
        "date": "2023-12-11T02:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\nOrder processing is a multi-step cycle not a two step one."
      },
      {
        "date": "2023-11-30T04:31:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2023-11-28T11:51:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2023-11-28T04:14:00.000Z",
        "voteCount": 4,
        "content": "B for sure"
      },
      {
        "date": "2023-11-22T14:36:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 409,
    "url": "https://www.examtopics.com/discussions/amazon/view/126861-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database. The company needs to launch the Lambda functions in a QA environment and in a production environment.<br><br>The company must not expose credentials within application code and must rotate passwords automatically.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials for both environments in AWS Systems Manager Parameter Store. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the application code of the Lambda functions, pull the credentials from the Parameter Store parameter by using the AWS SDK for Python (Boto3). Add a role to the Lambda functions to provide access to the Parameter Store parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials for both environments in AWS Secrets Manager with distinct key entry for the QA environment and the production environment. Turn on rotation. Provide a reference to the Secrets Manager key as an environment variable for the Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials for both environments in AWS Key Management Service (AWS KMS). Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environment variable for the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate S3 buckets for the QA environment and the production environment. Turn on server-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming pattern that gives each Lambda function\u2019s application code the ability to pull the correct credentials for the function's corresponding environment. Grant each Lambda function's execution role access to Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T04:35:00.000Z",
        "voteCount": 7,
        "content": "Answer B. Always remember - Automatic Password Rotation - AWS Secrets Manager!"
      },
      {
        "date": "2024-03-08T11:06:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-16T09:35:00.000Z",
        "voteCount": 1,
        "content": "Option : B is the correct answer\n\nWhile AWS Systems Manager Parameter Store is a valid service for storing configuration data, including secrets, using AWS KMS for encryption and Boto3 for retrieval, it lacks the built-in support for automatic rotation of secrets\n\nAWS KMS is primarily designed for managing cryptographic keys and does not provide built-in support for storing and rotating secrets like database credentials.\n\nWhile AWS KMS key rotation is available, it is intended for cryptographic key rotation rather than the rotation of sensitive data like passwords."
      },
      {
        "date": "2024-01-12T11:00:00.000Z",
        "voteCount": 1,
        "content": "The correct solution should be: \nStore the database credentials for both environments in AWS Secrets Manager with distinct key entry for the QA environment and the production environment. Enable a Lambda function to rotate the secrets regularly. Create a KMS key for each secret and use them to encrypt the credentials. Assign permissions to allow the business Lambda function to retrieve the credential from Secret manager and decrypt the credential with the KMS key.\n\nB is not ideal but is the only acceptable answer: \n\u201cTurn on rotation.\u201d: In Secret Manager, you must enable a Lambda function to rotate the credential\n\u201cProvide a reference to the Secrets Manager key as an environment variable for the Lambda functions. \u201c permission must be set to allow the Lambda function to use the Key to decrypt the credential."
      },
      {
        "date": "2024-01-11T13:14:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-11-28T04:15:00.000Z",
        "voteCount": 3,
        "content": "\"rotate passwords automatically\" -&gt; AWS Secrets Manager"
      },
      {
        "date": "2023-11-22T17:40:00.000Z",
        "voteCount": 2,
        "content": "AWS Secrets Manager with Rotation Enabled:"
      },
      {
        "date": "2023-11-22T14:39:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-11-22T13:13:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2023-11-22T00:32:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 410,
    "url": "https://www.examtopics.com/discussions/amazon/view/126862-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS Control Tower to manage AWS accounts in an organization in AWS Organizations. The company has an OU that contains accounts. The company must prevent any new or existing Amazon EC2 instances in the OU's accounts from gaining a public IP address.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure all instances in each account in the OU to use AWS Systems Manager. Use a Systems Manager Automation runbook to prevent public IP addresses from being attached to the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the AWS Control Tower proactive control to check whether instances in the OU's accounts have a public IP address. Set the AssociatePublicIpAddress property to False. Attach the proactive control to the OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config custom rule that detects instances that have a public IP address. Configure a remediation action that uses an AWS Lambda function to detach the public IP addresses from the instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-26T11:31:00.000Z",
        "voteCount": 8,
        "content": "Option C -  From AWS doc page  \"Don't use AWS Organizations to update service control policies (SCPs) attached to an OU that is registered with AWS Control Tower. Doing so could result in the controls entering an unknown state, which will require you to repair your landing zone or re-register your OU in AWS Control Tower. Instead, you can create new SCPs and attach those to the OUs rather than editing the SCPs that AWS Control Tower has created.\" \n\nhttps://docs.aws.amazon.com/controltower/latest/userguide/orgs-guidance.html"
      },
      {
        "date": "2024-02-12T10:46:00.000Z",
        "voteCount": 6,
        "content": "Voting for B: SCP will cause a state drift, since company already use Control Tower"
      },
      {
        "date": "2024-08-01T08:59:00.000Z",
        "voteCount": 2,
        "content": "Adding a new SCP will not cause drift. Modifying an existing SCP that was created by CT would, which is not the case here."
      },
      {
        "date": "2024-10-11T11:09:00.000Z",
        "voteCount": 2,
        "content": "\"The company MUST PREVENT...\"\nProactive controls do not directly prevent the action of attaching a public IP.  They are applicable to resources that are specifically PROVISIONED THROUGH AWS SERVICE CATALOG. They do not have the ability to broadly prevent all EC2 instances in an organization from obtaining a public IP, especially those created outside of Service Catalog.\nAlso, as user VerRi says here in the comments, how will AWS Control Tower proactive control \"check whether instances IN the OU's accounts have a public IP address.\"?\nOption C is the best solution because it uses an SCP, which is a preventive control that directly stops the creation or modification of EC2 instances with public IP addresses in all accounts under the specified OU. This ensures compliance with the requirement of preventing public IP addresses on EC2 instances."
      },
      {
        "date": "2024-09-12T20:48:00.000Z",
        "voteCount": 2,
        "content": "B. AWS Control Tower's Active Controls primarily focus on security related best practices such as IAM policies, security group rules, etc., rather than directly controlling the public IP addresses of EC2 instances. Although custom proactive control can be created, setting the associatePublicIpAddress property to False is usually done through API calls or CLI/SDK when starting an EC2 instance, rather than through proactive control in AWS Control Tower.\nC. AWS Service Control Policies (SCPs) are a mechanism provided by AWS Organizations for implementing access control to AWS services at the OU level. SCP can restrict the ability to request public IP addresses when launching EC2 instances within OU accounts, as well as limit the permission to modify existing instances to attach public IP addresses. This fully meets the company's needs as it ensures the implementation of a unified strategy at the OU level without the need to manage each account or instance separately."
      },
      {
        "date": "2024-06-15T10:07:00.000Z",
        "voteCount": 1,
        "content": "SCP prevent but not remediate existing. So correct is B with CT"
      },
      {
        "date": "2024-05-20T03:44:00.000Z",
        "voteCount": 2,
        "content": "Going for B as Control Tower permissions have to be managed using Controls but not SCPs which causes drifts."
      },
      {
        "date": "2024-03-30T06:56:00.000Z",
        "voteCount": 4,
        "content": "B is a bit weird because proactive control is used to check NEW resources. \nIt is weird to say \"Check whether instances IN the OU's accounts have a public IP address.\"."
      },
      {
        "date": "2024-03-21T03:29:00.000Z",
        "voteCount": 2,
        "content": "C.\nB is not correct since Control Tower doesn't have this capability."
      },
      {
        "date": "2024-03-08T11:29:00.000Z",
        "voteCount": 2,
        "content": "Option B is the right option."
      },
      {
        "date": "2024-03-02T06:28:00.000Z",
        "voteCount": 3,
        "content": "NOT B -- These controls are referred to as proactive because they check your resources \u2013**BEFORE** the resources are deployed \u2013 to determine whether the new resources will comply with the controls that are activated in your environment.\n\nThis control applies only to a new network interface created by means of the NetworkInterfaces property, where a NetworkInterfaceId has not been specified.\nBest answer is C"
      },
      {
        "date": "2024-02-15T10:26:00.000Z",
        "voteCount": 2,
        "content": "It is B"
      },
      {
        "date": "2024-02-11T08:45:00.000Z",
        "voteCount": 1,
        "content": "C ans https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-with-ec2-instance-connect-endpoint.html"
      },
      {
        "date": "2024-02-09T06:08:00.000Z",
        "voteCount": 5,
        "content": "C incorrect: Because SCP will surely block creation of instances with Public IP but will not resolve the existing ones. ALso will create a drift in Control Tower\nB is correct"
      },
      {
        "date": "2024-02-05T17:24:00.000Z",
        "voteCount": 4,
        "content": "Making changes to SCPs outside if Control Tower causes state drift.\nhttps://docs.aws.amazon.com/controltower/latest/userguide/external-resources.html\n\nControl Tower has Proactive Controls to cover the requirements\nhttps://docs.aws.amazon.com/controltower/latest/userguide/ec2-rules.html#ct-ec2-pr-9-description\nhttps://docs.aws.amazon.com/controltower/latest/userguide/ec2-rules.html#ct-ec2-pr-8-description"
      },
      {
        "date": "2024-01-15T20:26:00.000Z",
        "voteCount": 1,
        "content": "Option C. Not sure why Option D mentioned as correct."
      },
      {
        "date": "2024-01-14T23:17:00.000Z",
        "voteCount": 1,
        "content": "c will only prevent new instances from gaining a public IP. What if the instances already have public ips?"
      },
      {
        "date": "2024-01-11T13:16:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 411,
    "url": "https://www.examtopics.com/discussions/amazon/view/130043-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying a third-party web application on AWS. The application is packaged as a Docker image. The company has deployed the Docker image as an AWS Fargate service in Amazon Elastic Container Service (Amazon ECS). An Application Load Balancer (ALB) directs traffic to the application.<br><br>The company needs to give only a specific list of users the ability to access the application from the internet. The company cannot change the application and cannot integrate the application with an identity provider. All users must be authenticated through multi-factor authentication (MFA).<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFConfigure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the users in AWS Identity and Access Management (IAM). Attach a resource policy to the Fargate service to require users to use MFA. Configure a listener rule on the ALB to require authentication through IAM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the users in AWS Identity and Access Management (IAM). Enable AWS IAM Identity Center (AWS Single Sign-On). Configure resource protection for the ALB. Create a resource protection rule to require users to use MFA.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user pool in AWS Amplify. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amplify hosted UI."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "U",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T17:53:00.000Z",
        "voteCount": 8,
        "content": "A?\nAs GPT says,\n\nIn this scenario, setting up a user pool in Amazon Cognito allows you to define the specific list of users who can access the application.\nYou can configure the user pool to require multi-factor authentication (MFA), ensuring an additional layer of security for user authentication.\nConfiguring the ALB listener rule to require authentication through the Amazon Cognito hosted UI means that users attempting to access the application through the ALB will be redirected to the Cognito hosted UI for authentication, where they'll need to provide their credentials and MFA code.\nThis setup ensures that only authenticated users from the specific user pool with MFA will have access to the application, meeting the requirements without modifying the application itself."
      },
      {
        "date": "2024-03-09T00:18:00.000Z",
        "voteCount": 3,
        "content": "As application can not be changed to integrate with Identity provider and users needs to be authenticated from internet  using Cognito is the only possible solution among the options."
      },
      {
        "date": "2024-02-11T08:54:00.000Z",
        "voteCount": 3,
        "content": "A ans https://repost.aws/knowledge-center/cognito-user-pool-alb-authentication"
      },
      {
        "date": "2024-01-23T08:28:00.000Z",
        "voteCount": 1,
        "content": "A sounds OK"
      },
      {
        "date": "2024-01-16T23:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\n\nALB authentication only integration with:\nCognito\nAWS_IAM\nLambda authorizer"
      },
      {
        "date": "2024-01-17T01:14:00.000Z",
        "voteCount": 4,
        "content": "No, I am wrong.\nBut answer is still A.\n\nAPI GW authentication only integration with:\nCognito\nAWS_IAM\nLambda authorizer\n\nALB authentication only integration with:\nCognito\nOIDC"
      },
      {
        "date": "2024-01-14T23:30:00.000Z",
        "voteCount": 4,
        "content": "web application = Cognito"
      },
      {
        "date": "2024-01-11T13:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2024-01-04T11:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2023-12-31T19:25:00.000Z",
        "voteCount": 1,
        "content": "B&amp;C is for accessing aws resources"
      },
      {
        "date": "2023-12-31T19:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 412,
    "url": "https://www.examtopics.com/discussions/amazon/view/132873-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is preparing to deploy a new security tool into several previously unused AWS Regions. The solutions architect will deploy the tool by using an AWS CloudFormation stack set. The stack set's template contains an IAM role that has a custom name. Upon creation of the stack set, no stack instances are created successfully.<br><br>What should the solutions architect do to deploy the stacks successfully?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Service Quotas console to request a quota increase for the number of CloudFormation stacks in each new Region in all relevant accounts. Specify the CAPABILITY_IAM capability during the creation of the stack set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify the CAPABILITY_NAMED_IAM capability and the SELF_MANAGED permissions model during the creation of the stack set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify an administration role ARN and the CAPABILITY_IAM capability during the creation of the stack set."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T20:03:00.000Z",
        "voteCount": 6,
        "content": "Some stack templates might include resources that can affect permissions in your AWS account; for example, by creating new AWS Identity and Access Management (IAM) users. For those stacks, you must explicitly acknowledge this by specifying one of these capabilities.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_CreateStack.html"
      },
      {
        "date": "2024-02-18T01:11:00.000Z",
        "voteCount": 5,
        "content": "Question says \"several previously unused AWS Regions\"  so you have to enable them under the Account first ? \nAnd the CAPABILITY_NAMED_IAM for the custom name"
      },
      {
        "date": "2024-03-09T00:23:00.000Z",
        "voteCount": 1,
        "content": "A seems to be the right choice"
      },
      {
        "date": "2024-02-13T08:38:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\nThe following resources require you to specify CAPABILITY_IAM or CAPABILITY_NAMED_IAM: AWS::IAM::Group, AWS::IAM::InstanceProfile, AWS::IAM::Policy, and AWS::IAM::Role. If the application contains IAM resources with custom names, you must specify CAPABILITY_NAMED_IAM.\nWith self-managed permissions, you create the AWS Identity and Access Management (IAM) roles required by StackSets to deploy across accounts and AWS Regions.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\nhttps://docs.aws.amazon.com/serverlessrepo/latest/devguide/acknowledging-application-capabilities.html"
      },
      {
        "date": "2024-02-13T08:43:00.000Z",
        "voteCount": 2,
        "content": "nop, it's A. \nB y \"Enable the new Regions in all relevant accounts. \" they mean: \nCreate the necessary IAM service roles in your administrator and target accounts to define the permissions you want. \nThe A IS CORRECT."
      },
      {
        "date": "2024-02-07T22:20:00.000Z",
        "voteCount": 1,
        "content": "Proper answer is - A\n\nWe want to create Cloudformation stack that contains IAM role with custom name  - so we need to set CAPABILITY_NAMED_IAM"
      },
      {
        "date": "2024-02-05T09:19:00.000Z",
        "voteCount": 3,
        "content": "Correct A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 413,
    "url": "https://www.examtopics.com/discussions/amazon/view/132877-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that uses an Amazon Aurora PostgreSQL DB cluster for the application's database. The DB cluster contains one small primary instance and three larger replica instances. The application runs on an AWS Lambda function. The application makes many short-lived connections to the database's replica instances to perform read-only operations.<br><br>During periods of high traffic, the application becomes unreliable and the database reports that too many connections are being established. The frequency of high-traffic periods is unpredictable.<br><br>Which solution will improve the reliability of the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the proxy. Update the Lambda function to connect to the proxy endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the max_connections setting on the DB cluster's parameter group. Reboot all the instances in the DB cluster. Update the Lambda function to connect to the DB cluster endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure instance scaling for the DB cluster to occur when the DatabaseConnections metric is close to the max connections setting. Update the Lambda function to connect to the Aurora reader endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the Aurora Data API on the proxy. Update the Lambda function to connect to the proxy endpoint."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T20:25:00.000Z",
        "voteCount": 14,
        "content": "lambda -&gt; rds-proxy -&gt; aurora replica(s) read-only endpoint\n\nhttps://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\nhttps://aws.amazon.com/about-aws/whats-new/2021/03/amazon-rds-proxy-adds-read-only-endpoints-for-amazon-aurora-replicas/\n\nRDS Data API is used with  Aurora Serverless\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html#data-api.limitations"
      },
      {
        "date": "2024-03-09T00:27:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-02-11T09:13:00.000Z",
        "voteCount": 2,
        "content": "A is ans lambda -&gt; rds-proxy -&gt; aurora replica(s) read-only endpoint"
      },
      {
        "date": "2024-02-06T08:42:00.000Z",
        "voteCount": 1,
        "content": "rds-proxy"
      },
      {
        "date": "2024-02-05T09:30:00.000Z",
        "voteCount": 4,
        "content": "correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 414,
    "url": "https://www.examtopics.com/discussions/amazon/view/132878-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company is mounting IoT sensors in all of its stores worldwide. During the manufacturing of each sensor, the company\u2019s private certificate authority (CA) issues an X.509 certificate that contains a unique serial number. The company then deploys each certificate to its respective sensor.<br><br>A solutions architect needs to give the sensors the ability to send data to AWS after they are installed. Sensors must not be able to send data to AWS until they are installed.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. During manufacturing, call the RegisterThing API operation and specify the template and parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions state machine that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Specify the Step Functions state machine to validate parameters. Call the StartThingRegistrationTask API operation during installation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Include parameter validation in the template. Provision a claim certificate and a private key for each device that uses the CA. Grant AWS IoT Core service permissions to update AWS IoT things during provisioning."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T20:46:00.000Z",
        "voteCount": 8,
        "content": "In addition to validating the bootstrap certificate presented by devices, Fleet Provisioning also provides Lambda-based provisioning hooks that enable appropriate validation for pertinent device attributes. Examples of device attributes could include a serial number ...\n\nhttps://aws.amazon.com/blogs/iot/how-to-automate-onboarding-of-iot-devices-to-aws-iot-core-at-scale-with-fleet-provisioning/"
      },
      {
        "date": "2024-04-19T13:01:00.000Z",
        "voteCount": 1,
        "content": "Correct ans - C."
      },
      {
        "date": "2024-03-09T00:48:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-02-28T19:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/iot/latest/developerguide/iot-provision.html"
      },
      {
        "date": "2024-02-28T19:53:00.000Z",
        "voteCount": 1,
        "content": "AWS provides several different ways to provision a device and install unique client certificates on it. This section describes each way and how to select the best one for your IoT solution. These options are described in detail in the whitepaper titled Device Manufacturing and Provisioning with X.509 Certificates in AWS IoT Core."
      },
      {
        "date": "2024-02-28T19:51:00.000Z",
        "voteCount": 3,
        "content": "cANSGiven the requirements, Option C is the most suitable solution:\n\nIt combines serial number validation using a Lambda function.\nThe pre-provisioning hook ensures validation before registration.\nThe allow-auto-registration parameter provides fine-grained control over auto-registration."
      },
      {
        "date": "2024-02-13T08:58:00.000Z",
        "voteCount": 1,
        "content": "Devices can be manufactured with a provisioning claim certificate and private key (which are special purpose credentials) embedded in them. If these certificates are registered with AWS IoT, the service can exchange them for unique device certificates that the device can use for regular operations\nhttps://docs.aws.amazon.com/iot/latest/developerguide/provision-wo-cert.html#claim-based"
      },
      {
        "date": "2024-02-11T09:30:00.000Z",
        "voteCount": 1,
        "content": "C is ans"
      },
      {
        "date": "2024-02-05T09:32:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 415,
    "url": "https://www.examtopics.com/discussions/amazon/view/132879-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.<br><br>The software engineers have decided to use AWS CodePipeline to manage their build and deployment process.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T21:06:00.000Z",
        "voteCount": 5,
        "content": "They use Jenkins.  X-Ray is for debugging not unit testing.  Seamless deploys and rollbacks mean blue/green deployments.  That leaves Answer B:\n\nhttps://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/"
      },
      {
        "date": "2024-03-09T00:53:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-13T09:05:00.000Z",
        "voteCount": 1,
        "content": "B, no-brainer"
      },
      {
        "date": "2024-02-11T05:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-02-06T08:48:00.000Z",
        "voteCount": 2,
        "content": "AWS CodeBuild can be used to conduct unit testing. CodeBuild is a managed service that compiles your source code, runs tests, and produces deployable application artifacts. You can create reports in CodeBuild that contain details about tests that are run during builds. These tests can include unit tests, configuration tests, and functional tests"
      },
      {
        "date": "2024-02-06T04:34:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer B \nhttps://aws.amazon.com/about-aws/whats-new/2018/05/aws-codepipeline-supports-push-events-from-github-via-webhooks/\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html"
      },
      {
        "date": "2024-02-05T09:44:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 416,
    "url": "https://www.examtopics.com/discussions/amazon/view/132884-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software as a service (SaaS) company has developed a multi-tenant environment. The company uses Amazon DynamoDB tables that the tenants share for the storage layer. The company uses AWS Lambda functions for the application services.<br><br>The company wants to offer a tiered subscription model that is based on resource consumption by each tenant. Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambda functions. The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account. The company wants to allocate the DynamoDB costs to each tenant to match that tenant's resource consumption.<br><br>Which solution will provide a granular view of the DynamoDB cost for each tenant with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate a new tag that is named tenant ID with each table in DynamoDB. Activate the tag as a cost allocation tag in the AWS Billing and Cost Management console. Deploy new Lambda function code to log the tenant ID in Amazon CloudWatch Logs. Use the AWS CUR to separate DynamoDB consumption cost for each tenant ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. Deploy another Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new partition key that associates DynamoDB items with individual tenants. Deploy a Lambda function to populate the new column as part of each transaction. Deploy another Lambda function to calculate the tenant costs by using Amazon Athena to calculate the number of tenant items from DynamoDB and the overall DynamoDB cost from the AWS CUR. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Lambda function to log the tenant ID, the size of each response, and the duration of the transaction call as custom metrics to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to query the custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs and to calculate the tenant costs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-09T01:05:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-22T17:51:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2024-02-11T05:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-02-07T23:33:00.000Z",
        "voteCount": 4,
        "content": "AWS Cost Explorer API vs cost calculator is really all you need to consider here."
      },
      {
        "date": "2024-02-07T23:35:00.000Z",
        "voteCount": 2,
        "content": "API can automate it, cost calculator is a manual process and never ideal for something like this."
      },
      {
        "date": "2024-02-07T08:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-02-05T22:41:00.000Z",
        "voteCount": 4,
        "content": "Answer B:  LEAST operational effort and fine grained approach.  \nhttps://aws.amazon.com/blogs/apn/optimizing-cost-per-tenant-visibility-in-saas-solutions/\n\nRCU and WCU metrics are already logged in CloudWatch.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html"
      },
      {
        "date": "2024-02-05T09:53:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 417,
    "url": "https://www.examtopics.com/discussions/amazon/view/132886-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company\u2019s security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.<br><br>Which solution will ensure that existing and future objects in the S3 bucket are protected?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExplicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-08T02:17:00.000Z",
        "voteCount": 1,
        "content": "A\nassume role to provide short-term credential"
      },
      {
        "date": "2024-03-27T04:47:00.000Z",
        "voteCount": 2,
        "content": "Option A:  Amazon S3 now allows you to enable S3 Object Lock for existing buckets with just a few clicks and to enable S3 Replication for buckets using S3 Object Lock\n\nhttps://aws.amazon.com/about-aws/whats-new/2023/11/amazon-s3-enabling-object-lock-buckets/#:~:text=To%20lock%20existing%20objects%2C%20you,of%20objects%20at%20a%20time."
      },
      {
        "date": "2024-03-10T11:55:00.000Z",
        "voteCount": 2,
        "content": "The question is, as so often, misleading. None of the alternatives deal with _access_, only with modification."
      },
      {
        "date": "2024-03-09T01:16:00.000Z",
        "voteCount": 4,
        "content": "Option D is the only option that addresses security risk. Option A is not addressing this - Replicating existing bucket to another bucket does not eliminate the risk due to original bucket credential leak."
      },
      {
        "date": "2024-02-29T21:30:00.000Z",
        "voteCount": 2,
        "content": "The question is looking for solution for \u201cconcerned that an attacker could gain access to the AWS account through leaked long-term credentials\u201d. \nNone of the answer is addressing the concern of \u201cAccess\u201d Through \u201cleaked long-term credentials\u201d.\nThe is question doesn\u2019t mention anything about data loss concerns, while, all the answers are providing protection for deleting the data."
      },
      {
        "date": "2024-05-27T14:08:00.000Z",
        "voteCount": 1,
        "content": "creating new account accessed by security team members is action taken to avoid the risk through leaked long-term credentials of existing account so Option A"
      },
      {
        "date": "2024-09-01T17:18:00.000Z",
        "voteCount": 1,
        "content": "Attacker can just take the data and leave it intact. The damage is done."
      },
      {
        "date": "2024-02-10T02:46:00.000Z",
        "voteCount": 4,
        "content": "S3 Object Lock - prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely, adding a layer of protection against malicious or accidental deletion.\nReplication - to a new account limits the risk of a single point of compromise; even if attackers gain access to the original account, they cannot alter or delete the locked objects in the replicated bucket.\nVersioning - keeps multiple versions of an object in an S3 bucket, providing additional security and recovery options."
      },
      {
        "date": "2024-02-07T08:37:00.000Z",
        "voteCount": 2,
        "content": "Answer is D.  It's the only one that specifically addresses the issue.  The question never said only the security team needs access."
      },
      {
        "date": "2024-02-07T23:40:00.000Z",
        "voteCount": 1,
        "content": "The answer is a. It's the only one that prevents the data from being deleted by attackers that get access using long term credential. GuardDuty is a monitoring system. By itself, it doesn't actually stop anything from happening. It also likely wouldn't catch use of existing long-term credentials as malicious."
      },
      {
        "date": "2024-02-10T02:48:00.000Z",
        "voteCount": 1,
        "content": "Enabling GuardDuty with S3 protection and adding a lifecycle rule to delete objects after 1 year focuses on monitoring for threats and managing object lifecycle but:\n\nDoes not prevent the deletion or alteration of objects by an attacker who has gained access.\nS3 protection in GuardDuty helps identify suspicious access patterns but after-the-fact rather than preventing unauthorized changes."
      },
      {
        "date": "2024-02-05T22:49:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/s3-cross-account-replication-object-lock"
      },
      {
        "date": "2024-02-05T19:20:00.000Z",
        "voteCount": 3,
        "content": "A ans :\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html"
      },
      {
        "date": "2024-02-05T09:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 418,
    "url": "https://www.examtopics.com/discussions/amazon/view/132896-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.<br><br>A security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.<br><br>A solutions architect must design a solution to ensure that all backend services respond to only authenticated users.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T22:51:00.000Z",
        "voteCount": 9,
        "content": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html"
      },
      {
        "date": "2024-03-09T01:19:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-02-27T08:18:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      },
      {
        "date": "2024-02-07T08:38:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2024-02-05T11:19:00.000Z",
        "voteCount": 3,
        "content": "correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 419,
    "url": "https://www.examtopics.com/discussions/amazon/view/132897-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company creates an AWS Control Tower landing zone to manage and govern a multi-account AWS environment. The company's security team will deploy preventive controls and detective controls to monitor AWS services across all the accounts. The security team needs a centralized view of the security state of all the accounts.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the AWS Control Tower management account, use AWS CloudFormation StackSets to deploy an AWS Config conformance pack to all accounts in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon Detective for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Detective.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the AWS Control Tower management account, deploy an AWS CloudFormation stack set that uses the automatic deployment option to enable Amazon Detective for the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-27T05:00:00.000Z",
        "voteCount": 2,
        "content": "Option D:  Enable AWS Security Hub  and use Central Configuration for multiple AWS account and delegated Sec Hub Admin.   \"Central configuration is a Security Hub feature that helps you set up and manage Security Hub across multiple AWS accounts and AWS Regions &amp; From the delegated Security Hub administrator account, you can specify how the Security Hub service, security standards, and security controls are configured in your organization accounts and organizational units (OUs) across Regions\"\n\n(1) https://docs.aws.amazon.com/securityhub/latest/userguide/central-configuration-intro.html\n(2) https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-setup-prereqs.html"
      },
      {
        "date": "2024-03-09T01:26:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-02-27T08:21:00.000Z",
        "voteCount": 3,
        "content": "centralized view == security hub"
      },
      {
        "date": "2024-02-13T08:53:00.000Z",
        "voteCount": 2,
        "content": "D\n\nhttps://aws.amazon.com/blogs/mt/centralized-dashboard-for-aws-config-and-aws-security-hub/"
      },
      {
        "date": "2024-02-06T04:54:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer A\nhttps://aws.amazon.com/blogs/mt/extend-aws-control-tower-governance-using-aws-config-conformance-packs/"
      },
      {
        "date": "2024-02-05T22:54:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_delegated_admin.html"
      },
      {
        "date": "2024-02-05T11:22:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 420,
    "url": "https://www.examtopics.com/discussions/amazon/view/132898-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that develops consumer electronics with offices in Europe and Asia has 60 TB of software images stored on premises in Europe. The company wants to transfer the images to an Amazon S3 bucket in the ap-northeast-1 Region. New software images are created daily and must be encrypted in transit. The company needs a solution that does not require custom development to automatically transfer all existing and new software images to Amazon S3.<br><br>What is the next step in the transfer process?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Snowball device to transfer the images with the S3 bucket as the target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T22:58:00.000Z",
        "voteCount": 9,
        "content": "https://aws.amazon.com/blogs/storage/synchronizing-your-data-to-amazon-s3-using-aws-datasync/"
      },
      {
        "date": "2024-03-27T05:08:00.000Z",
        "voteCount": 1,
        "content": "Option A:   Additional info on AWS DataSync and S3 transfer \n\nhttps://aws.amazon.com/blogs/storage/migrating-hundreds-of-tb-of-data-to-amazon-s3-with-aws-datasync/"
      },
      {
        "date": "2024-03-09T01:32:00.000Z",
        "voteCount": 1,
        "content": "Option A. Option D is feasible but this needs custom development that company does not want to do."
      },
      {
        "date": "2024-03-02T15:55:00.000Z",
        "voteCount": 2,
        "content": "Easy to pick A as the answer, since all others are invalid.  Though, the images are in on premise,  the solution should at least mention VPN or direct connect."
      },
      {
        "date": "2024-02-10T03:09:00.000Z",
        "voteCount": 3,
        "content": "AWS DataSync - is a managed data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, as well as between AWS storage services. It supports encryption in transit and can be configured to transfer data to Amazon S3 automatically, handling both existing and new files efficiently. DataSync can be set up without requiring any custom development, making it a strong fit for the company's requirements. However Snowball it is not suited for the ongoing daily transfer of new software images due to the physical shipment of the device involved."
      },
      {
        "date": "2024-02-05T11:26:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 421,
    "url": "https://www.examtopics.com/discussions/amazon/view/132901-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has a web application that uses Amazon API Gateway. AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign.<br><br>A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB.<br><br>What change should the solutions architect make to improve the current response times as the web application becomes more popular?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the concurrency limit of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement DynamoDB auto scaling on the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the API Gateway throttle limit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-create the DynamoDB table with a better-partitioned primary index."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-09T01:33:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-07T10:07:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2024-02-06T16:37:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
      },
      {
        "date": "2024-02-05T11:29:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 422,
    "url": "https://www.examtopics.com/discussions/amazon/view/132903-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that has a web frontend. The application runs in the company's on-premises data center and requires access to file storage for critical data. The application runs on three Linux VMs for redundancy. The architecture includes a load balancer with HTTP request-based routing.<br><br>The company needs to migrate the application to AWS as quickly as possible. The architecture on AWS must be highly available.<br><br>Which solution will meet these requirements with the FEWEST changes to the architecture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type in three Availability Zones. Use Amazon S3 to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use the Fargate launch type in three Availability Zones. Use Amazon FSx for Lustre to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Amazon EC2 instances in three AWS Regions. Use Amazon Elastic Block Store (Amazon EBS) for file storage. Enable Cross-Region Replication (CRR) for all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-27T05:20:00.000Z",
        "voteCount": 1,
        "content": "Option B -  \"Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances.\"\n\n https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEFS.html"
      },
      {
        "date": "2024-03-09T01:37:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-10T03:34:00.000Z",
        "voteCount": 4,
        "content": "B - is the best solution to meet the requirements with the fewest changes to the architecture. It maintains the application's architecture by using EC2 instances for compute, EFS for shared file storage across instances (mirroring on-premises file storage capabilities), and an ALB for HTTP request-based routing, ensuring a smooth transition to AWS with high availability."
      },
      {
        "date": "2024-02-07T10:06:00.000Z",
        "voteCount": 2,
        "content": "Answer - B"
      },
      {
        "date": "2024-02-06T16:43:00.000Z",
        "voteCount": 3,
        "content": "Answer B: FEWEST changes to the architecture"
      },
      {
        "date": "2024-02-05T11:31:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 423,
    "url": "https://www.examtopics.com/discussions/amazon/view/132905-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate an on-premises data center to AWS. The company currently hosts the data center on Linux-based VMware VMs. A solutions architect must collect information about network dependencies between the VMs. The information must be in the form of a diagram that details host IP addresses, hostnames, and network connection information.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Application Discovery Service Agentless Collector for server data collection. Export the network diagrams from the AWS Migration Hub in .png format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS Application Migration Service agent on the on-premises servers for data collection. Use AWS Migration Hub data in Workload Discovery on AWS to generate network diagrams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS Application Migration Service agent on the on-premises servers for data collection. Export data from AWS Migration Hub in .csv format into an Amazon CloudWatch dashboard to generate network diagrams."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-06T16:55:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/migrationhub/latest/ug/network-diagram-prerequisites.html"
      },
      {
        "date": "2024-04-22T03:48:00.000Z",
        "voteCount": 2,
        "content": "\"AWS Application Discovery Service Discovery Agent must be running on all of the on-premises servers that you want mapped in the diagram.\""
      },
      {
        "date": "2024-07-08T01:28:00.000Z",
        "voteCount": 2,
        "content": "A is correct.\nAgentless can not collect network connection data(network dependency)\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html"
      },
      {
        "date": "2024-07-04T23:54:00.000Z",
        "voteCount": 1,
        "content": "A, for sure.\nIt's called AWS Application Discovery Agent, and not AWS Application Migration Service agent"
      },
      {
        "date": "2024-06-29T07:09:00.000Z",
        "voteCount": 1,
        "content": "AWS ADS can't publish data to AWS Migration Hub without permission granted. B is wrong because question doesn't mention that all the vms are managed by vcenter plus missing grant permisson to ADS"
      },
      {
        "date": "2024-06-24T05:21:00.000Z",
        "voteCount": 1,
        "content": "You don't need agents in VMware environment"
      },
      {
        "date": "2024-04-25T21:00:00.000Z",
        "voteCount": 2,
        "content": "VMware VMs = Agentless"
      },
      {
        "date": "2024-03-27T05:26:00.000Z",
        "voteCount": 2,
        "content": "Option A:  AWS Application Discovery Service and agent types. \n\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html"
      },
      {
        "date": "2024-03-09T01:42:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-02-29T18:16:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer. We need agent install in order to generate network diagrams."
      },
      {
        "date": "2024-02-05T11:33:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 424,
    "url": "https://www.examtopics.com/discussions/amazon/view/132908-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events, the application has a much higher workload than normal. Users notice slow response times during the peak periods because of many database connections. The company needs to improve the scalable performance and availability of the database.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora, and add a read replica. Add a database connection pool outside of the Lambda handler function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora, and add a read replica. Use Amazon Route 53 weighted records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-06T16:58:00.000Z",
        "voteCount": 6,
        "content": "Answer D:\nhttps://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html"
      },
      {
        "date": "2024-03-30T08:29:00.000Z",
        "voteCount": 3,
        "content": "Moving database connection settings outside of the Lambda handler function may allow Lambda to reuse the connection, but RDS Proxy is better."
      },
      {
        "date": "2024-03-09T01:44:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-02-07T09:57:00.000Z",
        "voteCount": 2,
        "content": "D is a correct answer"
      },
      {
        "date": "2024-02-05T11:41:00.000Z",
        "voteCount": 4,
        "content": "correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 425,
    "url": "https://www.examtopics.com/discussions/amazon/view/132909-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application\u2019s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.<br><br>A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-18T03:22:00.000Z",
        "voteCount": 2,
        "content": "Option A\nFor me it's quite clear: \"The solutions architect must migrate the application data to AWS to its new location WHILE STILL ALLOWING the on-premises application to access the data.\""
      },
      {
        "date": "2024-06-29T03:46:00.000Z",
        "voteCount": 4,
        "content": "Not right. As per question requirements \"A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage\""
      },
      {
        "date": "2024-03-27T05:36:00.000Z",
        "voteCount": 2,
        "content": "Option D :   AWS Architecture Blog\n\nhttps://aws.amazon.com/blogs/architecture/connect-amazon-s3-file-gateway-using-aws-privatelink-for-amazon-s3/"
      },
      {
        "date": "2024-03-09T01:50:00.000Z",
        "voteCount": 2,
        "content": "Option D."
      },
      {
        "date": "2024-02-09T07:40:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is D"
      },
      {
        "date": "2024-02-07T10:24:00.000Z",
        "voteCount": 3,
        "content": "the application must continue to have access to the data through SMB = Storage Gateway"
      },
      {
        "date": "2024-02-07T09:55:00.000Z",
        "voteCount": 3,
        "content": "I guess that proper answer is - D\n\nWe need to implement solition that uses Amazon S3 bucket for shared storage, but during migration phase - data should be avliable throught SMB. Only D option fits in this requirements"
      },
      {
        "date": "2024-02-06T17:02:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/storagegateway/file/"
      },
      {
        "date": "2024-02-06T16:11:00.000Z",
        "voteCount": 1,
        "content": "S3 bucket"
      },
      {
        "date": "2024-02-05T11:52:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 426,
    "url": "https://www.examtopics.com/discussions/amazon/view/132927-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A global company has a mobile app that displays ticket barcodes. Customers use the tickets on the mobile app to attend live events. Event scanners read the ticket barcodes and call a backend API to validate the barcode data against data in a database. After the barcode is scanned, the backend logic writes to the database's single table to mark the barcode as used.<br><br>The company needs to deploy the app on AWS with a DNS name of api.example.com. The company will host the database in three AWS Regions around the world.<br><br>Which solution will meet these requirements with the LOWEST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Container Service (Amazon ECS) clusters that are in the same Regions as the database. Create an accelerator in AWS Global Accelerator to route requests to the nearest ECS cluster. Create an Amazon Route 53 record that maps api.example.com to the accelerator endpoint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Kubernetes Service (Amazon EKS) clusters that are in the same Regions as the database. Create an Amazon CloudFront distribution with the three clusters as origins. Route requests to the nearest EKS cluster. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a CloudFront function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a Lambda@Edge function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-07T08:11:00.000Z",
        "voteCount": 6,
        "content": "D is the proper answer\n\nCloudFront Functions - can be used only for manipulation with requests data\nCloudFront Lambda@Edge functions - can be used for anything, because this is a regular lambda function"
      },
      {
        "date": "2024-02-05T12:50:00.000Z",
        "voteCount": 6,
        "content": "Correct Answer is A"
      },
      {
        "date": "2024-06-01T08:59:00.000Z",
        "voteCount": 1,
        "content": "Option: D"
      },
      {
        "date": "2024-05-15T05:49:00.000Z",
        "voteCount": 5,
        "content": "D withoud any doubt.\nFor this simple data, a DynamoDB Table is enought; so global table is perfect.\nFor check logic, cloudfront function is not enought because it is only for manipulate HTTP header or something very very light (code, metadata etc.). So for calling some backend layer (querying DynamoDB in this case) we need a Lambda@Edge function, which is much complete."
      },
      {
        "date": "2024-03-09T02:00:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-02-07T10:38:00.000Z",
        "voteCount": 4,
        "content": "D. Lambda@Edge"
      },
      {
        "date": "2024-02-06T17:17:00.000Z",
        "voteCount": 5,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-external-data-in-lambdaedge/\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/lambdaedge-design-best-practices/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 427,
    "url": "https://www.examtopics.com/discussions/amazon/view/132961-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A medical company is running a REST API on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group behind an Application Load Balancer (ALB). The ALB runs in three public subnets, and the EC2 instances run in three private subnets. The company has deployed an Amazon CloudFront distribution that has the ALB as the only origin.<br><br>Which solution should a solutions architect recommend to enhance the origin security?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL rule with an IP match condition of the CloudFront service IP address ranges. Associate the web ACL with the ALMove the ALB into the three private subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore a random string in AWS Systems Manager Parameter Store. Configure Parameter Store automatic rotation for the string. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Inspect the value of the custom HTTP header, and block access in the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Shield Advanced Create a security group policy to allow connections from CloudFront service IP address ranges. Add the policy to AWS Shield Advanced, and attach the policy to the ALB."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-06T17:37:00.000Z",
        "voteCount": 5,
        "content": "In this blog post, you\u2019ll see how to use CloudFront custom headers, AWS WAF, and AWS Secrets Manager to restrict viewer requests from accessing your CloudFront origin resources directly.\n\nhttps://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/"
      },
      {
        "date": "2024-05-16T04:16:00.000Z",
        "voteCount": 1,
        "content": "D is the correct Answer"
      },
      {
        "date": "2024-06-12T06:24:00.000Z",
        "voteCount": 1,
        "content": "you cannot directly add a security group to AWS Shield Advanced. BTW, what is security group policy?"
      },
      {
        "date": "2024-03-09T02:05:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-02-07T10:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2024-02-07T07:58:00.000Z",
        "voteCount": 2,
        "content": "A - is a proper answer\nhttps://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/"
      },
      {
        "date": "2024-02-05T14:31:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 428,
    "url": "https://www.examtopics.com/discussions/amazon/view/132963-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "To abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The solutions architect is required to provide access to the data stored in AWS to the company\u2019s global WAN network. The security team mandates that no traffic accessing this data should traverse the public internet.<br><br>How should the solutions architect design a highly available solution that meets the requirements and is cost-effective?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T06:29:00.000Z",
        "voteCount": 4,
        "content": "DX + DXGW"
      },
      {
        "date": "2024-05-17T20:36:00.000Z",
        "voteCount": 3,
        "content": "between C and D: TGW is a regional service. So D is the answer"
      },
      {
        "date": "2024-03-09T02:11:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-02-23T18:41:00.000Z",
        "voteCount": 4,
        "content": "D:\n\nNeed direct connect gateway to share with other regions"
      },
      {
        "date": "2024-02-10T03:56:00.000Z",
        "voteCount": 3,
        "content": "D - offers a blend of high availability (through redundancy with two DX connections), cost-effectiveness (by reducing the number of DX connections required), and simplicity (by avoiding the complexity of managing a transit VPC or multiple peering connections)."
      },
      {
        "date": "2024-02-07T10:52:00.000Z",
        "voteCount": 1,
        "content": "C. transit VPC"
      },
      {
        "date": "2024-04-06T14:57:00.000Z",
        "voteCount": 4,
        "content": "D\nTransit gateway is regional service.\nWe need DX gateway here"
      },
      {
        "date": "2024-02-06T17:48:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html"
      },
      {
        "date": "2024-02-07T22:48:00.000Z",
        "voteCount": 1,
        "content": "It seems to suggest you pay extra to do it that way. This is asking for the most cost-effective option."
      },
      {
        "date": "2024-02-07T22:49:00.000Z",
        "voteCount": 2,
        "content": "\"With the previous two options, you pay for Direct Connect pricing. For this option, you also pay for the Transit Gateway attachment and data processing charges.\""
      },
      {
        "date": "2024-02-05T14:35:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 429,
    "url": "https://www.examtopics.com/discussions/amazon/view/132965-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has developed an application that is running Windows Server on VMware vSphere VMs that the company hosts on premises. The application data is stored in a proprietary format that must be read through the application. The company manually provisioned the servers and the application.<br><br>As part of its disaster recovery plan, the company wants the ability to host its application on AWS temporarily if the company's on-premises environment becomes unavailable. The company wants the application to return to on-premises hosting after a disaster recovery event is complete. The RPO is 5 minutes.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS DataSync. Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and attach the EBS volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an AWS Storage Gateway file gateway. Replicate the data to an Amazon S3 bucket. When the on-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic Block Store (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Amazon FSx for Windows File Server file system on AWS. Replicate the data to the file system. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and use AWS::CloudFormation::Init commands to mount the Amazon FSx file shares."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-27T06:13:00.000Z",
        "voteCount": 3,
        "content": "Option B:  AWS Elastic Disaster Recovery performance Failover and Failback \n\nhttps://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html"
      },
      {
        "date": "2024-03-21T04:07:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer. AWS Elastic Disaster Recovery aligns with low operational overhead and 5-minute RPO. It takes care of ongoing replication\n\nA: Incorrect, DataSync lacks comprehensive DR capabilities and requires manual provisioning.\nC: Incorrect, introduces complexity and doesn't support a 5-minute RPO for DR.\nD: Incorrect, FSx lacks automated DR solutions for VMware vSphere VMs, increasing overhead."
      },
      {
        "date": "2024-03-09T02:12:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-10T11:39:00.000Z",
        "voteCount": 1,
        "content": "I selected option D, option B is requires enabling and configuring AWS disaster recovery service, monitoring replication status. In the even of a disaster. The replication of EC2 instances needs to be managed and maintained even where not in used."
      },
      {
        "date": "2024-02-17T12:17:00.000Z",
        "voteCount": 1,
        "content": "The RPO is 5 minutes."
      },
      {
        "date": "2024-02-07T10:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2024-02-07T04:08:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B - because only this option will provide LEAST amount of operational overhead.\n\nA - is out, because DataSync can't replicate data to EBS volumes\nC - is out, because AWS Backup can't restore not managed data from S3 to EBS\nD - is out, because it is not provide a way HOW we will replicate data from on-premise to FSx. Also, it is require additional amount of operational overhead"
      },
      {
        "date": "2024-02-06T17:50:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/disaster-recovery/"
      },
      {
        "date": "2024-02-05T14:47:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 430,
    "url": "https://www.examtopics.com/discussions/amazon/view/132972-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a highly available data collection application on Amazon EC2 in the eu-north-1 Region. The application collects data from end-user devices and writes records to an Amazon Kinesis data stream and a set of AWS Lambda functions that process the records. The company persists the output of the record processing to an Amazon S3 bucket in eu-north-1. The company uses the data in the S3 bucket as a data source for Amazon Athena.<br><br>The company wants to increase its global presence. A solutions architect must launch the data collection capabilities in the sa-east-1 and ap-northeast-1 Regions. The solutions architect deploys the application, the Kinesis data stream, and the Lambda functions in the two new Regions. The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis.<br><br>During testing of the new setup, the solutions architect notices a significant lag on the arrival of data from the new Regions to the S3 bucket.<br><br>Which solution will improve this lag time the MOST?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each of the two new Regions, set up the Lambda functions to run in a VPC. Set up an S3 gateway endpoint in that VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on S3 Transfer Acceleration on the S3 bucket in eu-north-1. Change the application to use the new S3 accelerated endpoint when the application uploads data to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket in each of the two new Regions. Set the application in each new Region to upload to its respective S3 bucket. Set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory requirements of the Lambda functions to ensure that they have multiple cores available. Use the multipart upload feature when the application uploads data to Amazon S3 from Lambda."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T23:23:00.000Z",
        "voteCount": 17,
        "content": "s3 transfer acceleration is not supported in eu-north-1 region yet\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html"
      },
      {
        "date": "2024-04-13T19:40:00.000Z",
        "voteCount": 5,
        "content": "S3 Transfer Acceleration achieves acceleration by AWS routing traffic to AWS cloudfront edge location and transfer aws network backbone. The key speed improvement is AWS network backbone. \nWhile, in this case, the application in the two new regions is the client to write the output to S3 in EU, which means the client is already on AWS, the data transportation is already on AWS network backbone. So S3 Transfer Acceleration is not helping at all. B is out.\nC is the best answer."
      },
      {
        "date": "2024-10-15T23:00:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is C because Amazon Kinesis Data Streams cannot directly leverage S3 Transfer Acceleration. S3 Transfer Acceleration is typically used for accelerating transfers from clients to S3 and doesn't support Kinesis Data Streams directly."
      },
      {
        "date": "2024-09-14T00:32:00.000Z",
        "voteCount": 1,
        "content": "While C would work, it adds complexity by requiring separate S3 buckets in each Region and replicating data, which could increase cost and delay due to replication schedules."
      },
      {
        "date": "2024-06-03T17:39:00.000Z",
        "voteCount": 2,
        "content": "No matter how fast you run, you cannot run from Tokyo to Stockholm as fast as someone 10km from Stockholm."
      },
      {
        "date": "2024-05-08T14:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is C but feel this is one they definitely want people to get wrong. No one is going to memorize which regions every feature is supported in, it is something anyone would look up."
      },
      {
        "date": "2024-05-03T04:47:00.000Z",
        "voteCount": 3,
        "content": "I would choose B but it cannot be B because S3 Transfer Acceleration is not supported in eu-north-1. This leaves C as the only viable option."
      },
      {
        "date": "2024-05-01T02:48:00.000Z",
        "voteCount": 3,
        "content": "agree with  bjexamprep"
      },
      {
        "date": "2024-04-06T13:38:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is C.\n\nS3 transfer acceleration is not yet supported in the eu-north-1 region, as wyeedh1 commented.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html"
      },
      {
        "date": "2024-03-30T07:56:00.000Z",
        "voteCount": 2,
        "content": "wyeedh1"
      },
      {
        "date": "2024-03-26T23:59:00.000Z",
        "voteCount": 3,
        "content": "see  wyeedh1 answer"
      },
      {
        "date": "2024-03-23T08:53:00.000Z",
        "voteCount": 5,
        "content": "\"improve this lag time the MOST\" means improve the time for \"uploading files\" not \"uploading the files to the destination.\" Uploading the files to the bucket in the same region is faster than transferring them to other regions."
      },
      {
        "date": "2024-03-09T02:23:00.000Z",
        "voteCount": 3,
        "content": "Option B"
      },
      {
        "date": "2024-03-03T17:32:00.000Z",
        "voteCount": 3,
        "content": "A: wrong. for s3 gateway endpoint it's not possible to access the S3 buckets from another VPC/Region\nB: typical scenario for s3 transfer acceleration.\nC: possible. But extra steps. And not sure s3 replication will be faster. So is wrong.\nD: wrong."
      },
      {
        "date": "2024-03-02T04:43:00.000Z",
        "voteCount": 2,
        "content": "I agree for the reason \"requirement to centralize the data analysis\" we cant have S3 s in other regions"
      },
      {
        "date": "2024-02-23T08:41:00.000Z",
        "voteCount": 4,
        "content": "as they said \"The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis\". So, B should be the answer"
      },
      {
        "date": "2024-02-13T12:37:00.000Z",
        "voteCount": 2,
        "content": "I think that the LAG 7is caused due to network path running to S3 public interfaces. The gateway endpoint can enchance the network path and reduce the LAG.\n\nhttps://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 431,
    "url": "https://www.examtopics.com/discussions/amazon/view/132984-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability.<br><br>Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only.<br><br>Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC peering connection from each business unit VPC to the shared VPAccept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T07:44:00.000Z",
        "voteCount": 5,
        "content": "B is the answer for me \nOnly way to get around overlapping IP range is using endpoint service"
      },
      {
        "date": "2024-07-18T23:10:00.000Z",
        "voteCount": 1,
        "content": "only option to get around of IP overlapping \nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/"
      },
      {
        "date": "2024-05-01T07:17:00.000Z",
        "voteCount": 1,
        "content": "A is actually. they never mentioned cost effect or less effort solution.\n\nwhen they are not mentioned anything we need to prefer best option"
      },
      {
        "date": "2024-07-01T16:23:00.000Z",
        "voteCount": 1,
        "content": "\"This requires that automatic route propagation to Transit Gateway be disabled as not all of the subnets in each VPC should be advertised.\" so it is B"
      },
      {
        "date": "2024-05-21T19:46:00.000Z",
        "voteCount": 3,
        "content": "Not possible, because TGW does not support overlapping ranges"
      },
      {
        "date": "2024-03-09T14:13:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2024-02-08T06:41:00.000Z",
        "voteCount": 2,
        "content": "B is the answer"
      },
      {
        "date": "2024-02-07T03:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nApplication already uses NLB so this is a best way for solve that task"
      },
      {
        "date": "2024-02-06T20:34:00.000Z",
        "voteCount": 4,
        "content": "https://www.examtopics.com/discussions/amazon/view/46708-exam-aws-certified-solutions-architect-professional-topic-1/\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/"
      },
      {
        "date": "2024-02-06T16:36:00.000Z",
        "voteCount": 1,
        "content": "VPC Endpoint Service can do the job"
      },
      {
        "date": "2024-02-05T18:58:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 432,
    "url": "https://www.examtopics.com/discussions/amazon/view/132985-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.<br><br>All data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.<br><br>A solutions architect needs to determine the architecture that the company will use for the website on AWS.<br><br>Which solution will meet these requirements with the LEAST effort to migrate?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-15T23:11:00.000Z",
        "voteCount": 1,
        "content": "B is an AWS Solutions/Sales team to a customer answer"
      },
      {
        "date": "2024-06-23T02:01:00.000Z",
        "voteCount": 3,
        "content": "B is best to manage, but is it easiest to migrate? you still need to adjust manifest file for managed node groups and ECR repo. D is lift and shift."
      },
      {
        "date": "2024-03-27T06:59:00.000Z",
        "voteCount": 2,
        "content": "Option B:  Some additional migration to EKS info\n(1) https://aws.amazon.com/blogs/architecture/field-notes-migrating-a-self-managed-kubernetes-cluster-on-ec2-to-amazon-eks/\n\n(2) https://aws.amazon.com/blogs/containers/migrating-from-self-managed-kubernetes-to-amazon-eks-here-are-some-key-considerations/"
      },
      {
        "date": "2024-03-09T14:17:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-09T05:45:00.000Z",
        "voteCount": 1,
        "content": "B is the best option"
      },
      {
        "date": "2024-02-08T06:41:00.000Z",
        "voteCount": 1,
        "content": "It is B"
      },
      {
        "date": "2024-02-07T03:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is B - because only in that case - we don't need to do any changes in application\n\nA - is out, because we will need to create deployments for many micro-services\nC - is out, because we will need to create ecs deployments for many micro-services\nD - is out, because it will require a lot of overhead and efforts for self-managed K8S setup"
      },
      {
        "date": "2024-02-06T20:42:00.000Z",
        "voteCount": 4,
        "content": "Answer B: LEAST effort to migrate\nMinor changes to the manifest files seems like the least amount of work compared to what needs to be done in the other answers."
      },
      {
        "date": "2024-02-05T19:02:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 433,
    "url": "https://www.examtopics.com/discussions/amazon/view/133225-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses a mobile app on AWS to run online contests. The company selects a winner at random at the end of each contest. The contests run for variable lengths of time. The company does not need to retain any data from a contest after the contest is finished.<br><br>The company uses custom code that is hosted on Amazon EC2 instances to process the contest data and select a winner. The EC2 instances run behind an Application Load Balancer and store contest entries on Amazon RDS DB instances. The company must design a new architecture to reduce the cost of running the contests.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate storage of the contest entries to Amazon DynamoDB. Create a DynamoDB Accelerator (DAX) cluster. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. At the end of the contest, delete the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the storage of the contest entries to Amazon Redshift. Rewrite the code as AWS Lambda functions. At the end of the contest, delete the Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon ElastiCache for Redis cluster in front of the RDS DB instances to cache the contest entries. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. Set the ElastiCache TTL attribute on each entry to expire each entry at the end of the contest.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-07T07:40:00.000Z",
        "voteCount": 5,
        "content": "Lambda for choosing a winner (it's a short-term task) and TTL (known before the contest starts) make sense."
      },
      {
        "date": "2024-07-18T23:25:00.000Z",
        "voteCount": 2,
        "content": "Option D is the most cost Efficient"
      },
      {
        "date": "2024-07-07T23:36:00.000Z",
        "voteCount": 1,
        "content": "remove entire DynamoDB and DAX is used for reduce RCU"
      },
      {
        "date": "2024-06-25T05:59:00.000Z",
        "voteCount": 2,
        "content": "Should be A. With Variable lengths of time, Lambda is not the right choice."
      },
      {
        "date": "2024-06-12T06:54:00.000Z",
        "voteCount": 3,
        "content": "\"the contests run for variable lengths of time\" does not mean that those time periods are not known. We do not need a fixed value for TTL but can use Lambda to change timestamp depending on each contest."
      },
      {
        "date": "2024-06-01T09:41:00.000Z",
        "voteCount": 2,
        "content": "Option D, The option A doesn't fulfill the requirement of cost effectiveness as there is no reason to use DAX"
      },
      {
        "date": "2024-05-15T07:24:00.000Z",
        "voteCount": 2,
        "content": "Correct option is A here.\nFirst of all, we don't know how much time the contest will last as per requirements, so fix the TTL it's a mistake.\nSecond point, we can delete the entire table as we didn't the file after the end of the context, so no data to retain.\nFinally, for web contest, we don't know how much users will be online, and providing a cache layer might be a good solution."
      },
      {
        "date": "2024-04-22T06:25:00.000Z",
        "voteCount": 2,
        "content": "D - correct."
      },
      {
        "date": "2024-04-16T23:16:00.000Z",
        "voteCount": 2,
        "content": "D. There is no in memory cache requirement here for DAX. So omitting A"
      },
      {
        "date": "2024-04-13T19:54:00.000Z",
        "voteCount": 3,
        "content": "The question is looking for \u201cMOST cost-effectively\u201d. I assume DAX + DynamoDB is cheaper than DynamoDB only. Cause DAX should be able to reduce the RCU cost and improve performance. This is an online contest, which means the RCU could be very high.\n\"the contests run for variable lengths of time\" means you can't set a fix TTL for the record entry. And even you can have a fix time, the record being submitted 1 min before the deadline will still be kept for the TTL and keep generating cost."
      },
      {
        "date": "2024-04-06T16:05:00.000Z",
        "voteCount": 4,
        "content": "Vote for A here\nreason as specified by zouwelaar"
      },
      {
        "date": "2024-04-02T23:18:00.000Z",
        "voteCount": 4,
        "content": "You are forgetting that the contests run for variable lengths of time. So Lambda and TTL are out."
      },
      {
        "date": "2024-04-07T11:39:00.000Z",
        "voteCount": 2,
        "content": "Assuming each contest still has a set time from the start, D is most cost efficient, TTL is set based on each contest time. Lambda is only used to add/fetch entries and select random winner, runtime is minimal. I go with D here"
      },
      {
        "date": "2024-03-09T14:19:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2024-02-29T20:09:00.000Z",
        "voteCount": 3,
        "content": "Time To Live (TTL) for DynamoDB is a cost-effective method for deleting items that are no longer relevant. TTL allows you to define a per-item expiration timestamp that indicates when an item is no longer needed. DynamoDB automatically deletes expired items within a few days of their expiration time, without consuming write throughput."
      },
      {
        "date": "2024-02-29T20:09:00.000Z",
        "voteCount": 2,
        "content": "D : https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
      },
      {
        "date": "2024-02-09T05:46:00.000Z",
        "voteCount": 4,
        "content": "Lambda to save cost"
      },
      {
        "date": "2024-02-09T01:24:00.000Z",
        "voteCount": 4,
        "content": "D is the most cost-effective solution. It leverages DynamoDB for efficient, scalable storage with automatic data expiration via TTL and AWS Lambda for flexible, event-driven processing. This setup minimizes costs by using resources only when needed and automatically scaling to match demand without the need for manual intervention or over-provisioning."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 434,
    "url": "https://www.examtopics.com/discussions/amazon/view/132986-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has implemented a new security requirement. According to the new requirement, the company must scan all traffic from corporate AWS instances in the company's VPC for violations of the company's security policies. As a result of these scans, the company can block access to and from specific IP addresses.<br><br>To meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies. The company installs approved proxy server software on these EC2 instances. The company modifies the route tables on all subnets to use the corresponding EC2 instances with proxy software as the default route. The company also creates security groups that are compliant with the security policies and assigns these security groups to the EC2 instances.<br><br>Despite these configurations, the traffic of the EC2 instances in their private subnets is not being properly forwarded to the internet.<br><br>What should a solutions architect do to resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable source/destination checks on the EC2 instances that run the proxy software.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-06T21:05:00.000Z",
        "voteCount": 8,
        "content": "Answer A:\nProxies like NATs will need SrcDestCheck disabled\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck"
      },
      {
        "date": "2024-10-14T02:13:00.000Z",
        "voteCount": 1,
        "content": "With A i am missing the route to the internet via a NAT Gateway or NAT Instance via a ENI, with D i miss the scr/dst check"
      },
      {
        "date": "2024-09-26T03:09:00.000Z",
        "voteCount": 1,
        "content": "\"the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies.\" How could a Proxy in a private Subnet communicate with the Internet? So we need a second network card with connection to an IGW. Anwser D"
      },
      {
        "date": "2024-04-05T10:26:00.000Z",
        "voteCount": 2,
        "content": "While disabling security checks might seem like a solution, it's not recommended for production environments as it weakens security. The issue lies in routing, not security"
      },
      {
        "date": "2024-02-09T05:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, proxy"
      },
      {
        "date": "2024-02-07T03:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is - A"
      },
      {
        "date": "2024-02-05T19:05:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 435,
    "url": "https://www.examtopics.com/discussions/amazon/view/132987-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.<br><br>What should the company do to meet this new requirement with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T08:44:00.000Z",
        "voteCount": 8,
        "content": "D - SAM cannot used for importing and currently we are already using Cloudformation\nB - Stacksets used to create multiple stacks and currently we are using Cloudformation\nA - CDK , we will need to change all the entire stack from Cloudformation to CDK\nC - We can import existing resources in Cloudformation: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html"
      },
      {
        "date": "2024-10-11T14:08:00.000Z",
        "voteCount": 1,
        "content": "\"Use the stack set to import the VPC into the stack.\"\nCloudFormation stack sets are designed for deploying stacks across multiple accounts and regions. They are NOT meant for importing existing resources into a single VPC or stack."
      },
      {
        "date": "2024-04-22T06:43:00.000Z",
        "voteCount": 2,
        "content": "C - correct."
      },
      {
        "date": "2024-04-22T06:42:00.000Z",
        "voteCount": 2,
        "content": "C - correct."
      },
      {
        "date": "2024-03-30T09:01:00.000Z",
        "voteCount": 3,
        "content": "B means to create a stack set just for VPC, we don't need a stack set to handle just 1 resource"
      },
      {
        "date": "2024-03-09T14:37:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-17T13:25:00.000Z",
        "voteCount": 3,
        "content": "I discarded B, because IMO stack sets are not needed."
      },
      {
        "date": "2024-02-09T05:49:00.000Z",
        "voteCount": 1,
        "content": "Create a CloudFormation stack"
      },
      {
        "date": "2024-02-08T06:49:00.000Z",
        "voteCount": 1,
        "content": "agree B"
      },
      {
        "date": "2024-02-15T13:31:00.000Z",
        "voteCount": 1,
        "content": "Changed to C"
      },
      {
        "date": "2024-02-07T03:29:00.000Z",
        "voteCount": 4,
        "content": "I guess C\n\nA - is out, because CDK not allow to import any exists resources\nB - is out, becuase StackSets are used only for create multiple stacks and manage them from a single stack\nD - is out, because AWS SAM cli - can't be used for import resources in CF"
      },
      {
        "date": "2024-02-06T21:10:00.000Z",
        "voteCount": 2,
        "content": "Answer B: Because CloudFormation is already in use.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html"
      },
      {
        "date": "2024-02-06T21:08:00.000Z",
        "voteCount": 1,
        "content": "Answer A:\nhttps://aws.amazon.com/blogs/devops/how-to-import-existing-resources-into-aws-cdk-stacks/\nhttps://docs.aws.amazon.com/cdk/v2/guide/cli.html#cli-import"
      },
      {
        "date": "2024-02-06T21:10:00.000Z",
        "voteCount": 1,
        "content": "Changing my answer to B:\nAnswer B: Because CloudFormation is already in use.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html"
      },
      {
        "date": "2024-02-05T19:06:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 436,
    "url": "https://www.examtopics.com/discussions/amazon/view/132988-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has developed a new release of a popular video game and wants to make it available for public download. The new release package is approximately 5 GB in size. The company provides downloads for existing releases from a Linux-based, publicly facing FTP site hosted in an on-premises data center. The company expects the new release will be downloaded by users worldwide. The company wants a solution that provides improved download performance and low transfer costs, regardless of a user's location.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on each of the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Set Requester Pays for the S3 bucket. Publish the game download URL for users to download the package."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-09T14:40:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2024-02-09T05:56:00.000Z",
        "voteCount": 2,
        "content": "C. S3 is the best option, no need for requestor pays"
      },
      {
        "date": "2024-02-08T06:52:00.000Z",
        "voteCount": 2,
        "content": "It is C"
      },
      {
        "date": "2024-02-06T21:13:00.000Z",
        "voteCount": 2,
        "content": "Answer C:"
      },
      {
        "date": "2024-02-05T19:08:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 437,
    "url": "https://www.examtopics.com/discussions/amazon/view/132989-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an application in the cloud that consists of a database and a website. Users can post data to the website, have the data processed, and have the data sent back to them in an email. Data is stored in a MySQL database running on an Amazon EC2 instance. The database is running in a VPC with two private subnets. The website is running on Apache Tomcat in a single EC2 instance in a different VPC with one public subnet. There is a single VPC peering connection between the database and website VPC.<br><br>The website has suffered several outages during the last month due to high traffic.<br><br>Which actions should a solutions architect take to increase the reliability of the application? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an additional VPC peering connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the MySQL database to Amazon Aurora with one Aurora Replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision two NAT gateways in the database VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the Tomcat server to the database VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional public subnet in a different Availability Zone in the website VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-06T21:21:00.000Z",
        "voteCount": 5,
        "content": "Answer: ACF\nThese increase reliability of the app.\nF.  Create an additional public subnet in a different Availability Zone in the website VPC.\nA. Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer.\nC. Migrate the MySQL database to Amazon Aurora with one Aurora Replica.\n\nThese do not.\nB. Provision an additional VPC peering connection.\nD. Provision two NAT gateways in the database VPC.\nE. Move the Tomcat server to the database VPC.  (good idea for security, but we're after reliability)"
      },
      {
        "date": "2024-03-09T14:44:00.000Z",
        "voteCount": 1,
        "content": "Option A, C, F"
      },
      {
        "date": "2024-02-17T08:17:00.000Z",
        "voteCount": 2,
        "content": "You cant move Ec2 directly to another VPC  need to migrate between VPCs"
      },
      {
        "date": "2024-02-08T06:55:00.000Z",
        "voteCount": 2,
        "content": "agree ACF"
      },
      {
        "date": "2024-02-07T22:12:00.000Z",
        "voteCount": 2,
        "content": "B - not correct, because will not give us any benefit\nD - not correct, because will not give us any benefit\nE - looks not correct, because if we move website into database VPC - this VPC don't contains any public subnet, so it will be inaccessible"
      },
      {
        "date": "2024-02-05T19:19:00.000Z",
        "voteCount": 4,
        "content": "correct answer is ACF"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 438,
    "url": "https://www.examtopics.com/discussions/amazon/view/132872-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.<br><br>After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.<br><br>While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.<br><br>Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-22T14:02:00.000Z",
        "voteCount": 5,
        "content": "same question as page1 question 10"
      },
      {
        "date": "2024-03-09T14:50:00.000Z",
        "voteCount": 1,
        "content": "A and E"
      },
      {
        "date": "2024-02-08T06:58:00.000Z",
        "voteCount": 2,
        "content": "it is AE"
      },
      {
        "date": "2024-02-06T21:27:00.000Z",
        "voteCount": 3,
        "content": "Answer: AE\nAll other answers won't help for transient failures\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html#custom-error-pages-procedure"
      },
      {
        "date": "2024-02-06T18:45:00.000Z",
        "voteCount": 1,
        "content": "answer is A and E"
      },
      {
        "date": "2024-02-05T19:26:00.000Z",
        "voteCount": 3,
        "content": "correct answer is A and E"
      },
      {
        "date": "2024-02-05T09:15:00.000Z",
        "voteCount": 1,
        "content": "A,B, ans"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 439,
    "url": "https://www.examtopics.com/discussions/amazon/view/132990-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a new AWS account in the same AWS Region. Both accounts are members of the same organization in AWS Organizations.<br><br>The company must minimize database service interruption before the company performs DNS cutover to the new database.<br><br>Which migration strategy will meet this requirement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T07:07:00.000Z",
        "voteCount": 2,
        "content": "A is unnecessary"
      },
      {
        "date": "2024-03-21T04:32:00.000Z",
        "voteCount": 2,
        "content": "The question says to choose two alternatives - but it doesn't say that they must work in conjunction. I.e., separate answers that stand on their own.\n\nB is best, but A works too. Thus A+B."
      },
      {
        "date": "2024-03-09T14:53:00.000Z",
        "voteCount": 1,
        "content": "A and B both are valid options."
      },
      {
        "date": "2024-03-03T12:20:00.000Z",
        "voteCount": 3,
        "content": "This question should have a single answer. A and C are both using a kind of back/restore strategy, and they cannot capture the changes happens during the restore stage. D is using Application Migration Service, which is not suitable for DB migration. Only B can do this job."
      },
      {
        "date": "2024-02-07T22:01:00.000Z",
        "voteCount": 1,
        "content": "This really should be a single answer, or it should say which solutions would meet this requirement. But yes A and B are both possible."
      },
      {
        "date": "2024-02-07T02:56:00.000Z",
        "voteCount": 3,
        "content": "I guess that the right answer - A \\ B\nA - Snapshots can be easily shared cross AWS accounts\nB - With AWS DMS - you can sync databases\nC  - Out because - as I understood - you can't just SHARE AWS Backup with another AWS Account, you need to setup cross account AWS backup to store backups in both accounts\nD - Out because AWS Application migration service - can't migrate RDS databases"
      },
      {
        "date": "2024-02-06T21:34:00.000Z",
        "voteCount": 1,
        "content": "Answer AB:  A is unnecessary, we really only need B.  It works either way.\nhttps://aws.amazon.com/blogs/database/cross-account-amazon-aurora-postgresql-and-amazon-rds-for-postgresql-migration-with-reduced-downtime-using-aws-dms/"
      },
      {
        "date": "2024-02-06T18:47:00.000Z",
        "voteCount": 1,
        "content": "have to us DMS or snapshot for DB migration"
      },
      {
        "date": "2024-02-05T19:29:00.000Z",
        "voteCount": 3,
        "content": "correct answer is A and B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 440,
    "url": "https://www.examtopics.com/discussions/amazon/view/132992-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.<br><br>The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.<br><br>The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.<br><br>Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate VPC peering connections between all the company's VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPAssociate the endpoint service with the NLB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC peering connection between the company's management VPC and each customer's VPC."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T14:31:00.000Z",
        "voteCount": 1,
        "content": "AWS Transit Gateway supports peering between transit gateways in different regions. This means that you can connect a Transit Gateway in one region to another Transit Gateway in a different region. This feature is known as Transit Gateway Peering (not VPC peering).\nAWS Transit Gateway also allows you to associate VPCs from different AWS accounts to the same transit gateway using AWS Resource Access Manager (RAM)"
      },
      {
        "date": "2024-08-11T16:27:00.000Z",
        "voteCount": 2,
        "content": "AAAAAAAAAAACCCCCCCCC"
      },
      {
        "date": "2024-05-21T09:38:00.000Z",
        "voteCount": 3,
        "content": "AC; AWS Transit Gateway allows you to connect resources across different AWS regions. Here\u2019s how you can achieve this:\n\nCreate Transit Gateways:\nBegin by creating Transit Gateways in the respective regions where you want to establish peering.\nEnsure that the necessary VPCs are attached to each Transit Gateway.\nEnable Peering:\nNavigate to the AWS Management Console and select the Transit Gateway service.\nInitiate the peering connection between the two Transit Gateways in different regions.\nUpdate Route Tables:\nConfigure the route tables associated with each Transit Gateway to allow traffic between the regions.\nSecurity Groups and Network ACLs:\nAdjust security groups and network ACLs to permit the necessary traffic flow.\nConnectivity Testing:\nVerify connectivity by testing communication between resources in different regions."
      },
      {
        "date": "2024-05-21T00:45:00.000Z",
        "voteCount": 1,
        "content": "As titi_r explained"
      },
      {
        "date": "2024-04-23T00:20:00.000Z",
        "voteCount": 2,
        "content": "B \u2013 Correct, even that it will be a routing madness. The default VPC peering quota is 50, but increasable after request to 125. So, the company will be able to peer its 50 VPCs, but it must request a quota increase for a higher number - that\u2019s not mentioned in the answer. And also what\u2019s happening when/if they require more than 125 VPCs at one point?\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-connection-quotas.html\n-\nC \u2013 Correct. The PrivateLink endpoint service will provide a one-way access from each customer's VPC to the company's management VPC.\n https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/use-case-examples.html"
      },
      {
        "date": "2024-08-30T04:19:00.000Z",
        "voteCount": 1,
        "content": "\"The number of VPCs that the company will use to host the solution will continue to increase as the solution grows\" - it can go beyond 125 as well"
      },
      {
        "date": "2024-08-11T16:30:00.000Z",
        "voteCount": 1,
        "content": "Creating a transit gateway peering allows VPCs in different regions to connect."
      },
      {
        "date": "2024-04-23T00:21:00.000Z",
        "voteCount": 3,
        "content": "\u0410 \u2013 Incorrect. It\u2019s not possible to attach a VPC from one Region to a TGW in another Region. You can only attach a VPC to a TGW in the same Region; additionally you can peer that TGW with another one, located in a different Region.\nhttps://content.cloudthat.com/resources/wp-content/uploads/2022/11/Picture220.png"
      },
      {
        "date": "2024-08-27T18:20:00.000Z",
        "voteCount": 2,
        "content": "Transit Gateway allow both inter-region and intra-region VPC peering as per this AWS document - https://docs.aws.amazon.com/vpc/latest/tgw/tgw-peering.html#:~:text=AWS%20Region%20considerations-,Transit%20gateway%20peering%20attachments%20in%20Amazon%20VPC%20Transit%20Gateways,and%20specify%20a%20transit%20gateway."
      },
      {
        "date": "2024-04-23T00:21:00.000Z",
        "voteCount": 1,
        "content": "D \u2013 Incorrect. Unknown if even possible, but more COMPLEX than answer \u201cB\u201d anyway. The default Site-to-Site VPN connections per VGW quota is only 10 (it's increasable, but the actual limit is not stated in the AWS documentation), however the company will need more than 50 and this sounds unrealistic. The default Site-to-Site VPN connections per Region quota is 50 \u2013 it will also require a request for quota increase.\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/vpn-limits.html\n-\nE \u2013 Incorrect. In this case customer VPCs will not be able to communicate with each other, but only with the management VPC."
      },
      {
        "date": "2024-04-21T10:02:00.000Z",
        "voteCount": 4,
        "content": "It SHOULD be transit gateway but it isn't. The VPCs are hosted in several accounts and regions. You can't attach all VPCs in  one transit gateway. You need several peered transit gws per region which is not the case here. Correct: B,C"
      },
      {
        "date": "2024-04-21T10:08:00.000Z",
        "voteCount": 3,
        "content": "Actually you need a transit gw per VPC region and they must be peered...... \nVery tricky question... Correct: B,C"
      },
      {
        "date": "2024-04-05T11:31:00.000Z",
        "voteCount": 1,
        "content": "NLB and PrivateLink offer benefits, they are overkill for this scenario. NLB is for distributing traffic across multiple instances, which isn't necessary here. PrivateLink creates a private connection for a service within a VPC, but it's a more complex solution than a simple peering connection for the management VPC."
      },
      {
        "date": "2024-03-09T15:00:00.000Z",
        "voteCount": 2,
        "content": "A and C"
      },
      {
        "date": "2024-02-08T07:14:00.000Z",
        "voteCount": 3,
        "content": "answer AC"
      },
      {
        "date": "2024-02-06T21:45:00.000Z",
        "voteCount": 3,
        "content": "Answer AC:\nTransit Gateway and Private Link for the WIN!"
      },
      {
        "date": "2024-02-05T19:42:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer A and C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 441,
    "url": "https://www.examtopics.com/discussions/amazon/view/132869-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has multiple lines of business (LOBs) that roll up to the parent company. The company has asked its solutions architect to develop a solution with the following requirements:<br>\u2022\tProduce a single AWS invoice for all of the AWS accounts used by its LOBs.<br>\u2022\tThe costs for each LOB account should be broken out on the invoice.<br>\u2022\tProvide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.<br>\u2022\tEach LOB account should be delegated full administrator permissions, regardless of the governance policy.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement service quotas to define the services and features that are permitted and apply the quotas to each LOB. as appropriate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that allows only approved services and features, then apply the policy to the LOB accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable consolidated billing in the parent account's billing console and link the LOB accounts."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-05T20:44:00.000Z",
        "voteCount": 11,
        "content": "E: Is wrong --&gt; Consolidated billing is already enabled by default when you create an organization\n\nB: obvious\nD: since there are no OUs, I assume that the SCP applies to each LOB account"
      },
      {
        "date": "2024-02-28T01:37:00.000Z",
        "voteCount": 7,
        "content": "I choose BE\nD conficts with the last requirement"
      },
      {
        "date": "2024-05-31T06:25:00.000Z",
        "voteCount": 1,
        "content": "I agree with you on that point"
      },
      {
        "date": "2024-05-09T11:50:00.000Z",
        "voteCount": 2,
        "content": "D does not conflict, they can be full administrators in their accounts but not have access to all services, one does not conflict with the other"
      },
      {
        "date": "2024-08-19T00:22:00.000Z",
        "voteCount": 1,
        "content": "B D\nD is required for governance"
      },
      {
        "date": "2024-08-04T03:32:00.000Z",
        "voteCount": 2,
        "content": "For SAP 01 version D and E were in the same option, so the answer is B - D+E"
      },
      {
        "date": "2024-06-12T07:19:00.000Z",
        "voteCount": 1,
        "content": "B D because Consolidated billing is already enabled by default when you create an organization"
      },
      {
        "date": "2024-05-26T07:44:00.000Z",
        "voteCount": 1,
        "content": "It should chose 3 ......... A + E . And D is require for governance"
      },
      {
        "date": "2024-05-26T07:45:00.000Z",
        "voteCount": 2,
        "content": "Sorry B + E . And  D is require for governance"
      },
      {
        "date": "2024-05-21T05:18:00.000Z",
        "voteCount": 3,
        "content": "We have to read carefully the question, it says \"Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.\", Organizations itself provide that ability but the question is not saying anything about to apply SCPs immediatelly to the OUs but just to have that ability. So for me D is discarded due to that."
      },
      {
        "date": "2024-05-03T04:53:00.000Z",
        "voteCount": 1,
        "content": "BE for me"
      },
      {
        "date": "2024-04-14T13:08:00.000Z",
        "voteCount": 2,
        "content": "The number of answers can not satisfy the question objectives.\n\u201cProduce a single AWS invoice for all of the AWS accounts used by its LOBs.\u201d B\n\u201cThe costs for each LOB account should be broken out on the invoice.\u201d E is trying to address this requirement, but has flaw. \n\u201cProvide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.\u201d D, but the policy should be applied to OU instead of Account.\n\u201cEach LOB account should be delegated full administrator permissions, regardless of the governance policy.\u201d NO answer is addressing this requirement."
      },
      {
        "date": "2024-04-09T05:18:00.000Z",
        "voteCount": 2,
        "content": "I think there should bbe choose 3 options here. BDE"
      },
      {
        "date": "2024-03-30T20:38:00.000Z",
        "voteCount": 4,
        "content": "It should be BE. D just clearly broke the requirement of \"each LOB account should be delegated full administrator permissions, regardless of the governance policy.\"."
      },
      {
        "date": "2024-03-20T01:07:00.000Z",
        "voteCount": 1,
        "content": "Could it be A, C? Service quotas is a terrible way of restricting features but it's the only one that satisfies the last 2 requirements."
      },
      {
        "date": "2024-03-22T04:54:00.000Z",
        "voteCount": 2,
        "content": "Forget it, answer is BD. In the SCP you can grant an exception to not limit the admin accounts."
      },
      {
        "date": "2024-03-09T15:06:00.000Z",
        "voteCount": 2,
        "content": "B and D"
      },
      {
        "date": "2024-03-03T19:59:00.000Z",
        "voteCount": 1,
        "content": "The question asks for billing at LOB level not at a company level. So A, not B."
      },
      {
        "date": "2024-02-27T10:20:00.000Z",
        "voteCount": 2,
        "content": "Can't be A, unless it changes to OU"
      },
      {
        "date": "2024-02-11T04:18:00.000Z",
        "voteCount": 1,
        "content": "I think something is missing in this question related to tags."
      },
      {
        "date": "2024-02-09T06:41:00.000Z",
        "voteCount": 3,
        "content": "B and E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 442,
    "url": "https://www.examtopics.com/discussions/amazon/view/132993-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect has deployed a web application that serves users across two AWS Regions under a custom domain. The application uses Amazon Route 53 latency-based routing. The solutions architect has associated weighted record sets with a pair of web servers in separate Availability Zones for each Region.<br><br>The solutions architect runs a disaster recovery scenario. When all the web servers in one Region are stopped, Route 53 does not automatically redirect users to the other Region.<br><br>Which of the following are possible root causes of this issue? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe weight for the Region where the web servers were stopped is higher than the weight for the other Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOne of the web servers in the secondary Region did not pass its HTTP health check.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLatency resource record sets cannot be used in combination with weighted resource record sets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe setting to evaluate target health is not turned on for the latency alias resource record set that is associated with the domain in the Region where the web servers were stopped.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn HTTP health check has not been set up for one or more of the weighted resource record sets associated with the stopped web servers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-03T21:53:00.000Z",
        "voteCount": 3,
        "content": "Route 53 latency-based routing does not inherently perform health checks. If a web server in one region goes down, Route 53 won't automatically redirect traffic to the other region unless health checks are properly configured and associated with your DNS records."
      },
      {
        "date": "2024-03-09T15:21:00.000Z",
        "voteCount": 1,
        "content": "Option D and E"
      },
      {
        "date": "2024-02-10T11:11:00.000Z",
        "voteCount": 1,
        "content": "DE are the correct answers for the given scenario"
      },
      {
        "date": "2024-02-06T22:17:00.000Z",
        "voteCount": 3,
        "content": "Answer DE:\n\nAn antique/classic question, answers are in a different order and wording slightly changed.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html"
      },
      {
        "date": "2024-02-05T21:17:00.000Z",
        "voteCount": 2,
        "content": "correct answer is D an E"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 443,
    "url": "https://www.examtopics.com/discussions/amazon/view/136546-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A flood monitoring agency has deployed more than 10,000 water-level monitoring sensors. Sensors send continuous data updates, and each update is less than 1 MB in size. The agency has a fleet of on-premises application servers. These servers receive updates from the sensors, convert the raw data into a human readable format, and write the results to an on-premises relational database server. Data analysts then use simple SQL queries to monitor the data.<br><br>The agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These maintenance tasks, which include updates and patches to the application servers, cause downtime. While an application server is down, data is lost from sensors because the remaining servers cannot handle the entire workload.<br><br>The agency wants a solution that optimizes operational overhead and costs. A solutions architect recommends the use of AWS IoT Core to collect the sensor data.<br><br>What else should the solutions architect recommend to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to .csv format and store it in an Amazon S3 bucket. Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to Apache Parquet format and store it in an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-18T20:17:00.000Z",
        "voteCount": 5,
        "content": "Kinesis Data Firehose is well-suited for ingesting and processing streaming data at scale, such as the continuous updates from the water-level monitoring sensors. It can reliably capture and deliver data to various destinations, including S3, without requiring additional application code.\n\nStoring the data in Apache Parquet format in S3 offers several benefits. Parquet is a columnar storage format optimized for analytics workloads, providing efficient compression and query performance. This format is suitable for data analysis and querying using tools like Athena.\n\nUsing AWS Lambda to transform the data from Kinesis Data Firehose into Parquet format reduces the maintenance effort associated with managing traditional servers. Lambda automatically scales with the incoming workload, ensuring continuous data processing without downtime."
      },
      {
        "date": "2024-10-11T15:12:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most suitable solution as it leverages serverless and scalable services (Kinesis Data Firehose, Lambda, S3, and Athena) to handle data ingestion, transformation, and analysis with minimal operational overhead and optimized costs."
      },
      {
        "date": "2024-08-15T08:52:00.000Z",
        "voteCount": 2,
        "content": "It says convert to human readable, that isn't Parquet, its CSV"
      },
      {
        "date": "2024-10-11T15:11:00.000Z",
        "voteCount": 1,
        "content": "The statement does not say that it is necessary to continue storing the data in human readable form.\nThe statement says that the agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These are the requirements."
      },
      {
        "date": "2024-05-11T05:08:00.000Z",
        "voteCount": 4,
        "content": "Lamda functions integrates with Data Firehouse better than sending the data to Apache Flink and then implement a solution to transform the data into the Parquet Format to be sent to S3. From AWS Documentation: \"With Amazon Managed Service for Apache Flink, you can use Java, Scala, Python, or SQL to process and analyze streaming data\". So, Flink does not make any authomatic data transformation. The correct option is B."
      },
      {
        "date": "2024-05-08T22:50:00.000Z",
        "voteCount": 1,
        "content": "D seems to be the correct option as its a managed service"
      },
      {
        "date": "2024-05-03T04:54:00.000Z",
        "voteCount": 2,
        "content": "B for me"
      },
      {
        "date": "2024-04-23T07:13:00.000Z",
        "voteCount": 3,
        "content": "\u201cB\u201d seems to be the correct ans. Amazon Data Firehose can ingest data streams from IoT and convert them to into Parquet format using Lambda function. The destination of the stream can be S3.\nhttps://aws.amazon.com/firehose/\nhttps://d1.awsstatic.com/pdp-how-it-works-assets/Product-Pate-Diagram-Amazon-Kinesis-Data-Firehose%402x.39ea068e48494676c0f4386535f85a966e9ac252.png"
      },
      {
        "date": "2024-04-17T00:55:00.000Z",
        "voteCount": 1,
        "content": "D seems a better fit as Apache flink is a managed services for steaming as well as transformation. makes things simpler"
      },
      {
        "date": "2024-04-02T06:01:00.000Z",
        "voteCount": 4,
        "content": "Both B and D are work.\nB - KDF&amp;Lambda for data transformation\nD - KDA for real-time analysis"
      },
      {
        "date": "2024-03-30T07:14:00.000Z",
        "voteCount": 1,
        "content": "Using a managed service for data transformation optimizes operational overhead."
      },
      {
        "date": "2024-03-20T15:12:00.000Z",
        "voteCount": 3,
        "content": "\"human readable format\", I go with CSV"
      },
      {
        "date": "2024-03-20T14:10:00.000Z",
        "voteCount": 3,
        "content": "Although option D call work, it introduces unnecessary complexity for the given scenario."
      },
      {
        "date": "2024-03-20T10:07:00.000Z",
        "voteCount": 2,
        "content": "Answer is D."
      },
      {
        "date": "2024-03-18T19:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 444,
    "url": "https://www.examtopics.com/discussions/amazon/view/136548-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instances running across multiple Availability Zones (AZs) in a Region backed by an Amazon RDS MySQL Multi-AZ deployment. Target group health checks are configured to use HTTP and pointed at the product catalog page. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.<br><br>Recently, the application experienced an outage. Auto Scaling continuously replaced the instances during the outage. A subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times.<br><br>Which of the following changes together would remediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-06T17:59:00.000Z",
        "voteCount": 7,
        "content": "My answer is a bit difference.\nIt doesn't mentioned read-only scnerio so read replica in A may not able to help\ncompare B and C, both pro and con. I lean to B from both the real world and in this particual question to bypass the database"
      },
      {
        "date": "2024-04-09T05:44:00.000Z",
        "voteCount": 7,
        "content": "https://www.examtopics.com/discussions/amazon/view/46826-exam-aws-certified-solutions-architect-professional-topic-1/"
      },
      {
        "date": "2024-07-07T22:13:00.000Z",
        "voteCount": 1,
        "content": "B and E works as expection.\nDue to web app is normal, A and C can not help\nCloudWatch alarm can not support so complicated action"
      },
      {
        "date": "2024-06-06T07:41:00.000Z",
        "voteCount": 2,
        "content": "When you talking about a product catalog page, you need to cache information (text, images) to leverage the load on the data tier. ElastiCache is the solution (or a CDN)."
      },
      {
        "date": "2024-05-27T18:57:00.000Z",
        "voteCount": 1,
        "content": "A &amp; E is correct answer as the monitoring aspect is not part of the problem statement"
      },
      {
        "date": "2024-05-16T08:02:00.000Z",
        "voteCount": 2,
        "content": "A and B are my answers.\nA instead of E only because we have no tips that indicate the necessity to have a caching systems. For example, it say \"high load on database\", but nothing about same record read more more time.\nSo for me A and B"
      },
      {
        "date": "2024-05-03T04:56:00.000Z",
        "voteCount": 3,
        "content": "BE for me"
      },
      {
        "date": "2024-04-23T07:23:00.000Z",
        "voteCount": 4,
        "content": "B and E."
      },
      {
        "date": "2024-04-08T10:53:00.000Z",
        "voteCount": 5,
        "content": "\"for future growth\" -&gt; E (cache in front of the DB)"
      },
      {
        "date": "2024-04-07T07:03:00.000Z",
        "voteCount": 4,
        "content": "Selected Answer: BE\nB: Will improve web app\nD: Will improve database high load issue and query response times."
      },
      {
        "date": "2024-04-02T06:13:00.000Z",
        "voteCount": 2,
        "content": "TCP check is like a heartbeat check, too rough."
      },
      {
        "date": "2024-03-26T19:52:00.000Z",
        "voteCount": 2,
        "content": "read replica to reduce load.\ntcp check to see if ec2 is reachable"
      },
      {
        "date": "2024-03-22T12:59:00.000Z",
        "voteCount": 2,
        "content": "I agree with A and B ."
      },
      {
        "date": "2024-03-20T14:29:00.000Z",
        "voteCount": 3,
        "content": "A and B are my picks"
      },
      {
        "date": "2024-03-20T10:39:00.000Z",
        "voteCount": 1,
        "content": "By the way, ExamTopics, we get a 503 when submitting voting comments. Please change my previous submission to that type, specifying A and C."
      },
      {
        "date": "2024-03-20T10:37:00.000Z",
        "voteCount": 2,
        "content": "First of all: the instances are terminated because the load on the DB is too high.\n\nA: Best way to reduce the load on the DB. It doesn't notify admins, though, which means we then need either B or C (D and E do not notify admins).\nB: Health checks do not burden the site as they are done relatively seldom, so trying to reduce load by using a page that's lighter on the DB is not very relevant. Notifies admins.\nC: TCP checks are lighter on the load than HTTP checks, which means the already slight overhead for health checks is reduced further than in B. Notifies admins. This is preferable to B, but both feel inconsequential.\nD: Recovery actions in this situation are out of scope. This alternative is there to confuse.\nE: An alternative to A, but it has operational overhead in that the application must be changed to use the cache. A is more straightforward.\n\nThus A and C."
      },
      {
        "date": "2024-03-20T01:24:00.000Z",
        "voteCount": 3,
        "content": "B. Health check is failing because the application cannot read from the DB, event though the EC2 instance is fine. As a result the ALB is terminating the EC2 instance unnecessarily."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 445,
    "url": "https://www.examtopics.com/discussions/amazon/view/136549-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an on-premises data center and is using Kubernetes to develop a new solution on AWS. The company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters for its development and test environments.<br><br>The EKS control plane and data plane for production workloads must reside on premises. The company needs an AWS managed solution for Kubernetes management.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Amazon EKS Anywhere on the company's hardware in the on-premises data center. Deploy the production workloads on an EKS Anywhere cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using an extended cluster configuration on the Outposts server for the production workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an AWS Outposts server in the on-premises data center. Install Amazon EKS Anywhere on the Outposts server. Deploy the production workloads on an EKS Anywhere cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-23T13:11:00.000Z",
        "voteCount": 10,
        "content": "A\n3 things to consider fromr question requirement\ncontrol plane location - onprem\ndata plane location - onprem\nmanagement - AWS\nEKS anywhere it managed by customer so BD out\nhttps://anywhere.eks.amazonaws.com/docs/concepts/eksafeatures/#comparing-amazon-eks-anywhere-to-amazon-eks\n\nExtended clusters \u2013 Run the Kubernetes control plane in an AWS Region and nodes on your Outpost.\nLocal clusters \u2013 Run the Kubernetes control plane and nodes on your Outpost\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-deployment-options.html\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-outposts.html"
      },
      {
        "date": "2024-10-11T15:34:00.000Z",
        "voteCount": 1,
        "content": "With EKS LOCAL clusters on Outposts, you can run the Amazon EKS control plane and data plane entirely on the Outposts hardware on-premises.Also, it's an AWS managed solution."
      },
      {
        "date": "2024-08-19T01:25:00.000Z",
        "voteCount": 1,
        "content": "It's A, EKS Everywhere = Operational Overhead, and it said it needs to be managed by AWS"
      },
      {
        "date": "2024-08-06T19:47:00.000Z",
        "voteCount": 1,
        "content": "B\nOption A (AWS Outposts with local EKS cluster): While this can run EKS on premises, managing an AWS Outposts server and the associated EKS infrastructure might involve more overhead compared to EKS Anywhere.\nOption C (AWS Outposts with extended EKS cluster): Similar to option A, this involves managing the Outposts server and integrating it with your on-premises environment, which could increase the complexity and operational overhead.\nOption D (AWS Outposts with EKS Anywhere): This combines managing an Outposts server with deploying EKS Anywhere, which adds unnecessary complexity and overhead compared to deploying EKS Anywhere directly on the company's hardware."
      },
      {
        "date": "2024-07-27T13:23:00.000Z",
        "voteCount": 1,
        "content": "EKS anywhere!!!"
      },
      {
        "date": "2024-10-11T15:32:00.000Z",
        "voteCount": 1,
        "content": "Can't be B. The company needs an AWS managed solution for Kubernetes management. EKS Anywhere is not an AWS managed solution. It's a self-managed solution."
      },
      {
        "date": "2024-06-29T10:05:00.000Z",
        "voteCount": 2,
        "content": "Vote A because by using outpostM EKS is AWS managed service but running on local. Question require AWS managed solution for Kubernetes management. If EKS Anywhere with control plane on prem not AWS cloud, then it's self managed cluster."
      },
      {
        "date": "2024-06-23T03:16:00.000Z",
        "voteCount": 1,
        "content": "A - Control Plane has to be on-prem (not case for Outpost)"
      },
      {
        "date": "2024-05-27T19:08:00.000Z",
        "voteCount": 2,
        "content": "B - Correct. The requirement is to use on-premise hardware with AWS managed EKS that means EKS Anywhere which leverages on-premise hardware with full control over both the control plane and data plane"
      },
      {
        "date": "2024-04-23T07:39:00.000Z",
        "voteCount": 1,
        "content": "A - correct."
      },
      {
        "date": "2024-03-27T08:17:00.000Z",
        "voteCount": 2,
        "content": "Option A:  requirement is ask for AWS Managed Solution  and \n AWS Outpost give you that option https://docs.aws.amazon.com/managedservices/latest/userguide/outposts.html\n\nNot Option B : Unlike Amazon EKS in AWS Cloud, EKS Anywhere is a user-managed product that runs on user-managed infrastructure. You are responsible for cluster lifecycle \noperations and maintenance of your EKS Anywhere clusters.\nhttps://anywhere.eks.amazonaws.com/docs/overview/"
      },
      {
        "date": "2024-03-26T19:42:00.000Z",
        "voteCount": 1,
        "content": "https://anywhere.eks.amazonaws.com/docs/concepts/eksafeatures/#:~:text=With%20Amazon%20EKS%20on%20Outposts,with%20EKS%20Anywhere%20automation%20tooling."
      },
      {
        "date": "2024-03-23T02:00:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A.\nIt is not C because EKS Anywhere cluster is a customer-managed product that runs on customer-managed infrastructure. \nRef: https://aws.amazon.com/eks/eks-anywhere/faqs/"
      },
      {
        "date": "2024-03-22T21:31:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A\nYou can use Amazon EKS to run on-premises Kubernetes applications on AWS Outposts. You can deploy Amazon EKS on Outposts in the following ways:\n\n    Extended clusters \u2013 Run the Kubernetes control plane in an AWS Region and nodes on your Outpost.\n\n    Local clusters \u2013 Run the Kubernetes control plane and nodes on your Outpost.\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-outposts.html"
      },
      {
        "date": "2024-03-21T04:14:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A: when deploying EKS on an Outpost server in a local cluster configuration, the control plane and data plane reside on-premises, but the control plane is AWS-managed.\n\nB is incorrect. Although for EKS-A, the control plane and data plane reside on-premises, it is not AWS-managed but completely customer-managed (both control plane and data plane).\n\nC is incorrect because in an extended cluster configuration on AWS Outpost, the control plane runs inside the AWS cloud, not on the outpost server on-premises.\n\nD is incorrect because you do not combine EKS-A and Outpost."
      },
      {
        "date": "2024-03-20T19:09:00.000Z",
        "voteCount": 2,
        "content": "Answer is B.  The requirement is that both control plane and data plane will reside on premise.  If you deploy EKS using extended cluster the control plane lies within AWS region.  You need a local cluster for the control plane to reside on outpost. \nPlease refer to url below. \nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-outposts.html"
      },
      {
        "date": "2024-03-20T15:24:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/eks/latest/userguide/eks-deployment-options.html"
      },
      {
        "date": "2024-03-20T14:45:00.000Z",
        "voteCount": 2,
        "content": "This option provides an AWS-managed solution for Kubernetes management on-premises without the additional complexity of managing an Outposts server."
      },
      {
        "date": "2024-04-12T08:01:00.000Z",
        "voteCount": 1,
        "content": "Does AWS manage Amazon EKS Anywhere clusters or cluster infrastructure capacity?\n\nNo. Unlike Amazon EKS in AWS Cloud, Amazon EKS Anywhere is a customer-managed product that runs on customer-managed infrastructure. You are responsible for cluster lifecycle operations and maintenance of your Amazon EKS Anywhere clusters and the cluster infrastructure capacity.\nhttps://aws.amazon.com/eks/eks-anywhere/faqs/"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 446,
    "url": "https://www.examtopics.com/discussions/amazon/view/136551-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations to manage its development environment. Each development team at the company has its own AWS account. Each account has a single VPC and CIDR blocks that do not overlap.<br><br>The company has an Amazon Aurora DB cluster in a shared services account. All the development teams need to work with live data from the DB cluster.<br><br>Which solution will provide the required connectivity to the DB cluster with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Resource Access Manager (AWS RAM) resource share for the DB cluster. Share the DB cluster with all the development accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway in the shared services account. Create an AWS Resource Access Manager (AWS RAM) resource share for the transit gateway. Share the transit gateway with all the development accounts. Instruct the developers to accept the resource share. Configure networking.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer (ALB) that points to the IP address of the DB cluster. Create an AWS PrivateLink endpoint service that uses the ALB. Add permissions to allow each development account to connect to the endpoint service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection in the shared services account. Configure networking. Use AWS Marketplace VPN software in each development account to connect to the Site-to-Site VPN connection."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T15:52:00.000Z",
        "voteCount": 9,
        "content": "The question asks about working with live data and providing CONNECTIVITY to the DB cluster. B is the correct as it provides both"
      },
      {
        "date": "2024-04-06T18:15:00.000Z",
        "voteCount": 8,
        "content": "B\nI originally chose A since I thoughtAurora DB cluster is sharable\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur\nBut as Verri mentioned, with that share, it only allow you to CLONE the db rather than use it as live\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Managing.Clone.Cross-Account"
      },
      {
        "date": "2024-07-07T22:00:00.000Z",
        "voteCount": 1,
        "content": "for live data, it should be B"
      },
      {
        "date": "2024-05-16T11:57:00.000Z",
        "voteCount": 1,
        "content": "For me it's A.\nWe need to use the RAM only for the Aurora DB. We don't need to peer the VPCs with TransitGateway. Also less ops effort is option A. So Option B is unuseful complicated."
      },
      {
        "date": "2024-04-23T11:54:00.000Z",
        "voteCount": 3,
        "content": "Correct ans \"B\"."
      },
      {
        "date": "2024-04-01T17:50:00.000Z",
        "voteCount": 1,
        "content": "Seemed A since B requires a lot setup work"
      },
      {
        "date": "2024-03-29T09:15:00.000Z",
        "voteCount": 1,
        "content": "LEAST operational overhead is \"A\".\nYou can share DB Cluster. https://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur"
      },
      {
        "date": "2024-07-05T07:54:00.000Z",
        "voteCount": 1,
        "content": "Live data is catch here, A is for clone"
      },
      {
        "date": "2024-03-24T18:44:00.000Z",
        "voteCount": 5,
        "content": "A: Sharing DB cluster with RAM allows you to CLONE a shared, centrally managed DB cluster\nC: PrivateLink needs NLB not ALB\nD: WTF"
      },
      {
        "date": "2024-03-23T13:27:00.000Z",
        "voteCount": 1,
        "content": "I will go for A as the ref link provided by JOKERO \nif not, the transit gateway would be ideal too."
      },
      {
        "date": "2024-07-05T07:55:00.000Z",
        "voteCount": 1,
        "content": "Live data is catch here, A is for clone"
      },
      {
        "date": "2024-03-20T12:58:00.000Z",
        "voteCount": 4,
        "content": "C is wrong because for Private Link you need to use NLB not ALB.\n\nCorrect answer is B."
      },
      {
        "date": "2024-03-20T08:26:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur"
      },
      {
        "date": "2024-03-19T17:37:00.000Z",
        "voteCount": 6,
        "content": "AWS PrivateLink requires an NLB (Network Load Balancer). Since the question mentions that IP addresses should not overlap, sharing via Transit Gateway might be a good approach."
      },
      {
        "date": "2024-03-18T21:19:00.000Z",
        "voteCount": 1,
        "content": "Utilizing AWS PrivateLink to enable private connectivity between VPCs without the need for public IP addresses or internet gateways. Creating an ALB pointing to the DB cluster's IP address and then creating a PrivateLink endpoint service that uses the ALB allows each development account to securely connect to the DB cluster. This approach minimizes operational overhead and simplifies network connectivity."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 447,
    "url": "https://www.examtopics.com/discussions/amazon/view/136552-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company used AWS CloudFormation to create all new infrastructure in its AWS member accounts. The resources rarely change and are properly sized for the expected load. The monthly AWS bill is consistent.<br><br>Occasionally, a developer creates a new resource for testing and forgets to remove the resource when the test is complete. Most of these tests last a few days before the resources are no longer needed.<br><br>The company wants to automate the process of finding unused resources. A solutions architect needs to design a solution that determines whether the cost in the AWS bill is increasing. The solution must help identify resources that cause an increase in cost and must automatically notify the company's operations team.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on billing alerts. Use AWS Cost Explorer to determine the costs for the past month. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on billing alerts. Use AWS Cost Explorer to determine the average monthly costs for the past 3 months. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of Linked account. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T12:13:00.000Z",
        "voteCount": 10,
        "content": "Ans \"D\" - more granular.\n\nQ: What is the difference between a linked account monitor in a payer account, and a services monitor in a linked account?\n\nA linked account monitor in a payer account will monitor the spend of all services, in total, for that linked account.\nA services monitor in a linked account will monitor the individual spend for each service for that linked account.\nFor example, if there is a spike in S3 spending, but a dip in EC2 spending of the same amount (net neutral change), the linked account monitor in the payer account will not detect this because it is monitoring the total account spend across all services. However, the services monitor in the linked account would detect the S3 spike since it is monitoring each service spend individually."
      },
      {
        "date": "2024-05-17T20:37:00.000Z",
        "voteCount": 1,
        "content": "Thanks for  the explanation"
      },
      {
        "date": "2024-04-23T12:13:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/#:~:text=What%20is%20the%20difference%20between%20a%20linked%20account%20monitor%20in%20a%20payer%20account,%20and%20a%20services%20monitor%20in%20a%20linked%20account"
      },
      {
        "date": "2024-10-11T15:44:00.000Z",
        "voteCount": 1,
        "content": "For those who think the correct answer is C: A Linked Account monitor detects anomalies at the account level. While it can identify which account has unusual spending, it does NOT pinpoint the SPECIFIC SERVICES or RESOURCES causing the increase, as the statement requires."
      },
      {
        "date": "2024-08-14T18:09:00.000Z",
        "voteCount": 1,
        "content": "\"identify resources that cause an increase in cost \"\nD for sure"
      },
      {
        "date": "2024-07-27T13:35:00.000Z",
        "voteCount": 1,
        "content": "Monitoring at the AWS service level can be useful, but it may not provide the same comprehensive view of costs across accounts as the linked account monitor type.\nTherefore, option C provides a more adaptive and comprehensive solution for detecting cost anomalies and notifying the operations team."
      },
      {
        "date": "2024-05-03T05:00:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2024-04-17T02:42:00.000Z",
        "voteCount": 1,
        "content": "D. \nAn AWS service monitor will be applicable to all customers since it tracks and detects anomalies across any service they deploy\nhttps://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/#:~:text=An%20AWS%20service%20monitor%20will%20be%20applicable%20to%20all%20customers%20since%20it%20tracks%20and%20detects%20anomalies%20across%20any%20service%20they%20deploy"
      },
      {
        "date": "2024-04-07T23:48:00.000Z",
        "voteCount": 3,
        "content": "c: identify abnormal accounts\nd: identify abnormal service, which is desired."
      },
      {
        "date": "2024-04-06T18:22:00.000Z",
        "voteCount": 3,
        "content": "vote D here\nA linked account monitor can track up to 10 different linked accounts. A linked account monitor tracks spending aggregated across all of the designated linked accounts. For example, if a linked account monitor tracks Account A and Account B, and then Account A\u2019s usage spikes while Account B\u2019s usage dips by the same amount, there will be no anomaly detected because it is a net neutral change.\nref\nhttps://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/"
      },
      {
        "date": "2024-03-29T19:10:00.000Z",
        "voteCount": 1,
        "content": "C more granular"
      },
      {
        "date": "2024-03-27T08:33:00.000Z",
        "voteCount": 1,
        "content": "Option C and Not Option D :  Linked account - This monitor evaluates the total spend of an individual, or group of, member accounts. If your Organizations need to segment spend by team, product, services, or environment, this monitor is useful. The maximum number of member accounts that you can select for each monitor is 10.\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/getting-started-ad.html#monitor-type-def"
      },
      {
        "date": "2024-03-24T19:16:00.000Z",
        "voteCount": 1,
        "content": "I will go with C because the scenario says, \"to create all new infrastructure in its AWS member accounts.\""
      },
      {
        "date": "2024-03-23T13:40:00.000Z",
        "voteCount": 4,
        "content": "D seems more granular to detect thich resource in which account generated the bill.\nC seem only care about the balance across accounts as below\n\"linked account monitor can track up to 10 different linked accounts. A linked account monitor tracks spending aggregated across all of the designated linked accounts. For example, if a linked account monitor tracks Account A and Account B, and then Account A\u2019s usage spikes while Account B\u2019s usage dips by the same amount, there will be no anomaly detected because it is a net neutral change\"  \nhttps://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/"
      },
      {
        "date": "2024-03-20T12:56:00.000Z",
        "voteCount": 3,
        "content": "Answer is between C and D.\n\nChoosing monitor type AWS Services is more appropriate than Linked Account because the monitor AWS Services monitors all resources including resources from all member accounts of the organization. Also with Linked Accounts you can only add max 10 accounts to a single monitor. Therefore answer D is correct."
      },
      {
        "date": "2024-03-20T10:56:00.000Z",
        "voteCount": 4,
        "content": "On reconsideration: D, as it deals with the individual services in an account, not just the total cost."
      },
      {
        "date": "2024-03-20T10:53:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer.\nD is not granular enough."
      },
      {
        "date": "2024-03-18T21:26:00.000Z",
        "voteCount": 3,
        "content": "AWS Cost Anomaly Detection is specifically designed to detect unusual spending patterns or anomalies in AWS costs. By creating a cost monitor with a monitor type of Linked account, the solution focuses on the entire AWS account's spending, which is suitable for identifying unexpected increases in costs due to unused resources. Setting a threshold for cost variance allows the operations team to receive notifications when there are significant deviations from the expected spending pattern."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 448,
    "url": "https://www.examtopics.com/discussions/amazon/view/136553-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.<br><br>The solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR). The DR copy has an RPO of less than 1 hour.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-09T12:31:00.000Z",
        "voteCount": 15,
        "content": "So practically everyone here is wrong. because it is A. Here is why B is wrong because one there is no such thing as bursting mode for Lustre that is an EFS thing, but also Backup will not work for the RPO. C is wrong obviously because GP3 can't be shared. D is wrong because Datasync tasks cannot be scheduled for any more frequent then hourly so no D is wrong because you cannot schedule data sync tasks less then hourly so you don't meet the RPO. So all of those are easily wrong because they have bad information. They fooled everyone on A because all they say is the 'Active working set is 100GB\" not the entire filesystem. EFS accumulates bursting credits so for every 100GB of filesystem size you can burst up to 300MiBps for up to 72 minutes. So you provision 75MiBps because that would average out over time so you aren't being overcharged for the provisioned size."
      },
      {
        "date": "2024-07-07T21:31:00.000Z",
        "voteCount": 2,
        "content": "A scheduled task runs at a frequency that you specify, with a minimum interval of 1 hour.\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html"
      },
      {
        "date": "2024-06-29T10:41:00.000Z",
        "voteCount": 3,
        "content": "A. EFS support cross region replication. e4bc18e already point why D is wrong."
      },
      {
        "date": "2024-06-12T16:41:00.000Z",
        "voteCount": 3,
        "content": "big thank to e4bc18e"
      },
      {
        "date": "2024-05-20T08:39:00.000Z",
        "voteCount": 3,
        "content": "A\nSolution write by e4bc18e"
      },
      {
        "date": "2024-04-23T13:28:00.000Z",
        "voteCount": 2,
        "content": "D is correct.\n\n\"You can use DataSync to transfer files between two FSx for OpenZFS file systems, and also move data to a file system in a different AWS Region or AWS account. You can also use DataSync with FSx for OpenZFS file systems for other tasks. For example, you can perform one-time data migrations, periodically ingest data for distributed workloads, and schedule replication for data protection and recovery.\"\nhttps://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/migrate-files-to-fsx-datasync.html"
      },
      {
        "date": "2024-05-09T12:23:00.000Z",
        "voteCount": 2,
        "content": "This is wrong a Datasync task cannot be schedule for any more frequent then one hour so the under 1 hour RPO is not met."
      },
      {
        "date": "2024-05-18T05:23:00.000Z",
        "voteCount": 1,
        "content": "@e4bc18e, it seems you are right. Indeed, DataSync can go as granular as 1 hour.\nFound this:\n\"If the file system\u2019s baseline throughput exceeds the Provisioned throughput amount, then it automatically uses the Bursting throughput...\"\nFor 1 TiB of metered data in Standard storage, it can burst to 300 MiBps read-only for 12 hours per day.\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes"
      },
      {
        "date": "2024-04-07T07:26:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html#fsx-aggregate-perf"
      },
      {
        "date": "2024-04-23T13:25:00.000Z",
        "voteCount": 1,
        "content": "\u201cB\u201d is wrong because with AWS Backup you can do a backup as frequent as 1 hour, but the RPO must be less than 1 hour.\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html#create-backup-plan-console"
      },
      {
        "date": "2024-03-28T06:24:00.000Z",
        "voteCount": 1,
        "content": "D:  \nThe throughput is related to size of the EFS, but the question said the active set of the data will be only up to 100GB, with that size, the throughout will be lower than requested.\nso D:"
      },
      {
        "date": "2024-03-24T19:50:00.000Z",
        "voteCount": 3,
        "content": "D involves managing separate file systems that do not natively offer a \"single location\" experience across regions without additional configuration and replication mechanisms."
      },
      {
        "date": "2024-03-23T14:25:00.000Z",
        "voteCount": 4,
        "content": "D\n\na sneaky question since my first impression is go for A but it is wrong due to the 75M throughput mode. What's the calculation here? one region has 3 AZ? so 75x3=225?. EFS is not provisioned in that way. Even that, the 225 is the total throughput where question asked 225 for read. Implied the total would be more like 225+XXX. Anyway, A is wrong.\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nC is wrong since EBS multi attach don't support gp3\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-volumes-multi.html"
      },
      {
        "date": "2024-03-23T14:27:00.000Z",
        "voteCount": 3,
        "content": "B is wrong where the hourly AWS backup job won't meet the RPO requirement (less than 1 hour)\n\nThe backup frequency determines how often AWS Backup creates a snapshot backup. Using the console, you can choose a frequency of every hour, 12 hours, daily, weekly, or monthly. You can also create a cron expression that creates snapshot backups as frequently as hourly. Using the AWS Backup CLI, you can schedule snapshot backups as frequently as hourly\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html"
      },
      {
        "date": "2024-03-20T11:02:00.000Z",
        "voteCount": 1,
        "content": "D is the answer. A would also have worked."
      },
      {
        "date": "2024-03-18T21:33:00.000Z",
        "voteCount": 1,
        "content": "Amazon FSx for OpenZFS is a fully managed file system service that supports native replication between regions, making it well-suited for DR scenarios with a low RPO requirement. Using AWS DataSync for replication every 10 minutes ensures that the DR copy stays up to date with minimal data loss. This solution provides the required read throughput, data replication, and DR capabilities with less operational overhead."
      },
      {
        "date": "2024-05-09T12:26:00.000Z",
        "voteCount": 1,
        "content": "Wrong Datasync tasks cannot be scheduled to be more frequent then hourly, so you cannot schedule data sync tasks to be every 10 Minutes. Apparently everyone is forgetting about burst credits for EFS.  Probably something a little missing but it only says the \"Active working set\" is 100GB\" not the entire filesystem. For every 100GB of data of provisioned EFS space you can burst to 300MiBps for 72 minutes."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 449,
    "url": "https://www.examtopics.com/discussions/amazon/view/136558-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to gather data from an experiment in a remote location that does not have internet connectivity. During the experiment, sensors that are connected to a local network will generate 6 TB of data in a proprietary format over the course of 1 week. The sensors can be configured to upload their data files to an FTP server periodically, but the sensors do not have their own FTP server. The sensors also do not support other protocols. The company needs to collect the data centrally and move the data to object storage in the AWS Cloud as soon as possible after the experiment.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowball Edge Compute Optimized device. Connect the device to the local network. Configure AWS DataSync with a target bucket name, and unload the data over NFS to the device. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Create a shell script that periodically downloads data from each sensor. After the experiment, return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowcone device. Connect the device to the local network. Configure the device to use Amazon FSx. Configure the sensors to upload data to the device. Configure AWS DataSync on the device to synchronize the uploaded data with an Amazon S3 bucket. Return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T16:49:00.000Z",
        "voteCount": 2,
        "content": "Since the sensors only support FTP for data upload, installing and configuring an FTP server on the EC2 instance is essential. This setup allows the sensors to periodically upload their data files to the Snowcone device."
      },
      {
        "date": "2024-05-17T20:57:00.000Z",
        "voteCount": 1,
        "content": "Snowcone is specialized for huge data migration."
      },
      {
        "date": "2024-03-30T21:29:00.000Z",
        "voteCount": 2,
        "content": "Snowcone edge computing + FTP data transfer"
      },
      {
        "date": "2024-03-24T19:56:00.000Z",
        "voteCount": 2,
        "content": "C, because of FTP"
      },
      {
        "date": "2024-03-23T14:34:00.000Z",
        "voteCount": 1,
        "content": "agree on C, since need FTP server which is the only supported method. AWS snowball seems support EC2 too, but not in any answer"
      },
      {
        "date": "2024-03-20T11:06:00.000Z",
        "voteCount": 1,
        "content": "C, for FTP."
      },
      {
        "date": "2024-03-20T01:55:00.000Z",
        "voteCount": 2,
        "content": "C is the only one which uses FTP"
      },
      {
        "date": "2024-03-18T21:49:00.000Z",
        "voteCount": 2,
        "content": "Sensors only support FTP protocol. Leverage the native capabilities of Snowcone and EC2, providing an efficient method for collecting data."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 450,
    "url": "https://www.examtopics.com/discussions/amazon/view/136559-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.<br><br>Each business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.<br><br>Which solution will meet these requirements with the LEAST operational complexity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account\u2019s data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T11:09:00.000Z",
        "voteCount": 8,
        "content": "Correct: D\nThe option talks about LEAST operational complexity not LEAST operational overhead. Option B is quite complex"
      },
      {
        "date": "2024-03-20T11:13:00.000Z",
        "voteCount": 7,
        "content": "LEAST operational complexity, considering the report already is available in the bucket: B. After the initial setup, the process is fully automatic, which means the operational complexity involving separate actions by account managers isn't needed."
      },
      {
        "date": "2024-10-11T16:16:00.000Z",
        "voteCount": 1,
        "content": "In addition to what user Dgix commented, the fact that the S3 bucket must be in the account that creates the CUR does not make option B unfeasible. On the contrary, this option already assumes that the initial configuration of the bucket and the processing of the CUR report happen in the management account. Option B remains the recommended solution because it: Automates the data segmentation process. Ensures compliance with documentation by keeping the S3 bucket in the management account. Simplifies access control by using bucket policies to ensure that each account sees only its own data. Meets the requirement of lower operational complexity by centralizing the processing of the CUR. Therefore, even with the restriction that the S3 bucket must be in the management account, option B remains the best choice to meet the business requirements with the least operational effort."
      },
      {
        "date": "2024-08-21T06:15:00.000Z",
        "voteCount": 2,
        "content": "B sounds like quite the adventure."
      },
      {
        "date": "2024-08-21T06:19:00.000Z",
        "voteCount": 1,
        "content": "\"Each business unit can have access to only its own cost and utilization data\""
      },
      {
        "date": "2024-08-12T11:10:00.000Z",
        "voteCount": 1,
        "content": "B would be very complex to parse the incoming files and separate by prefix. Then managing all the individual prefix shares. For that reason D seems like a better choice. Also the question mentions having the right permissions setup so they can configure their own CUR."
      },
      {
        "date": "2024-07-07T03:39:00.000Z",
        "voteCount": 2,
        "content": "Answer: Option D\n\nFirst Reason: The Cost and Usage Report (CUR) cannot be set up for cross-account delivery. According to the AWS documentation, \u201cThe account that creates the Cost and Usage Report must also own the Amazon S3 bucket that AWS sends the reports to.\u201d This means each account must set up its own S3 bucket to receive its respective CUR.\nhttps://docs.aws.amazon.com/cur/latest/userguide/cur-consolidated-billing.html\nSecond Reason: The question asks for the solution with the least operational complexity. Option D simplifies the process by allowing each account to independently manage its own CUR setup without requiring complex configurations or custom Lambda functions."
      },
      {
        "date": "2024-06-12T17:08:00.000Z",
        "voteCount": 3,
        "content": "After some investigation, I found A could be a suitable choice, however it lacks a few details\nBy using AWS RAM, you can share the S3 bucket (or specific prefixes within the bucket) containing the Cost and Usage Report with the member accounts.\nEach member account can set up Athena queries to access and analyze their own cost and utilization data from the shared S3 bucket. This approach ensures that each business unit can view its own data without accessing other units' data.\n\nB: too complicated\nC: Cost Explorer doesn't provide the raw cost and usage data that might be needed for detailed analysis with Athena.\nD: multiple Cost and Usage Reports, one for each account =&gt; out"
      },
      {
        "date": "2024-06-12T17:01:00.000Z",
        "voteCount": 2,
        "content": "The question asks for LEAST operational complexity\nBut it seems that only the most complex option can solve the problem"
      },
      {
        "date": "2024-05-17T07:38:00.000Z",
        "voteCount": 4,
        "content": "Why B? The question talk about LEAST operations. D for me"
      },
      {
        "date": "2024-03-24T20:04:00.000Z",
        "voteCount": 1,
        "content": "The most straightforward option"
      },
      {
        "date": "2024-03-23T14:41:00.000Z",
        "voteCount": 2,
        "content": "B\nI don't like this type of question that shows the current AWS limit which need to use sneaky way, like lambda, to automate the process. This should be a potential new feature that AWS should improve in future since the billing and report is such a common scenrio as in the question."
      },
      {
        "date": "2024-03-18T22:00:00.000Z",
        "voteCount": 1,
        "content": "With the Lambda to extract and separate each member account's cost and utilization data from the central Cost and Usage Report stored in the S3 bucket and S3 events to trigger the Lambda function, the process is automated and requires minimal ongoing management. Each member account can be given access only to its own prefix within the S3 bucket, ensuring that each business unit can only access its own cost data.  Other options involve higher operational complexity and overhead."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 451,
    "url": "https://www.examtopics.com/discussions/amazon/view/136560-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is designing an AWS environment for a manufacturing application. The application has been successful with customers, and the application's user base has increased. The company has connected the AWS environment to the company's on-premises data center through a 1 Gbps AWS Direct Connect connection. The company has configured BGP for the connection.<br><br>The company must update the existing network connectivity solution to ensure that the solution is highly available, fault tolerant, and secure.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data in transit and provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision another Direct Connect connection between the company's on-premises data center and AWS to increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the Direct Connect connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure multiple private VIFs. Load balance data across the VIFs between the on-premises data center and AWS to provide resilience.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-23T00:30:00.000Z",
        "voteCount": 10,
        "content": "MACsec is only supported on 10gbps and 100gbps Direct Connect \nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-mac-sec-getting-started.html"
      },
      {
        "date": "2024-09-05T16:44:00.000Z",
        "voteCount": 1,
        "content": "This URL was updated as it supports 400gbps. (it does not change the answer)."
      },
      {
        "date": "2024-09-05T16:44:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/directconnect/latest/UserGuide/MACsec.html"
      },
      {
        "date": "2024-06-03T22:33:00.000Z",
        "voteCount": 1,
        "content": "mentioned by oayoade."
      },
      {
        "date": "2024-04-24T00:13:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\nTo encrypt data over DX, you use MACsec for 10 Gbps and 100 Gbps links, and S2S VPN for slower links (e.g. 1 Gbps).\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html\nhttps://repost.aws/knowledge-center/create-vpn-direct-connect\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/adding-macsec-security-to-aws-direct-connect-connections/"
      },
      {
        "date": "2024-04-06T18:35:00.000Z",
        "voteCount": 1,
        "content": "vote for D too"
      },
      {
        "date": "2024-04-04T21:35:00.000Z",
        "voteCount": 1,
        "content": "D as mentioned by oayoade."
      },
      {
        "date": "2024-03-27T18:11:00.000Z",
        "voteCount": 1,
        "content": "same as oayosde mentioned,"
      },
      {
        "date": "2024-03-26T03:22:00.000Z",
        "voteCount": 2,
        "content": "as  oayoade says we need at least 10gbps to use MACsec, so option D"
      },
      {
        "date": "2024-03-24T13:42:00.000Z",
        "voteCount": 1,
        "content": "D as mentioned by oayoade."
      },
      {
        "date": "2024-03-21T09:46:00.000Z",
        "voteCount": 2,
        "content": "MACSec is the difference here for the additional security for Direct Connect."
      },
      {
        "date": "2024-03-20T18:11:00.000Z",
        "voteCount": 1,
        "content": "A because dynamic IP is more resilence than static IP"
      },
      {
        "date": "2024-03-20T12:48:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer.\nD uses static routing which is less suitable."
      },
      {
        "date": "2024-03-20T02:39:00.000Z",
        "voteCount": 2,
        "content": "With A the VPN is dependent on the DX connection, so not adding any resilience. VPN is encrypted by default, D."
      },
      {
        "date": "2024-03-18T23:41:00.000Z",
        "voteCount": 1,
        "content": "Solution: A\nIf we look at the request \"MOST cost-effectively\" we can eliminate the answer under B.\nIf we look at this part of the requirement \"the solution is highly available, fault tolerant\" we can eliminate C.\nIf we look at this part \"The company has configured BGP for the connection\" and \"the solution is ... secure\" we can eliminate D, because the current Direct Connect connection is not encrypted and answer under D does not offer a solution to encrypt the traffic.\nBase on this answer under A is right choice."
      },
      {
        "date": "2024-03-18T22:09:00.000Z",
        "voteCount": 3,
        "content": "Provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection. More cost effective than the static Site-to-Site VPN in Option D (which does not have the MACsec encryption for additional security)."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 452,
    "url": "https://www.examtopics.com/discussions/amazon/view/136587-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to modernize an application and migrate the application to AWS. The application stores user profile data as text in a single table in an on-premises MySQL database.<br><br>After the modernization, users will use the application to upload video files that are up to 4 GB in size. Other users must be able to download the video files from the application. The company needs a video storage solution that provides rapid scaling. The solution must not affect application performance.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora PostgreSQL by using AWS Database Migration Service (AWS DMS). Store the videos as base64-encoded strings in a TEXT column in the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Keyspaces (for Apache Cassandra) by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 object identifier in the corresponding Amazon Keyspaces entry.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as base64-encoded strings in the corresponding DynamoDB item."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T20:25:00.000Z",
        "voteCount": 1,
        "content": "No doubt"
      },
      {
        "date": "2024-03-24T13:46:00.000Z",
        "voteCount": 1,
        "content": "B\n4GB in file size would be S3\nAmazon Keyspaces (for Apache Cassandra) is not relevant at all"
      },
      {
        "date": "2024-03-20T12:49:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2024-03-19T04:02:00.000Z",
        "voteCount": 2,
        "content": "Storing the videos as objects in S3 is scalable and cost-effective for storing large files. DynamoDB can store video metadata (including the S3 key), allowing for efficient retrieval and management of the videos."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 453,
    "url": "https://www.examtopics.com/discussions/amazon/view/136589-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.<br><br>The company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.<br><br>A solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross-Region Replication for the file system."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-02T05:02:00.000Z",
        "voteCount": 1,
        "content": "https://community.aws/content/2iCkeS4XUmYdFf8Mlz6C7DFg5K3/protecting-amazon-s3-using-aws-backup"
      },
      {
        "date": "2024-07-11T22:46:00.000Z",
        "voteCount": 1,
        "content": "I checked the AWS Backup console and you cannot setup backup plan less than 1 hour, so 30 min backup(B) will be excluded."
      },
      {
        "date": "2024-04-24T02:30:00.000Z",
        "voteCount": 1,
        "content": "Answer A."
      },
      {
        "date": "2024-04-11T03:30:00.000Z",
        "voteCount": 4,
        "content": "C is not supported, see here: https://docs.aws.amazon.com/aws-backup/latest/devguide/backup-feature-availability.html#features-by-resource\nB is not possible (minimum is 1 hour, according to https://aws.amazon.com/blogs/storage/automating-backups-and-optimizing-backup-costs-for-amazon-efs-using-aws-backup/#:~:text=cron%20expression%20that%20creates%20backups%20as%20frequently%20as%20hourly).\nSo I vote for A"
      },
      {
        "date": "2024-03-24T22:03:00.000Z",
        "voteCount": 4,
        "content": "The default backup plan is once a day, which cannot meet the RPO, so C and D are out. \nWe need both EventBridge and Lambda functions to frequently backup the EFS, so B is out."
      },
      {
        "date": "2024-03-24T14:06:00.000Z",
        "voteCount": 2,
        "content": "B\nUsing the AWS Backup console, you can choose a frequency of every 12 hours, daily, weekly, or monthly. You can also create a cron expression that creates backups as frequently as hourly\nref:\nhttps://aws.amazon.com/blogs/storage/automating-backups-and-optimizing-backup-costs-for-amazon-efs-using-aws-backup/\n\nPITR is not supported for EFS mentioned by djangoUnchained, so C is out\nFrom AWS console, the most frequently backup is daily."
      },
      {
        "date": "2024-10-14T07:52:00.000Z",
        "voteCount": 1,
        "content": "A: I've tried it and it doesn't work, you get an error message \"\nError in some rules due to : The interval between backup jobs shouldn't be less than 60 minutes.\""
      },
      {
        "date": "2024-03-22T13:21:00.000Z",
        "voteCount": 1,
        "content": "Answer C."
      },
      {
        "date": "2024-03-20T13:11:00.000Z",
        "voteCount": 3,
        "content": "First of all, using the existing default backup plan means backups only once a day, which disqualifies both C and D. We are thus left with A and B, which both fulfil the RPO. B is slightly more wasteful in that 30-minute backups are overkill. Also, B requires a custom cron task to be set up using EventBridge as it is a non-standard one for AWS Backup.\n\nA, however, can be accomplished without extra operational overhead. Therefore, A."
      },
      {
        "date": "2024-03-19T04:09:00.000Z",
        "voteCount": 1,
        "content": "Creating a new IAM role and updating the KMS key policy to allow the role to use the key ensures that the backup mechanism has the necessary permissions for encryption. Enabling continuous backups for point-in-time recovery to increases the likelihood of being able to recover deleted documents within the specified RPO of 100 minutes."
      },
      {
        "date": "2024-03-20T02:50:00.000Z",
        "voteCount": 3,
        "content": "It seems PITR is not supported for EFS https://docs.aws.amazon.com/aws-backup/latest/devguide/point-in-time-recovery.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 454,
    "url": "https://www.examtopics.com/discussions/amazon/view/136593-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket. Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T14:20:00.000Z",
        "voteCount": 5,
        "content": "D\nSTS seems to be the answer\nhttps://advancedweb.hu/aws-how-to-secure-access-keys-with-mfa/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html"
      },
      {
        "date": "2024-03-24T22:14:00.000Z",
        "voteCount": 4,
        "content": "access keys with AWS CLI will just skip the MFA"
      },
      {
        "date": "2024-03-20T13:16:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer, as STS is required here."
      },
      {
        "date": "2024-03-19T04:19:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C are incorrect - Using IAM access keys with the AWS CLI would bypass the requirement for MFA.\n\nNot B - MFA should be required for specific actions, not just when assuming a role or group."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 455,
    "url": "https://www.examtopics.com/discussions/amazon/view/136565-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to migrate 60 on-premises legacy applications to AWS. The applications are based on the NET Framework and run on Windows.<br><br>The company needs a solution that minimizes migration time and requires no application code changes. The company also does not want to manage the infrastructure.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the containerized applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances. Use the EC2 instances to deploy and manage the applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Kubernetes Service (Amazon EKS) with the Fargate launch type to host the containerized applications."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T11:20:00.000Z",
        "voteCount": 1,
        "content": "Elastic Beanstalk abstracts the infrastructure, so the company won\u2019t need to manage EC2 instances, scaling, load balancing, or patching. Elastic Beanstalk takes care of these tasks automatically, which fits the requirement of not managing infrastructure. This is not the same thing as serverless (which is NOT a requirement), as Zas1 commented.\nThe answer can't be A because refactoring=code change, as asquared16 have already commented."
      },
      {
        "date": "2024-08-16T06:50:00.000Z",
        "voteCount": 1,
        "content": "Refactoring = Code Change"
      },
      {
        "date": "2024-04-24T02:55:00.000Z",
        "voteCount": 2,
        "content": "Getting started with Windows .NET on Elastic Beanstalk\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/dotnet-getstarted.html"
      },
      {
        "date": "2024-04-17T04:46:00.000Z",
        "voteCount": 1,
        "content": "No, AWS Elastic Beanstalk is not a serverless platform."
      },
      {
        "date": "2024-05-20T09:09:00.000Z",
        "voteCount": 2,
        "content": "B, Not word Serverless appears: \"The company also does not want to manage the infrastructure.\" \nhttps://aws.amazon.com/elasticbeanstalk\nQuickly launch web applications: Deploy scalable web applications in minutes without the complexity of provisioning and managing underlying infrastructure."
      },
      {
        "date": "2024-03-24T22:23:00.000Z",
        "voteCount": 2,
        "content": "This is a typical Beanstalk feature.&nbsp;\nRefactoring and containerizing applications often involve some level of code change."
      },
      {
        "date": "2024-03-24T14:27:00.000Z",
        "voteCount": 1,
        "content": "I vote for B\nwhen googling Windows Web Application Migration Assistant, all top 3 are using EB.\nhttps://github.com/awslabs/windows-web-app-migration-assistant\nCompare to EC2 in C, the question mentioned do not manage infrastructure\nSee below wording\nWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html"
      },
      {
        "date": "2024-03-24T14:29:00.000Z",
        "voteCount": 1,
        "content": "AC\nAWS Toolkit will change code in some way\nhttps://aws.amazon.com/visual-studio-net/"
      },
      {
        "date": "2024-03-23T02:55:00.000Z",
        "voteCount": 1,
        "content": "A\nNot B as company does not want to manage the infra."
      },
      {
        "date": "2024-03-20T13:21:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B, use Beanstalk. It's a classic use for Beanstalk: remember - no application changes is a requirement.\n\nA involves quite a bit of work and application changes. AWS Toolkit for .NET is a help, but there's operational overhead. Also, moving to ECS Fargate, serverless as it is, requires containerising the application, which also adds overhead."
      },
      {
        "date": "2024-03-19T04:27:00.000Z",
        "voteCount": 1,
        "content": "Refactoring the applications and containerizing them using AWS Toolkit for .NET Refactoring allows for easy migration without needing to modify application code. Using Amazon ECS with the Fargate launch type is optimized for running containers (when comparing to #D) and allows the provisioning and scaling of containers. #A provides a streamlined migration process with minimal management overhead."
      },
      {
        "date": "2024-03-18T23:56:00.000Z",
        "voteCount": 2,
        "content": "Solution: B\nIf you look at the request \"Company needs a solution that minimizes migration time and requires no changes to application code,\" you can eliminate the answer under A &amp; D (refactoring suggested).\nThe answers under B &amp; C are fine, but the \"minimize migration time\" part, the better solution is under B."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 456,
    "url": "https://www.examtopics.com/discussions/amazon/view/136597-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to run large batch-processing jobs on data that is stored in an Amazon S3 bucket. The jobs perform simulations. The results of the jobs are not time sensitive, and the process can withstand interruptions.<br><br>Each job must process 15-20 GB of data when the data is stored in the S3 bucket. The company will store the output from the jobs in a different Amazon S3 bucket for further analysis.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a serverless data pipeline. Use AWS Step Functions for orchestration. Use AWS Lambda functions with provisioned capacity to process the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Batch compute environment that includes Amazon EC2 On-Demand Instances and Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy for the Spot Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Elastic Kubernetes Service (Amazon EKS) to run the processing jobs. Use managed node groups that contain a combination of Amazon EC2 On-Demand Instances and Spot Instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T22:28:00.000Z",
        "voteCount": 5,
        "content": "\"large batch-processing jobs\" -&gt; Batch\n\"not time sensitive, and the process can withstand interruptions\" -&gt; Spot"
      },
      {
        "date": "2024-04-29T10:02:00.000Z",
        "voteCount": 1,
        "content": "Option B -  AWS Blog  \nhttps://aws.amazon.com/blogs/compute/cost-effective-batch-processing-with-amazon-ec2-spot/"
      },
      {
        "date": "2024-03-24T14:58:00.000Z",
        "voteCount": 1,
        "content": "B\nC is wrong due to the following\nAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nhttps://docs.aws.amazon.com/batch/latest/userguide/allocation-strategies.html"
      },
      {
        "date": "2024-03-20T13:24:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B."
      },
      {
        "date": "2024-03-19T04:31:00.000Z",
        "voteCount": 1,
        "content": "AWS Batch with Spot instances given not time sensitive"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 457,
    "url": "https://www.examtopics.com/discussions/amazon/view/136604-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.<br><br>The company has a Microsoft Hyper-V environment on premises and has compute capacity available. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.<br><br>The company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-29T10:16:00.000Z",
        "voteCount": 2,
        "content": "Option B:  AWS Blog - \nhttps://aws.amazon.com/blogs/storage/protect-your-file-and-backup-archives-using-aws-datasync-and-amazon-s3-glacier/\n\nHow do I use AWS DataSync to archive cold data?  -  https://aws.amazon.com/datasync/faqs/"
      },
      {
        "date": "2024-03-30T22:02:00.000Z",
        "voteCount": 1,
        "content": "Deploy the DataSync agent to the source."
      },
      {
        "date": "2024-03-20T13:30:00.000Z",
        "voteCount": 3,
        "content": "A is out because of Glacier Instant Retrieval (milliseconds)\nB is the correct answer: goes directly to Glacier Deep Archive\nC needlessly stores data in S3 Standard for a day\nD is an awkward use case."
      },
      {
        "date": "2024-03-19T04:46:00.000Z",
        "voteCount": 1,
        "content": "deploy the AWS DataSync in Hyper-V env, use more cost effice S3 Glacier Deep Archive"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 458,
    "url": "https://www.examtopics.com/discussions/amazon/view/136611-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to record key performance indicators (KPIs) from its application as part of a strategy to convert to a user-based licensing schema. The application is a multi-tier application with a web-based UI. The company saves all log files to Amazon CloudWatch by using the CloudWatch agent. All logins to the application are saved in a log file.<br><br>As part of the new license schema, the company needs to find out how many unique users each client has on a daily basis, weekly basis, and monthly basis.<br><br>Which solution will provide this information with the LEAST change to the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the application logic to make each successful login generate a call to the AWS SDK to increment a custom metric that records user name and client name dimensions in CloudWatch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CloudWatch agent to extract successful login metrics from the logs. Additionally, configure the CloudWatch agent to save the successful login metrics as a custom metric that uses the user name and client name as dimensions for the metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T09:14:00.000Z",
        "voteCount": 1,
        "content": "was at first for A but then for D.. ChatGPT is also for D: \n\nD: This option provides the most flexibility and capability for processing data. AWS Lambda can process the incoming log stream to apply more complex logic, such as checking for and ignoring duplicate entries within a set time frame (daily, weekly, monthly) before incrementing the metrics. This allows for the implementation of logic to ensure that users are only counted once per period, effectively tracking unique logins.\n\nConclusion:\nAmong the given options, Option D using an AWS Lambda function is best equipped to handle the requirement of counting unique user logins accurately over specified periods. Lambda functions offer the flexibility to implement any necessary logic to filter duplicates and manage counts over time, aligning with the need to track unique users on a daily, weekly, and monthly basis."
      },
      {
        "date": "2024-09-17T11:35:00.000Z",
        "voteCount": 2,
        "content": "A is not because Metric filters can't directly solve the problem of counting unique users across different time periods. They can count how many logins happened, but not how many distinct users logged in during those time periods."
      },
      {
        "date": "2024-04-08T18:45:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html"
      },
      {
        "date": "2024-03-30T22:13:00.000Z",
        "voteCount": 1,
        "content": "With existing logs, we don't have to make changes to the application."
      },
      {
        "date": "2024-03-24T16:05:00.000Z",
        "voteCount": 1,
        "content": "I would go for A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html"
      },
      {
        "date": "2024-03-22T13:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is C."
      },
      {
        "date": "2024-03-20T13:34:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer: it has the least changes to the application. C and D are rubbish."
      },
      {
        "date": "2024-03-19T05:02:00.000Z",
        "voteCount": 1,
        "content": "No app code change by configuring the agent to extract &amp; save successful login metrics as custom metrics with user name and client name dimensions.\n\n#A and #B requires app changes.\n#D needs additional lamba infra and increase complexity"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 459,
    "url": "https://www.examtopics.com/discussions/amazon/view/136613-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using GitHub Actions to run a CI/CD pipeline that accesses resources on AWS. The company has an IAM user that uses a secret key in the pipeline to authenticate to AWS. An existing IAM role with an attached policy grants the required permissions to deploy resources.<br><br>The company\u2019s security team implements a new requirement that pipelines can no longer use long-lived secret keys. A solutions architect must replace the secret key with a short-lived solution.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM SAML 2.0 identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Update GitHub to use SAML authentication for the pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Cognito identity pool. Configure the authentication provider to use GitHub. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub authentication provider. Configure the pipeline to use Cognito as its authentication provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trust anchor to AWS Private Certificate Authority. Generate a client certificate to use with AWS IAM Roles Anywhere. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Configure the pipeline to use the credential helper tool and to reference the client certificate public key to assume the new IAM role."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-20T13:38:00.000Z",
        "voteCount": 7,
        "content": "A is incorrect because GitHub doesn't support the aging SAML protocol.\nB is correct because GitHub does support OIDC.\nC is hysterically overengineered for this use case.\nD even more so."
      },
      {
        "date": "2024-03-23T22:17:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/devops/integrating-with-github-actions-ci-cd-pipeline-to-deploy-a-web-app-to-amazon-ec2/"
      },
      {
        "date": "2024-03-24T16:10:00.000Z",
        "voteCount": 3,
        "content": "B\nas in your KB link:\nThe GitHub Actions workflows must access resources in your AWS account. Here we are using IAM OpenID Connect identity provider and IAM role with IAM policies to access CodeDeploy and Amazon S3 bucket. OIDC lets your GitHub Actions workflows access resources in AWS without needing to store the AWS credentials as long-lived GitHub secrets"
      },
      {
        "date": "2024-03-30T22:18:00.000Z",
        "voteCount": 1,
        "content": "A and D are out because of sts:AssumeRole. \nB with the least operational overhead."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 460,
    "url": "https://www.examtopics.com/discussions/amazon/view/136615-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a web-crawling process on a list of target URLs to obtain training documents for machine learning training algorithms. A fleet of Amazon EC2 t2.micro instances pulls the target URLs from an Amazon Simple Queue Service (Amazon SQS) queue. The instances then write the result of the crawling algorithm as a .csv file to an Amazon Elastic File System (Amazon EFS) volume. The EFS volume is mounted on all instances of the fleet.<br><br>A separate system adds the URLs to the SQS queue at infrequent rates. The instances crawl each URL in 10 seconds or less.<br><br>Metrics indicate that some instances are idle when no URLs are in the SQS queue. A solutions architect needs to redesign the architecture to optimize costs.<br><br>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the web-crawling process to store results in Amazon Neptune.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the web-crawling process to store results in Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T16:16:00.000Z",
        "voteCount": 2,
        "content": "BE\nlamda + S3\nthe process don't need a database"
      },
      {
        "date": "2024-03-20T13:44:00.000Z",
        "voteCount": 3,
        "content": "A is utter rubbish - scaling out is not what we need\nB is optimal in terms of cost\nC and D involve fairly expensive databases not suitable for this use case. Moreover, Neptune must run in a VPC.\nE is optimal in terms of accessibility and cost"
      },
      {
        "date": "2024-03-19T05:19:00.000Z",
        "voteCount": 1,
        "content": "use lambda instead of a fleet of EC2, and store the results into cost-effective S3"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 461,
    "url": "https://www.examtopics.com/discussions/amazon/view/136621-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to migrate its website from an on-premises data center to AWS. The website consists of a load balancer, a content management system (CMS) that runs on a Linux operating system, and a MySQL database.<br><br>The CMS requires persistent NFS-compatible storage for a file system. The new solution on AWS must be able to scale from 2 Amazon EC2 instances to 30 EC2 instances in response to unpredictable traffic increases. The new solution also must require no changes to the website and must prevent data loss.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic File System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Deploy the CMS to AWS Elastic Beanstalk with a Network Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EBS volume to the EC2 instances. Create an Amazon RDS for MySQL database in the Elastic Beanstalk environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic File System (Amazon EFS) file system. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create a Network Load Balancer to distribute traffic. Create an Amazon Aurora MySQL database. Use an EC2 Auto Scaling scale-in lifecycle hook to mount the EFS file system to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create an Application Load Balancer to distribute traffic. Create an Amazon ElastiCache for Redis cluster to support the MySQL database. Use EC2 user data to attach the EBS volume to the EC2 instances."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-01T18:54:00.000Z",
        "voteCount": 4,
        "content": "C is wrong because lifehook cannot mount EFS"
      },
      {
        "date": "2024-03-30T22:33:00.000Z",
        "voteCount": 2,
        "content": "B and D are out because NFS-&gt;EFS\nC scale-in lifecycle hook to mount the EFS?????"
      },
      {
        "date": "2024-03-27T19:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-03-25T11:40:00.000Z",
        "voteCount": 3,
        "content": "A\nEBS is out first.\nFor C, the NLB is weired but couldn't say its wrong. The scale-in policy to mount EFS is wrong, since mounting task should happens during scale-out process."
      },
      {
        "date": "2024-03-24T04:27:00.000Z",
        "voteCount": 2,
        "content": "B and D are out because Amazon EBS is not NFS-compatible\nC is out because scale-in lifecycle hook triggers when the instance is about to terminate - no point of mounting the EFS file system here\n\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html"
      },
      {
        "date": "2024-03-20T17:26:00.000Z",
        "voteCount": 2,
        "content": "A because I think Network Load Balancer is not the answer for this case"
      },
      {
        "date": "2024-03-20T14:06:00.000Z",
        "voteCount": 1,
        "content": "B and D are out because of EBS Multi-Attach volumes not working across AZs and have a max number of 16 instances in one zone.\n\nA is the correct answer because of no code changes (yes!).\nC is not optimal because of the NLB which isn't optimal as it doesn't support HTTP/HTTPS as such, working on the TCP level and doesn't do path-based routing. Also, having to set up autoscaling explicitly adds overhead.\n\nTherefore, A."
      },
      {
        "date": "2024-03-19T06:12:00.000Z",
        "voteCount": 1,
        "content": "Change to #C since #A could involve website changes"
      },
      {
        "date": "2024-03-19T05:59:00.000Z",
        "voteCount": 1,
        "content": "EFS for persistent storage, Beanstalk for deploying with ALB and auto-scaling"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 462,
    "url": "https://www.examtopics.com/discussions/amazon/view/136624-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.<br><br>The company\u2019s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.<br><br>A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALIn the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-25T11:49:00.000Z",
        "voteCount": 5,
        "content": "C\nA is out since periodic lambda will have data loss\nB is out since ALB is regional service. Can't add EC2 to ALB if in different region\nD is out since hourly backup will have data loss"
      },
      {
        "date": "2024-10-08T00:19:00.000Z",
        "voteCount": 1,
        "content": "I\u00b4ll go with B, because\n1. C did not promote the Read Replica into a Standalone instance.\n2. C did not redirect S3 traffic to the separate region, if the main region fails.\nCost is not in focus, so we can preinstall the needed EC2 instances."
      },
      {
        "date": "2024-10-08T00:19:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/de/blogs/aws/amazon-rds-for-mysql-promote-read-replica/"
      },
      {
        "date": "2024-03-24T04:45:00.000Z",
        "voteCount": 1,
        "content": "A is out because relational database is suited here\nD is out because ElastiCache is not required and hourly snapshots of the RDS DB instance would not minimise data loss\nB is out because as per the requirements (no RTO is mentioned), there is no need to launch EC2 instances in DR site and keep them idle"
      },
      {
        "date": "2024-03-20T14:11:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2024-03-19T23:14:00.000Z",
        "voteCount": 2,
        "content": "This solution involves creating a Read Replica of the RDS DB instance in another region and directing the finance team to execute queries on it, minimizing application performance impact. AMIs of EC2 instances are created and copied for rapid deployment. S3 Cross-Region Replication ensures data safety. In a disaster, the Read Replica becomes a standalone DB, and EC2 instances from AMIs with a new ALB serve the application, all reconfigured to the new S3 bucket. This approach addresses disaster recovery, minimizes data loss, and mitigates query-induced performance issues with minimal application changes."
      },
      {
        "date": "2024-03-19T06:07:00.000Z",
        "voteCount": 1,
        "content": "Read recplica for reporting, CRR to replicate S3 in another region, launch EC2 from AMI and ALB and promote the read replicate in the separate region furing DR"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 463,
    "url": "https://www.examtopics.com/discussions/amazon/view/136627-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has many services running in its on-premises data center. The data center is connected to AWS using AWS Direct Connect (DX) and an IPSec VPN. The service data is sensitive and connectivity cannot traverse the internet. The company wants to expand into a new market segment and begin offering its services to other companies that are using AWS.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC Endpoint Service that accepts HTTP or HTTPS traffic, host it behind an Application Load Balancer, and make the service available over DX.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an internet gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a NAT gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-26T11:49:00.000Z",
        "voteCount": 1,
        "content": "A is the correct option. There is no direct support for ALB with Private Link / VPC Endpoint service. ALB can be a target group for NLB so, we can use ALB with NLB but not ALB directly. Check this page for more details  - https://aws.amazon.com/about-aws/whats-new/2021/09/application-load-balancer-aws-privatelink-static-ip-addresses-network-load-balancer/"
      },
      {
        "date": "2024-08-21T05:07:00.000Z",
        "voteCount": 2,
        "content": "What do we know that makes B not a valid answer? It feels like the question is missing something."
      },
      {
        "date": "2024-09-14T22:03:00.000Z",
        "voteCount": 1,
        "content": "VPC Endpoint doesn't directly work with ALB, so B is wrong"
      },
      {
        "date": "2024-07-12T01:23:00.000Z",
        "voteCount": 2,
        "content": "A, for sure.\nConnectivity cannot traverse the internet"
      },
      {
        "date": "2024-06-03T23:15:00.000Z",
        "voteCount": 2,
        "content": "A, VPC endpoint used with NLB"
      },
      {
        "date": "2024-03-30T22:42:00.000Z",
        "voteCount": 2,
        "content": "VPC endpoint + NLB = PrivateLink"
      },
      {
        "date": "2024-03-27T19:03:00.000Z",
        "voteCount": 1,
        "content": "A, VPC endpoint used with NLB"
      },
      {
        "date": "2024-03-25T12:07:00.000Z",
        "voteCount": 4,
        "content": "A\nThis is a privatelink scenrio. Can't find a hard evidence but the Privatelink seem can only work with NLB. If need ALB, it will be Privatelink -&gt; NLB -&gt; ALB\none evidence is the link lasithasilva709 posted\nanother evidence is compare of ALB/NLB\nhttps://aws.amazon.com/elasticloadbalancing/features/?nc=sn&amp;loc=2&amp;dn=1\n3rd evidence\nhttps://aws.amazon.com/about-aws/whats-new/2021/09/application-load-balancer-aws-privatelink-static-ip-addresses-network-load-balancer/"
      },
      {
        "date": "2024-04-07T00:21:00.000Z",
        "voteCount": 2,
        "content": "Also in question only mentioned services but doesn't mention port, where TCP (NLB) can cover all ports but HTTP/HTTPS (ALB) is restricted"
      },
      {
        "date": "2024-03-24T05:00:00.000Z",
        "voteCount": 1,
        "content": "My understanding is that NLB should be used for a VPC endpoint service.\n\nHere are some resources:\n1. To use AWS PrivateLink, create a Network Load Balancer for your application in your VPC, and create a VPC endpoint service configuration pointing to that load balancer. \nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/aws-privatelink.html\n2. https://aws.amazon.com/blogs/networking-and-content-delivery/application-load-balancer-type-target-group-for-network-load-balancer/"
      },
      {
        "date": "2024-03-22T16:36:00.000Z",
        "voteCount": 2,
        "content": "Answer is A.\nMany services is a key word , option B is for http and https."
      },
      {
        "date": "2024-03-20T14:19:00.000Z",
        "voteCount": 2,
        "content": "B is just a safe as A \u2014 TCP is not inherently safer. However, HTTPS and HTTP are much more commonly used when providing services to other companies. As we don't have any information as to the nature of the service, a safer bet (pun intended) is B."
      },
      {
        "date": "2024-03-19T06:22:00.000Z",
        "voteCount": 2,
        "content": "#C &amp; #D are out given the connectivity cannot traverse the internet. #A enables secure VPC endpoint to privately expose to other companies' VPCs without traversing the internet, and TCP to provide more controlled and secure comm protocol for sensitive data"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 464,
    "url": "https://www.examtopics.com/discussions/amazon/view/136629-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. A solutions architect must design a solution in which only administrator roles are allowed to use IAM actions. However, the solutions architect does not have access to all the AWS accounts throughout the company.<br><br>Which solution meets these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that applies to all the AWS accounts to allow IAM actions only for administrator roles. Apply the SCP to the root OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CloudTrail to invoke an AWS Lambda function for each event that is related to IAM actions. Configure the function to deny the action if the user who invoked the action is not an administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet an IAM permissions boundary that allows IAM actions. Attach the permissions boundary to every administrator role across all the AWS accounts."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T01:57:00.000Z",
        "voteCount": 2,
        "content": "If an SCP allows certain IAM actions specifically for administrator roles or groups, it implicitly denies those actions for all other roles and users in the accounts where the SCP is applied.\n\nYou do not need to explicitly deny the actions for non-administrator roles and users. The implicit deny happens automatically because SCPs define the maximum permissible permissions.\n\nSo it is A. With C at every new role you have to define it. And Administrators by default have in their IAM permission the capacity to do the modifications."
      },
      {
        "date": "2024-03-25T12:25:00.000Z",
        "voteCount": 3,
        "content": "C\nusing SCP deny"
      },
      {
        "date": "2024-03-20T14:23:00.000Z",
        "voteCount": 4,
        "content": "A: SCPs don't allow, they deny\nB: is reactive, not preventive\nC: is correct\nD: Boundary Permissions don't allow, they set maximum permissions."
      },
      {
        "date": "2024-03-19T06:27:00.000Z",
        "voteCount": 3,
        "content": "Applying SCP to the root OU with specified deny rule"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 465,
    "url": "https://www.examtopics.com/discussions/amazon/view/136641-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company hosts some applications in a VPC in the company's shared services account.<br><br>The company has attached a transit gateway to the VPC in the shared services account.<br><br>The company is developing a new capability and has created a development environment that requires access to the applications that are in the shared services account. The company intends to delete and recreate resources frequently in the development account. The company also wants to give a development team the ability to recreate the team's connection to the shared services account as required.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transit gateway in the development account. Create a transit gateway peering request to the shared services account. Configure the shared services transit gateway to automatically accept peering connections.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on automatic acceptance for the transit gateway in the shared services account. Create a VPC endpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account. Configure the endpoint service to automatically accept connection requests. Provide the endpoint details to the development team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gateway attachment when the development account makes an attachment request. Use AWS Network Manager to share the transit gateway in the shared services account with the development account. Accept the transit gateway in the development account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-19T05:31:00.000Z",
        "voteCount": 1,
        "content": "The dev account has frequent changes and needs to connect with the ShareServices account hence connection request is from Dev -&gt; SS"
      },
      {
        "date": "2024-06-12T18:58:00.000Z",
        "voteCount": 3,
        "content": "A is incorrect: creating and managing another transit gateway in the development account and setting up peering. This adds unnecessary complexity and management overhead.\n\nB is correct: the development account can create transit gateway attachments without needing manual intervention every time an attachment is made. \n\nC is incorrect: Not usecase of VPC endpoints. VPC endpoints are typically used for connecting to AWS services privately without traversing the public internet. This option does not align well with the requirement to access applications in a VPC through a transit gateway.\n\nD is incorrect:  too complicated"
      },
      {
        "date": "2024-05-18T12:30:00.000Z",
        "voteCount": 2,
        "content": "\"B\" is correct.\n\"C\" is wrong: Endpoint services require either a Network Load Balancer or a Gateway Load Balancer., However, the answer does not mention the creation of a NLB.\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/create-endpoint-service.html"
      },
      {
        "date": "2024-03-25T12:35:00.000Z",
        "voteCount": 3,
        "content": "B\nAuto accept shared attachments\nhttps://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html\nThen, create create TGW attachment in dev account"
      },
      {
        "date": "2024-03-20T14:34:00.000Z",
        "voteCount": 3,
        "content": "B is correct.\nA is wrong becase TGW peering is done between regions, not accounts.\nC is rubbish\nD is overengineered and weird, using Network Manager for sharing the TGW rather than RAM which is best practice."
      },
      {
        "date": "2024-08-19T05:32:00.000Z",
        "voteCount": 1,
        "content": "Intra region peering is allowed, A is also valid"
      },
      {
        "date": "2024-03-19T07:06:00.000Z",
        "voteCount": 2,
        "content": "Provide the flexibility needed for the development team to recreate their connection to the shared services account"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 466,
    "url": "https://www.examtopics.com/discussions/amazon/view/136643-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS. The company has successfully tested a few sample workloads on AWS. The company also has created an AWS Site-to-Site VPN connection to a VPC. A solutions architect needs to generate a total cost of ownership (TCO) report for the migration of all the workloads from the data center.<br><br>Simple Network Management Protocol (SNMP) has been enabled on each VM in the data center. The company cannot add more VMs in the data center and cannot install additional software on the VMs. The discovery data must be automatically imported into AWS Migration Hub.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Application Migration Service agentless service and the AWS Migration Hub Strategy Recommendations to generate the TCO report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Hub to generate the TCO report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Migration Readiness Assessment tool inside the VPC. Configure Migration Evaluator to generate the TCO report."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-08T22:53:00.000Z",
        "voteCount": 3,
        "content": "agentless collector to scan the on-premise VMs using SNMP:\nhttps://d1.awsstatic.com/migration-evaluator-resources/agentless_collector_overview.pdf\n\nHere lists Migration Evaluator as one of the tools for tco report:\nhttps://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/aws-pricingtco-tools.html"
      },
      {
        "date": "2024-03-25T12:56:00.000Z",
        "voteCount": 1,
        "content": "B\nMigration Evaluator will generate a report. The finacial forecast (TCO) is included. Example of report can be found here\nhttps://aws.amazon.com/migration-evaluator/resources/"
      },
      {
        "date": "2024-03-22T16:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is C.\nhttps://aws.amazon.com/migration-hub/faqs/\nMigration Hub is the AWS service that analyzes collected data and produces the TCO report."
      },
      {
        "date": "2024-03-20T14:40:00.000Z",
        "voteCount": 1,
        "content": "A doesn't do TCO reports\nB is correct, uses SNMP and generates the report\nC doesn't do TCO reports\nD doesn't do TCO reports that way"
      },
      {
        "date": "2024-03-19T07:17:00.000Z",
        "voteCount": 1,
        "content": "agentless collector to scan the on-premise VMs using SNMP to gather the data and generate the TCO report"
      },
      {
        "date": "2024-03-19T07:16:00.000Z",
        "voteCount": 2,
        "content": "agentless collector to scan the on-premise VMs using SNMP to gather the data and generate the TCO report"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 467,
    "url": "https://www.examtopics.com/discussions/amazon/view/136646-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company that is developing a mobile game is making game assets available in two AWS Regions. Game assets are served from a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in each Region. The company requires game assets to be fetched from the closest Region. If game assets become unavailable in the closest Region, they should be fetched from the other Region.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution. Create an origin group with one origin for each ALB. Set one of the origins as primary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Route 53 health check for each ALCreate a Route 53 failover routing record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Amazon CloudFront distributions, each with one ALB as the origin. Create an Amazon Route 53 failover routing record pointing to the two CloudFront distributions. Set the Evaluate Target Health value to Yes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T07:01:00.000Z",
        "voteCount": 8,
        "content": "A - Need to set cache behaviour for another origin\nB - Failover routing record cannot point to 2 ALBs\nC - Works but does not meet the requirement. By default, when there is no unhealthy distribution, the traffic will always be sent to the primary but not the closest region.\nD - Sending the traffic to the closest region unless the closest region becomes unhealthy"
      },
      {
        "date": "2024-03-25T05:54:00.000Z",
        "voteCount": 5,
        "content": "It is either A or D but both are not perfect. In A Cloudfront will always fetch from the primary region not the closest region. In D latency based routing might not choose the closest region but the one with best latency. For the following reasons I go with A:\n\nA is correct: the user will always use the nearest edge location in the closest region to fetch the game assets. Cloudfront will either respond from the cache or load the game assets from the primary origin (or in case the primary origin is not available from the fail over origin).\n\nB is incorrect because the game assets would always be fetched from the primary region.\n\nC is wrong because this setup is essentially the same as A but much more complicated.\n\nD is wrong because latency based routing does not necessarily choose the nearest region but the region with the best latency (geolocation routing policy would be the correct to fulfill the requirement)"
      },
      {
        "date": "2024-10-15T00:56:00.000Z",
        "voteCount": 1,
        "content": "A - This option leverages Amazon CloudFront, which is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. By setting up an origin group with the ALBs as origins and designating a primary origin, CloudFront automatically routes traffic to the second origin if the primary is unavailable. This setup addresses both the latency (by serving content from the nearest location due to CloudFront's global presence) and failover requirements effectively.\n\nB - does not serve geographic location\nC - works to but A is less complex\nD - It lacks a CDN which if preffered for this kind of solution"
      },
      {
        "date": "2024-10-12T13:17:00.000Z",
        "voteCount": 1,
        "content": "Option D is the best solution because it uses Route 53 latency alias records to ensure that users access the closest AWS Region and provides failover capability with health checks, fulfilling both the proximity and reliability requirements.\nFor option A to be correct, it should be described as CloudFront with Origin Groups. This allows you to set up a primary and a secondary origin. If the primary origin (the ALB in the primary Region) becomes unavailable, CloudFront automatically switches to the secondary origin (the ALB in the secondary Region).\nThis provides the failover functionality, which means if the assets are unavailable in one Region, they can be fetched from the other Region."
      },
      {
        "date": "2024-09-25T07:36:00.000Z",
        "voteCount": 1,
        "content": "D - closest region indicate DNS based on latency. Not a failover scenario."
      },
      {
        "date": "2024-08-14T06:28:00.000Z",
        "voteCount": 1,
        "content": "There are many factors in this question. But this \"The company requires game assets to be fetched from the closest Region\" points to 'D' latency based routing R53."
      },
      {
        "date": "2024-07-20T04:43:00.000Z",
        "voteCount": 2,
        "content": "Option D incorrect because there is no automatic failover like in cloudFront option A"
      },
      {
        "date": "2024-06-07T06:43:00.000Z",
        "voteCount": 3,
        "content": "We are talking about \"GAME ASSETS\" that by definitions are STATIC CONTENT. So for this reason we should keep A or C (CDN). The correct is A."
      },
      {
        "date": "2024-04-25T03:31:00.000Z",
        "voteCount": 1,
        "content": "Answer: D."
      },
      {
        "date": "2024-04-01T04:25:00.000Z",
        "voteCount": 4,
        "content": "It is A or D.\nNot A because the request will be always routed to the primary origin, the requirement wants it to be routed to the closest region."
      },
      {
        "date": "2024-03-25T13:08:00.000Z",
        "voteCount": 1,
        "content": "I vote for D\nreason same as Dgix mentioned in the correction, since question request game asset fetched from closed region"
      },
      {
        "date": "2024-03-24T06:28:00.000Z",
        "voteCount": 3,
        "content": "In option D, game will always be fetched from on region except the if the primary region fails.  Option A allows for multiple origin, and caching."
      },
      {
        "date": "2024-03-24T06:31:00.000Z",
        "voteCount": 1,
        "content": "Sorry I can't correct my misspelled"
      },
      {
        "date": "2024-03-22T17:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nQuestion ask in case of failover , not to monitor latency."
      },
      {
        "date": "2024-03-20T15:17:00.000Z",
        "voteCount": 3,
        "content": "Moderator, please change my previous answer to D."
      },
      {
        "date": "2024-03-20T14:58:00.000Z",
        "voteCount": 3,
        "content": "A, as CloudFront does both geographical proximity and origin failover and serves the assets from the closest region to the client. B doesn't do geo proximity. C is overengineered and does the same as A. D is viable, but uses DNS rather than CDN.\n\nSo it's either A or D. Of the two, A is simpler."
      },
      {
        "date": "2024-03-20T15:17:00.000Z",
        "voteCount": 4,
        "content": "However... in A all downloads will be from one of the origins to the CloudFront edge locations. In D, they will come from the region with the lowest latency unless one region's health checks fail. With A, cache time becomes somewhat important. If it is short, one region - the primary - will be accessed quite often and the secondary never, unless the primary fails. That's not geographical proximity at all. \n\nWith D, which doesn't use caching to \"gloss over\" the fact that only one region is used, downloads always come from the lowest latency region.\n\nBoth A and D are valid, depending on how you look at it. But I will change my mind to D, as CloudFront really just obscures the main point, that of from what regions asset downloads _originate_, quite apart from caching."
      },
      {
        "date": "2024-03-19T07:24:00.000Z",
        "voteCount": 2,
        "content": "CF for closest region, Route 53 failover to route and failover when one of the CFs is unavailable."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 468,
    "url": "https://www.examtopics.com/discussions/amazon/view/136647-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.<br><br>A security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.<br><br>Which solution will meet these requirements with the LARGEST performance improvement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Athena workgroup without data usage control limits. Use Athena engine version 2."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T04:45:00.000Z",
        "voteCount": 3,
        "content": "Option C :  https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-vpc-flow-logs-parquet-hive-prefixes-partitioned-files/\n\nNew features to make it faster, easier and more cost efficient to store and run analytics on your Amazon VPC Flow Logs"
      },
      {
        "date": "2024-03-25T13:12:00.000Z",
        "voteCount": 3,
        "content": "C\nUsing AWS Athena with parquet files is faster and cheaper than using other formats like CSV and JSON based file structures, according to AWS Athena pricing \"compressing your data allows Athena to scan less data, and converting your data to columnar formats allows Athena to selectively read only required columns to process the data, which leads to cost savings and improved performance\nhttps://www.linkedin.com/pulse/aws-athena-parquet-vs-csv-ahmed-fayed/"
      },
      {
        "date": "2024-03-24T05:28:00.000Z",
        "voteCount": 2,
        "content": "Apache Parquet is compressed, efficient columnar data representation\nhttps://parquet.apache.org/docs/overview/motivation/"
      },
      {
        "date": "2024-03-20T15:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2024-03-19T07:31:00.000Z",
        "voteCount": 1,
        "content": "Apache Parquet format to enable a highly optimized columnar storage format and partitioning by hour for improving the Athena query performance"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 469,
    "url": "https://www.examtopics.com/discussions/amazon/view/136648-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to establish a dedicated connection between its on-premises infrastructure and AWS. The company is setting up a 1 Gbps AWS Direct Connect connection to its account VPC. The architecture includes a transit gateway and a Direct Connect gateway to connect multiple VPCs and the on-premises infrastructure.<br><br>The company must connect to VPC resources over a transit VIF by using the Direct Connect connection.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the 1 Gbps Direct Connect connection to 10 Gbps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdvertise the on-premises network prefixes over the transit VIF.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdvertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Direct Connect connection's MACsec encryption mode attribute to must_encrypt.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair with the Direct Connect connection."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T22:34:00.000Z",
        "voteCount": 1,
        "content": "BC\nMACsec is ruled out as this needs 10 gbps https://aws.amazon.com/about-aws/whats-new/2021/03/aws-direct-connect-announces-macsec-encryption-for-dedicated-10gbps-and-100gbps-connections-at-select-locations/"
      },
      {
        "date": "2024-03-25T13:23:00.000Z",
        "voteCount": 3,
        "content": "BC\njust need to add routing at both sides.\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-gateways.html\nhttps://docs.aws.amazon.com/vpc/latest/tgw/tgw-prefix-lists.html\nADE seems not relevant"
      },
      {
        "date": "2024-03-20T17:59:00.000Z",
        "voteCount": 3,
        "content": "BC because we need to ardvertise the VPC prefixes and on-premise prefixes so the on-premise and VPC can connected"
      },
      {
        "date": "2024-03-20T15:28:00.000Z",
        "voteCount": 2,
        "content": "B and C are correct. \nA is not required, D needlessly involves encryption, and E doesn't create connectivity."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 470,
    "url": "https://www.examtopics.com/discussions/amazon/view/136650-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to use Amazon WorkSpaces in combination with thin client devices to replace aging desktops. Employees use the desktops to access applications that work with Clinical trial data. Corporate security policy states that access to the applications must be restricted to only company branch office locations. The company is considering adding an additional branch office in the next 6 months.<br><br>Which solution meets these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T12:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct. B is incorrect because WAF web ACLs don't work with Amazon Workspaces. A web access control list (web ACL) gives you fine-grained control over all of the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, AWS AppSync, Amazon Cognito, AWS App Runner, and AWS Verified Access resources.\nhttps://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html"
      },
      {
        "date": "2024-06-12T19:11:00.000Z",
        "voteCount": 2,
        "content": "This is not usecase of AWS Firewall Manager and web ACL, and A work"
      },
      {
        "date": "2024-05-30T09:20:00.000Z",
        "voteCount": 1,
        "content": "B. AWS Firewall Manager and web ACL: While this could work, it is generally used for managing rules across multiple AWS accounts and resources, which might be an overcomplication for this specific use case. It is more complex to set up and manage compared to IP access control groups."
      },
      {
        "date": "2024-05-18T01:14:00.000Z",
        "voteCount": 1,
        "content": "From an operational simplicity point of view (which is what is required) it is clearly B. \nIt is much easier to manage IPs with Firewall manager than in a custom way, which by the way remains vague. For me, the correct answer is B."
      },
      {
        "date": "2024-08-14T06:35:00.000Z",
        "voteCount": 1,
        "content": "A would be done once for the entire WorkSpaces directory so it would be easy to manage and done centrally. https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html#associate-ip-access-control-group"
      },
      {
        "date": "2024-04-25T10:04:00.000Z",
        "voteCount": 2,
        "content": "Answer: A\nFrom the AWS Console: \"Create an IP access control group that you can add to a WorkSpaces Directory. Users will only be able to access WorkSpaces from these IP addresses.\""
      },
      {
        "date": "2024-04-17T22:48:00.000Z",
        "voteCount": 3,
        "content": "A\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html"
      },
      {
        "date": "2024-04-11T18:45:00.000Z",
        "voteCount": 1,
        "content": "Using AWS Firewall Manager to create a web ACL rule with an IPSet containing the list of public addresses from the branch office locations and associating it with the WorkSpaces directory is the most operationally efficient solution. AWS Firewall Manager allows you to centrally manage and apply web access control lists (web ACLs) across multiple AWS resources, including WorkSpaces. This approach ensures that the access control policy is consistently applied across the WorkSpaces environment, and it can be easily updated as the company adds a new branch office location in the next 6 months."
      },
      {
        "date": "2024-04-07T14:40:00.000Z",
        "voteCount": 1,
        "content": "According to ChatGPT:\n\n\"Among these options, option B, using AWS Firewall Manager to create a web ACL rule with an IPSet, offers the most operational efficiency. It allows for centralized management of access control rules across multiple WorkSpaces and easily scales to accommodate future changes, such as adding a new branch office. Additionally, it aligns with the company's security policy by restricting access based on IP addresses. Therefore, option B is the best choice.\""
      },
      {
        "date": "2024-03-25T13:27:00.000Z",
        "voteCount": 4,
        "content": "A\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html"
      },
      {
        "date": "2024-03-22T17:31:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A."
      },
      {
        "date": "2024-03-20T18:04:00.000Z",
        "voteCount": 1,
        "content": "correct answer A , need to add ip public for the branch offices to restrict access from branch offices only"
      },
      {
        "date": "2024-03-20T15:32:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer. It is the most operationally efficient as it uses IP access control groups."
      },
      {
        "date": "2024-03-20T13:21:00.000Z",
        "voteCount": 2,
        "content": "Trust me"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 471,
    "url": "https://www.examtopics.com/discussions/amazon/view/136653-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company uses AWS Organizations. The company runs two firewall appliances in a centralized networking account. Each firewall appliance runs on a manually configured highly available Amazon EC2 instance. A transit gateway connects the VPC from the centralized networking account to VPCs of member accounts. Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet.<br><br>During a recent incident, a badly configured script initiated the termination of both firewall appliances. During the rebuild of the firewall appliances, the company wrote a new script to configure the firewall appliances at startup.<br><br>The company wants to modernize the deployment of the firewall appliances. The firewall appliances need the ability to scale horizontally to handle increased traffic when the network expands. The company must continue to use the firewall appliances to comply with company policy. The provider of the firewall appliances has confirmed that the latest version of the firewall code will work with all AWS services.<br><br>Which combination of steps should the solutions architect recommend to meet these requirements MOST cost-effectively? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate VPC endpoints in each member account. Update the route tables to point to the VPC endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-30T19:40:00.000Z",
        "voteCount": 11,
        "content": "Refer this https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/\n\nThe endpoint is created in the centralized account only."
      },
      {
        "date": "2024-04-25T14:22:00.000Z",
        "voteCount": 4,
        "content": "No doubt that \u201cA\u201d and \u201cC\u201d are correct.\n\nE \u2013 it\u2019s a valid config, but it\u2019s against any logic \u2013 having a TGW and at the same time paying for GWLBEs in each member account\u2019s VPC.\nF \u2013 The answer says \u201cUpdate the route tables in each member account to point to the VPC endpoints.\u201d \u2013 this is NOT possible. The route tables of the member/spoke accounts point to the TGW\u2019s ENI (for 0.0.0.0/0) in their own VPC; they cannot point to the (GWLB) VPC endpoints in another VPC.\nCheck the route table of Spoke1 VPC in below diagram \u2013 Destination: 0.0.0.0/0, Target: tgw-id (NOT vpce-az-a-id):\nhttps://d2908q01vomqb2.cloudfront.net/5b384ce32d8cdef02bc3a139d4cac0a22bb029e8/2022/04/14/GWLB_TGW_FIGURE2.jpg\n-\nP.S. Who wrote this question is an incompetent."
      },
      {
        "date": "2024-09-11T23:14:00.000Z",
        "voteCount": 1,
        "content": "A has VPC Endpoint Service in central VPC\nThen we should have VPC endpoints in member accounts"
      },
      {
        "date": "2024-08-07T15:31:00.000Z",
        "voteCount": 1,
        "content": "Main discussion about E and F"
      },
      {
        "date": "2024-07-21T12:29:00.000Z",
        "voteCount": 2,
        "content": "Main discussion about E and F\nit combine Member VPC, Centralize networking, Endpoint Service, VPC Endpoint\nAccoring to statement and answer A and C, that mean\nTransit-GW is in memeber VPC\nFirewall in Centralize VPC which alread has Endpoint Service in PrivateLink, So, MUST have VPC Endpoint in Memeber account, not Centralized\nAnother important is 'Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet ', which prevent use one IP from transit-GW as endpoint."
      },
      {
        "date": "2024-07-07T00:00:00.000Z",
        "voteCount": 1,
        "content": "Main discussion about E and F\nit combine Member VPC, Centralize networking, Endpoint Service, VPC Endpoint\nAccoring to statement and answer A and C, that mean\nTransit-GW is in memeber VPC\nFirewall in Centralize VPC which alread has Endpoint Service in PrivateLink, So, MUST have VPC Endpoint in Memeber account, not Centralized \nAnother important is 'Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet ', which prevent use one IP from transit-GW as endpoint."
      },
      {
        "date": "2024-07-02T13:45:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html"
      },
      {
        "date": "2024-06-03T23:45:00.000Z",
        "voteCount": 3,
        "content": "Having multiple VPC endpoints will make connection unscalable"
      },
      {
        "date": "2024-05-20T10:16:00.000Z",
        "voteCount": 2,
        "content": "F discard because update route. Explain \"titi_r\""
      },
      {
        "date": "2024-05-12T12:36:00.000Z",
        "voteCount": 4,
        "content": "A - Gateway Load Balancer is LB type used to redirect traffic to traffic inspection devices like firewalls, this is done via GENEVE network protocol. (correct)\nB - NLB could not be used, NLB does not support GENEVE protocol. (incorrect)\nC - ASG is the way to go for this scenario, in addition could be add Autoscaling policies to add more instances during traffic spikes and reduce when no traffic spikes (correct)\nD - Launch wizard work directly with resource EC2 and EBS, I didn't see any integration with ASG (incorrect)\nE - Works but it's not cost effective, VPCE have a price of 0.01$/hour/az each, so if you have GWLB in multi-az you would pay (1VPCE * number of AZs * number of member account) (incorrect - not cost effective)\nF - Since transit gateway is used, all traffic could be routed to the centralized networking account, and in there 0.0.0.0/0 traffic would go to the GWLB endpoints, so instead of multiple vpc endpoints you would only have 1VPCE * number of AZs (correct)"
      },
      {
        "date": "2024-05-09T19:02:00.000Z",
        "voteCount": 2,
        "content": "gateway loadbalancer endpoint needs to be in the spoke VPC. https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/"
      },
      {
        "date": "2024-05-06T20:54:00.000Z",
        "voteCount": 4,
        "content": "VPC endpoint service in central account\nVPC endpoint in memeber account\nF is wrong"
      },
      {
        "date": "2024-04-23T12:11:00.000Z",
        "voteCount": 1,
        "content": "More logical"
      },
      {
        "date": "2024-04-07T14:46:00.000Z",
        "voteCount": 2,
        "content": "Why B might also be considered:\n\nB. Deploy a Network Load Balancer in the centralized networking account: This would distribute incoming traffic across multiple instances of the firewall appliances deployed in the centralized networking account, providing scalability and high availability. Using AWS PrivateLink for endpoint services ensures that communication between member accounts and the centralized networking account remains within the AWS network, enhancing security and performance. However, this option may not be as cost-effective as option C alone because it involves additional costs associated with deploying and managing a Network Load Balancer. But it could be considered if high availability and scalability are prioritized over cost-effectiveness."
      },
      {
        "date": "2024-03-28T08:51:00.000Z",
        "voteCount": 2,
        "content": "ACE\nE pairs up with end point service in A."
      },
      {
        "date": "2024-03-25T14:05:00.000Z",
        "voteCount": 3,
        "content": "ACE\nVPC endpoint service in central account\nVPC endpoint in memeber account"
      },
      {
        "date": "2024-03-22T17:40:00.000Z",
        "voteCount": 3,
        "content": "I am thinking between E and F , E is not cost efficient but F is."
      },
      {
        "date": "2024-03-22T05:42:00.000Z",
        "voteCount": 4,
        "content": "Why would you create the VPC endpoint in the centralized account? The goal is to connect the member accounts to the centralized accounts. F is wrong.\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/getting-started.html"
      },
      {
        "date": "2024-04-08T06:56:00.000Z",
        "voteCount": 2,
        "content": "All the accounts are already connected and forwarding traffic to the centralized account. In that case you only need to create a endpoint in the central VPC"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 472,
    "url": "https://www.examtopics.com/discussions/amazon/view/136654-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect must implement a multi-Region architecture for an Amazon RDS for PostgreSQL database that supports a web application. The database launches from an AWS CloudFormation template that includes AWS services and features that are present in both the primary and secondary Regions.<br><br>The database is configured for automated backups, and it has an RTO of 15 minutes and an RPO of 2 hours. The web application is configured to use an Amazon Route 53 record to route traffic to the database.<br><br>Which combination of steps will result in a highly available architecture that meets all the requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the primary Region, create a health check on the database that will invoke an AWS Lambda function when a failure is detected. Program the Lambda function to recreate the database from the latest database snapshot in the secondary Region and update the Route 53 host records for the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to copy the latest automated backup to the secondary Region every 2 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a hot standby database in the secondary Region. Use an AWS Lambda function to restore the secondary database to the latest RDS automatic backup in the event that the primary database fails."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-03T23:48:00.000Z",
        "voteCount": 2,
        "content": "Classic question"
      },
      {
        "date": "2024-03-24T18:44:00.000Z",
        "voteCount": 1,
        "content": "Option C involves copying the latest automated backup to the secondary Region every 2 hours, which does not provide a standby database instance and may not meet the RTO requirement."
      },
      {
        "date": "2024-03-20T15:44:00.000Z",
        "voteCount": 2,
        "content": "A is required to meet the RTO as well as the RPO.\nB will not meet the RTO.\nC meets the RPO but doesn't handle failover\nD handles failover\nE is incomplete and says nothing of how backups arrive in the secondary region and will most likely not meet the RTO."
      },
      {
        "date": "2024-03-19T08:12:00.000Z",
        "voteCount": 2,
        "content": "cross region read replicate at secondary region, promote during failover, together with Route 53 failover routing policy"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 473,
    "url": "https://www.examtopics.com/discussions/amazon/view/136655-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An ecommerce company runs an application on AWS. The application has an Amazon API Gateway API that invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.<br><br>During the company\u2019s most recent flash sale, a sudden increase in API calls negatively affected the application's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that time and noticed a significant increase in Lambda invocations and database connections. The CPU utilization also was high on the DB instance.<br><br>What should the solutions architect recommend to optimize the application's performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function to connect to the database outside of the function's handler. Check for an existing database connection before creating a new connection."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-26T02:31:00.000Z",
        "voteCount": 8,
        "content": "Some guys got confused whether it's possible to create a DB proxy from the Lambda console Yes, you CAN create a proxy from within the Lambda console: open a function -&gt; Configuration -&gt; RDS databases -&gt; Add Proxy, then select a radio button with two options:\n- Create a new database proxy\n- Choose an existing database proxy\n\nSaid that, \"C\" is correct."
      },
      {
        "date": "2024-05-02T23:14:00.000Z",
        "voteCount": 1,
        "content": "Create a new database not RDS proxy!\nUse an existing database\nCreate a new database"
      },
      {
        "date": "2024-03-19T13:39:00.000Z",
        "voteCount": 7,
        "content": "https://repost.aws/knowledge-center/lambda-rds-database-proxy"
      },
      {
        "date": "2024-10-12T14:22:00.000Z",
        "voteCount": 2,
        "content": "\"Step 3\n1.    Open the Functions page in the LAMBDA CONSOLE.\n2.    In Functions, choose your Lambda function.\n3.    Choose Configuration, and then choose ADD DATABASE PROXIES.\n4.    Enter the following variables:\nProxy identifier: The name of the proxy.\nRDS DB instance:  A supported MySQL or PostgreSQL DB instance or cluster.\nSecret: The Secrets Manager that you created.\nIAM role: The IAM role that you created.\nAuthentication: Choose Password to connect with database credentials or choose Execution role to use the function's IAM credentials for authentication.\n5.    Choose Add.\""
      },
      {
        "date": "2024-08-05T07:34:00.000Z",
        "voteCount": 1,
        "content": "With the current options for my is D, because you can't create a RDS Proxy from Lmabda function console, unless the C is misspelled and the answer is only AWS Console"
      },
      {
        "date": "2024-06-30T07:55:00.000Z",
        "voteCount": 1,
        "content": "Vote D because answer C is incorrect without mention increasing db server qty increase behind proxy. No improvement by just changing endpoint from db to db proxy. \nD can help by reusing the db connection instead of one connection per thread. Lambda by default is parallel run inside the handler."
      },
      {
        "date": "2024-05-30T09:46:00.000Z",
        "voteCount": 2,
        "content": "C, is wrong \nCreating Proxy: Within the RDS console, find the \"Proxies\" section and click on \"Create proxy\"."
      },
      {
        "date": "2024-04-30T06:29:00.000Z",
        "voteCount": 3,
        "content": "Option C:  Read AWS Blog -   Using Amazon RDS Proxy w/ AWS Lambda \nhttps://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\n\nRead Section \" Create and attach a proxy to a Lambda function \"\nNext, use the Lambda console to Add a Database proxy to a Lambda function.\n\nSign into the AWS Lambda console and open the Lambda function you would like to enable RDS Proxy. This Lambda function needs to be configured for access to the same VPC and Subnets as your RDS database."
      },
      {
        "date": "2024-04-22T00:18:00.000Z",
        "voteCount": 2,
        "content": "C, is wrong \nCreating Proxy: Within the RDS console, find the \"Proxies\" section and click on \"Create proxy\"."
      },
      {
        "date": "2024-04-14T21:50:00.000Z",
        "voteCount": 1,
        "content": "A) Incorrect because the issue is at database level\nB) Partially correct but there's one step missed because you have to modify endpoint for the lambda function\nC) This is the tricky one, it's almost correct and the best option except for one comment, it's said \"Create an RDS proxy by using the Lambda console\" the RDS proxy is not created in the lambda console but in the RDS console....\nD) Totally correct, https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\n\nSaid that, going for D but I found this question very tricky...."
      },
      {
        "date": "2024-05-27T22:24:00.000Z",
        "voteCount": 1,
        "content": "I've changed my mind as it's possible to create an RDS proxy by using the Lambda console and regarding the D option, this will improve the lambda performance but not the RDS so going for option C definitely."
      },
      {
        "date": "2024-03-26T11:37:00.000Z",
        "voteCount": 3,
        "content": "BCD all looks good.\nI vote for C"
      },
      {
        "date": "2024-03-26T11:38:00.000Z",
        "voteCount": 1,
        "content": "not B, redis is NOSQL so no relevant to this question"
      },
      {
        "date": "2024-03-26T11:52:00.000Z",
        "voteCount": 1,
        "content": "umm, NVM\nhttps://newsletter.simpleaws.dev/p/elasticache-redis-cache-rds"
      },
      {
        "date": "2024-03-22T05:58:00.000Z",
        "voteCount": 2,
        "content": "Almost answered C before realizing it was a trap. You don't create RDS Proxies from the LAMBDA console, it is done from the RDS console. D is the best answer."
      },
      {
        "date": "2024-03-25T10:27:00.000Z",
        "voteCount": 3,
        "content": "It's not a trap. It _is_ possible to create the RDS Proxy from within the lambda console."
      },
      {
        "date": "2024-03-20T15:49:00.000Z",
        "voteCount": 4,
        "content": "C, for the reasons oayoade links to. \nD is a trap: moving the DB connection outside of the handler obviates the need for keeping track of DB connections. However, C is an even better alternative."
      },
      {
        "date": "2024-03-19T08:19:00.000Z",
        "voteCount": 1,
        "content": "check to re-use any existing DB connection across multiple invocations of Lambda function"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 474,
    "url": "https://www.examtopics.com/discussions/amazon/view/136656-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A retail company wants to improve its application architecture. The company's applications register new orders, handle returns of merchandise, and provide analytics. The applications store retail data in a MySQL database and an Oracle OLAP analytics database. All the applications and databases are hosted on Amazon EC2 instances.<br><br>Each application consists of several components that handle different parts of the order process. These components use incoming data from different sources. A separate ETL job runs every week and copies data from each application to the analytics database.<br><br>A solutions architect must redesign the architecture into an event-driven solution that uses serverless services. The solution must provide updated analytics in near real time.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the individual applications as microservices to Amazon Elastic Container Service (Amazon ECS) containers that use AWS Fargate. Keep the retail MySQL database on Amazon EC2. Move the analytics database to Amazon Neptune. Use Amazon Simple Queue Service (Amazon SQS) to send all the incoming data to the microservices and the analytics database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group for each application. Specify the necessary number of EC2 instances in each Auto Scaling group. Migrate the retail MySQL database and the analytics database to Amazon Aurora MySQL. Use Amazon Simple Notification Service (Amazon SNS) to send all the incoming data to the correct EC2 instances and the analytics database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the individual applications as microservices to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use AWS Fargate. Migrate the retail MySQL database to Amazon Aurora Serverless MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use Amazon EventBridge to send all the incoming data to the microservices and the analytics database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the individual applications as microservices to Amazon AppStream 2.0. Migrate the retail MySQL database to Amazon Aurora MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use AWS IoT Core to send all the incoming data to the microservices and the analytics database."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-20T15:51:00.000Z",
        "voteCount": 2,
        "content": "C is serverless. \nD is rubbish."
      },
      {
        "date": "2024-03-19T13:36:00.000Z",
        "voteCount": 2,
        "content": "\"serverless\""
      },
      {
        "date": "2024-03-19T08:27:00.000Z",
        "voteCount": 4,
        "content": "#A - SQS is not for near real time. MySQL on EC2 is not serverless\n#B is not serverless\n#D is incorrect - Appstream for desktop app streaming and IoT Core for IoT"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 475,
    "url": "https://www.examtopics.com/discussions/amazon/view/136658-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is planning a migration from an on-premises data center to the AWS Cloud. The company plans to use multiple AWS accounts that are managed in an organization in AWS Organizations. The company will create a small number of accounts initially and will add accounts as needed. A solutions architect must design a solution that turns on AWS CloudTrail in all AWS accounts.<br><br>What is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that creates a new CloudTrail trail in all AWS accounts in the organization. Invoke the Lambda function daily by using a scheduled action in Amazon EventBridge.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new CloudTrail trail in the organization's management account. Configure the trail to log all events for all AWS accounts in the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new CloudTrail trail in all AWS accounts in the organization. Create new trails whenever a new account is created. Define an SCP that prevents deletion or modification of trails. Apply the SCP to the root OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Systems Manager Automation runbook that creates a CloudTrail trail in all AWS accounts in the organization. Invoke the automation by using Systems Manager State Manager."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-28T02:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-03-26T12:01:00.000Z",
        "voteCount": 3,
        "content": "B\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html"
      },
      {
        "date": "2024-03-20T15:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2024-03-19T08:30:00.000Z",
        "voteCount": 3,
        "content": "#B is the most operational efficient"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 476,
    "url": "https://www.examtopics.com/discussions/amazon/view/136659-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A software development company has multiple engineers who are working remotely. The company is running Active Directory Domain Services (AD DS) on an Amazon EC2 instance. The company's security policy states that all internal, nonpublic services that are deployed in a VPC must be accessible through a VPN. Multi-factor authentication (MFA) must be used for access to a VPN.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection. Configure integration between a VPN and AD DS. Use an Amazon WorkSpaces client with MFA support enabled to establish a VPN connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple AWS Site-to-Site VPN connections by using AWS VPN CloudHub. Configure integration between AWS VPN CloudHub and AD DS. Use AWS Copilot to establish a VPN connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon WorkLink endpoint. Configure integration between Amazon WorkLink and AD DS. Enable MFA in Amazon WorkLink. Use AWS Client VPN to establish a VPN connection."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-15T06:27:00.000Z",
        "voteCount": 1,
        "content": "ACD are wrong.\n\nBut for B, it is also not perfect. AD Connector is for connecting between ADDS on premises and AWS. In this case, the ADDS is on AWS's EC2. Do you really need AD Connector?"
      },
      {
        "date": "2024-07-13T06:27:00.000Z",
        "voteCount": 1,
        "content": "No doubt that answer B will collect all the events from accounts in the organizations. But the requirement is \"A solutions architect must design a solution that turns on AWS CloudTrail in all AWS accounts.\" Can answer B turn on AWS CloudTrail in all AWS accounts.?"
      },
      {
        "date": "2024-04-26T07:38:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. \n\nClient VPN provides Active Directory support by integrating with AWS Directory Service. Client VPN supports multi-factor authentication (MFA) when it's enabled for AWS Managed Microsoft AD or AD Connector.\nhttps://docs.aws.amazon.com/vpn/latest/clientvpn-admin/ad.html\n\nC. WHY Copilot?\nD. Worklink is Provide secure mobile access to your internal websites and web apps."
      },
      {
        "date": "2024-03-20T15:57:00.000Z",
        "voteCount": 4,
        "content": "A: Site-to-Site VPN is for connecting networks, not giving users access.\nB is correct.\nC is rubbish: AWS Copilot is for deploying containers (and it's bloody good!)\nD is also rubbish: WorkLink is for website and webapp access, not VPN access."
      },
      {
        "date": "2024-03-19T13:33:00.000Z",
        "voteCount": 2,
        "content": "has to be B"
      },
      {
        "date": "2024-03-19T08:43:00.000Z",
        "voteCount": 3,
        "content": "#A - workspaces client for remote desktop access and not for VPN\n#C - AWS VPN CloudHub for connecting multiple on-premises or offices, and not for individual VPN connection\n#D - WorkLink for secure access from mobile devices and not for VPN connection"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 477,
    "url": "https://www.examtopics.com/discussions/amazon/view/138593-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a three-tier web application in an on-premises data center. The frontend is served by an Apache web server, the middle tier is a monolithic Java application, and the storage tier is a PostgreSQL database.<br><br>During a recent marketing promotion, customers could not place orders through the application because the application crashed. An analysis showed that all three tiers were overloaded. The application became unresponsive, and the database reached its capacity limit because of read operations. The company already has several similar promotions scheduled in the near future.<br><br>A solutions architect must develop a plan for migration to AWS to resolve these issues. The solution must maximize scalability and must minimize operational effort<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the frontend so that static assets can be hosted on Amazon S3. Use Amazon CloudFront to serve the frontend to customers. Connect the frontend to the Java application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRehost the Apache web server of the frontend on Amazon EC2 instances that are in an Auto Scaling group. Use a load balancer in front of the Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) to host the static assets that the Apache web server needs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRehost the Java application in an AWS Elastic Beanstalk environment that includes auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the Java application, Develop a Docker container to run the Java application. Use AWS Fargate to host the container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to replatform the PostgreSQL database to an Amazon Aurora PostgreSQL database. Use Aurora Auto Scaling for read replicas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRehost the PostgreSQL database on an Amazon EC2 instance that has twice as much memory as the on-premises server."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-15T04:23:00.000Z",
        "voteCount": 1,
        "content": "I would preferred container over beanstalk but this are examen questions ACE"
      },
      {
        "date": "2024-09-20T01:01:00.000Z",
        "voteCount": 1,
        "content": "The question says they have incoming campaigns in NEAR future.  Doesn't this means Rehost options are better?"
      },
      {
        "date": "2024-08-25T08:35:00.000Z",
        "voteCount": 1,
        "content": "A, C &amp; E. For A - You can connect S3 with backend Java application. It is a known pattern and also published here - https://aws.amazon.com/blogs/storage/extending-java-applications-to-directly-access-files-in-amazon-s3-without-recompiling/"
      },
      {
        "date": "2024-05-23T14:06:00.000Z",
        "voteCount": 3,
        "content": "A -&gt; Correct. Frontend could be hosted on s3 so we don't need a EC2\nB -&gt; False. That's expensive, and requires operational effort (ex: patches, ...)\nC -&gt; Correct. Elastic Beanstalk would reduce operational effort of patches and other stuff and also support native scaling\nD -&gt; False. Would be a great answer if it mentioned \"Service Autoscaling\" for fargate. Since it does not mention, we have to assume that would be only 1 fargate task.\nE -&gt; Correct. Aurora RDS would reduce operational effort and would also allow scaling read replicas.\nF -&gt; False. Requires very operational effort to allow DB reads scaling\nF"
      },
      {
        "date": "2024-05-12T22:59:00.000Z",
        "voteCount": 1,
        "content": "ACE best choices"
      },
      {
        "date": "2024-04-29T23:29:00.000Z",
        "voteCount": 1,
        "content": "BCF is correct"
      },
      {
        "date": "2024-04-26T07:30:00.000Z",
        "voteCount": 3,
        "content": "I chose ACE, but I don't know why it's not D. If you tried to reduce the development effort, it wouldn't have been D, but if you want to reduce the operation effort, I think D is definitely the answer to some extent. However, I chose C because I thought using app refactoring -&gt; java container development -&gt; EKS Fargate was cumbersome."
      },
      {
        "date": "2024-05-23T13:59:00.000Z",
        "voteCount": 1,
        "content": "A single fargate task will not handle peak traffic. In this options it's not mentioned \"Service Autoscaling\" for fargate, so as is it means that would be a single fargate task\n\nhttps://repost.aws/knowledge-center/ecs-fargate-service-auto-scaling"
      },
      {
        "date": "2024-04-19T22:04:00.000Z",
        "voteCount": 1,
        "content": "1. AWS CloudFront (CDN)\n2. AWS Elastic Beanstalk\n3. DMS \n4. Aurora Auto Scaling"
      },
      {
        "date": "2024-04-18T01:21:00.000Z",
        "voteCount": 1,
        "content": "BDE\nB for Web Servers ASG with EFS for scale to share Linux apache servers\nD for App Servers - Fargate reduces ops efforts\nE for DB"
      },
      {
        "date": "2024-04-15T16:31:00.000Z",
        "voteCount": 1,
        "content": "This is what my company does, and there is no mention here of enabling S3 static web hosting"
      },
      {
        "date": "2024-04-15T02:02:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect because you can\u00b4t connect a bucket S3 to a Java application so going with Apcache Web Servers with autoscaling. Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. It supports Java applications and can automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring. The using of Aurora PostgreSQL is pretty obvious. Said that going for BCE."
      },
      {
        "date": "2024-08-25T08:34:00.000Z",
        "voteCount": 1,
        "content": "You can connect S3 with backend Java application. It is a known pattern and also published here - https://aws.amazon.com/blogs/storage/extending-java-applications-to-directly-access-files-in-amazon-s3-without-recompiling/"
      },
      {
        "date": "2024-04-13T11:05:00.000Z",
        "voteCount": 4,
        "content": "In general, Beanstalk is the best option if your priorities are SIMPLICITY and low cost.\nMeanwhile, Fargate is better if you want more control over how your application is hosted, your budget is not especially tight, and your application can be containerized."
      },
      {
        "date": "2024-04-13T04:35:00.000Z",
        "voteCount": 1,
        "content": "ACE are correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 478,
    "url": "https://www.examtopics.com/discussions/amazon/view/138594-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is deploying a new application on AWS. The application consists of an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and an Amazon Elastic Container Registry (Amazon ECR) repository. The EKS cluster has an AWS managed node group.<br><br>The company's security guidelines state that all resources on AWS must be continuously scanned for security vulnerabilities.<br><br>Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate AWS Security Hub. Configure Security Hub to scan the EKS nodes and the ECR repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate Amazon Inspector to scan the EKS nodes and the ECR repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a new Amazon EC2 instance and install a vulnerability scanning tool from AWS Marketplace. Configure the EC2 instance to scan the EKS nodes. Configure Amazon ECR to perform a basic scan on push.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on the EKS nodes. Configure the CloudWatch agent to scan continuously. Configure Amazon ECR to perform a basic scan on push."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-31T18:37:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, not B is focused primarily on scanning Amazon EC2 instances for vulnerabilities and does not natively support scanning Amazon EKS nodes or Amazon ECR repositories"
      },
      {
        "date": "2024-05-30T10:14:00.000Z",
        "voteCount": 3,
        "content": "A. Activate AWS Security Hub: While AWS Security Hub aggregates security findings from various AWS services, it is not primarily designed for continuous scanning of EKS nodes or ECR repositories. Security Hub is more suited for compliance checks and aggregation of security alerts from multiple sources."
      },
      {
        "date": "2024-05-23T13:54:00.000Z",
        "voteCount": 4,
        "content": "A -&gt; False. Security Hub is just a Finding aggregator of other services like AWS config, Inspector, Macie, ..., even security hub controls are in the end config rules.\nB -&gt; True. Inspector scans EC2, ECR, lambda functions (either layer analysis, either deep scan of the code), ...\nC -&gt; False. Has a lot of effort. Plus \"perform a basic scan on push\" is a deprecated thing, inspector should be used.\nD -&gt; False. CW Agent does not report vulns. Inspector uses SSM Agent to perform vulnerability scans. Plus \"perform a basic scan on push\" is a deprecated thing, inspector should be used."
      },
      {
        "date": "2024-04-26T07:01:00.000Z",
        "voteCount": 2,
        "content": "Configuration and vulnerability analysis in Amazon EKS\n- You can use Amazon Inspector to check for unintended network accessibility of your nodes and for vulnerabilities on those Amazon EC2 instances.\nhttps://docs.aws.amazon.com/eks/latest/userguide/configuration-vulnerability-analysis.html\n\nAmazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure.\nhttps://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html\n\nSo, answer is B."
      },
      {
        "date": "2024-04-19T22:07:00.000Z",
        "voteCount": 1,
        "content": "EKS nodes == EC2 , ECR repository = AWS Inspector"
      },
      {
        "date": "2024-04-18T01:29:00.000Z",
        "voteCount": 1,
        "content": "B. Inspector"
      },
      {
        "date": "2024-04-15T16:17:00.000Z",
        "voteCount": 1,
        "content": "Inspector not suppot for eks"
      },
      {
        "date": "2024-04-15T02:09:00.000Z",
        "voteCount": 3,
        "content": "Security hub integrates many Security features but the scaning itself is done by Amazon Inspector so going for B."
      },
      {
        "date": "2024-04-13T11:17:00.000Z",
        "voteCount": 3,
        "content": "You can use Amazon Inspector to check for unintended network accessibility of your nodes and for vulnerabilities on those Amazon EC2 instances.\nhttps://docs.aws.amazon.com/eks/latest/userguide/configuration-vulnerability-analysis.html"
      },
      {
        "date": "2024-04-13T06:53:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer for the given scenario"
      },
      {
        "date": "2024-04-13T04:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 479,
    "url": "https://www.examtopics.com/discussions/amazon/view/138597-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to improve the reliability of its ticketing application. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster. The company uses Amazon CloudFront to serve the application. A single ECS service of the ECS cluster is the CloudFront distribution\u2019s origin.<br><br>The application allows only a specific number of active users to enter a ticket purchasing flow. These users are identified by an encrypted attribute in their JSON Web Token (JWT). All other users are redirected to a waiting room module until there is available capacity for purchasing.<br><br>The application is experiencing high loads. The waiting room module is working as designed, but load on the waiting room is disrupting the applications availability.<br>This disruption is negatively affecting the application's ticket sale transactions.<br><br>Which solution will provide the MOST reliability for ticket sale transactions during periods of high load?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Ensure that the ticketing service uses the JWT information and appropriately forwards requests to the waiting room service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Make the ticketing pod part of a StatefulSet. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Create a CloudFront function that inspects the JWT information and appropriately forwards requests to the ticketing service or the waiting room service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Use AWS App Mesh by provisioning the App Mesh controller for Kubernetes. Enable mTLS authentication and service-to-service authentication for communication between the ticketing pod and the waiting room pod. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T13:46:00.000Z",
        "voteCount": 10,
        "content": "CFFunctions:You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html"
      },
      {
        "date": "2024-09-14T23:12:00.000Z",
        "voteCount": 1,
        "content": "A. No mention of finer control at the CloudFront level\nB. When it comes to migrating to EKS, it may bring additional complexity and cost.\nC. It combines the flexibility of ECS and the edge computing capability of CloudFront.\nD. It involves complex migration, configuration, and authentication mechanisms."
      },
      {
        "date": "2024-06-12T19:47:00.000Z",
        "voteCount": 2,
        "content": "Option A involves creating a separate service in the ECS cluster for the waiting room but relies on the ticketing service to forward requests to the waiting room service based on JWT information. This approach still puts some load and decision-making logic on the ticketing service, which can affect its performance during high load periods."
      },
      {
        "date": "2024-05-20T13:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-04-13T04:43:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 480,
    "url": "https://www.examtopics.com/discussions/amazon/view/138598-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is creating an AWS CloudFormation template from an existing manually created non-production AWS environment. The CloudFormation template can be destroyed and recreated as needed. The environment contains an Amazon EC2 instance. The EC2 instance has an instance profile that the EC2 instance uses to assume a role in a parent account.<br><br>The solutions architect recreates the role in a CloudFormation template and uses the same role name. When the CloudFormation template is launched in the child account, the EC2 instance can no longer assume the role in the parent account because of insufficient permissions<br><br>What should the solutions architect do to resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Ensure that the target role ARN in the existing statement that allows the sts:AssumeRole action is correct. Save the trust policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Add a statement that allows the sts:AssumeRole action for the root principal of the child account. Save the trust policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation stack again. Specify only the CAPABILITY_NAMED_IAM capability.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation stack again. Specify the CAPABILITY_IAM capability and the CAPABILITY_NAMED_IAM capability."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-27T13:22:00.000Z",
        "voteCount": 6,
        "content": "Answer is A . \nThe error occurs because the trust relationship in the parent account that allows the EC2 instance to assume a role may have been broken or misconfigured. This can happen when a role is recreated with a different ARN but the same role name. The trust policy must be updated to reflect the correct ARN.\nOption A addresses this by ensuring that the trust policy in the parent account contains the correct ARN for the role in the child account, allowing the sts:AssumeRole action.\nOption B, which allows the root principal to assume the role, is risky and should be avoided due to security implications."
      },
      {
        "date": "2024-04-27T13:05:00.000Z",
        "voteCount": 5,
        "content": "It is A.\nB is incorrect because specifying the root principal opens access up to all principals in the child account that are allowed to use sts."
      },
      {
        "date": "2024-09-14T23:53:00.000Z",
        "voteCount": 1,
        "content": "This option is directly related to the issue mentioned in the explanation. When a character is recreated in a sub account, its ARN will change, but the character name may remain unchanged. If the ARN in the trust policy is not updated to reflect the new ARN, the EC2 instance will not be able to successfully assume that role. Therefore, updating the trust policy to include the correct ARN is the key to solving the problem. The 'correct ARN' here should refer to the ARN currently held by the character recreated in the parent account. That is to say, the corresponding ARN in the parent account policy needs to be updated.\nOption B adds a statement that allows the sub account root principal to perform the sts: AssemeRole operation. This is usually not the best practice, as allowing the root account to directly assume roles would pose security risks. The root account is the most powerful identity in AWS accounts and should be strictly protected."
      },
      {
        "date": "2024-08-12T06:22:00.000Z",
        "voteCount": 1,
        "content": "It's B for sure"
      },
      {
        "date": "2024-07-27T18:25:00.000Z",
        "voteCount": 2,
        "content": "(A) Because EC2 Instance 's role must be added as a trusted principal so that the parent role can trust it"
      },
      {
        "date": "2024-07-15T04:28:00.000Z",
        "voteCount": 2,
        "content": "The issue is that the EC2 instance in the child account cannot assume the role in the parent account due to insufficient permissions, even though the role has been recreated in the CloudFormation template with the same name.\n\nEditing the trust policy of the role in the parent account and ensuring the target role ARN is correct does not grant the necessary permissions for the child account to assume the role. The trust policy governs which principals (accounts, users, roles, or services) are allowed to assume the role.\n\nIn this case, the correct solution is option B"
      },
      {
        "date": "2024-06-04T00:22:00.000Z",
        "voteCount": 1,
        "content": "it's custom IAM role name so C"
      },
      {
        "date": "2024-05-25T14:12:00.000Z",
        "voteCount": 4,
        "content": "Should be \"A\"."
      },
      {
        "date": "2024-04-26T17:14:00.000Z",
        "voteCount": 1,
        "content": "In IAM roles, use the Principal element in the role trust policy to specify who can assume the role. For cross-account access, you must specify the 12-digit identifier of the trusted account. [\u2026]\nWhen you allow access to a different account, an administrator in that account must then grant access to an identity (IAM user or role) in that account. When you specify an AWS account, you can use the account ARN (arn:aws:iam::account-ID:root), or a shortened form that consists of the \"AWS\": prefix followed by the account ID."
      },
      {
        "date": "2024-04-26T17:14:00.000Z",
        "voteCount": 1,
        "content": "E.g.:\n\"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" }\n\"Principal\": { \"AWS\": \"123456789012\" }\nThe account ARN and the shortened account ID behave the same way. Both delegate permissions to the account. Using the account ARN in the Principal element does not limit permissions to only the root user of the account.\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html\nhttps://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles\n\nA or B?!"
      },
      {
        "date": "2024-04-18T01:42:00.000Z",
        "voteCount": 2,
        "content": "C\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_CreateStack.html#:~:text=If%20you%20have%20IAM%20resources%20with%20custom%20names%2C%20you%20must%20specify%20CAPABILITY_NAMED_IAM"
      },
      {
        "date": "2024-04-15T04:37:00.000Z",
        "voteCount": 2,
        "content": "The solutions architect should ensure that the trust relationship of the role in the parent account allows the child account to assume the role. The trust relationship is defined in the role's trust policy. The trust policy should specify the AWS account ID of the child account as a Principal."
      },
      {
        "date": "2024-04-13T04:46:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 481,
    "url": "https://www.examtopics.com/discussions/amazon/view/138599-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company's web application has reliability issues. The application serves customers globally. The application runs on a single Amazon EC2 instance and performs read-intensive operations on an Amazon RDS for MySQL database.<br><br>During high load, the application becomes unresponsive and requires a manual restart of the EC2 instance. A solutions architect must improve the application's reliability.<br><br>Which solution will meet this requirement with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudFront distribution. Specify the EC2 instance as the distribution\u2019s origin. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the application on EC2 instances that are in an Auto Scaling group. Place the EC2 instances behind an Elastic Load Balancing (ELB) load balancer. Replace the database service with Amazon Aurora. Use Aurora Replicas for the read-intensive operations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy AWS Global Accelerator. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to AWS Lambda functions. Create read replicas for the RDS for MySQL database. Use the read replicas for the read-intensive operations."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-26T22:35:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2024-05-23T13:48:00.000Z",
        "voteCount": 2,
        "content": "Obviously B"
      },
      {
        "date": "2024-05-22T04:19:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-04-26T17:50:00.000Z",
        "voteCount": 1,
        "content": "Answer: B."
      },
      {
        "date": "2024-04-26T07:18:00.000Z",
        "voteCount": 3,
        "content": "The answer is B.\n\n- Automatically restart using health check when not responding -&gt;&gt;ASG\n- Global customers and read-intensive &gt;&gt; 'Read Replica' should be available.\n\nA: EC2 is still alone\nC: EC2 is still alone\nD: It's not a minimum development effort"
      },
      {
        "date": "2024-04-19T22:17:00.000Z",
        "voteCount": 1,
        "content": "D. Migrate the application to AWS Lambda functions. Create read replicas for the RDS for MySQL database. Use the read replicas for the read-intensive operations.\n\nHere's why the other options require more development effort:\n\nA. CloudFront with Multi-AZ RDS: This requires setting up CloudFront and configuring it to point to the EC2 instance. It also requires switching to a Multi-AZ RDS deployment, which might involve downtime.\nB. Auto Scaling with ELB and Aurora: This requires the most effort. You need to migrate the application to run on multiple EC2 instances managed by Auto Scaling, set up an ELB to distribute traffic, migrate the database to Aurora, and configure Aurora Replicas.\nC. Global Accelerator with Multi-AZ RDS: Similar to option A, this involves setting up Global Accelerator and requires a Multi-AZ RDS deployment."
      },
      {
        "date": "2024-04-27T13:08:00.000Z",
        "voteCount": 3,
        "content": "You are dangerously stupid. It is B"
      },
      {
        "date": "2024-09-05T17:45:00.000Z",
        "voteCount": 1,
        "content": "This comment made me laugh! I would also go with B."
      },
      {
        "date": "2024-04-18T02:05:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-17T20:20:00.000Z",
        "voteCount": 4,
        "content": "RDS need readable standby instance"
      },
      {
        "date": "2024-04-14T07:37:00.000Z",
        "voteCount": 1,
        "content": "B is obviously correct for LEAST effort"
      },
      {
        "date": "2024-04-13T11:44:00.000Z",
        "voteCount": 1,
        "content": "Customers globally sounds good Global Accelerator\nMore Work develope migrate app to Lambda\nAmazon Relational Database Service (Amazon RDS) for PostgreSQL and for MySQL now support a new Amazon RDS Multi-AZ deployment option with one primary and two readable STANDBY database (DB) instances across three Availability Zones (AZs). \nhttps://pages.awscloud.com/Deep-dive-on-Amazon-RDS-Multi-AZ-with-two-readable-standbys_2022_0408-DAT_OD.html\nFor a standard accelerator, you can add one or more regional resources, such as load balancers or EC2 instances endpoints,\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-get-started.html"
      },
      {
        "date": "2024-05-26T03:59:00.000Z",
        "voteCount": 1,
        "content": "B  Excuse me...\n\"Use the standby DB instance for the read-intensive operations\" --&gt; NOT"
      },
      {
        "date": "2024-04-13T04:48:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 482,
    "url": "https://www.examtopics.com/discussions/amazon/view/142919-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to use an AWS Transfer Family SFTP-enabled server with an Amazon S3 bucket to receive updates from a third-party data supplier. The data is encrypted with Pretty Good Privacy (PGP) encryption. The company needs a solution that will automatically decrypt the data after the company receives the data.<br>A solutions architect will use a Transfer Family managed workflow. The company has created an IAM service role by using an IAM policy that allows access to AWS Secrets Manager and the S3 bucket. The role\u2019s trust relationship allows the transfer amazonaws.com service to assume the role.<br><br>What should the solutions architect do next to complete the solution for automatic decryption?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the PGP public key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the nominal step. Associate the workflow with the Transfer Family server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the PGP private key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the exception handler. Associate the workflow with the SFTP user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the PGP private key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the nominal step. Associate the workflow with the Transfer Family server.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the PGP public key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the exception handler. Associate the workflow with the SFTP user."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-25T11:28:00.000Z",
        "voteCount": 7,
        "content": "The answer should be \"C\" because you store the \"private\" key in Secrets Manager"
      },
      {
        "date": "2024-07-15T04:31:00.000Z",
        "voteCount": 1,
        "content": "C correct"
      },
      {
        "date": "2024-07-10T11:21:00.000Z",
        "voteCount": 1,
        "content": "C, for sure."
      },
      {
        "date": "2024-07-05T05:09:00.000Z",
        "voteCount": 2,
        "content": "C, for sure.\nIn the context of AWS Transfer Family managed workflows, a \"\"nominal step\"\" refers to one of the predefined steps that you can include in a managed workflow to automate file transfer and processing tasks.\nAn \"\"exception-handling step\"\" is a specific type of step designed to handle errors or exceptions that occur during the execution of a workflow."
      },
      {
        "date": "2024-07-02T17:00:00.000Z",
        "voteCount": 2,
        "content": "C is correct b/c private key is what is required for decryption"
      },
      {
        "date": "2024-06-30T08:39:00.000Z",
        "voteCount": 1,
        "content": "Agree with Zapper1234 plus the permission is granted to transfer family server."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 483,
    "url": "https://www.examtopics.com/discussions/amazon/view/142914-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating infrastructure for its massive multiplayer game to AWS. The game\u2019s application features a leaderboard where players can see rankings in real time. The leaderboard requires microsecond reads and single-digit-millisecond write latencies. The datasets are single-digit terabytes in size and must be available to accept writes in less than a minute if a primary node failure occurs.<br><br>The company needs a solution in which data can persist for further analytical processing through a data pipeline.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon ROS database with a read replica. Configure the application to point writes to the writer endpoint. Configure the application to point reads to the reader endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon MemoryDB for Redis cluster in Muit-AZ mode Configure the application to interact with the primary node.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple Redis nodes on Amazon EC2 instances that are spread across multiple Availability Zones. Configure backups to Amazon S3."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-25T07:20:00.000Z",
        "voteCount": 7,
        "content": "memobrydb for ultra-fast Redis performance, so C"
      },
      {
        "date": "2024-10-15T04:45:00.000Z",
        "voteCount": 1,
        "content": "where is A?"
      },
      {
        "date": "2024-07-06T13:33:00.000Z",
        "voteCount": 3,
        "content": "MEM DB for gaming leaderboard with related latency requirement"
      },
      {
        "date": "2024-07-03T01:27:00.000Z",
        "voteCount": 2,
        "content": "It is C for sure"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 484,
    "url": "https://www.examtopics.com/discussions/amazon/view/142915-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running several applications in the AWS Cloud. The applications are specific to separate business units in the company. The company is running the components of the applications in several AWS accounts that are in an organization in AWS Organizations.<br><br>Every cloud resource in the company\u2019s organization has a tag that is named BusinessUnit. Every tag already has the appropriate value of the business unit name.<br><br>The company needs to allocate its cloud costs to different business units. The company also needs to visualize the cloud costs for each business unit.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create a cost allocation tag that is named BusinessUnit. Also in the management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each member account, create a cost allocation tag that is named BusinessUnit. In the organization\u2019s management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. Create an Amazon CloudWatch dashboard for visualization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create a cost allocation tag that is named BusinessUnit. In each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. In the management account, create an Amazon CloudWatch dashboard for visualization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each member account, create a cost allocation tag that is named BusinessUnit. Also in each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-25T07:21:00.000Z",
        "voteCount": 6,
        "content": "You need Athena and Quicksight so I say A"
      },
      {
        "date": "2024-08-15T19:26:00.000Z",
        "voteCount": 2,
        "content": "Create tag in management account\nUse Athena + QuickSight"
      },
      {
        "date": "2024-07-05T05:12:00.000Z",
        "voteCount": 1,
        "content": "A, for sure"
      },
      {
        "date": "2024-07-04T09:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is \"A\""
      },
      {
        "date": "2024-07-02T17:19:00.000Z",
        "voteCount": 1,
        "content": "A is the best answer"
      },
      {
        "date": "2024-06-29T05:41:00.000Z",
        "voteCount": 1,
        "content": "You need Athena and Quicksight to visualize CUR"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 485,
    "url": "https://www.examtopics.com/discussions/amazon/view/142920-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function. and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.<br><br>As more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda.<br><br>Which combination of changes will resolve these issues? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the write capacity units to the DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory available to the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the payload size from the smart meters to send more data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T12:31:00.000Z",
        "voteCount": 6,
        "content": "I would go with Increasing the write capacity units to the DynamoDB table and Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches. I think that processing the data in batches is much better than increasing the lambda functions memory."
      },
      {
        "date": "2024-06-25T11:33:00.000Z",
        "voteCount": 5,
        "content": "AB because the more memory a Lambda funtion has the faster it reacts"
      },
      {
        "date": "2024-07-06T13:29:00.000Z",
        "voteCount": 3,
        "content": "Kinesis allows to process data in batches, which can help reduce the number of requests and the load on your Lambda functions and DynamoDB."
      },
      {
        "date": "2024-06-30T07:06:00.000Z",
        "voteCount": 3,
        "content": "A and D"
      },
      {
        "date": "2024-06-29T03:04:00.000Z",
        "voteCount": 2,
        "content": "need batch execution so i think AD is correct"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 486,
    "url": "https://www.examtopics.com/discussions/amazon/view/142921-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company recently completed a successful proof of concept of Amazon WorkSpaces. A solutions architect needs to make the solution highly available across two AWS Regions. Amazon WorkSpaces is deployed in a failover Region, and a hosted zone is deployed in Amazon Route 53.<br><br>What should the solutions architect do to configure high availability for the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in the primary Region. Create a Route 53 multivalue answer routing policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a connection alias in the primary Region. Associate the connection alias with a directory in the primary Region. Create a Route 53 weighted routing policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a connection alias in the primary Region Associate the connection alias with a directory in the failover Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-29T05:38:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/workspaces/latest/adminguide/cross-region-redirection.html#cross-region-redirection-associate-connection-aliases"
      },
      {
        "date": "2024-07-06T13:18:00.000Z",
        "voteCount": 2,
        "content": "I think you mean A with link that you provided"
      },
      {
        "date": "2024-06-30T05:38:00.000Z",
        "voteCount": 3,
        "content": "you are right."
      },
      {
        "date": "2024-06-29T03:55:00.000Z",
        "voteCount": 6,
        "content": "A\nhttps://docs.aws.amazon.com/ja_jp/workspaces/latest/adminguide/cross-region-redirection.html"
      },
      {
        "date": "2024-10-12T15:56:00.000Z",
        "voteCount": 1,
        "content": "\"You can associate a connection alias with ONLY ONE directory per AWS Region.\"\nSo, it can't be D."
      },
      {
        "date": "2024-08-13T06:54:00.000Z",
        "voteCount": 1,
        "content": "For A: Potential latency increase due to cross-region access\nFor B:Potential data consistency issues if failover occurs\n\nI choose A"
      },
      {
        "date": "2024-07-21T11:23:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-07-21T02:53:00.000Z",
        "voteCount": 2,
        "content": "I will go with option A"
      },
      {
        "date": "2024-07-14T09:38:00.000Z",
        "voteCount": 4,
        "content": "A for sure"
      },
      {
        "date": "2024-07-04T09:13:00.000Z",
        "voteCount": 3,
        "content": "Answer is A - \nA. Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
      },
      {
        "date": "2024-06-29T03:09:00.000Z",
        "voteCount": 3,
        "content": "A\nhttps://docs.aws.amazon.com/ja_jp/workspaces/latest/adminguide/cross-region-redirection.html"
      },
      {
        "date": "2024-06-25T11:45:00.000Z",
        "voteCount": 4,
        "content": "Believe the answer is A becuase the two distinct Workspaces directories would give you two IP's for truc failover"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 487,
    "url": "https://www.examtopics.com/discussions/amazon/view/142922-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company plans to migrate many VMs from an on-premises environment to AWS. The company requires an initial assessment of the on-premises environment before the migration, a visualization of the dependencies between applications that run on the VMs, and a report that provides an assessment of the on-premises environment.<br><br>To get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraints<br><br>Which solution will provide the company with the required information with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight When the QuickSight report is generated, download the Quick Insights assessment report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T02:21:00.000Z",
        "voteCount": 8,
        "content": "For VMs use agentless unless you need to track network"
      },
      {
        "date": "2024-10-09T06:05:00.000Z",
        "voteCount": 2,
        "content": "requirement is to discover application dependency. So that needs to track network. So it should be A."
      },
      {
        "date": "2024-07-20T08:40:00.000Z",
        "voteCount": 6,
        "content": "Option C is not the least operation overhead. it requires Application Discovery agentless Collector + Export &amp; Upload of the report. also, Discovery agentless will not gather data for all dependencies."
      },
      {
        "date": "2024-10-12T16:08:00.000Z",
        "voteCount": 2,
        "content": "\"AGENT-BASED discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers. The agent installer is available for Windows and Linux operating systems. It collects static configuration data, detailed time-series system-performance information, INBOUND AND OUTBOUND NETWORK CONNECTIONS, and processes that are running.\""
      },
      {
        "date": "2024-10-10T12:21:00.000Z",
        "voteCount": 2,
        "content": "The question clearly states, \"The company has the ability to install collector software in its on-premises environment without any constraints\""
      },
      {
        "date": "2024-09-20T07:22:00.000Z",
        "voteCount": 1,
        "content": "They have the capability of accepting installs on their VM so I will go with A. It can not be the agent less because there is no mention that they can not allow installs on their VMs"
      },
      {
        "date": "2024-08-12T03:34:00.000Z",
        "voteCount": 1,
        "content": "the key word here is \"many\" VMs, hence C."
      },
      {
        "date": "2024-07-16T06:38:00.000Z",
        "voteCount": 1,
        "content": "it should be A, as you can see on this link: https://aws.amazon.com/migration-evaluator/\nagentless collector should be used, it is simple."
      },
      {
        "date": "2024-07-16T06:41:00.000Z",
        "voteCount": 1,
        "content": "sorry i wanted to say C"
      },
      {
        "date": "2024-07-04T18:17:00.000Z",
        "voteCount": 1,
        "content": "Option C: Use Agentless for VMs"
      },
      {
        "date": "2024-06-27T12:40:00.000Z",
        "voteCount": 5,
        "content": "\"To get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraints\", so I understand from the question that the next step would be installing the AWS Application Discovery Agent on each on-premises VM, so for me the answer is A."
      },
      {
        "date": "2024-06-25T11:48:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer becuase you would use migration evaluater"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 488,
    "url": "https://www.examtopics.com/discussions/amazon/view/142923-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambda functions that contain the logic for the API methods. The company\u2019s internal applications use the API for core functionality and business logic. The company\u2019s customers use the API to access data from their accounts. Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.<br><br>The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.<br><br>What should a solutions architect do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS WAF to protect both APIs. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze both APIs. Configure Amazon GuardDuty to block malicious attempts to access the APIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS WAF to protect the API Gateway AP! Configure Amazon Inspector to protect the legacy API. Configure Amazon GuardDuty to block malicious attempts to access the APIs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T00:24:00.000Z",
        "voteCount": 7,
        "content": "GuardDuty only monitors but doesn't block malicious attempts. So answer is C"
      },
      {
        "date": "2024-07-05T05:24:00.000Z",
        "voteCount": 1,
        "content": "C, for sure.\nAWS GuardDuty is a monitoring and threat detection service and does not directly block malicious activities. GuardDuty is designed to continuously monitor and analyze your AWS accounts and workloads for potential threats using machine learning, anomaly detection, and integrated threat intelligence."
      },
      {
        "date": "2024-06-30T09:31:00.000Z",
        "voteCount": 3,
        "content": "Not A because the question only say \"Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.\" There is no ALB or cloudfront mentioned so WAF can't be attached to EC2 directly."
      },
      {
        "date": "2024-06-27T12:49:00.000Z",
        "voteCount": 2,
        "content": "\"The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.\", so I understand that we have to protect BOTH, and GuardDuty does not block anything... The answer for me is A"
      },
      {
        "date": "2024-06-30T05:52:00.000Z",
        "voteCount": 2,
        "content": "how are you going to attache WAF to ec2? :)"
      },
      {
        "date": "2024-06-25T11:50:00.000Z",
        "voteCount": 1,
        "content": "B becuase this protects both API's"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 489,
    "url": "https://www.examtopics.com/discussions/amazon/view/142924-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a serverless ecommerce application on AWS. The application uses Amazon API Gateway to invoke AWS Lambda Java functions. The Lambda functions connect to an Amazon RDS for MySQL database to store data.<br><br>During a recent sale event, a sudden increase in web traffic resulted in poor API performance and database connection failures. The company needs to implement a solution to minimize the latency for the Lambda functions and to support bursts in traffic.<br><br>Which solution will meet these requirements with the LEAST amount of change to the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the code of the Lambda functions so that the Lambda functions open the database connection outside of the function handler. Increase the provisioned concurrency for the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the provisioned concurrency for the Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom parameter group. Increase the value of the max_connections parameter. Associate the custom parameter group with the RDS DB instance and schedule a reboot. Increase the reserved concurrency for the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the reserved concurrency for the Lambda functions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T00:45:00.000Z",
        "voteCount": 6,
        "content": "Provisioned Concurrency - makes sure Lambda functions could handle traffic bursts\nRDS proxy endpoint - intelligently manages connections to a relational database ( Amazon RDS here)  So, the answer - B"
      },
      {
        "date": "2024-06-27T12:55:00.000Z",
        "voteCount": 3,
        "content": "yes, I would go with \"Provisioned concurrency\" too."
      },
      {
        "date": "2024-10-12T16:25:00.000Z",
        "voteCount": 1,
        "content": "Connection Pooling: RDS Proxy helps manage database connections efficiently by pooling and reusing connections. Reduced Latency: By reducing the overhead of establishing new connections for each Lambda invocation, RDS Proxy improves the overall performance, minimizing the latency experienced by your application during traffic bursts. Provisioned Concurrency: Increasing the provisioned concurrency for the Lambda functions ensures they are always ready to handle spikes in traffic, reducing cold start times. Seamless Integration: The integration with AWS Lambda and Secrets Manager ensures that the database credentials are securely managed. IAM Permissions: The use of AWS Identity and Access Management (IAM) permissions to control access to the proxy ensures security and compliance, making this solution secure and scalable."
      },
      {
        "date": "2024-07-06T12:03:00.000Z",
        "voteCount": 3,
        "content": "B is correct for least operation(not cost effectively) in question"
      },
      {
        "date": "2024-06-30T09:43:00.000Z",
        "voteCount": 2,
        "content": "A is right answer. D is wrong because the provisioned Concurrency is the term to keep pre-allocated warm instances of lambda not reserved concurrency.  B missing auto scaling the db."
      },
      {
        "date": "2024-08-19T12:10:00.000Z",
        "voteCount": 1,
        "content": "\"LEAST amount of change to the application\"\nB doesn't need auto-scaling if you're using RDS Proxy"
      },
      {
        "date": "2024-06-30T05:29:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2024-06-27T13:01:00.000Z",
        "voteCount": 4,
        "content": "If cost is a primary concern, Reserved Concurrency could be a more economical choice. Provisioned Concurrency is designed to provide more control over your Lambda function\u2019s performance and scalability. Answer is B."
      },
      {
        "date": "2024-06-26T17:09:00.000Z",
        "voteCount": 4,
        "content": "Option D with \"reserved concurrency\" can be more cost-effective and flexible for handling sudden traffic bursts, as it ensures a minimum number of instances without the potential over-provisioning of provisioned concurrency."
      },
      {
        "date": "2024-06-25T11:54:00.000Z",
        "voteCount": 1,
        "content": "Sorry, meant A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 490,
    "url": "https://www.examtopics.com/discussions/amazon/view/142969-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.<br><br>Which step should the solutions architect take to resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the subnet route table with a route to the interface endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the private DNS option on the VPC attributes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the security group on the interface endpoint to allow connectivity to the AWS services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T00:49:00.000Z",
        "voteCount": 5,
        "content": "ensures proper DNS resolution for VPC endpoints."
      },
      {
        "date": "2024-10-15T05:20:00.000Z",
        "voteCount": 1,
        "content": "B .. .because we had exact this problem once. C would be right if name would be resolved to a private IP, but as described it is not, it resolves to the public ip, so B"
      },
      {
        "date": "2024-08-24T10:19:00.000Z",
        "voteCount": 1,
        "content": "Sorry, Ignore my previous comment. private DNS would solve the issue. Option B is correct"
      },
      {
        "date": "2024-08-24T09:54:00.000Z",
        "voteCount": 1,
        "content": "C (security group) is correct. Private DNS resolution is neither a mandatory pre-requisite to use interface endpoints nor a requirement in this question. If you read the question again, resolving to a public IP is a distractor which makes us think that private DNS (option B) is the correct option. The real problem is the 2nd issue of the question - not able to connect which is a security group configuration issue. Even if you don't want to use private DNS, your interface endpoint will still work however, without security group rule configured, you can't use interface endpoint at all. Check this document for a list of pre-requisites - https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html and 2nd point says \"To use private DNS...\" which implies you may or may not want to use Private DNS however, 4th pre-requisite \"Create a security group....\" is mandatory."
      },
      {
        "date": "2024-07-30T08:18:00.000Z",
        "voteCount": 1,
        "content": "Here in prerequisites for interface endpoint:\nTo use private DNS, you must enable DNS hostnames and DNS resolution for your VPC. For more information, see View and update DNS attributes in the Amazon VPC User Guide.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html"
      },
      {
        "date": "2024-06-27T13:05:00.000Z",
        "voteCount": 1,
        "content": "Private DNS for Interface Endpoints. Answer B."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 491,
    "url": "https://www.examtopics.com/discussions/amazon/view/142925-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is developing a latency-sensitive application. Part of the application includes several AWS Lambda functions that need to initialize as quickly as possible. The Lambda functions are written in Java and contain initialization code outside the handlers to load libraries, initialize classes, and generate unique IDs.<br><br>Which solution will meet the startup performance requirement MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove all the initialization code to the handlers for each Lambda function. Activate Lambda SnapStart for each Lambda function. Configure SnapStart to reference the $LATEST version of each Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish a version of each Lambda function. Create an alias for each Lambda function. Configure each alias to point to its corresponding version. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding alias.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish a version of each Lambda function. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding version. Activate Lambda SnapStar for the published versions of the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Lambda functions to add a pre-snapshot hook. Move the code that generates unique IDs into the handlers. Publish a version of each Lambda function. Activate Lambda SnapStart for the published versions of the Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-30T15:00:00.000Z",
        "voteCount": 5,
        "content": "While option B improves startup performance, it is generally more expensive than SnapStart because it keeps environments warm continuously."
      },
      {
        "date": "2024-10-12T17:35:00.000Z",
        "voteCount": 1,
        "content": "For those who think option C is correct: as dzidis commented, \"SnapStart does NOT support PROVISIONED CONCURRENCY\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/snapstart.html"
      },
      {
        "date": "2024-08-28T17:51:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/compute/reducing-java-cold-starts-on-aws-lambda-functions-with-snapstart/"
      },
      {
        "date": "2024-08-24T07:45:00.000Z",
        "voteCount": 1,
        "content": "D is correct and as dzidis referred to AWS document, you can't use both provisioned concurrency and SnapStart for the same function version. Therefore, C can't be a correct option. Refer to this section of document for more details:\nhttps://docs.aws.amazon.com/lambda/latest/dg/snapstart.html#snapstart-concurrency"
      },
      {
        "date": "2024-08-15T01:54:00.000Z",
        "voteCount": 3,
        "content": "You can't use both SnapStart and provisioned concurrency on the same function version.\n\nTherefore cannot be C.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/snapstart.html"
      },
      {
        "date": "2024-07-06T11:18:00.000Z",
        "voteCount": 3,
        "content": "D\nCombining provisioned concurrency with SnapStart is redundant\nWhile provisioned concurrency reduces cold start latency, it is more costly compared to SnapStart because it keeps a set number of instances warm and ready to handle requests, even when not in use."
      },
      {
        "date": "2024-07-05T02:19:00.000Z",
        "voteCount": 1,
        "content": "It is C"
      },
      {
        "date": "2024-07-02T19:10:00.000Z",
        "voteCount": 1,
        "content": "Provisioned concurrency \u2013 This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Provisioned concurrency is useful for reducing cold start latencies for functions. Configuring provisioned concurrency incurs additional charges to your AWS account."
      },
      {
        "date": "2024-06-27T13:11:00.000Z",
        "voteCount": 2,
        "content": "\"Lambda SnapStart for Java can improve startup performance for latency-sensitive applications by up to 10x at no extra cost\". Answer C."
      },
      {
        "date": "2024-06-27T01:05:00.000Z",
        "voteCount": 1,
        "content": "Leverages both versioning and provisioned concurrency. Also Lambda SnapStart for improved startup performance."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 492,
    "url": "https://www.examtopics.com/discussions/amazon/view/142926-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VM Import feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned an Amazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPC and has a public IP address assigned.<br><br>The EC2 instance does not appear as a managed instance in the AWS Systems Manager console.<br><br>Which combination of steps should the solutions architect take to troubleshoot this issue? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that Systems Manager Agent is installed on the instance and is running.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the instance is assigned an appropriate IAM role for Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify the existence of a VPC endpoint on the VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerity that the AWS Application Discovery Agent is configured.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify the correct configuration of service-linked roles for Systems Manager."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T01:16:00.000Z",
        "voteCount": 8,
        "content": "Answer:AB\nSSM Agent - must for communication between EC2 instances and Systems Manager\nAppropriate IAM role allows the instance to interact with Systems Manager services"
      },
      {
        "date": "2024-06-29T06:45:00.000Z",
        "voteCount": 2,
        "content": "correct."
      },
      {
        "date": "2024-09-20T09:13:00.000Z",
        "voteCount": 1,
        "content": "I will go with A and B because with SSM agent it has to be installed on the VM and there has to be a role for it that allows it to interact with systems manager"
      },
      {
        "date": "2024-09-12T23:15:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-instance-permissions.html"
      },
      {
        "date": "2024-07-03T23:03:00.000Z",
        "voteCount": 1,
        "content": "I also go for A and B. A, the agent for sure and I think B because without the role it would definitly have no access to that instance. I don't know why D should be related to the scenario."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 493,
    "url": "https://www.examtopics.com/discussions/amazon/view/142927-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS CloudFormation as its deployment tool for all applications. It stages all application binaries and templates within Amazon S3 buckets with versioning enabled. Developers have access to an Amazon EC2 instance that hosts the integrated development environment (IDE). The developers download the application binaries from Amazon S3 to the EC2 instance, make changes, and upload the binaries to an S3 bucket after running the unit tests locally. The developers want to improve the existing deployment mechanism and implement CI/CD using AWS CodePipeline.<br><br>The developers have the following requirements:<br>\u2022\tUse AWS CodeCommit for source control.<br>\u2022\tAutomate unit testing and security scanning.<br>\u2022\tAlert the developers when unit tests fail.<br>\u2022\tTurn application features on and off, and customize deployment dynamically as part of CI/CD.<br>\u2022\tHave the lead developer provide approval before deploying an application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to run unit tests and security scans. Use an Amazon EventBridge rule to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Cloud Development Kit (AWS CDK) constructs for different solution features, and use a manifest file to tum features on and off in the AWS CDK application. Use a manual approval stage in the pipeline to allow the lead developer to approve applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to run unit tests and security scans. Use Lambda in a subsequent stage in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Amplify plugins for different solution features and utilize user prompts to tum features on and off. Use Amazon SES in the pipeline to allow the lead developer to approve applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Jenkins to run unit tests and security scans. Use an Amazon EventBridge rule in the pipeline to send Amazon SES alerts to the developers when unit tests fail Use AWS CloudFormation nested stacks for different solution features and parameters to turn features on and off. Use AWS Lambda in the pipeline to allow the lead developer to approve applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy to run unit tests and security scans. Use an Amazon CloudWatch alarm in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use a manual approval stage in the pipeline to allow the lead developer to approve applications."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T01:23:00.000Z",
        "voteCount": 7,
        "content": "A- Yes\nB - No - Lambda not optimal for unit testing\nc- No - Jenkins needs separate management not part of the AWS native services\nD - No - CodeDeploy is for deployment, not to run unit tests and security scans"
      },
      {
        "date": "2024-08-23T04:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct option - https://docs.aws.amazon.com/codebuild/latest/userguide/test-reporting.html"
      },
      {
        "date": "2024-08-05T04:11:00.000Z",
        "voteCount": 1,
        "content": "I think A is the correct answer because I will be testing with codebuild."
      },
      {
        "date": "2024-07-06T13:55:00.000Z",
        "voteCount": 2,
        "content": "Codebuild looks good"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 494,
    "url": "https://www.examtopics.com/discussions/amazon/view/142930-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A global ecommerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes by using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company\u2019s on-premises application servers.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T01:28:00.000Z",
        "voteCount": 8,
        "content": "Answer C - a comprehensive solution for all the requirements for scalable storage, low-latency access, point-in-time backups, and iSCSI device support"
      },
      {
        "date": "2024-08-23T04:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct - https://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html"
      },
      {
        "date": "2024-07-03T04:39:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2024-07-02T22:11:00.000Z",
        "voteCount": 1,
        "content": "B with Amazon FSx File Gateway and S3 File Gateway, along with AWS Backup for data protection, best aligns with the company's requirements for scalable storage, low-latency access, point-in-time backups, and integration with on-premises applications."
      },
      {
        "date": "2024-06-27T13:23:00.000Z",
        "voteCount": 2,
        "content": "iSCSI ---&gt;  AWS Storage Gateway volume gateway"
      },
      {
        "date": "2024-06-25T12:12:00.000Z",
        "voteCount": 2,
        "content": "B because you need iSCSI interface"
      },
      {
        "date": "2024-06-27T04:31:00.000Z",
        "voteCount": 4,
        "content": "C...\"and must retain low-latency access to frequently accessed data\""
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 495,
    "url": "https://www.examtopics.com/discussions/amazon/view/142931-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that uses AWS Key Management Service (AWS KMS) to encrypt and decrypt data. The application stores data in an Amazon S3 bucket in an AWS Region. Company security policies require the data to be encrypted before the data is placed into the S3 bucket. The application must decrypt the data when the application reads files from the S3 bucket.<br><br>The company replicates the S3 bucket to other Regions. A solutions architect must design a solution so that the application can encrypt and decrypt data across Regions. The application must use the same key to decrypt the data in each Region.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a KMS multi-Region primary key. Use the KMS multi-Region primary key to create a KMS multi-Region replica key in each additional Region where the application is running. Update the application code to use the specific replica key in each Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new customer managed KMS key in each additional Region where the application is running. Update the application code to use the specific KMS key in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Private Certificate Authority to create a new certificate authority (CA) in the primary Region. Issue a new private certificate from the CA for the application\u2019s website URL. Share the CA with the additional Regions by using AWS Resource Access Manager (AWS RAM). Update the application code to use the shared CA certificates in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Parameter Store to create a parameter in each additional Region where the application is running. Export the key material from the KMS key in the primary Region. Store the key material in the parameter in each Region. Update the application code to use the key data from the parameter in each Region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T01:33:00.000Z",
        "voteCount": 9,
        "content": "A- straightforward  - encryption and decryption across regions using multi-region key"
      },
      {
        "date": "2024-08-23T04:11:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as per this AWS documentation - https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html#:~:text=A%20multi%2DRegion%20primary%20key%20is%20a%20KMS%20key%20that,primary%20key%20can%20be%20replicated."
      },
      {
        "date": "2024-07-03T04:43:00.000Z",
        "voteCount": 1,
        "content": "Answer A. AWS KMS multi-Region keys allow you to replicate keys across multiple Regions, ensuring that the same key material is available in each Region."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 496,
    "url": "https://www.examtopics.com/discussions/amazon/view/142932-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances run user data scripts to download critical content for the application from an Amazon S3 bucket.<br><br>The EC2 instances are launching correctly. However, after a period of time, the EC2 instances are terminated with the following error message: \u201cAn instance was taken out of service in response to an ELB system health check failure.\u201d EC2 instances continue to launch and be terminated because of Auto Scaling events in an endless loop.<br><br>The only recent change to the deployment is that the company added a large amount of critical content to the S3 bucket. The company does not want to alter the user data scripts in production.<br><br>What should a solutions architect do so that the production environment can deploy successfully?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the health check timeout for the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the health check path for the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the health check grace period for the Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T01:34:00.000Z",
        "voteCount": 9,
        "content": "D. Increase the health check grace period"
      },
      {
        "date": "2024-07-03T04:47:00.000Z",
        "voteCount": 5,
        "content": "Answer D. \nExtending the grace period allows the instances more time to complete their startup tasks, including downloading the additional content from S3, before health checks start. This solution does not require altering the user data scripts in production, which aligns with the company\u2019s requirements."
      },
      {
        "date": "2024-10-12T17:49:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: D - The grace period in Auto Scaling allows new instances time to finish initialization (like downloading data or running user scripts) before health checks start. By increasing this grace period, you provide more time for the EC2 instances to complete the startup process and avoid premature termination.\nIncorrect answer: B - This only extends the time the ALB waits for a response on a health check, but if the EC2 instance isn't ready to serve requests due to its long initialization time, it will still fail the health check."
      },
      {
        "date": "2024-09-15T20:57:00.000Z",
        "voteCount": 2,
        "content": "I prefer B over D:\nThe problem is that EC2 instances need to download content from S3 during the startup process, which may take some time. If ALB's health check is performed during this period and the EC2 instance is unable to respond to the health check request due to incomplete downloads, ALB may consider the instance unhealthy and remove it from the service. This may trigger the auto scaling group to start new instances, creating an endless loop."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 497,
    "url": "https://www.examtopics.com/discussions/amazon/view/142995-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to move some on-premises Oracle databases to AWS. The company has chosen to keep some of the databases on premises for business compliance reasons.<br><br>The on-premises databases contain spatial data and run cron jobs for maintenance. The company needs to connect to the on-premises systems directly from AWS to query data as a foreign table.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon DynamoDB global tables with auto scaling enabled. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to DynamoDB. Create an AWS Lambda function to move the spatial data to Amazon S3. Query the data by using Amazon Athena. Use Amazon EventBridge to schedule jobs in DynamoDB for maintenance. Use Amazon API Gateway for foreign table support.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS for Microsoft SQL Server DB instance. Use native replication to move the data from on premises to the DB instance. Use the AWS Schema Conversion Tool (AWS SCT) to modify the SQL Server schema as needed after replication. Move the spatial data to Amazon Redshift. Use stored procedures for system maintenance. Create AWS Glue crawlers to connect to the on-premises Oracle databases for foreign table support.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch Amazon EC2 instances to host the Oracle databases. Place the EC2 instances in an Auto Scaling group. Use AWS Application Migration Service to move the data from on premises to the EC2 instances and for real-time bidirectional change data capture (CDC) synchronization. Use Oracle native spatial data support. Create an AWS Lambda function to run maintenance jobs as part of an AWS Step Functions workflow. Create an internet gateway for foreign table support.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS for PostgreSQL DB instance. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to the DB instance. Use PostgreSQL native spatial data support. Run cron jobs on the DB instance for maintenance. Use AWS Direct Connect to connect the DB instance to the on-premises environment for foreign table support.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T17:54:00.000Z",
        "voteCount": 1,
        "content": "PostgreSQL natively supports spatial data through the PostGIS extension. This makes it well-suited for handling spatial data from the Oracle databases. AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) are both effective tools for migrating schema and data from Oracle to Amazon RDS for PostgreSQL, ensuring minimal disruption during the migration process. Amazon RDS for PostgreSQL allows the use of cron jobs directly on the instance through extensions like pg_cron for scheduling maintenance tasks. AWS Direct Connect provides a dedicated, secure connection between the on-premises systems and the AWS environment. This low-latency link will allow querying data from on-premises Oracle databases as foreign tables in the PostgreSQL instance without going through the internet, which supports compliance and performance needs."
      },
      {
        "date": "2024-08-15T09:47:00.000Z",
        "voteCount": 1,
        "content": "I didn't realize SCT could convert from Oracle to things like Aurora MySQL and PostgreSQL. But since that is true and a direct connect also makes sense, I'd go D. https://aws.amazon.com/dms/schema-conversion-tool/"
      },
      {
        "date": "2024-06-30T10:24:00.000Z",
        "voteCount": 2,
        "content": "It's Data Migration Service that provides real-time bidirectional change data capture (CDC) synchronization"
      },
      {
        "date": "2024-06-27T17:26:00.000Z",
        "voteCount": 3,
        "content": "Option D is the most appropriate because it leverages AWS services effectively to migrate and manage the databases while ensuring compatibility with spatial data types and maintaining connectivity for querying on-premises data."
      },
      {
        "date": "2024-06-27T17:25:00.000Z",
        "voteCount": 2,
        "content": "Option D is the most appropriate because it leverages AWS services effectively to migrate and manage the databases while ensuring compatibility with spatial data types and maintaining connectivity for querying on-premises data."
      },
      {
        "date": "2024-06-27T13:30:00.000Z",
        "voteCount": 3,
        "content": "PostgreSQL has native support for spatial data, which is required by the company, so answer for me is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 498,
    "url": "https://www.examtopics.com/discussions/amazon/view/142996-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "Accompany runs an application on Amazon EC2 and AWS Lambda. The application stores temporary data in Amazon S3. The S3 objects are deleted after 24 hours.<br><br>The company deploys new versions of the application by launching AWS CloudFormation stacks. The stacks create the required resources. After validating a new version, the company deletes the old stack. The deletion of an old development stack recently failed. A solutions architect needs to resolve this issue without major architecture changes.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation stack to add a DeletionPolicy attribute with a value of Snapshot for the S3 bucket resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to create an Amazon Elastic File System (Amazon EFS) file system to store temporary files instead of Amazon S3. Configure the Lambda functions to run in the same VPC as the EFS file system."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T18:00:00.000Z",
        "voteCount": 1,
        "content": "By using a Lambda function as a custom resource, you can ensure that the Lambda function deletes the objects in the S3 bucket before CloudFormation attempts to delete the bucket itself. Adding the DependsOn attribute ensures that the S3 bucket resource will not be deleted until the Lambda function has completed its task of clearing out all objects from the bucket, thus avoiding any errors caused by attempting to delete a non-empty S3 bucket. Options B and C: These options will not work because the DeletionPolicy attribute does NOT trigger the deletion of the OBJECTS INSIDE THE BUCKET. It ONLY determines what happens to the BUCKET RESOURCE ITSELF, not its contents. The stack deletion will still fail if objects remain in the bucket. Option D introduces significant architectural changes, which are unnecessary for solving the stack deletion issue."
      },
      {
        "date": "2024-07-12T06:13:00.000Z",
        "voteCount": 3,
        "content": "A, for sure.\nDeletionPolicy: Delete: The DeletionPolicy attribute in CloudFormation is used to specify what should happen to a resource when the stack is deleted. The value Delete indicates that CloudFormation should delete the resource (in this case, the S3 bucket) when the stack is deleted.\nNon-Empty Buckets: The problem with this approach is that CloudFormation cannot delete an S3 bucket if it contains any objects. The DeletionPolicy: Delete does not change this behavior; it only specifies that the bucket should be deleted, which will still fail if the bucket is not empty."
      },
      {
        "date": "2024-07-05T09:40:00.000Z",
        "voteCount": 2,
        "content": "A, for sure.\nDeletionPolicy: Delete: The DeletionPolicy attribute in CloudFormation is used to specify what should happen to a resource when the stack is deleted. The value Delete indicates that CloudFormation should delete the resource (in this case, the S3 bucket) when the stack is deleted.\nNon-Empty Buckets: The problem with this approach is that CloudFormation cannot delete an S3 bucket if it contains any objects. The DeletionPolicy: Delete does not change this behavior; it only specifies that the bucket should be deleted, which will still fail if the bucket is not empty."
      },
      {
        "date": "2024-07-05T02:28:00.000Z",
        "voteCount": 2,
        "content": "you can\u00b4t delete a S3 bucket with objects in it. So A is correct"
      },
      {
        "date": "2024-07-03T19:16:00.000Z",
        "voteCount": 1,
        "content": "By setting the Deletion Policy attribute to Delete in the stack, you ensure that the S3 bucket and its contents are deleted when the CloudFormation stack is deleted. This best option for the scenario and  and aligns with the desired behavior of removing old resources when the stack is deleted."
      },
      {
        "date": "2024-07-03T16:16:00.000Z",
        "voteCount": 2,
        "content": "IT SHOULD BE A"
      },
      {
        "date": "2024-07-03T04:55:00.000Z",
        "voteCount": 3,
        "content": "I will go for A.\nUsing Lambda function as a custom resource ensures that the S3 bucket is emptied before the stack is deleted. DependsOn Attribute ensures the Lambda function runs and completes before attempting to delete the S3 bucket, thus preventing deletion failure."
      },
      {
        "date": "2024-07-02T21:07:00.000Z",
        "voteCount": 1,
        "content": "When you specify a DeletionPolicy attribute with a value of Delete for an S3 bucket in a CloudFormation template, CloudFormation will delete the bucket and all its contents during stack deletion. This approach addresses the issue of the stack deletion failing due to the bucket not being empty."
      },
      {
        "date": "2024-06-30T10:36:00.000Z",
        "voteCount": 1,
        "content": "Votes C. After s3 snapshot, cloud formation will proceed s3 bucket deletion. A is right but compare to c it doesn't match the requirement in the question. \"resolve this issue without major architecture changes.\"\nAlso the data become useless only after 24 hours. A delete everything regardless. C is better."
      },
      {
        "date": "2024-06-29T07:19:00.000Z",
        "voteCount": 4,
        "content": "it should be A"
      },
      {
        "date": "2024-06-27T13:35:00.000Z",
        "voteCount": 1,
        "content": "\"DependsOn\" attribute ensures that the Lambda function will always be invoked before the S3 bucket is deleted in a CloudFormation. Answer A."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 499,
    "url": "https://www.examtopics.com/discussions/amazon/view/142933-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that stores user-uploaded videos in an Amazon S3 bucket that uses S3 Standard storage. Users access the videos frequently in the first 180 days after the videos are uploaded. Access after 180 days is rare. Named users and anonymous users access the videos.<br><br>Most of the videos are more than 100 MB in size. Users often have poor internet connectivity when they upload videos, resulting in failed uploads. The company uses multipart uploads for the videos.<br><br>A solutions architect needs to optimize the S3 costs of the application.<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the S3 bucket to be a Requester Pays bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Transfer Acceleration to upload the videos to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Lifecycle configuration o expire incomplete multipart uploads 7 days after initiation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Lifecycle configuration to transition objects to S3 Glacier Instant Retrieval after 1 day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Lifecycle configuration to transition objects to S3 Standard-infrequent Access (S3 Standard- IA) after 180 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T18:06:00.000Z",
        "voteCount": 1,
        "content": "C: Incomplete multipart uploads consume unnecessary storage and increase costs, especially with large files like videos. Setting a lifecycle policy to automatically delete incomplete uploads after 7 days helps reduce unnecessary storage costs without manual intervention.\nE: The videos are frequently accessed in the first 180 days and rarely accessed after that. Transitioning the videos to S3 Standard-IA after 180 days reduces costs while still providing immediate retrieval when needed. S3 Standard-IA is designed for infrequently accessed data and offers lower storage costs than S3 Standard while maintaining fast access times.\nWhy not B? S3 Transfer Acceleration improves upload speeds for users with poor internet connectivity, but it INCURS ADDITIONAL COSTS. While this can help with uploads, it does NOT directly optimize STORAGE COSTS, which is the main goal here."
      },
      {
        "date": "2024-08-16T04:12:00.000Z",
        "voteCount": 1,
        "content": "CE for sure.\n\nWhy not B?\nThe root cause for failed upload is due to that fact that user has poor internet connectivity. That's not something transfer accelerator can help with, maybe the user need to find a better internet provider, or use Elon's star link"
      },
      {
        "date": "2024-07-03T20:29:00.000Z",
        "voteCount": 1,
        "content": "CE are the best options.  In option C, incomplete upload should be deleted to save costs."
      },
      {
        "date": "2024-07-03T04:58:00.000Z",
        "voteCount": 1,
        "content": "From the cost management perspective, the answer should be CE"
      },
      {
        "date": "2024-06-29T04:07:00.000Z",
        "voteCount": 3,
        "content": "Cost Optimization so CE"
      },
      {
        "date": "2024-06-29T04:06:00.000Z",
        "voteCount": 2,
        "content": "Cost Optimization so CE"
      },
      {
        "date": "2024-06-27T13:40:00.000Z",
        "voteCount": 2,
        "content": "B - for transfers of files over long distances between your client and an S3 bucket | E - for reducing cost for data that is accessed less frequently."
      },
      {
        "date": "2024-06-27T01:52:00.000Z",
        "voteCount": 4,
        "content": "C - optimizes the S3 storage costs effectively \nE - address frequent access in the first 180 days and infrequent access afterward"
      },
      {
        "date": "2024-06-25T12:31:00.000Z",
        "voteCount": 3,
        "content": "B and E are the only ones that make sense to me"
      },
      {
        "date": "2024-06-29T07:24:00.000Z",
        "voteCount": 1,
        "content": "B is not related to cost optimization."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 500,
    "url": "https://www.examtopics.com/discussions/amazon/view/142934-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs an ecommerce web application on AWS. The web application is hosted as a static website on Amazon S3 with Amazon CloudFront for content delivery. An Amazon API<br>Gateway API invokes AWS Lambda functions to handle user requests and order processing for the web application The Lambda functions store data in an Amazon ROS for MySQL DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.<br><br>Recently, the website has experienced SQL injection and web exploit attempts. Customers also report that order processing time has increased during periods of peak usage. During these periods, the Lambda functions often have cold starts. As the company grows, the company needs to ensure scalability and low-latency access during traffic peaks. The company also must optimize the database costs and add protection against the SQL injection and web exploit attempts.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Lambda functions to have an increased timeout value during peak periods. Use RDS Reserved Instances for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory of the Lambda functions, Transition to Amazon Redshift for the database. Integrate Amazon Inspector with CloudFront to protect against the SQL injection and web exploit attempts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Lambda functions with provisioned concurrency for compute during peak periods, Transition to Amazon Aurora Serverless for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Lambda functions with provisioned concurrency for compute during peak periods. Use RDS Reserved Instances for the database. Integrate AWS WAF with CloudFront to protect against the SQL injection and web exploit attempts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T01:59:00.000Z",
        "voteCount": 7,
        "content": "D - AWS WAF for SQL injection and web exploit protection"
      },
      {
        "date": "2024-10-12T18:31:00.000Z",
        "voteCount": 1,
        "content": "It's D: Provisioned Concurrency ensures that Lambda functions are pre-warmed and ready to handle requests instantly, which reduces the \"cold start\" problem. RDS Reserved Instances for Amazon RDS will help reduce the database cost. Since the workload has been consistent over the past 12 months, Reserved Instances provide a cost-effective solution by offering significant discounts compared to On-Demand pricing. AWS WAF protects the application from web exploits such as SQL injection and cross-site scripting (XSS)."
      },
      {
        "date": "2024-09-27T02:18:00.000Z",
        "voteCount": 1,
        "content": "waf for sql injection and web exploits"
      },
      {
        "date": "2024-09-18T23:22:00.000Z",
        "voteCount": 1,
        "content": "expanding business needs serverless database so Aurora in option C is the best"
      },
      {
        "date": "2024-09-20T02:32:00.000Z",
        "voteCount": 1,
        "content": "It looksxlike I was looking at different question as no mention in the question to use serverless database. I will go with D"
      },
      {
        "date": "2024-08-22T21:40:00.000Z",
        "voteCount": 1,
        "content": "using shield advanced will enable the basic features of WAF for free as well, so C"
      },
      {
        "date": "2024-09-15T04:30:00.000Z",
        "voteCount": 1,
        "content": "Shield Advanced is not free, and it's used against DDoS, not SQL injection."
      },
      {
        "date": "2024-08-09T05:17:00.000Z",
        "voteCount": 2,
        "content": "Regardless of the diabolical wording of the question. Forget about whether it's WAF or Shield Advance, it's 'C' because it drills down to saying \"the company is now expecting growth and needs to ensure scalability\", this pushes us to Aurora Serverless. DB usage was consistent last year, it no longer is."
      },
      {
        "date": "2024-07-06T07:21:00.000Z",
        "voteCount": 2,
        "content": "AWS WAF instead of AWS Shield"
      },
      {
        "date": "2024-07-05T09:46:00.000Z",
        "voteCount": 2,
        "content": "D, for sure.\nTo protect against SQL injection attacks, AWS WAF (Web Application Firewall) is the appropriate service to use, not AWS Shield Advanced."
      },
      {
        "date": "2024-06-27T13:45:00.000Z",
        "voteCount": 2,
        "content": "Lambda functions with provisioned concurrency for compute during peak periods + Aurora Serverless + AWS Shield Advanced, I don't see any better choice. Answer C."
      },
      {
        "date": "2024-06-25T12:35:00.000Z",
        "voteCount": 1,
        "content": "C - using Lambda concuraancy with Aurora Serverless solves a bunch of the issues"
      },
      {
        "date": "2024-06-29T07:29:00.000Z",
        "voteCount": 2,
        "content": "it is D, no need for AWS Shield Advanced, WAF is sufficient."
      },
      {
        "date": "2024-06-30T02:00:00.000Z",
        "voteCount": 2,
        "content": "it is D, AWS Shield Advanced is not required; AWS WAF can be used to protect against common web exploits such as SQL injection and cross-site scripting (XSS) attacks."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 501,
    "url": "https://www.examtopics.com/discussions/amazon/view/142997-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company runs a web application on a single Amazon EC2 instance. End users experience slow application performance during times of peak usage, when CPU utilization is consistently more than 95%.<br><br>A user data script installs required custom packages on the EC2 instance. The process of launching the instance takes several minutes.<br><br>The company is creating an Auto Scaling group that has mixed instance groups, varied CPUs, and a maximum capacity limit. The Auto Scaling group will use a launch template for various configuration options. The company needs to decrease application latency when new instances are launched during auto scaling.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a predictive scaling policy. Use an instance maintenance policy to run the user data script. Set the default instance warmup time to 0 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a dynamic scaling policy. Use lifecycle hooks to run the user data script. Set the default instance warmup time to 0 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a predictive scaling policy. Enable warm pools for the Auto Scaling group. Use an instance maintenance policy to run the user data script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a dynamic scaling policy. Enable warm pools for the Auto Scaling group. Use lifecycle hooks to run the user data script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-15T19:06:00.000Z",
        "voteCount": 1,
        "content": "Agree with D\nThere is no mention of predictable peak period. Since there's a known metric where user experience skpwness, dynamic scaling should be used. \nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html#:~:text=A%20dynamic%20scaling%20policy%20instructs,CloudWatch%20alarm%20is%20in%20ALARM.\n\nUse warm pool to reduce latency and cost of unnecessary standby instance. \nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\n\nUse lifecycle hook due to the need to install custom packages\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\n\nMore reference:\nhttps://aws.amazon.com/blogs/compute/introducing-instance-maintenance-policy-for-amazon-ec2-auto-scaling/"
      },
      {
        "date": "2024-07-06T07:17:00.000Z",
        "voteCount": 1,
        "content": "D is correcyt"
      },
      {
        "date": "2024-07-03T16:13:00.000Z",
        "voteCount": 1,
        "content": "Answer : D"
      },
      {
        "date": "2024-07-03T11:18:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2024-06-30T01:49:00.000Z",
        "voteCount": 1,
        "content": "B\nAWS Database Migration Service can convert Oracle and SQL Server to Amazon RDS for MySQL and Amazon RDS for PostgreSQL stored procedures.\n\nD\nAWS Database Migration Service (AWS DMS) performs data migration.\n\nThe answer is DB."
      },
      {
        "date": "2024-06-30T15:57:00.000Z",
        "voteCount": 1,
        "content": "looks like this is an answer for different question"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 502,
    "url": "https://www.examtopics.com/discussions/amazon/view/142971-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to migrate its on-premises database fleet to Amazon RDS. The company is currently using a mixture of Microsoft SQL Server, MySQL, and Oracle databases. Some of the databases have custom schemas and stored procedures.<br><br>Which combination of steps should the company take for the migration? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migration Evaluator Quick Insights to analyze the source databases and to identify the stored procedures that need to be migrated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Migration Service to analyze the source databases and to identify the stored procedures that need to be migrated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to analyze the source databases for changes that are required\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate the source databases to Amazon RDS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to migrate the data from the source databases to Amazon RDS."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T02:17:00.000Z",
        "voteCount": 7,
        "content": "Answer CD"
      },
      {
        "date": "2024-09-15T20:51:00.000Z",
        "voteCount": 1,
        "content": "A. AWS does not have a service directly named 'Migration Evaluator Quick Insights'.\nB. AWS Application Migration Service is primarily used for migrating virtual machines rather than analyzing them.\nC. AWS SCT can analyze the source database and identify potential architecture changes needed for successful migration to Amazon RDS.\nD. AWS DMS is a data migration service.\nE. AWS DataSync is a service used for quickly migrating large amounts of data between local storage and AWS storage services, with a focus on file level data migration. But database migration requires maintaining data integrity, relationships, constraints, and so on."
      },
      {
        "date": "2024-07-22T00:59:00.000Z",
        "voteCount": 1,
        "content": "answer CD"
      },
      {
        "date": "2024-07-03T11:19:00.000Z",
        "voteCount": 1,
        "content": "Answer CD"
      },
      {
        "date": "2024-06-30T01:53:00.000Z",
        "voteCount": 1,
        "content": "B\nAWS Database Migration Service can convert Oracle and SQL Server to Amazon RDS for MySQL and Amazon RDS for PostgreSQL stored procedures.\n\nD\nAWS Database Migration Service (AWS DMS) performs data migration.\n\nThe answer is DB."
      },
      {
        "date": "2024-06-28T12:05:00.000Z",
        "voteCount": 4,
        "content": "C and D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 503,
    "url": "https://www.examtopics.com/discussions/amazon/view/142972-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating its blog platform to AWS. The company's on-premises servers connect to AWS through an AWS Site-to-Site VPN connection. The blog content is updated several times a day by multiple authors and is served from a file share on a network-attached storage (NAS) server.<br><br>The company needs to migrate the blog platform without delaying the content updates. The company has deployed Amazon EC2 instances across multiple Availability Zones to run the blog platform behind an Application Load Balancer. The company also needs to move 200 TB of archival data from its on-premises servers to Amazon S3 as soon as possible.<br><br>Which combination of stops will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a weekly cron job in Amazon EventBridge. Use the cron job to invoke an AWS Lambda function to update the EC2 instances from the NAS server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume for the EC2 instances to share for content access. Write code to synchronize the EBS volume with the NAS server weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMount an Amazon Elastic File System (Amazon EFS) file system to the on-premises servers to act as the NAS server. Copy the blog data to the EFS file system. Mount the EFS file system to the C2 instances to serve the content.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowball Edge Storage Optimized device. Copy the static data artifacts to the device. Ship the device to AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowcons SSD device. Copy the static data artifacts to the device. Ship the device to AWS."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T12:06:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D\n\nhttps://aws.amazon.com/about-aws/whats-new/2023/05/aws-snow-family-multi-pb-data-migration-210tb-device/"
      },
      {
        "date": "2024-07-03T11:22:00.000Z",
        "voteCount": 3,
        "content": "Answer is CD \nC. EFS provides a scalable, shared file storage that can be accessed by both on-premises servers and EC2 instances. This ensures that updates to the content are immediately available to the blog platform without the need for synchronization.\nD. Snowball Edge is designed for large-scale data transfers, providing an efficient way to move 200 TB of data to Amazon S3 quickly and securely."
      },
      {
        "date": "2024-06-30T01:40:00.000Z",
        "voteCount": 1,
        "content": "A, B\nThe NAS server has not been migrated.\n\nE\nAWS Snowcone does not have enough capacity.\nSnowball Edge Storage Optimized can handle up to 210 TB of NVMe capacity.\n\nThe answers are C and D."
      },
      {
        "date": "2024-06-28T12:10:00.000Z",
        "voteCount": 4,
        "content": "C And D"
      },
      {
        "date": "2024-06-27T02:43:00.000Z",
        "voteCount": 4,
        "content": "Answer CD"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 504,
    "url": "https://www.examtopics.com/discussions/amazon/view/142998-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company plans to migrate a legacy on-premises application to AWS. The application is a Java web application that runs on Apache Tomcat with a PostgreSQL database.<br><br>The company does not have access to the source code but can deploy the application Java Archive (JAR) files. The application has increased traffic at the end of each month.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch Amazon EC2 instances in multiple Availability Zones. Deploy Tomcat and PostgreSQL to all the instances by using Amazon Elastic File System (Amazon EFS) mount points. Use AWS Step Functions to deploy additional EC2 instances to scale for increased traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision Amazon Elastic Kubernetes Service (Amazon EKS) in an Auto Scaling group across multiple AWS Regions. Deploy Tomcat and PostgreSQL in the container images. Use a Network Load Balancer to scale for increased traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the Java application into Python-based containers. Use AWS Lambda functions for the application logic. Store application data in Amazon DynamoDB global tables. Use AWS Storage Gateway and Lambda concurrency to scale for increased traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to deploy the Tomcat servers with auto scaling in multiple Availability Zones. Store application data in an Amazon RDS for PostgreSQL database. Deploy Amazon CloudFront and an Application Load Balancer to scale for increased traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T11:25:00.000Z",
        "voteCount": 4,
        "content": "Answer D\nElastic Beanstalk: Provides an easy and managed way to deploy and scale web applications. It handles the deployment, capacity provisioning, load balancing, and auto-scaling automatically.\n\nAmazon RDS for PostgreSQL: Manages the database operations, providing automated backups, patching, and scaling, which reduces operational overhead.\n\nCloudFront and Application Load Balancer: Ensure that the application can handle increased traffic efficiently, distributing the load across multiple Availability Zones and providing low latency."
      },
      {
        "date": "2024-06-30T01:24:00.000Z",
        "voteCount": 1,
        "content": "D\nThe option with the least overhead is the use of AWS Elastic Beanstalk Tomcat."
      },
      {
        "date": "2024-06-27T14:09:00.000Z",
        "voteCount": 3,
        "content": "Upload the .jar straightforward to AWS Elastic Beanstalk. Answer D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 505,
    "url": "https://www.examtopics.com/discussions/amazon/view/142999-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating its on-premises IoT platform to AWS. The platform consists of the following components:<br><br>\u2022\tA MongoDB cluster as a data store for all collected and processed IoT data.<br>\u2022\tAn application that uses Message Queuing Telemetry Transport (MQTT) to connect to IoT devices every 5 minutes to collect data.<br>\u2022\tAn application that runs jobs periodically to generate reports from the IoT data. The jobs take 120-600 seconds to finish running.<br>\u2022\tA web application that runs on a web server. End users use the web application to generate reports that are accessible to the general public.<br><br>The company needs to migrate the platform to AWS to reduce operational overhead while maintaining performance.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Step Functions state machines with AUS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Configure an Amazon CloudFront distribution that has an S3 origin to serve the reports\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function. Program the Lambda function to connect to the IoT devices. process the data, and write the data to the data store. Configure a Lambda layer to temporarily store messages for processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Create an ingress controller on the EKS cluster to serve the reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect the IoT devices to AWS IoT Core to publish messages. Create an AWS IoT rule that runs when a message is received. Configure the rule to call an AWS Lambda function. Program the Lambda function to parse, transform, and store device message data to the data store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the MongoDB cluster to Amazon EC2 instances."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-28T12:18:00.000Z",
        "voteCount": 6,
        "content": "Step Functions and Lambda for Report Generation (A):\n\nAWS Step Functions and Lambda can manage the periodic jobs to generate reports with minimal operational overhead. By using Amazon S3 for storage and Amazon CloudFront for distribution, the solution provides scalability and reliability with minimal management.\nAWS IoT Core for MQTT Messaging (D):\n\nAWS IoT Core is a managed service that simplifies the connection and management of IoT devices. Using IoT rules and Lambda functions ensures efficient message processing and data storage with minimal overhead.\nAmazon DocumentDB for MongoDB Compatibility (E):\n\nAmazon DocumentDB is a managed database service compatible with MongoDB, which reduces the operational burden of managing a MongoDB cluster while maintaining performance and scalability."
      },
      {
        "date": "2024-07-06T00:15:00.000Z",
        "voteCount": 1,
        "content": "A, D and E  meet all requirements clearly"
      },
      {
        "date": "2024-06-27T14:14:00.000Z",
        "voteCount": 1,
        "content": "A - for preparing the reports and writing them to Amazon S3 | D - for handling connections from IoT devices | E -  supporting  MongoDB workloads."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 506,
    "url": "https://www.examtopics.com/discussions/amazon/view/143000-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company creates an Amazon API Gateway API and shares the API with an external development team. The API uses AWS Lambda functions and is deployed to a stage that is named Production.<br><br>The external development team is the sole consumer of the API. The API experiences sudden increases of usage at specific times, leading to concerns about increased costs. The company needs to limit cost and usage without reworking the Lambda functions.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the API to send requests to Amazon Simple Queue Service (Amazon SQS) queues instead of directly to the Lambda functions. Update the Lambda functions to consume messages from the queues and to process the requests. Set up the queues to invoke the Lambda functions when new messages arrive.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure provisioned concurrency for each Lambda function. Use AWS Application Auto Scaling to register the Lambda functions as targets. Set up scaling schedules to increase and decrease capacity to match changes in API usage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API Gateway API key and an AWS WAF Regional web ACL. Associate the web ACL with the Production stage. Add a rate-based rule to the web ACL. In the rule, specify the rate limit and a custom request aggregation that uses the X-API-Key header. Share the API key with the external development team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-22T00:15:00.000Z",
        "voteCount": 4,
        "content": "The most cost-effective solution to limit cost and usage for the API Gateway API with minimal code changes is:\n\nD. Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team.\n\nHere's why this approach is most cost-effective:\n\nAPI Key and Usage Plan: This restricts access to the API only for the development team using the provided API key. The usage plan allows defining throttling limits (maximum requests per unit time) and quotas (total requests allowed) for the API key. This controls resource utilization and costs.\nMinimal Code Changes: No modifications are required to the existing Lambda functions, reducing development effort."
      },
      {
        "date": "2024-07-12T07:32:00.000Z",
        "voteCount": 1,
        "content": "D, for sure.\nAPI Gateway Usage Plans allow you to set throttling limits and quotas on API keys. This directly controls the number of requests per second and per day that the external development team can make. It helps in managing costs by limiting the amount of Lambda invocations triggered by API requests."
      },
      {
        "date": "2024-07-06T00:11:00.000Z",
        "voteCount": 1,
        "content": "User plan is to define who and how much for API usage"
      },
      {
        "date": "2024-06-28T12:25:00.000Z",
        "voteCount": 4,
        "content": "Creating an API key and a usage plan allows you to control and limit the usage of the API. The usage plan lets you define throttling limits (requests per second) and quotas (total requests per day or month).\nBy associating the usage plan with the Production stage and the API key, you can enforce these limits on the external development team, ensuring that the API usage stays within the desired boundaries.\nThis approach directly addresses the concern of sudden increases in usage and helps control costs without requiring any changes to the existing Lambda functions or the overall architecture"
      },
      {
        "date": "2024-06-27T14:19:00.000Z",
        "voteCount": 1,
        "content": "The \"usage plan\" is the key here for me to access the API within the defined limits."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 507,
    "url": "https://www.examtopics.com/discussions/amazon/view/143001-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An entertainment company hosts a ticketing service on a fleet of Linux Amazon EC2 instances that are in an Auto Scaling group. The ticketing service uses a pricing file. The pricing file is stored in an Amazon S3 bucket that has S3 Standard storage. A central pricing solution that is hosted by a third party updates the pricing file.<br><br>The pricing file is updated every 1-15 minutes and has several thousand line items. The pricing file is downloaded to each EC2 instance when the instance launches.<br><br>The EC2 instances occasionally use outdated pricing information that can result in incorrect charges for customers.<br><br>Which solution will resolve this problem MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to update an Amazon DynamoDB table with new prices each time the pricing file is updated. Update the ticketing service to use DynramoDB to look up pricing\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to update an Amazon Elastic File System (Amazon EFS) file share with the pricing file each time the file is updated. Update the ticketing service to use Amazon EFS to access the pricing file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad Mountpoint for Amazon S3 onto the AMI of the EC2 instances. Configure Mountpoint for Amazon S3 to mount the S3 bucket that contains the pricing file. Update the ticketing service to point to the mount point and path to access the $3 object,\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS Multi-Attach to attach the volume to every EC2 instance. When a new EC2 instance launches, configure the new instance to update the pricing file on the EBS volume. Update the ticketing service to point to the new local source."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-28T12:31:00.000Z",
        "voteCount": 6,
        "content": "Mountpoint for Amazon S3: This solution allows the EC2 instances to directly access the S3 bucket as if it were a local file system. This ensures that the instances always access the latest version of the pricing file without having to download it each time.\nCost-Effective: This approach avoids the need to constantly download and store the file on each instance, which can save on both S3 GET requests and local storage costs.\nSimplicity: By mounting the S3 bucket, you ensure that all instances are using the most current file without additional logic or processes to manage file updates."
      },
      {
        "date": "2024-10-15T06:30:00.000Z",
        "voteCount": 1,
        "content": "none of them makes sense... if an S3 object is uploaded it is strongly consist since end 2020, eventual consistency is a matter of the past. So it doesn't matter if the lambda function get the trigger after upload and transfer the information to dynamodb (A) or EFS(b), or the ec2 instance get the object via blocklevel file access (C) or EBS (D). The consistency is being provided by the source system which is S3, so nothing helps here. From the cost perspective is C the cheapest"
      },
      {
        "date": "2024-10-13T08:16:00.000Z",
        "voteCount": 1,
        "content": "Mountpoint for Amazon S3 allows EC2 instances to treat an S3 bucket like a file system. This solution ensures that the EC2 instances always have access to the latest version of the pricing file, as the file is directly accessed from S3. You avoid downloading the file every time and reduce the risk of using outdated pricing data.\nS3 Consistency: Amazon S3 provides strong read-after-write consistency, so any update to the pricing file in S3 will be immediately visible to all EC2 instances accessing the file via the mount point.\nCost Efficiency: By using Mountpoint for Amazon S3, you leverage S3's cost-effective storage and avoid additional infrastructure like DynamoDB or Elastic File System (EFS). This solution does not require copying data to another storage system, minimizing overhead."
      },
      {
        "date": "2024-09-18T23:44:00.000Z",
        "voteCount": 1,
        "content": "the question is asking about cost effectiveness so why choose A to add additional service like Dynamodb . I will go for option C"
      },
      {
        "date": "2024-09-15T21:50:00.000Z",
        "voteCount": 1,
        "content": "A. DynamoDB provides fast data access and query capabilities, suitable for frequently read but infrequently updated data.\nB. EFS may not be suitable for frequent small file updates, and its cost may be higher than using DynamoDB.\nC. This solution can directly read pricing files from S3, but it does not solve the problem of outdated pricing data being used by old instances even after the pricing files are updated.\nD. EBS is not good at Multi Attach to multiple EC2 instances, and it can increase complexity and cost."
      },
      {
        "date": "2024-08-11T09:26:00.000Z",
        "voteCount": 3,
        "content": "Option A is the correct answer."
      },
      {
        "date": "2024-07-24T00:51:00.000Z",
        "voteCount": 1,
        "content": "There is no need to move away from S3"
      },
      {
        "date": "2024-06-27T14:25:00.000Z",
        "voteCount": 4,
        "content": "DynamoDB in this scenario looks cheaper than EFS. Answer A"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 508,
    "url": "https://www.examtopics.com/discussions/amazon/view/143002-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has an application that uses Amazon EC2 instances in an Auto Scaling group. The quality assurance (QA) department needs to launch a large number of short-lived environments to test the application. The application environments are currently launched by the manager of the department using an AWS CloudFormation template. To launch the stack, the manager uses a role with permission to use CloudFormation, EC2, and Auto Scaling APIs. The manager wants to allow testers to launch their own environments, but does not want to grant broad permissions to each user.<br><br>Which set up would achieve these goals?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to assume the manager\u2019s role and add a policy that restricts the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Service Catalog product from the environment template. Add a launch constraint to the product with the existing role. Give users in the QA department permission to use AWS Service Catalog APIs only. Train users to launch the template from the AWS Service Catalog console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to use CloudFormation and S3 APIs, with conditions that restrict the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Elastic Beanstalk application from the environment template. Give users in the QA department permission to use Elastic Beanstalk permissions only. Train users to launch Elastic Beanstalk environments with the Elastic Beanstalk CLI, passing the existing role to the environment as a service role."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-28T12:38:00.000Z",
        "voteCount": 6,
        "content": "B is the answer"
      },
      {
        "date": "2024-06-27T14:28:00.000Z",
        "voteCount": 1,
        "content": "Service Catalog, answer B"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 509,
    "url": "https://www.examtopics.com/discussions/amazon/view/143003-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using a single AWS Region for its ecommerce website. The website includes a web application that runs on several Amazon EC2 instances behind an Application Load Balancer (ALB). The website also includes an Amazon DynamoDB table. A custom domain name in Amazon Route 53 is linked to the ALB. The company created an SSL/TLS certificate in AWS Certificate Manager (ACM) and attached the certificate to the ALB. The company is not using a content delivery network as part of its design.<br><br>The company wants to replicate its entire application stack in a second Region to provide disaster recovery, plan for future growth, and provide improved access time to users. A solutions architect needs to implement a solution that achieves these goals and minimizes administrative overhead.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template for the current infrastructure design. Use parameters for important system values, including Region. Use the CloudFormation template to create the new infrastructure in the second Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Management Console to document the existing infrastructure design in the first Region and to create the new infrastructure in the second Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Route 53 hosted zone record for the application to use weighted routing. Send 50% of the traffic to the ALB in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Route 53 hosted zone record for the application to use latency-based routing. Send traffic to the ALB in each Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the configuration of the existing DynamoDB table by enabling DynamoDB Streams. Add the second Region to create a global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DynamoDB table. Enable DynamoDB Streams for the new table. Add the second Region to create a global table. Copy the data from the existing DynamoDB table to the new table as a one-time operation."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-06T21:01:00.000Z",
        "voteCount": 2,
        "content": "A, D , E is correct"
      },
      {
        "date": "2024-06-28T13:20:00.000Z",
        "voteCount": 3,
        "content": "A and D And E"
      },
      {
        "date": "2024-06-27T14:30:00.000Z",
        "voteCount": 1,
        "content": "A-D-E makes more sense to me here"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 510,
    "url": "https://www.examtopics.com/discussions/amazon/view/143004-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to create a single Amazon S3 bucket for its data scientists to store work-related documents. The company uses AWS IAM Identity Center to authenticate all users. A group for the data scientists was created.<br><br>The company wants to give the data scientists access to only their own work. The company also wants to create monthly reports that show which documents each user accessed.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom IAM Identity Center permission set to grant the data scientists access to an S3 bucket prefix that matches their username tag. Use a policy to limit access to paths with the ${aws:PrincipalTag/userName}/* condition.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM Identity Center role for the data scientists group that has Amazon S3 read access and write access. Add an S3 bucket policy that allows access to the IAM Identity Center role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CloudTrail to log S3 data events and deliver the logs to an S3 bucket. Use Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CloudTrail to log S3 management events to CloudWatch. Use Amazon Athena\u2019s CloudWatch connector to query the logs and generate reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 access logging to EMR File System (EMRFS). Use Amazon S3 Select to query logs and generate reports."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-28T13:23:00.000Z",
        "voteCount": 4,
        "content": "A and C"
      },
      {
        "date": "2024-06-27T14:33:00.000Z",
        "voteCount": 1,
        "content": "IAM Identity Center permission + Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports, answer A-C"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 511,
    "url": "https://www.examtopics.com/discussions/amazon/view/143048-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company hosts a data-processing application on Amazon EC2 instances. The application polls an Amazon Elastic File System (Amazon EFS) file system for newly uploaded files. When a new file is detected, the application extracts data from the file and runs logic to select a Docker container image to process the file. The application starts the appropriate container image and passes the file location as a parameter.<br><br>The data processing that the container performs can take up to 2 hours. When the processing is complete, the code that runs inside the container writes the file back to Amazon EFS and exits.<br><br>The company needs to refactor the application to eliminate the EC2 instances that are running the containers.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an Amazon EventBridge rule that starts the appropriate Fargate task. Configure the EventBridge rule to run when files are added to the EFS file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Update and containerize the container selection logic to run as a Fargate service that starts the appropriate Fargate task. Configure an EFS event notification to invoke the Fargate service when files are added to the EFS file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an AWS Lambda function that starts the appropriate Fargate task. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the Lambda function when objects are created.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Lambda container images for the processing. Configure Lambda functions to use the container images. Extract the container selection logic to run as a decision Lambda function that invokes the appropriate Lambda processing function. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the decision Lambda function when objects are created."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-28T00:32:00.000Z",
        "voteCount": 1,
        "content": "EFS event notification to invoke the Fargate"
      },
      {
        "date": "2024-07-14T10:40:00.000Z",
        "voteCount": 2,
        "content": "EFS event notification to invoke the Fargate --&gt; EFS don't have event notification like s3"
      },
      {
        "date": "2024-07-05T23:15:00.000Z",
        "voteCount": 2,
        "content": "C\nEventBridge can not monitor EFS event directly, not A"
      },
      {
        "date": "2024-06-29T00:28:00.000Z",
        "voteCount": 2,
        "content": "C\nEFS cannot notify events and Lambda cannot do container execution for 2 hours."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 512,
    "url": "https://www.examtopics.com/discussions/amazon/view/143049-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A media company has a 30-T8 repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset Management (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people\u2019s faces. A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to AWS.<br><br>The company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.<br><br>How can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T09:08:00.000Z",
        "voteCount": 1,
        "content": "AWS Rekognition is capable of processing both images and videos from the following sources: Amazon S3, directly passed images, Kinesis Video Streams. Although option C mentions Kinesis Video Streams, it would be an overkill for a scenario where videos are not being streamed in real time, and it introduces unnecessary complexity. So the solution using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system is A."
      },
      {
        "date": "2024-09-28T00:39:00.000Z",
        "voteCount": 1,
        "content": "Use the MAM solution to extract the videos from the current archive"
      },
      {
        "date": "2024-08-22T20:14:00.000Z",
        "voteCount": 1,
        "content": "It is not possible to directly invoke Amazon Rekognition to process videos that are stored on an AWS Storage Gateway (whether it's a file gateway or tape gateway). Amazon Rekognition processes videos that are stored in Amazon S3."
      },
      {
        "date": "2024-08-21T01:31:00.000Z",
        "voteCount": 1,
        "content": "Yes the videos were stored on tape on premise, but the solution requires active processing later on, this can't be done on Tape GW; won't make sense."
      },
      {
        "date": "2024-08-21T01:44:00.000Z",
        "voteCount": 1,
        "content": "Okay, it's B. You cannot directly migrate videos from a tape library to an AWS Storage Gateway file gateway appliance."
      },
      {
        "date": "2024-08-06T10:24:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/aws/file-interface-to-aws-storage-gateway/"
      },
      {
        "date": "2024-07-23T02:03:00.000Z",
        "voteCount": 3,
        "content": "Use the MAM solution to extract the videos from the current archive and push them into"
      },
      {
        "date": "2024-07-21T07:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is B: \"These videos are stored on tape in an on-premises tape library....\""
      },
      {
        "date": "2024-07-06T08:33:00.000Z",
        "voteCount": 1,
        "content": "ape Gateway is designed for offline data transfer, not ideal for actively accessed videos."
      },
      {
        "date": "2024-07-04T10:15:00.000Z",
        "voteCount": 4,
        "content": "Answer is B - Use Tape Gateway, then Lambda and Rekognition can be used to process and index the data for the MAM system. \n\n\nhttps://aws.amazon.com/storagegateway/vtl/\nhttps://aws.amazon.com/blogs/aws/file-interface-to-aws-storage-gateway/"
      },
      {
        "date": "2024-06-30T04:40:00.000Z",
        "voteCount": 4,
        "content": "B - Tape Gateway"
      },
      {
        "date": "2024-06-29T00:31:00.000Z",
        "voteCount": 2,
        "content": "A\nA tape gateway appliance is required.\nAWS Storage Gateway also requires S3 or other storage."
      },
      {
        "date": "2024-06-30T17:18:00.000Z",
        "voteCount": 1,
        "content": "so you mean B ???"
      },
      {
        "date": "2024-07-02T15:11:00.000Z",
        "voteCount": 2,
        "content": "A - Amazon Rekognition and Tape Gateway cannot communicate directly; S3 is required."
      },
      {
        "date": "2024-07-04T10:16:00.000Z",
        "voteCount": 2,
        "content": "Storage Gateway stores the tape library in S3."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 513,
    "url": "https://www.examtopics.com/discussions/amazon/view/143050-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company needs to optimize the cost of an AWS environment that contains multiple accounts in an organization in AWS Organizations. The company conducted cost optimization activities 3 years ago and purchased Amazon EC2 Standard Reserved Instances that recently expired.<br><br>The company needs EC2 instances for 3 more years. Additionally, the company has deployed a new serverless workload.<br><br>Which strategy will provide the company with the MOST cost savings?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase the same Reserved Instances for an additional 3-year term with All Upfront payment. Purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a 1-year Compute Savings Plan with No Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the Compute Savings Plan.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Region. Purchase a 3-year Compute Savings Plan with No Upfront payment in the management account to cover any additional compute costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase a 3-year EC2 Instance Savings Plan with All Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the EC2 Instance Savings Plan."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-14T00:18:00.000Z",
        "voteCount": 6,
        "content": "Since there are no special requirements in the problem statement other than cost (e.g., workload consistency, specific instance size, future changes), simply select the least expensive plan.\n\nCompared to on-demand instances, Reserved Instances (RI) offer discounts of up to 75%, Compute Savings Plans up to 66%, and EC2 Instance Savings Plans up to 72%. In each case, the higher the upfront payment, the greater the discount."
      },
      {
        "date": "2024-09-16T19:48:00.000Z",
        "voteCount": 1,
        "content": "The mention of serverless workload is just distraction. It can be for a new system and meanwhile it may not be commercially or operationally viable to move the current ec2 workload to serverless, so it's normal to continue to run it with cost effectiveness for the next 3 years."
      },
      {
        "date": "2024-09-01T11:21:00.000Z",
        "voteCount": 1,
        "content": "Option C seems better. Because with Option A, provides comprehensive coverage but involves upfront payment and potentially redundant cost coverage if Compute Savings Plans are also being purchased. The combination might be less cost-efficient due to the overlap in coverage."
      },
      {
        "date": "2024-07-14T10:47:00.000Z",
        "voteCount": 2,
        "content": "It's clear, RI for EC2 and compute saving for serverless"
      },
      {
        "date": "2024-07-04T11:14:00.000Z",
        "voteCount": 3,
        "content": "A because it is said that 3 years ago they optimized costs and came to that conclusion... so just renew reserved instances and for compute use compute plan because that is the best add for serverless..."
      },
      {
        "date": "2024-06-29T00:34:00.000Z",
        "voteCount": 4,
        "content": "A\nNeed a Compute Savings Plan for serverless workloads\nThe least expensive way to run EC2 instances for 3 years would be either a Reserved Instance or EC2 Instance Savings Plan."
      },
      {
        "date": "2024-07-02T07:25:00.000Z",
        "voteCount": 1,
        "content": "+1 for 3Y All Upfront RI, which will give best discount ever"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 514,
    "url": "https://www.examtopics.com/discussions/amazon/view/143031-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company operates a static content distribution platform that serves customers globally. The customers consume content from their own AWS accounts.<br><br>The company serves its content from an Amazon S3 bucket. The company uploads the content from its on-premises environment to the S3 bucket by using an S3 File Gateway.<br><br>The company wants to improve the platform\u2019s performance and reliability by serving content from the AWS Region that is geographically closest to customers. The company must route the on-premises data to Amazon S3 with minimal latency and without public internet exposure.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement S3 Multi-Region Access Points\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Cross-Region Replication (CRR) to copy content to different Regions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that tracks the routing of clients to Regions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Site-to-Site VPN connection to connect to a Multi-Region Access Point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-28T14:44:00.000Z",
        "voteCount": 5,
        "content": "A and E"
      },
      {
        "date": "2024-07-02T14:55:00.000Z",
        "voteCount": 1,
        "content": "A,E is correct.\nOn-premise data needed to be routed to Amazon S3 with minimal latency and without exposing it to the public Internet."
      },
      {
        "date": "2024-10-15T07:04:00.000Z",
        "voteCount": 1,
        "content": "A. Implement S3 Multi-Region Access Points and B. Use S3 Cross-Region Replication (CRR) to copy content to different Regions.\n\nThe combination of (A) and (B) allows the company to serve content from the closest region to the end-user and ensures that the data is replicated across multiple regions to support this.\nMulti-Region Access Points simplify the access and management of the data while CRR ensures that the content is available across these regions.\nThis setup provides a straightforward and managed solution to meet the requirement of geographical content routing with minimal operational overhead.\nFor all that voting for E... how does AWS DirectConnect and \"LEAST operational overhead\" fit toghether."
      },
      {
        "date": "2024-10-13T09:19:00.000Z",
        "voteCount": 1,
        "content": "A: S3 Multi-Region Access Points allow customers to access Amazon S3 data from multiple AWS Regions with the lowest latency. These access points automatically route requests to the closest region based on the user's location. This helps optimize performance and increases reliability by dynamically routing traffic to the most optimal region. \nE: AWS PrivateLink ensures private connectivity between AWS services and on-premises resources without traversing the public internet.\nB: Although CRR replicates data across Regions, it does NOT optimize performance by routing users to the closest Region dynamically. \nC: While Lambda can handle some routing logic, this option adds more operational overhead compared to using built-in features like S3 Multi-Region Access Points.\nD: It can't be because one of the requirements is \"without public internet exposure.\""
      },
      {
        "date": "2024-09-16T20:12:00.000Z",
        "voteCount": 2,
        "content": "The company wants to improve the platform\u2019s performance and reliability by serving content from the AWS Region that is geographically closest to customers: s3 multi region access point. (A) \n\nThe company must route the on-premises data to Amazon S3 with minimal latency and without public internet exposure: PrivateLink and Direct Connect to the MRAP. (E)"
      },
      {
        "date": "2024-09-16T00:15:00.000Z",
        "voteCount": 1,
        "content": "Option A: Multi regional access points are mainly used to access S3 data across multiple regions, rather than solving data transmission problems.\nOption B: Allows companies to automatically asynchronously replicate data from S3 buckets to S3 buckets in other AWS regions.\nOption E: AWS PrivateLink provides a secure and private way to access AWS services without using the public Internet."
      },
      {
        "date": "2024-07-05T22:44:00.000Z",
        "voteCount": 2,
        "content": "A and E is correct to meet latency and private network"
      },
      {
        "date": "2024-07-05T10:37:00.000Z",
        "voteCount": 2,
        "content": "A, E, for sure"
      },
      {
        "date": "2024-06-29T00:39:00.000Z",
        "voteCount": 2,
        "content": "A and B\nMulti-region access configuration allows content to be served from each region closest to the customer's AWS access using a cross-region replica of the AWS global network."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 515,
    "url": "https://www.examtopics.com/discussions/amazon/view/143032-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is migrating its data center to the AWS Cloud and needs to complete the migration as quickly as possible. The company has many applications that are running on hundreds of VMware VMs in the data center. Each VM is configured with a shared Windows folder that contains common shared files. The file share is larger than 100 GB in size.<br><br>The company\u2019s compliance team requires a change request to be fled and approved for every software installation and modification to each VM. The company has an AWS Direct Connect connection with 10 GB of bandwidth between AWS and the data center.<br><br>Which set of steps should the company take to complete the migration in the LEAST amount of time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VM ImporvExport to create images of each VM. Use AWS Application Migration Service to manage and view the images. Copy the Windows file share data to an Amazon Elastic File System (Amazon EFS) file system. After migration, remap the file share to the EFS file system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS Application Discovery Service agentless appliance to VMware vCenter. Review the portfolio of discovered VMs in AWS Migration Hub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS Application Migration Service agentless appliance to VMware vCenter. Copy the Windows file share data to a new Amazon FSx for Windows File Server file system. After migration, remap the file share on each VM to the FSx for Windows File Server file system.<br>C. Create and review a portfolio in AWS Migration Hub. Order an AWS Snowcone device. Deploy AWS Application Migration Service to VMware vCenter and export all the VMs to the Snowcone device. Copy all Windows file share data to the Snowcone device. Ship the Snowcone device to AWS. Use Application Migration Service to deploy all the migrated instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS Application Discovery Service Agent and the AWS Application Migration Service Agent onto each VMware hypervisor directly. Review the portfolio in AWS Migration Hub. Copy each VM\u2019s file share data to a new Amazon FSx for Windows File Server file system. After migration, remap the file share on each VM to the FSx for Windows File Server file system."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T22:46:00.000Z",
        "voteCount": 6,
        "content": "C1 (not C2) is correct because Application Migration Agentless on vMCenter."
      },
      {
        "date": "2024-10-13T09:25:00.000Z",
        "voteCount": 1,
        "content": "C1 is correct. The AWS Application Migration Service (AMS) agentless appliance can quickly be deployed into the VMware environment via VMware vCenter. It simplifies migration by eliminating the need for installing agents on each individual VM, reducing time and complexity. Because AMS is agentless, you don\u2019t need to install additional software on the VMs themselves, which aligns with the company's requirement to file and approve change requests for any software installation or modifications on VMs. By copying the shared Windows folder to Amazon FSx for Windows File Server, the company can maintain a native Windows environment for file sharing. After migration, it will be easy to remap the file share from the VMs to the new FSx file system, which minimizes disruptions and downtime."
      },
      {
        "date": "2024-07-14T00:46:00.000Z",
        "voteCount": 1,
        "content": "A combination of A and B is one option.\nIn AWS, if there are five choices, the answer is usually two.\nIn the case of multiple choice, there is only one answer from ABCD.\nI have never seen a pattern where there is only one answer from ABCDE."
      },
      {
        "date": "2024-07-03T16:27:00.000Z",
        "voteCount": 2,
        "content": "IT should be C"
      },
      {
        "date": "2024-06-29T00:41:00.000Z",
        "voteCount": 1,
        "content": "B\nMigration can be completed in the shortest possible time using an agentless appliance"
      },
      {
        "date": "2024-06-30T17:31:00.000Z",
        "voteCount": 4,
        "content": "B focuses on discovery and does not address the migration process, correct answer is C"
      },
      {
        "date": "2024-06-28T14:54:00.000Z",
        "voteCount": 4,
        "content": "\u00e71 is the answer"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 516,
    "url": "https://www.examtopics.com/discussions/amazon/view/143065-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has multiple AWS accounts that are in an organization in AWS Organizations. The company needs to store AWS account activity and query the data from a central location by using SQL.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudTraii trail in each account. Specify CloudTrail management events for the trail. Configure CloudTrail to send the events to Amazon CloudWatch Logs. Configure CloudWatch cross-account observability. Query the data in CloudWatch Logs Insights.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a delegated administrator account to create an AWS CloudTrail Lake data store. Specify CloudTrail management events for the data store. Enable the data store for all accounts in the organization. Query the data in CloudTrail Lake.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a delegated administrator account to create an AWS CloudTral trail. Specify CloudTrail management events for the trail. Enable the trail for all accounts in the organization. Keep all other settings as default. Query the CloudTrail data from the CloudTrail event history page.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy AWS CloudTrail Lake data stores in each account. Specify CloudTrail management events for the data stores. Keep all other settings as default, Query the data in CloudTrail Lake."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T02:37:00.000Z",
        "voteCount": 2,
        "content": "To enable cloudtrail lake, you need to login with admin access to cloudtrail.\nhttps://aws.amazon.com/blogs/mt/announcing-aws-cloudtrail-lake-a-managed-audit-and-security-lake/"
      },
      {
        "date": "2024-07-09T11:41:00.000Z",
        "voteCount": 2,
        "content": "AWS CloudTrail Lake lets you run SQL-based queries on your events."
      },
      {
        "date": "2024-06-29T05:30:00.000Z",
        "voteCount": 3,
        "content": "B\nYou can aggregate events within an Organization by enabling it for all accounts in the Organization with AWS CloudTrail Lake."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 517,
    "url": "https://www.examtopics.com/discussions/amazon/view/143064-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is using AWS to develop and manage its production web application. The application includes an Amazon API Gateway HTTP API that invokes an AWS Lambda function. The Lambda function processes and then stores data in a database.<br><br>The company wants to implement user authorization for the web application in an integrated way. The company already uses a third-party identity provider that issues OAuth tokens for the company\u2019s other applications.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate the company\u2019s third-party identity provider with API Gateway. Configure an API Gateway Lambda authorizer to validate tokens from the identity provider. Require the Lambda authorizer on all API routes. Update the web application to get tokens from the identity provider and include the tokens in the Authorization header when calling the API Gateway HTTP API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate the company's third-party identity provider with AWS Directory Service. Configure Directory Service as an API Gateway authorizer to validate tokens from the identity provider. Require the Directory Service authorizer on all API routes. Configure AWS IAM Identity Center as a SAML 2.0 identity Provider. Configure the web application as a custom SAML 2.0 application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate the company\u2019s third-party identity provider with AWS IAM Identity Center. Configure API Gateway to use IAM Identity Center for zero-configuration authentication and authorization. Update the web application to retrieve AWS Security Token Service (AWS STS) tokens from IAM Identity Center and include the tokens in the Authorization header when calling the API Gateway HTTP API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate the company\u2019s third-party identity provider with AWS IAM Identity Center. Configure IAM users with permissions to call the API Gateway HTTP API. Update the web application to extract request parameters from the IAM users and include the parameters in the Authorization header when calling the API Gateway HTTP API."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T02:49:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/security/use-aws-lambda-authorizers-with-a-third-party-identity-provider-to-secure-amazon-api-gateway-rest-apis/"
      },
      {
        "date": "2024-09-17T02:46:00.000Z",
        "voteCount": 1,
        "content": "Building a Lambda authorizer allows users to access API Gateway resources by using their third-party credentials without having to configure additional services, such as Amazon Cognito. This can be particularly useful if your organization is using the third-party identity provider for single sign-on (SSO).\non.com/blogs/security/use-aws-lambda-authorizers-with-a-third-party-identity-provider-to-secure-amazon-api-gateway-rest-apis/"
      },
      {
        "date": "2024-07-14T08:10:00.000Z",
        "voteCount": 1,
        "content": "A, for sure.\nLambda authorizers can integrate with external identity providers, including OAuth2, OpenID Connect, and others, to validate tokens or credentials."
      },
      {
        "date": "2024-07-05T22:47:00.000Z",
        "voteCount": 2,
        "content": "A \nAPI GW + integrated Lambda Authorizor for Authen. and Author."
      },
      {
        "date": "2024-06-29T05:26:00.000Z",
        "voteCount": 2,
        "content": "A\nIt is reasonable to configure the API Gateway Lambda authorizer to validate tokens from identity providers."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 518,
    "url": "https://www.examtopics.com/discussions/amazon/view/143035-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company\u2019s security policy requires the EBS volumes to be encrypted.<br><br>The company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T22:49:00.000Z",
        "voteCount": 3,
        "content": "D is correct instead of A  because AWS support change account setting for EBS encryption"
      },
      {
        "date": "2024-07-02T20:16:00.000Z",
        "voteCount": 2,
        "content": "Use config to find unencrypted EBS. Change the default setting."
      },
      {
        "date": "2024-06-29T05:24:00.000Z",
        "voteCount": 2,
        "content": "D\nEnabling default encryption for EBSs prevents the creation of unencrypted EBSs."
      },
      {
        "date": "2024-06-28T15:27:00.000Z",
        "voteCount": 2,
        "content": "the answer is D"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 519,
    "url": "https://www.examtopics.com/discussions/amazon/view/143206-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is running a large containerized workload in the AWS Cloud. The workload consists of approximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS) to orchestrate the workload.<br><br>Recently the company\u2019s development team started using AWS Fargate instead of Amazon EC2 instances in the ECS cluster. In the past, the workload has come close to running the maximum number of EC2 instances that are available in the account.<br><br>The company is worried that the workload could reach the maximum number of ECS tasks that are allowed. A solutions architect must implement a solution that will notify the development team when Fargate reaches 80% of the maximum number of tasks.<br><br>What should the solutions architect do to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS cluster. Set an alarm for when the math expression sample count/SERVICE_QUOTA(service)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metric namespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to poll detailed metrics from the ECS cluster. When the number of running Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify the development team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule to evaluate whether the Fargate SERVICE_QUOTA is greater than 80. Use Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule is not compliant."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T14:32:00.000Z",
        "voteCount": 5,
        "content": "B\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Quotas-Visualize-Alarms.html"
      },
      {
        "date": "2024-09-26T23:24:00.000Z",
        "voteCount": 3,
        "content": "Why this is the correct choice:\nAWS/Usage metric namespace: This namespace provides detailed metrics for service usage and quotas, including the number of running ECS Fargate tasks. You can use this data to monitor how close you are to the service quotas.\nService quota monitoring: The math expression metric/SERVICE_QUOTA(metric)*100 allows you to calculate the percentage of the quota being used, making it easy to set an alarm when usage reaches 80%.\nCloudWatch Alarm: This is a native and efficient way to monitor service usage, and you can easily configure notifications via Amazon SNS to alert the development team when the threshold is crossed."
      },
      {
        "date": "2024-07-09T11:59:00.000Z",
        "voteCount": 2,
        "content": "Service Quota"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 520,
    "url": "https://www.examtopics.com/discussions/amazon/view/143390-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company has several AWS Lambda functions written in Python. The functions are deployed with the .zip package deployment type. The functions use a Lambda layer that contains common libraries and packages in a .zip file. The Lambda .zip packages and Lambda layer .zip file are stored in an Amazon S3 bucket.<br><br>The company must implement automatic scanning of the Lambda functions and the Lambda layer to identify CVEs. A subset of the Lambda functions must receive automated code scans to detect potential data leaks and other vulnerabilities. The code scans must occur only for selected Lambda functions, not all the Lambda functions.<br><br>Which combination of actions will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate Amazon Inspector. Start automated CVE scans.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate Lambda standard scanning and Lambda code scanning in Amazon Inspector.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty. Enable the Lambda Protection feature in GuardDuty.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable scanning in the Monitor settings of the Lambda functions that need code scans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTag Lambda functions that do not need code scans. In the tag, include a key of InspectorCodeExclusion and a value of LambdaCodeScanning.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Inspector to scan the 3 bucket that contains the Lambda .zip packages and the Lambda layer .zip file for code scans."
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T22:52:00.000Z",
        "voteCount": 5,
        "content": "A, B and E\nInspector for Lamda std scanning and code scanning\nLambda Function with monitor setting to code scan\nTag for conditional function, not for all functions"
      },
      {
        "date": "2024-10-13T09:58:00.000Z",
        "voteCount": 1,
        "content": "A: Amazon Inspector can automatically scan your Lambda functions for known vulnerabilities (CVEs) in the dependencies of the functions. This action will initiate the security scanning of Lambda functions and Lambda layers to detect vulnerabilities.\nB: Amazon Inspector provides enhanced scanning features for Lambda functions. This includes both standard scanning (for CVEs in dependencies and layers) and code scanning (for potential vulnerabilities, like data leaks, directly in the code). \nE: https://docs.aws.amazon.com/lambda/latest/dg/governance-code-scanning.html#:~:text=To%20exclude%20a%20Lambda%20function,Value%3ALambdaStandardScanning. \n\"To exclude a Lambda function from code scans, tag the function with the following key-value pair:\nKey:InspectorCodeExclusion\nValue:LambdaCodeScanning\""
      },
      {
        "date": "2024-09-14T00:35:00.000Z",
        "voteCount": 2,
        "content": "A: Need to Activate Amazon Inspector first\nB: For **CVE**, need to use **Lambda standard scanning**\nB: For **data leaks**, need to use Lambda code scanning\nE: Tag Lambda functions that do not need code scans"
      },
      {
        "date": "2024-09-06T01:08:00.000Z",
        "voteCount": 1,
        "content": "ABE, \nhttps://docs.aws.amazon.com/inspector/latest/user/scanning-lambda.html\nTo exclude a Lambda function from Lambda standard scanning, tag the function with the following key-value pair:\nKey:InspectorExclusion\nValue:LambdaStandardScanning"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 521,
    "url": "https://www.examtopics.com/discussions/amazon/view/143205-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is changing the way that it handles patching of Amazon EC2 instances in its application account. The company currently patches instances over the internet by using a NAT gateway in a VPC in the application account.<br><br>The company has EC2 instances set up as a patch source repository in a dedicated private VPC in a core account. The company wants to use AWS Systems Manager Patch Manager and the patch source repository in the core account to patch the EC2 instances in the application account. The company must prevent all EC2 instances in the application account from accessing the internet.<br><br>The EC2 instances in the application account need to access Amazon S3, where the application data is stored. These EC2 instances need connectivity to Systems Manager and to the patch source repository in the private VPC in the core account.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a network ACL that blocks outbound traffic on port 80. Associate the network ACL with all subnets in the application account. In the application account and the core account, deploy one EC2 instance that runs a custom VPN server. Create a VPN tunnel to access the private VPC. Update the route table in the application account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate private VIFs for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route table in the core account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a VPC peering connection to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a network ACL that blocks inbound traffic on port 80. Associate the network ACL with all subnets in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T03:22:00.000Z",
        "voteCount": 1,
        "content": "Aftee delete NAT gateway theres no need to block outbound port 80. Use vpc interface endpoint to keep traffic private. \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html"
      },
      {
        "date": "2024-08-31T09:55:00.000Z",
        "voteCount": 1,
        "content": "C is right \nHere's why:\n\n\n\nThe company needs to prevent all EC2 instances in the application account from accessing the internet, which means they can't use a NAT gateway.\n\nThey need to access Amazon S3 and Systems Manager, so creating VPC endpoints for these services is the way to go.\n\nA VPC peering connection between the two accounts will allow the EC2 instances in the application account to access the patch source repository in the core account.\n\nUpdating the route tables in both accounts is necessary to ensure that traffic is properly routed."
      },
      {
        "date": "2024-07-03T15:42:00.000Z",
        "voteCount": 3,
        "content": "answer : C"
      },
      {
        "date": "2024-07-02T14:29:00.000Z",
        "voteCount": 3,
        "content": "A, D\nA block of Port.80 is not enough.\nB\nprivate VIFs is inadequate.\nThe correct answer is C."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 522,
    "url": "https://www.examtopics.com/discussions/amazon/view/143548-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company in the United States (US) has acquired a company in Europe. Both companies use the AWS Cloud. The US company has built a new application with a microservices architecture. The US company is hosting the application across five VPCs in the us-east-2 Region. The application must be able to access resources in one VPC in the eu-west-1 Region.<br>However, the application must not be able to access any other VPCs.<br><br>The VPCs in both Regions have no overlapping CIDR ranges. All accounts are already consolidated in one organization in AWS Organizations.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one transit gateway in eu-west-1. Attach the VPCs in us-east-2 and the VPC in eu-west-1 to the transit gateway. Create the necessary route entries in each VPC so that the traffic is routed through the transit gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one transit gateway in each Region. Attach the involved subnets to the regional transit gateway. Create the necessary route entries in the associated route tables for each subnet so that the traffic is routed through the regional transit gateway. Peer the two transit gateways.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a full mesh VPC peering connection configuration between all the VPCs. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one VPC peering connection for each VPC in us-east-2 to the VPC in eu-west-1. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-08T02:06:00.000Z",
        "voteCount": 5,
        "content": "is most cost-effectively"
      },
      {
        "date": "2024-09-16T02:25:00.000Z",
        "voteCount": 1,
        "content": "VPC peer-to-peer connection is a free service in AWS used for communication between VPCs.\nAWS's Transit Gateway is mainly used for connecting across multiple VPCs or accounts and does not directly support cross regional VPC connections."
      },
      {
        "date": "2024-08-03T05:08:00.000Z",
        "voteCount": 1,
        "content": "Taking into account what solutions are possible, only A or B can do it, because we need a transit gateway to connect VPCs that are in different regions. You cannot peer both vpcs directly. And as for costing, A is more economic."
      },
      {
        "date": "2024-08-06T13:19:00.000Z",
        "voteCount": 2,
        "content": "After reconsidering, the answer is D. \nWith Inter-region VPC peering you can peer 2 VPCs in different regions. So, the most economic solution is D."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 523,
    "url": "https://www.examtopics.com/discussions/amazon/view/143190-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T10:17:00.000Z",
        "voteCount": 1,
        "content": "A: An Amazon SES configuration set allows you to capture event data related to email sending, such as delivery status, bounces, and complaints. By setting Amazon Kinesis Data Firehose as the destination, you can stream these logs to an Amazon S3 bucket.\nC: Once the logs are in the S3 bucket, Amazon Athena allows you to run SQL queries directly on the data stored in S3. This enables easy searching and filtering by recipient, subject, and time sent, without having to move or load the data into a database.\nB: CloudTrail logs API calls to Amazon SES but does NOT provide detailed information about email delivery, bounces, or complaints.\nD: While you can monitor metrics in CloudWatch, it\u2019s not a good fit for storing or searching detailed SES email logs.\nE: CloudWatch logs are not natively queryable using Athena."
      },
      {
        "date": "2024-09-16T02:47:00.000Z",
        "voteCount": 4,
        "content": "A. Amazon Data Firehose is configured as the target of SES configuration set, which can capture and transfer SES log data in real-time to Amazon S3 storage buckets.\nB. CloudTrail is primarily used to track AWS management operations, rather than service level operation logs.\nC. Amazon Athena allows you to directly analyze data stored in Amazon S3 using SQL queries.\nD. Amazon SES does not directly support sending logs to CloudWatch.\nE. Athena does not support directly querying logs in CloudWatch."
      },
      {
        "date": "2024-10-11T20:15:00.000Z",
        "voteCount": 1,
        "content": "Yes it does with the connector -&gt; https://docs.aws.amazon.com/athena/latest/ug/connectors-cloudwatch.html"
      },
      {
        "date": "2024-09-08T06:39:00.000Z",
        "voteCount": 1,
        "content": "Just It.\nhttps://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination-firehose.html"
      },
      {
        "date": "2024-08-15T11:16:00.000Z",
        "voteCount": 2,
        "content": "The answers seem pretty split on this. Based on this https://docs.aws.amazon.com/athena/latest/ug/querying-ses-logs.html I'd go A/C"
      },
      {
        "date": "2024-09-05T15:19:00.000Z",
        "voteCount": 1,
        "content": "I'd go A/C too."
      },
      {
        "date": "2024-07-23T06:13:00.000Z",
        "voteCount": 3,
        "content": "vote A&amp;C"
      },
      {
        "date": "2024-07-07T06:12:00.000Z",
        "voteCount": 4,
        "content": "A &amp; C is the correct answer."
      },
      {
        "date": "2024-07-05T09:07:00.000Z",
        "voteCount": 1,
        "content": "Athena can not direct query Cloudwatch log, so C is correct instead of E."
      },
      {
        "date": "2024-07-04T11:35:00.000Z",
        "voteCount": 3,
        "content": "A &amp; C. Cloudtrail does not track the emails."
      },
      {
        "date": "2024-07-04T05:06:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/messaging-and-targeting/how-to-log-amazon-ses-details-using-amazon-cloudwatch/"
      },
      {
        "date": "2024-07-08T15:17:00.000Z",
        "voteCount": 1,
        "content": "It cannot be D&amp;E because SES Event data with Cloudwatch is not able to retrieve recipient, mail headers, and timestamp, this is what it can as per this article: https://docs.aws.amazon.com/ses/latest/dg/event-publishing-retrieving-cloudwatch.html\n\nA&amp;C is a better choice as it is able to retrieve the information the questions is asking as per this article: https://docs.aws.amazon.com/ses/latest/dg/event-publishing-retrieving-firehose-contents.html"
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 524,
    "url": "https://www.examtopics.com/discussions/amazon/view/143186-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company migrated to AWS and uses AWS Business Support. The company wants to monitor the cost-effectiveness of Amazon EC2 instances across AWS accounts. The EC2 instances have tags for department, business unit, and environment. Development EC2 instances have high cost but low utilization.<br><br>The company needs to detect and stop any underutilized development EC2 instances. Instances are underutilized if they had 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon CloudWatch dashboards to monitor EC2 instance utilization based on tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Systems Manager to track EC2 instance utilization and report underutilized instances to Amazon CloudWatch. Filter the CloudWatch data by tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to run daily to retrieve utilization data for all EC2 instances. Save the data to an Amazon DynamoDB table. Create an Amazon QuickSight dashboard that uses the DynamoDB table as a data source to identify and stop underutilized development EC2 instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T10:22:00.000Z",
        "voteCount": 1,
        "content": "AWS Trusted Advisor provides insights into underutilized EC2 instances automatically, including recommendations for cost-saving based on utilization metrics like CPU and network usage. Since the company is using AWS Business Support, they already have access to Trusted Advisor, making this a low-overhead solution.\nAmazon EventBridge can be used to create a rule that detects when Trusted Advisor reports low-utilization instances. This avoids the need for custom-built CloudWatch dashboards or manual tracking.\nAWS Lambda can be triggered to handle the logic of stopping instances that meet the specific criteria of low CPU utilization and network I/O, filtering by tags for department, business unit, and environment. Lambda is serverless and scales automatically, so it minimizes operational overhead."
      },
      {
        "date": "2024-08-19T08:37:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#low-utilization-amazon-ec2-instances"
      },
      {
        "date": "2024-08-07T07:43:00.000Z",
        "voteCount": 3,
        "content": "This is exactly the same criteria provided by the Trusted Advisor:\nhttps://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#low-utilization-amazon-ec2-instances"
      },
      {
        "date": "2024-07-14T08:31:00.000Z",
        "voteCount": 2,
        "content": "C, for sure.\nTA for 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.\nAnd least operational overhead"
      },
      {
        "date": "2024-07-10T13:31:00.000Z",
        "voteCount": 1,
        "content": "A - involves continuous monitoring and potential updates to dashboards and metrics.\nC - minimizes ongoing maintenance by relying on Trusted Advisor's automated reports."
      },
      {
        "date": "2024-07-09T04:58:00.000Z",
        "voteCount": 1,
        "content": "A is not correct as it's missing setting up alarms for the \"detect\" part. I go with C."
      },
      {
        "date": "2024-07-05T08:59:00.000Z",
        "voteCount": 3,
        "content": "It would be A as correct answer\nTagging with EC2 instances for department, business unit, and environment . \n\nCloudWatch to collect and monitor CPU utilization and network I/O metrics.\n\nCreate CloudWatch Alarms to detect underutilized instances with composite alarmwith boh CPU utilization and network I/O are low.\n\nAWS Lambda Function to be triggered by the CloudWatch Alarms and check\nthe conditions (10% or less average daily CPU utilization and 5 MB or less network I/O) hold true for at least 4 of the past 14 days. Stop the instances that meet these criteria."
      },
      {
        "date": "2024-08-30T14:01:00.000Z",
        "voteCount": 1,
        "content": "Unless you create an alarm or something cloudwatch dashboard will not take any action to delete the instances, it will only show the metric data"
      },
      {
        "date": "2024-07-03T08:57:00.000Z",
        "voteCount": 1,
        "content": "Not c because AWS Trusted Advisor does not provide real-time utilization metrics suitable for detecting underutilized instances over a specific timeframe. It focuses more on best practices and recommendations rather than real-time operational metrics. Should be B"
      },
      {
        "date": "2024-07-09T12:37:00.000Z",
        "voteCount": 2,
        "content": "Will B monitor multi account ? NO  Ans is C , question says to stop of the instance is low for 4 days in last 14 days."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 525,
    "url": "https://www.examtopics.com/discussions/amazon/view/143134-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is hosting an application on AWS for a project that will run for the next 3 years. The application consists of 20 Amazon EC2 On-Demand Instances that are registered in a target group for a Network Load Balancer (NLB). The instances are spread across two Availability Zones. The application is stateless and runs 24 hours a day, 7 days a week.<br><br>The company receives reports from users who are experiencing slow responses from the application. Performance metrics show that the instances are at 10% CPU utilization during normal application use. However, the CPU utilization increases to 100% at busy times, which typically last for a few hours.<br><br>The company needs a new architecture to resolve the problem of slow responses from the application.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 20 and the desired capacity to 28. Purchase Reserved Instances for 20 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Spot Fleet that has a request type of request. Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to On-Demand. Specify the NLB when creating the Spot Fleet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Spot Fleet that has a request type of maintain. Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to Spot. Replace the NLB with an Application Load Balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 4 and the maximum capacity to 28. Purchase Reserved Instances for four instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T16:32:00.000Z",
        "voteCount": 1,
        "content": "It should be D"
      },
      {
        "date": "2024-07-02T05:43:00.000Z",
        "voteCount": 2,
        "content": "D\nD\nSince the CPU utilization of the instance is 10% during normal application use, a minimum capacity of 4 is required, which is the minimum configuration and most cost-effective."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 526,
    "url": "https://www.examtopics.com/discussions/amazon/view/143133-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.<br><br>The application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda, function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS loT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-14T11:15:00.000Z",
        "voteCount": 6,
        "content": "https://aws.amazon.com/blogs/iot/ingesting-enriched-iot-data-into-amazon-s3-using-amazon-kinesis-data-firehose/"
      },
      {
        "date": "2024-07-14T05:26:00.000Z",
        "voteCount": 5,
        "content": "In this application, sensor data is transmitted at the following intervals:\n\n1. Device to IoT Core (every 5 seconds)\n2. IoT Core to S3 (every 30 minutes)\n\nThe data load from IoT Core to S3 doesn't necessarily need to be real-time, and the most cost-effective solution is option A. Option A uses the simplest method to load data without using resources like Kinesis."
      },
      {
        "date": "2024-08-30T14:06:00.000Z",
        "voteCount": 2,
        "content": "Answer is C, hundreds of devices sending data every 5 seconds, if you have 100 devices you will trigger the lambda 1200 times in one minute,"
      },
      {
        "date": "2024-09-17T19:38:00.000Z",
        "voteCount": 1,
        "content": "No other applications are processing the sensor data from AWS IoT Core: \nUse AWS IoT Core Basic Ingest to ingest the sensor data to reduce messaging cost: B or D\nhttps://docs.aws.amazon.com/iot/latest/developerguide/iot-basic-ingest.html\n\nConfigure an AWS IoT rule action to write the data to Amazon KDF or  KDS? \"New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data.\" =&gt;near real time, stream data to s3, no need storage or replay, we shd use autoscaling and fully managed KDF."
      },
      {
        "date": "2024-09-16T05:34:00.000Z",
        "voteCount": 1,
        "content": "A. The advantage of this method is its simplicity and high real-time performance, as Lambda functions can immediately respond to IoT events. The cost of Lambda functions is based on execution time and resource usage, which is very economical for small data processing tasks.\nB. The 900 second buffer interval of Kinesis Data Firehose does not meet real-time requirements (data needs to be processed within 30 minutes, while the set buffer here is 15 minutes). In addition, introducing Kinesis Data Firehose adds additional cost and complexity, especially when Lambda functions can directly process data."
      },
      {
        "date": "2024-09-05T16:31:00.000Z",
        "voteCount": 1,
        "content": "Many devices sending data every 5 seconds, it's not necessary, due that you just need the data available in S3 within 30 minutes!"
      },
      {
        "date": "2024-08-30T06:01:00.000Z",
        "voteCount": 1,
        "content": "the data emitting time is 5 sec and lambda may take upto 15 mins to enrich the data , this detail is only captured in buffer section of KFS , hence going with B, If there is SQS queue in option A before lambda then i would have choosen that"
      },
      {
        "date": "2024-08-15T05:03:00.000Z",
        "voteCount": 2,
        "content": "As per this link it is B, firehose is used:\nhttps://aws.amazon.com/blogs/iot/ingesting-enriched-iot-data-into-amazon-s3-using-amazon-kinesis-data-firehose/"
      },
      {
        "date": "2024-08-13T04:03:00.000Z",
        "voteCount": 2,
        "content": "We need simple and cost effective so choosing A"
      },
      {
        "date": "2024-07-31T00:34:00.000Z",
        "voteCount": 3,
        "content": "I prefer B"
      },
      {
        "date": "2024-07-22T16:46:00.000Z",
        "voteCount": 4,
        "content": "Best Answer: B. Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.\nReasoning:\nCost-effective: IoT Core Basic Ingest is the most cost-effective option for high-volume, low-value data.\nLow latency: Kinesis Data Firehose with a 900-second buffering interval provides a balance between cost and latency, meeting the requirement of data availability in S3 within 30 minutes.\nScalability: Kinesis Data Firehose can handle high throughput, making it suitable for large volumes of sensor data.\nSimplicity: The solution involves a straightforward pipeline with minimal components."
      },
      {
        "date": "2024-07-20T10:35:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-07-04T06:50:00.000Z",
        "voteCount": 4,
        "content": "Vote A because only required minimum services are involved. IoT core topic to hold income data, Lambda to enrich data and save to s3. IoT rule call Lambda and consume the incoming data."
      },
      {
        "date": "2024-07-04T06:22:00.000Z",
        "voteCount": 1,
        "content": "D is a typical scenario"
      },
      {
        "date": "2024-07-02T14:12:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect.\nC is correct.\nAWS loT Core Basic Ingest \u2192 Cost optimization\nAmazon Kinesis Data Streams \u2192 Send every 5 seconds"
      },
      {
        "date": "2024-07-01T06:55:00.000Z",
        "voteCount": 3,
        "content": "A\nSimple and most cost-effective."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 527,
    "url": "https://www.examtopics.com/discussions/amazon/view/143132-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company is collecting data from a large set of IoT devices. The data is stored in an Amazon S3 data lake. Data scientists perform analytics on Amazon EC2 instances that run in two public subnets in a VPC in a separate AWS account.<br><br>The data scientists need access to the data lake from the EC2 instances. The EC2 instances already have an assigned role with permissions to access Amazon S3.<br>According to company policies, only authorized networks are allowed to have access to the IoT data.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway VPC endpoint for Amazon S3 in the data scientists\u2019 VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 access point in the data scientists' AWS account for the data lake.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EC2 instance role. Add a policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the VPC route table to route S3 traffic to an S3 access point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T02:23:00.000Z",
        "voteCount": 1,
        "content": "This question is really bad.\n\nIt feels like if A is selected, then E needs to be adjusted to enable access between VPC endpoints and the bucket directly\n\nOr if B is selected, then B needs to be reworded to say creating access point in data lake account, then E would be valid without any modification"
      },
      {
        "date": "2024-08-18T13:19:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E are correct options. A isn't correct because gateway VPC endpoint doesn't work outside of VPC. In this question, we are talking about 2 different accounts which implies 2 different VPCs as well"
      },
      {
        "date": "2024-08-16T20:56:00.000Z",
        "voteCount": 2,
        "content": "S3 Access Point should be created in destination account.\n\nYou need VPC endpoint to keep the network private.\n\nThis question might just assumed that the S3 access point is already created in destination account"
      },
      {
        "date": "2024-08-05T09:07:00.000Z",
        "voteCount": 3,
        "content": "S3 access point is used If you want to share your bucket with other accounts"
      },
      {
        "date": "2024-09-14T04:22:00.000Z",
        "voteCount": 1,
        "content": "You can also create a cross-account access point that's associated with a bucket in another AWS account, as long as you know the bucket name and the bucket owner's account ID. However, creating cross-account access points doesn't grant you access to data in the bucket until you are granted permissions from the bucket owner. The bucket owner must grant the access point owner's account (your account) access to the bucket through the bucket policy. For more information, see Granting permissions for cross-account access points.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/create-access-points.html"
      },
      {
        "date": "2024-07-30T22:39:00.000Z",
        "voteCount": 2,
        "content": "Gateway endpoint do not work cross account, so BE.\nowever, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway. For those scenarios, you must use an interface endpoint, which is available for an additional cost.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
      },
      {
        "date": "2024-07-25T18:42:00.000Z",
        "voteCount": 3,
        "content": "Anyone who is picking A/E - please realize DataAccessPointArn  ONLY WORKS when there is an access point created. \nA does NOT mention creating an Access Point. B is completely possible and combine with E restricts all traffic coming from the VPC that has the acccess point mentioned in B. \nB+E is the correct answer"
      },
      {
        "date": "2024-08-20T00:27:00.000Z",
        "voteCount": 2,
        "content": "B is completely wrong because the access point is created in the wrong account.\n\nYou need the access point to be created in the source s3 bucket account"
      },
      {
        "date": "2024-08-29T01:05:00.000Z",
        "voteCount": 1,
        "content": "I just try it. You can create a cross-account s3 access point"
      },
      {
        "date": "2024-07-24T00:42:00.000Z",
        "voteCount": 1,
        "content": "a,d gateway VPC endpoint needs config route table"
      },
      {
        "date": "2024-07-20T00:32:00.000Z",
        "voteCount": 1,
        "content": "A, E are correct"
      },
      {
        "date": "2024-07-13T11:17:00.000Z",
        "voteCount": 2,
        "content": "A, E for sure.\nOnly authorized networks are allowed to have access to the IoT data."
      },
      {
        "date": "2024-07-10T08:22:00.000Z",
        "voteCount": 3,
        "content": "Need access from different AWS account with restrictions. So it is BE"
      },
      {
        "date": "2024-07-03T16:43:00.000Z",
        "voteCount": 4,
        "content": "A. This step ensures that the traffic between the EC2 instances and the S3 data lake does not traverse the public internet, thereby meeting security requirements and reducing latency.\nE. This step ensures that the access to the data lake is restricted according to company policies. It leverages an S3 bucket policy to enforce access control based on specific conditions, thereby providing an additional layer of security."
      },
      {
        "date": "2024-07-03T16:42:00.000Z",
        "voteCount": 1,
        "content": "A. This step ensures that the traffic between the EC2 instances and the S3 data lake does not traverse the public internet, thereby meeting security requirements and reducing latency.\nC. This step ensures that the access to the data lake is restricted according to company policies. It leverages an S3 bucket policy to enforce access control based on specific conditions, thereby providing an additional layer of security."
      },
      {
        "date": "2024-07-01T06:51:00.000Z",
        "voteCount": 2,
        "content": "B\nS3 access points allow fine-grained control of access policies and network settings for specific S3 buckets.\nE\ns3:DataAccessPointArn must be used to set permissions on the S3 bucket side for going through the access point. role settings in C do not have settings to determine the access point on the bucket side."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 528,
    "url": "https://www.examtopics.com/discussions/amazon/view/143130-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "A company wants to migrate its website to AWS. The website uses containers that are deployed in an on-premises, self-managed Kubernetes cluster. All data for the website is stored in an on-premises PostgreSQL database.<br><br>The company has decided to migrate the on-premises Kubernetes cluster to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster will use EKS managed node groups with a static number of nodes. The company will also migrate the on-premises database to an Amazon RDS for PostgreSQL database.<br><br>A solutions architect needs to estimate the total cost of ownership (TCO) for this workload before the migration.<br><br>Which solution will provide the required TCO information?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch AWS Database Migration Service (AWS DMS) for the on-premises database. Generate an assessment report. Create an estimate in AWS Pricing Calculator for the costs of the EKS migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInitialize AWS Application Migration Service. Add the on-premises servers as source servers. Launch a test instance. Output a TCO report from Application Migration Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess the AWS Cloud Economics Center webpage to assess the AWS Cloud Value Framework. Create an AWS Cost and Usage report from the Cloud Value Framework."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-16T21:05:00.000Z",
        "voteCount": 3,
        "content": "Quick catch: when you see TCO, think about Migration Evaluator"
      },
      {
        "date": "2024-07-23T00:59:00.000Z",
        "voteCount": 2,
        "content": "Best Answer: A. Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.\nReasoning:\nComprehensive TCO Analysis: Migration Evaluator is specifically designed to assess migration projects and provides detailed cost estimates.\nAccurate Data: By collecting data from the on-premises environment, Migration Evaluator can generate more accurate cost estimates.\nScenario Modeling: The ability to configure scenarios allows for testing different migration options and their associated costs.\nQuick Insights Report: This provides a summarized overview of the potential TCO."
      },
      {
        "date": "2024-07-20T00:31:00.000Z",
        "voteCount": 1,
        "content": "B:Estimate AWS service costs\t\nA: Assess current environment and plan migration to AWS"
      },
      {
        "date": "2024-07-16T21:51:00.000Z",
        "voteCount": 2,
        "content": "Option B is incorrect because AWS Database Migration Service (AWS DMS) is used for migrating databases, not for estimating the TCO. Additionally, the AWS Pricing Calculator alone cannot provide a comprehensive TCO analysis for a complex migration scenario involving Kubernetes and databases.\nOption C is incorrect because AWS Application Migration Service is primarily used for migrating and modernizing applications, not for estimating the TCO of a migration."
      },
      {
        "date": "2024-07-10T14:12:00.000Z",
        "voteCount": 1,
        "content": "Option B (AWS DMS assessment + AWS Pricing Calculator) is typically more appropriate and practical."
      },
      {
        "date": "2024-07-12T10:41:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/aws-pricingtco-tools.html"
      },
      {
        "date": "2024-07-01T06:15:00.000Z",
        "voteCount": 3,
        "content": "A\nMigration Evaluator is used to estimate TCO."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 529,
    "url": "https://www.examtopics.com/discussions/amazon/view/143123-exam-aws-certified-solutions-architect-professional-sap-c02/",
    "body": "An events company runs a ticketing platform on AWS. The company\u2019s customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer\u2019s events.<br><br>The company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.<br><br>The ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.<br><br>The company needs to optimize the cost of the platform without decreasing the platform's availability.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway VPC endpoint for the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Transfer Acceleration on the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the predictive scaling policy with scheduled scaling policies for the scheduled events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-30T18:25:00.000Z",
        "voteCount": 5,
        "content": "Options A and E will meet the requirements most cost-effectively by leveraging the predictability of the workload of known customer events to optimize scaling operations and reducing data transfer costs"
      },
      {
        "date": "2024-09-27T02:27:00.000Z",
        "voteCount": 1,
        "content": "A. Create a gateway VPC endpoint for the S3 bucket: This will allow the ECS cluster to access the S3 bucket directly without the need for traffic to flow through the NAT gateway, reducing costs associated with NAT data transfer.\n\nB. Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances: By introducing Spot Instances with the same weight as On-Demand Instances, the company can take advantage of the cost savings from Spot Instances while maintaining the ability to scale with On-Demand Instances when needed.\n\nThese steps will reduce network traffic costs and take advantage of lower-cost compute options without compromising platform availability."
      },
      {
        "date": "2024-07-05T05:46:00.000Z",
        "voteCount": 2,
        "content": "Since it is scheduled event , E is correct ans"
      },
      {
        "date": "2024-07-05T05:03:00.000Z",
        "voteCount": 2,
        "content": "A: no charge for S3 access\nE: know details of date and time --- can scheduled for saving cost"
      },
      {
        "date": "2024-07-01T05:58:00.000Z",
        "voteCount": 3,
        "content": "A\nFor S3 communication in the same region, the communication fee is waived by using the gateway VPC endpoint.\nB\nAvailability is reduced when spot instances are used.\nC\nUsing on-demand capacity reservation increases costs.\nD\nUsing S3 Transfer Acceleration increases costs.\nE\nScheduled scaling policies allow resources to be used according to events.\n\nThe answers are A and E."
      }
    ],
    "examNameCode": "aws-certified-solutions-architect-professional-sap-c02",
    "topicNumber": "notopic"
  }
]