[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/25182-exam-aws-certified-database-specialty-topic-1-question-1/",
    "body": "A company has deployed an e-commerce web application in a new AWS account. An Amazon RDS for MySQL Multi-AZ DB instance is part of this deployment with a database-1.xxxxxxxxxxxx.us-east-1.rds.amazonaws.com endpoint listening on port 3306. The company's Database Specialist is able to log in to MySQL and run queries from the bastion host using these details.<br>When users try to utilize the application hosted in the AWS account, they are presented with a generic error message. The application servers are logging a `could not connect to server: Connection times out` error message to Amazon CloudWatch Logs.<br>What is the cause of this error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe user name and password the application is using are incorrect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe security group assigned to the application servers does not have the necessary rules to allow inbound connections from the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe security group assigned to the DB instance does not have the necessary rules to allow inbound connections from the application servers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe user name and password are correct, but the user is not authorized to use the DB instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-06T17:11:00.000Z",
        "voteCount": 7,
        "content": "Correct Answer: C"
      },
      {
        "date": "2024-04-01T07:04:00.000Z",
        "voteCount": 1,
        "content": "B. Since requirement is of handling random activity peaks"
      },
      {
        "date": "2023-08-19T17:22:00.000Z",
        "voteCount": 1,
        "content": "C. The security group assigned to the DB instance does not have the necessary rules to allow inbound connections from the application servers."
      },
      {
        "date": "2023-05-10T00:45:00.000Z",
        "voteCount": 2,
        "content": "I got this question in the exam:\nan aurora cluster has some replica in current region and other regions\nthe admin accidentally  delete primary cluster.\na. all replica are promoted to standalone\nb. all replica in other regions are promoted\nc. only replica in current region are promoted\nd. dont remember"
      },
      {
        "date": "2023-05-21T20:52:00.000Z",
        "voteCount": 1,
        "content": "Do you know what the correct answer was?"
      },
      {
        "date": "2023-09-18T07:27:00.000Z",
        "voteCount": 1,
        "content": "please try to remember option D"
      },
      {
        "date": "2022-04-28T19:23:00.000Z",
        "voteCount": 2,
        "content": "DB needs to allow inbound"
      },
      {
        "date": "2022-04-17T06:46:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: C"
      },
      {
        "date": "2021-11-06T02:23:00.000Z",
        "voteCount": 1,
        "content": "Anyone passed \"AWS Certified Database - Specialty\" exam recently, could you please share your experience of actual exam??\nHow many Questions were came from examtopics Q set of 145 in real exam?\n\nYour comments will be highly appreciated.. Thanks.."
      },
      {
        "date": "2021-11-07T04:46:00.000Z",
        "voteCount": 2,
        "content": "I gave exam today. Only 40 questions came from here out of 65."
      },
      {
        "date": "2021-11-26T19:35:00.000Z",
        "voteCount": 2,
        "content": "Guru - 40 questions from out of 112 or 149? I can able to access only 112. Can you possible to share the dump if you have 149 questions."
      },
      {
        "date": "2021-11-05T21:44:00.000Z",
        "voteCount": 1,
        "content": "Basically, it is a connection timeout error, so A, D can be deleted.\nB is wrong, because if it has a problem in SG rules, it should affect at outbound not inbound,\nHence C is correct, which is the inbound rule for DB instance SG."
      },
      {
        "date": "2021-11-06T09:01:00.000Z",
        "voteCount": 1,
        "content": "Are you planning for exam?\nWe can share study material, it would be beneficial for both. You can email me on \"awsdbguru at gmail\""
      },
      {
        "date": "2021-11-04T19:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nAnyone preparing for DB Speciality and want to do group study with us, comment below with email"
      },
      {
        "date": "2021-11-04T17:48:00.000Z",
        "voteCount": 2,
        "content": "its C for me"
      },
      {
        "date": "2021-11-03T05:49:00.000Z",
        "voteCount": 1,
        "content": "C ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-10-31T11:31:00.000Z",
        "voteCount": 1,
        "content": "It's C. I actually had to configure SG on my MYSQL instance to allow me connect from home PC with a SQL client :)"
      },
      {
        "date": "2021-10-27T12:32:00.000Z",
        "voteCount": 1,
        "content": "A company has an application that uses an Amazon DynamoDB table to store user dat\na. Every morning, a single-threaded process calls the DynamoDB API Scan operation to scan the entire table and generate a critical start-of-day report for management. A successful marketing campaign recently doubled the number of items in the table, and now the process takes too long to run and the report is not generated in time. A database specialist needs to improve the performance of the process. The database specialist notes that, when the process is running, 15% of the table\u2019s provisioned read capacity units (RCUs) are being used.\n\nWhat should the database specialist do?\nA. Enable auto scaling for the DynamoDB table.\nB. Use four threads and parallel DynamoDB API Scan operations.\nC. Double the table\u2019s provisioned RCUs.\nD. Set the Limit and Offset parameters before every call to the API."
      },
      {
        "date": "2021-10-28T12:20:00.000Z",
        "voteCount": 1,
        "content": "Looks like B"
      },
      {
        "date": "2021-10-30T16:45:00.000Z",
        "voteCount": 1,
        "content": "Yep, B looks right. Since autoscaling would increase the WCU/RCU which is not needed."
      },
      {
        "date": "2021-10-27T06:38:00.000Z",
        "voteCount": 1,
        "content": "A company is running an on-premises application comprised of a web tier, an application tier, and a\nMySQL database tier. The database is used primarily during business hours with random activity peaks\nthroughout the day. A database specialist needs to improve the availability and reduce the cost of the\nMySQL database tier as part of the company\u2019s migration to AWS.\nWhich MySQL database option would meet these requirements?\nA. Amazon RDS for MySQL with Multi-AZ\nB. Amazon Aurora Serverless MySQL cluster\nC. Amazon Aurora MySQL cluster\nD. Amazon RDS for MySQL with read replica"
      },
      {
        "date": "2021-10-29T04:34:00.000Z",
        "voteCount": 2,
        "content": "random activity peaks -&gt; Aurora Serverless -&gt; B"
      },
      {
        "date": "2021-12-26T09:51:00.000Z",
        "voteCount": 1,
        "content": "Should be B"
      },
      {
        "date": "2021-10-21T14:44:00.000Z",
        "voteCount": 1,
        "content": "A company is running an on-premises application comprised of a web tier, an application tier, and a\nMySQL database tier. The database is used primarily during business hours with random activity peaks\nthroughout the day. A database specialist needs to improve the availability and reduce the cost of the\nMySQL database tier as part of the company\u2019s migration to AWS.\nWhich MySQL database option would meet these requirements?\nA. Amazon RDS for MySQL with Multi-AZ\nB. Amazon Aurora Serverless MySQL cluster\nC. Amazon Aurora MySQL cluster\nD. Amazon RDS for MySQL with read replica"
      },
      {
        "date": "2021-10-21T13:53:00.000Z",
        "voteCount": 1,
        "content": "A database specialist needs to review and optimize an Amazon DynamoDB table that is experiencing performance issues. A thorough investigation by the database specialist reveals that the partition key is causing hot partitions, so a new partition key is created. The database specialist must effectively apply this new partition key to all existing and new data.\nHow can this solution be implemented?\nA. Use Amazon EMR to export the data from the current DynamoDB table to Amazon S3. Then use Amazon EMR again to import the data from Amazon S3 into a new DynamoDB table with the new partition key.\nB. Use AWS DMS to copy the data from the current DynamoDB table to Amazon S3. Then import the DynamoDB table to create a new DynamoDB table with the new partition key.\nC. Use the AWS CLI to update the DynamoDB table and modify the partition key.\nD. Use the AWS CLI to back up the DynamoDB table. Then use the restore-table-from-backup command and modify the partition key."
      },
      {
        "date": "2021-10-23T12:43:00.000Z",
        "voteCount": 1,
        "content": "B. Since requirement is of handling random activity peaks"
      },
      {
        "date": "2021-10-25T21:12:00.000Z",
        "voteCount": 1,
        "content": "Ignore the above answer. It's for another question :)"
      },
      {
        "date": "2021-10-29T08:19:00.000Z",
        "voteCount": 4,
        "content": "DDB cannot change Partition key of a table -&gt; C/D wrong.\nDMS cannot choose DDB as source -&gt; B wrong.\nSo A should be the correct one although it appears to need a lot of efforts."
      },
      {
        "date": "2021-10-20T12:16:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/67312-exam-aws-certified-database-specialty-topic-1-question-2/",
    "body": "An AWS CloudFormation stack that included an Amazon RDS DB instance was accidentally deleted and recent data was lost. A Database Specialist needs to add<br>RDS settings to the CloudFormation template to reduce the chance of accidental instance data loss in the future.<br>Which settings will meet this requirement? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet DeletionProtection to True\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet MultiAZ to True",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet TerminationProtection to True",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet DeleteAutomatedBackups to False\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet DeletionPolicy to Delete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet DeletionPolicy to Retain\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "CDF",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-22T12:24:00.000Z",
        "voteCount": 15,
        "content": "A - https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-rds-now-provides-database-deletion-protection/\nD - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\nF - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"
      },
      {
        "date": "2023-10-14T15:12:00.000Z",
        "voteCount": 1,
        "content": "got this Q in the exam"
      },
      {
        "date": "2023-07-27T09:15:00.000Z",
        "voteCount": 1,
        "content": "ADF\naccording to this link https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbinstance.html"
      },
      {
        "date": "2022-10-23T09:38:00.000Z",
        "voteCount": 1,
        "content": "ADF is the right one"
      },
      {
        "date": "2022-09-21T12:32:00.000Z",
        "voteCount": 1,
        "content": "TerminationProtection is a delete protection on CloudFormation instead of RDS. So ADF are correct answers."
      },
      {
        "date": "2022-07-05T18:49:00.000Z",
        "voteCount": 1,
        "content": "ADF is right . Deletion Policy ;- retain in for any resource attached to RDS ."
      },
      {
        "date": "2022-06-26T15:27:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"
      },
      {
        "date": "2022-04-30T09:08:00.000Z",
        "voteCount": 1,
        "content": "A. Set DeletionProtection to True\nx B. Set MultiAZ to True\nx C. Set TerminationProtection to True (it is for stack deletion)\nD. Set DeleteAutomatedBackups to False\nx E. Set DeletionPolicy to Delete\nF. Set DeletionPolicy to Retain"
      },
      {
        "date": "2022-03-05T13:07:00.000Z",
        "voteCount": 3,
        "content": "Very frequently asked question used  in different question banks.\nSet DeletionProtection of the Stack\nPrevent Automated Backups from being deleted\nSet deletion policy to Retain (default is snapshot)"
      },
      {
        "date": "2023-12-31T01:32:00.000Z",
        "voteCount": 1,
        "content": "The default of DeleteionPolicy may be \"delete\", which delete the resources"
      },
      {
        "date": "2022-02-04T07:06:00.000Z",
        "voteCount": 4,
        "content": "Are we not applying this to stack so should be C,D,F..not to RDS..there is no deletion protection at stack level"
      },
      {
        "date": "2022-04-30T09:07:00.000Z",
        "voteCount": 1,
        "content": "ADF\nhttps://aws.amazon.com/about-aws/whats-new/2017/09/aws-cloudformation-provides-stack-termination-protection/#:~:text=AWS%20CloudFormation%20now%20allows%20you,its%20status%2C%20will%20remain%20unchanged."
      },
      {
        "date": "2022-01-09T18:30:00.000Z",
        "voteCount": 1,
        "content": "the answer is ADF"
      },
      {
        "date": "2022-01-08T19:05:00.000Z",
        "voteCount": 1,
        "content": "C, D &amp; F"
      },
      {
        "date": "2021-12-09T10:52:00.000Z",
        "voteCount": 3,
        "content": "ACF is the correct answer.\nYou need  Set TerminationProtection to True for the RDS instance"
      },
      {
        "date": "2021-12-22T12:22:00.000Z",
        "voteCount": 3,
        "content": "RDS doesn't have TerminationProtection .. It has DeletionProtection. So, the correct options are A,D,F"
      },
      {
        "date": "2023-09-09T17:44:00.000Z",
        "voteCount": 1,
        "content": "TerminationProtection setting applies to stack instead of RDS instance."
      },
      {
        "date": "2021-12-07T06:14:00.000Z",
        "voteCount": 4,
        "content": "Option A, D, F\n\nRDS doenst have termination protection, only deletion protection"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/25011-exam-aws-certified-database-specialty-topic-1-question-3/",
    "body": "A Database Specialist is troubleshooting an application connection failure on an Amazon Aurora DB cluster with multiple Aurora Replicas that had been running with no issues for the past 2 months. The connection failure lasted for 5 minutes and corrected itself after that. The Database Specialist reviewed the Amazon<br>RDS events and determined a failover event occurred at that time. The failover process took around 15 seconds to complete.<br>What is the MOST likely cause of the 5-minute connection outage?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter a database crash, Aurora needed to replay the redo log from the last database checkpoint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe client-side application is caching the DNS data and its TTL is set too high\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter failover, the Aurora DB cluster needs time to warm up before accepting client connections",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere were no active Aurora Replicas in the Aurora DB cluster"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T20:33:00.000Z",
        "voteCount": 17,
        "content": "Answer: B\nThe question indicates failover has already taken place. The TTL is set too high."
      },
      {
        "date": "2021-11-01T13:16:00.000Z",
        "voteCount": 3,
        "content": "Agree, B. TTL."
      },
      {
        "date": "2024-03-13T15:36:00.000Z",
        "voteCount": 1,
        "content": "examtopics.com : please explain your answers C."
      },
      {
        "date": "2023-09-14T14:05:00.000Z",
        "voteCount": 1,
        "content": "B. The client-side application is caching the DNS data and its TTL is set too high"
      },
      {
        "date": "2023-01-25T11:35:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-redirected-endpoint-writer/"
      },
      {
        "date": "2023-01-17T05:04:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-01-14T04:20:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2022-11-02T14:22:00.000Z",
        "voteCount": 1,
        "content": "B is corret"
      },
      {
        "date": "2022-10-23T09:41:00.000Z",
        "voteCount": 1,
        "content": "B is the right one."
      },
      {
        "date": "2022-09-23T05:39:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-04-30T15:03:00.000Z",
        "voteCount": 2,
        "content": "B. The client-side application is caching the DNS data and its TTL is set too high\n\nIf your client application is caching the Domain Name Service (DNS) data of your DB instances, set a time-to-live (TTL) value of less than 30 seconds. Because the underlying IP address of a DB instance can change after a failover, caching the DNS data for an extended time can lead to connection failures if your application tries to connect to an IP address that no longer is in service."
      },
      {
        "date": "2022-03-07T04:50:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it). B is correct"
      },
      {
        "date": "2023-04-24T12:12:00.000Z",
        "voteCount": 1,
        "content": "how would you know that?\nin the result there is passed / not passed and a section %%"
      },
      {
        "date": "2022-02-25T18:59:00.000Z",
        "voteCount": 2,
        "content": "B\nIf your client application is caching the Domain Name Service (DNS) data of your DB instances, set a time-to-live (TTL) value of less than 30 seconds. Because the underlying IP address of a DB instance can change after a failover, caching the DNS data for an extended time can lead to connection failures if your application tries to connect to an IP address that no longer is in service."
      },
      {
        "date": "2021-11-06T10:43:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer, cache needs to be set for the lowest possible time."
      },
      {
        "date": "2021-11-04T22:14:00.000Z",
        "voteCount": 1,
        "content": "B ==&gt;&gt; Correct Answer"
      },
      {
        "date": "2021-11-04T18:10:00.000Z",
        "voteCount": 1,
        "content": "B; in Stephane's course he talks about Aurora-Postgres settings to keep client caching at minimum. Since there were Read Replicas in place, Aurora-side failover is &lt;1 min"
      },
      {
        "date": "2021-11-04T15:41:00.000Z",
        "voteCount": 2,
        "content": "Why \"Reveal Solution\" answer is mostly wrong ?  When i come to discussion i see the answers given by others which look correct/logical.  Is \"Reveal solution\" not reliable ?"
      },
      {
        "date": "2021-12-23T20:40:00.000Z",
        "voteCount": 2,
        "content": "\"Reveal solution\" is not reliable. Always read the discussion."
      },
      {
        "date": "2021-10-30T01:23:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/25029-exam-aws-certified-database-specialty-topic-1-question-4/",
    "body": "A company is deploying a solution in Amazon Aurora by migrating from an on-premises system. The IT department has established an AWS Direct Connect link from the company's data center. The company's Database Specialist has selected the option to require SSL/TLS for connectivity to prevent plaintext data from being set over the network. The migration appears to be working successfully, and the data can be queried from a desktop machine.<br>Two Data Analysts have been asked to query and validate the data in the new Aurora DB cluster. Both Analysts are unable to connect to Aurora. Their user names and passwords have been verified as valid and the Database Specialist can connect to the DB cluster using their accounts. The Database Specialist also verified that the security group configuration allows network from all corporate IP addresses.<br>What should the Database Specialist do to correct the Data Analysts' inability to connect?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestart the DB cluster to apply the SSL change.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the Data Analysts to download the root certificate and use the SSL certificate on the connection string to connect.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd explicit mappings between the Data Analysts' IP addresses and the instance in the security group assigned to the DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Data Analysts' local client firewall to allow network traffic to AWS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-14T14:23:00.000Z",
        "voteCount": 3,
        "content": "B. Instruct the Data Analysts to download the root certificate and use the SSL certificate on the connection string to connect.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/ssl-certificate-rotation-aurora-postgresql.html"
      },
      {
        "date": "2023-01-17T05:06:00.000Z",
        "voteCount": 4,
        "content": "Author from the Udemy.com practice test says B is the correct answer."
      },
      {
        "date": "2022-10-25T09:05:00.000Z",
        "voteCount": 1,
        "content": "B is the right one"
      },
      {
        "date": "2022-09-23T05:38:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2022-04-30T18:46:00.000Z",
        "voteCount": 2,
        "content": "B. Instruct the Data Analysts to download the root certificate and use the SSL certificate on the connection string to connect.\n\n To connect using SSL:\n\u2022 Provide the SSLTrust certificate (can be downloaded from AWS)\n\u2022 Provide SSL options when connecting to database\n\u2022 Not using SSL on a DB that enforces SSL would result in error\n\nB - Need root certificate and then need to specify --sql-ca = cert.pem --ssl-mode=verify_identity for example mysql\n\nWhen the require_secure_transport parameter is set to ON for a DB cluster, a database client can connect to it if it can establish an encrypted connection. Otherwise, an error message similar to the following is returned to the client:\n\nMySQL Error 3159 (HY000): Connections using insecure transport are prohibited while --require_secure_transport=ON."
      },
      {
        "date": "2022-03-06T10:11:00.000Z",
        "voteCount": 3,
        "content": "The answer is (B). what I am beginning to REALLY dislike  about some of these questions is the terrible grammar. This question is very similar to another question bank where the Database specialist is able to connect to the Aurora Cluster. The english they are using is \"Specialist MAY use their account to log in\" - this is artificially vague. in the pressure of time - such english just leaves a bad taste and its clear  many of the questions are not written by native english language speakers - which makes it frustrating"
      },
      {
        "date": "2021-12-28T02:50:00.000Z",
        "voteCount": 3,
        "content": "My point: Usually, you have no privileges to modify local firewall policy in a big cooperate. In the question, it has mentioned the connection can be made from desktop that means local firewall rules allow to access RDS."
      },
      {
        "date": "2021-11-07T04:41:00.000Z",
        "voteCount": 1,
        "content": "its B, SSL issue."
      },
      {
        "date": "2021-11-05T12:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer  ==&gt;&gt; B"
      },
      {
        "date": "2021-11-04T14:01:00.000Z",
        "voteCount": 1,
        "content": "B, you need a cert"
      },
      {
        "date": "2021-11-03T21:31:00.000Z",
        "voteCount": 3,
        "content": "Must be B, SSL issue."
      },
      {
        "date": "2021-11-03T21:03:00.000Z",
        "voteCount": 1,
        "content": "B\n\n\u2022 To connect using SSL:\n\u2022 Provide the SSLTrust certificate (can be downloaded from AWS)\n\u2022 Provide SSL options when connecting to database\n\u2022 Not using SSL on a DB that enforces SSL would result in error"
      },
      {
        "date": "2021-10-24T17:00:00.000Z",
        "voteCount": 1,
        "content": "D is not right this. The questions is on SSL/TLS encryption in transit - \n\nB - Need root certificate and then need to specify --sql-ca = cert.pem --ssl-mode=verify_identity for example mysql"
      },
      {
        "date": "2021-10-22T03:20:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-21T10:18:00.000Z",
        "voteCount": 1,
        "content": "When the require_secure_transport parameter is set to ON for a DB cluster, a database client can connect to it if it can establish an encrypted connection. Otherwise, an error message similar to the following is returned to the client:\n\nMySQL Error 3159 (HY000): Connections using insecure transport are prohibited while --require_secure_transport=ON."
      },
      {
        "date": "2021-10-20T16:55:00.000Z",
        "voteCount": 2,
        "content": "D. Aurora MySQL DB clusters must be created in an Amazon Virtual Private Cloud (VPC). To control which devices and Amazon EC2 instances can open connections to the endpoint and port of the DB instance for Aurora MySQL DB clusters in a VPC, you use a VPC security group. These endpoint and port connections can be made using Secure Sockets Layer (SSL). In addition, firewall rules at your company can control whether devices running at your company can open connections to a DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Security.html"
      },
      {
        "date": "2021-10-16T22:10:00.000Z",
        "voteCount": 3,
        "content": "Answer is B\nAs SSL parameter can be used in string \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/ssl-certificate-rotation-aurora-postgresql.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/25030-exam-aws-certified-database-specialty-topic-1-question-5/",
    "body": "A company is concerned about the cost of a large-scale, transactional application using Amazon DynamoDB that only needs to store data for 2 days before it is deleted. In looking at the tables, a Database Specialist notices that much of the data is months old, and goes back to when the application was first deployed.<br>What can the Database Specialist do to reduce the overall cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new attribute in each table to track the expiration time and create an AWS Glue transformation to delete entries more than 2 days old.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new attribute in each table to track the expiration time and enable DynamoDB Streams on each table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new attribute in each table to track the expiration time and enable time to live (TTL) on each table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Events event to export the data to Amazon S3 daily using AWS Data Pipeline and then truncate the Amazon DynamoDB table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T17:28:00.000Z",
        "voteCount": 13,
        "content": "Answer C. Enable TTL on a new attribute."
      },
      {
        "date": "2023-09-14T14:30:00.000Z",
        "voteCount": 2,
        "content": "C. Create a new attribute in each table to track the expiration time and enable time to live (TTL) on each table.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
      },
      {
        "date": "2023-09-01T07:43:00.000Z",
        "voteCount": 1,
        "content": "Enable TTL on a new attribute."
      },
      {
        "date": "2023-03-27T23:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-before-you-start.html correct answer C"
      },
      {
        "date": "2023-02-19T02:24:00.000Z",
        "voteCount": 1,
        "content": "Answer C. Enable TTL on a new attribute."
      },
      {
        "date": "2023-01-25T11:40:00.000Z",
        "voteCount": 2,
        "content": "Answer:C"
      },
      {
        "date": "2023-01-17T05:27:00.000Z",
        "voteCount": 2,
        "content": "Author from the Udemy.com practice test says C is the correct answer."
      },
      {
        "date": "2022-12-14T08:59:00.000Z",
        "voteCount": 1,
        "content": "Can someone please explain why A was a better solution for cost?"
      },
      {
        "date": "2022-06-23T06:05:00.000Z",
        "voteCount": 4,
        "content": "Answer:C"
      },
      {
        "date": "2022-04-30T13:19:00.000Z",
        "voteCount": 3,
        "content": "C. Create a new attribute in each table to track the expiration time and enable time to live (TTL) on each table."
      },
      {
        "date": "2021-11-04T18:13:00.000Z",
        "voteCount": 1,
        "content": "C: TTL is used in dynamo DB"
      },
      {
        "date": "2021-11-03T19:43:00.000Z",
        "voteCount": 1,
        "content": "its C, Dynamo DB TTL attribute for auto deletion"
      },
      {
        "date": "2021-11-03T02:29:00.000Z",
        "voteCount": 1,
        "content": "C ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-10-27T06:43:00.000Z",
        "voteCount": 2,
        "content": "My opinion:\n\nYou can include AWS glue to write a Deletion job to transform the data.\n\nTTL might be bound by the timefactor in the document"
      },
      {
        "date": "2021-11-03T09:50:00.000Z",
        "voteCount": 2,
        "content": "Super, I guess you are an aws product developer/architect. Nice answers, dont mislead people."
      },
      {
        "date": "2022-07-12T09:45:00.000Z",
        "voteCount": 2,
        "content": "I don't think your solution helps to minimize the total cost of ownership, the delete queries will scan the tables we will need to pay for that, and we will need to pay for Glue Jobs  executions and  aws says \"...TTL is provided at no extra cost as a means to reduce stored data volumes..\""
      },
      {
        "date": "2021-10-26T03:17:00.000Z",
        "voteCount": 1,
        "content": "Yes C is correct"
      },
      {
        "date": "2021-09-30T22:57:00.000Z",
        "voteCount": 1,
        "content": "For sure Answer C"
      },
      {
        "date": "2021-09-29T18:38:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/25032-exam-aws-certified-database-specialty-topic-1-question-6/",
    "body": "A company has an on-premises system that tracks various database operations that occur over the lifetime of a database, including database shutdown, deletion, creation, and backup.<br>The company recently moved two databases to Amazon RDS and is looking at a solution that would satisfy these requirements. The data could be used by other systems within the company.<br>Which solution will meet these requirements with minimal effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Events rule with the operations that need to be tracked on Amazon RDS. Create an AWS Lambda function to act on these rules and write the output to the tracking systems.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to trigger on AWS CloudTrail API calls. Filter on specific RDS API calls and write the output to the tracking systems.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate RDS event subscriptions. Have the tracking systems subscribe to specific RDS event system notifications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite RDS logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to act on these rules and write the output to the tracking systems."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T05:16:00.000Z",
        "voteCount": 8,
        "content": "Answer C:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html"
      },
      {
        "date": "2023-03-27T23:58:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2022-05-21T14:13:00.000Z",
        "voteCount": 1,
        "content": "NOt A ==&gt; an on-premises system"
      },
      {
        "date": "2022-04-30T10:11:00.000Z",
        "voteCount": 2,
        "content": "Create RDS event subscriptions. Have the tracking systems subscribe to specific RDS event system notifications.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html"
      },
      {
        "date": "2022-01-07T06:32:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't it be A since the tracking system sits on premise and C does not cater to write the output to the tracking system ?"
      },
      {
        "date": "2022-01-28T13:18:00.000Z",
        "voteCount": 1,
        "content": "Should be C, after restudying RDS Event"
      },
      {
        "date": "2021-12-07T06:50:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-11-04T20:47:00.000Z",
        "voteCount": 2,
        "content": "B also can be the answer, but question is with minimal effort, Hence the answer is C"
      },
      {
        "date": "2021-10-20T11:12:00.000Z",
        "voteCount": 1,
        "content": "Theek answer hai ==&gt;&gt; C"
      },
      {
        "date": "2021-10-13T06:14:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2021-10-06T04:07:00.000Z",
        "voteCount": 2,
        "content": "Looks like C is the one"
      },
      {
        "date": "2021-10-01T10:03:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-09-29T17:16:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-09-29T04:36:00.000Z",
        "voteCount": 4,
        "content": "C its correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/25044-exam-aws-certified-database-specialty-topic-1-question-7/",
    "body": "A clothing company uses a custom ecommerce application and a PostgreSQL database to sell clothes to thousands of users from multiple countries. The company is migrating its application and database from its on-premises data center to the AWS Cloud. The company has selected Amazon EC2 for the application and Amazon RDS for PostgreSQL for the database. The company requires database passwords to be changed every 60 days. A Database Specialist needs to ensure that the credentials used by the web application to connect to the database are managed securely.<br>Which approach should the Database Specialist take to securely manage the database credentials?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in a text file in an Amazon S3 bucket. Restrict permissions on the bucket to the IAM role associated with the instance profile only. Modify the application to download the text file and retrieve the credentials on start up. Update the text file every 60 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure IAM database authentication for the application to connect to the database. Create an IAM user and map it to a separate database user for each ecommerce user. Require users to update their passwords every 60 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in AWS Secrets Manager. Restrict permissions on the secret to only the IAM role associated with the instance profile. Modify the application to retrieve the credentials from Secrets Manager on start up. Configure the rotation interval to 60 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in an encrypted text file in the application AMI. Use AWS KMS to store the key for decrypting the text file. Modify the application to decrypt the text file and retrieve the credentials on start up. Update the text file and publish a new AMI every 60 days."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T02:52:00.000Z",
        "voteCount": 24,
        "content": "Answer C."
      },
      {
        "date": "2022-04-29T19:12:00.000Z",
        "voteCount": 5,
        "content": "Secret Manager\n-&gt; rotation 60 days\n-&gt;Secret access to  IAM roles for instance only \n-&gt; Apps refer Secret manager to get pwd on startup"
      },
      {
        "date": "2023-09-14T14:41:00.000Z",
        "voteCount": 2,
        "content": "C. Store the credentials in AWS Secrets Manager. Restrict permissions on the secret to only the IAM role associated with the instance profile. Modify the application to retrieve the credentials from Secrets Manager on start up. Configure the rotation interval to 60 days\n\nhttps://aws.amazon.com/secrets-manager/"
      },
      {
        "date": "2023-08-11T21:02:00.000Z",
        "voteCount": 2,
        "content": "now the answer is B,https://repost.aws/knowledge-center/users-connect-rds-iam, \"If your application is running on Amazon Elastic Compute Cloud (Amazon EC2), then you can use your EC2 instance profile credentials to access the database. You don't need to store database passwords on your instance.\" \nand \"Authentication tokens have a lifespan of 15 minutes, so you don't need to enforce password resets.\" meet the question criteria.\n\nC no longer best practice."
      },
      {
        "date": "2022-06-24T08:18:00.000Z",
        "voteCount": 2,
        "content": "If people think of B, the only reason they should move away from it is, IAM DB Authentication tokens can be valid only for 15 mins.\n\nAnswer is C"
      },
      {
        "date": "2022-04-13T07:16:00.000Z",
        "voteCount": 2,
        "content": "Answer C."
      },
      {
        "date": "2022-03-05T07:18:00.000Z",
        "voteCount": 3,
        "content": "Obviously  C.  How do the owners allow 80% wrong answers and  not correct them?"
      },
      {
        "date": "2022-02-24T16:03:00.000Z",
        "voteCount": 2,
        "content": "secret manager"
      },
      {
        "date": "2022-01-06T03:42:00.000Z",
        "voteCount": 1,
        "content": "B seems to be right choice. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"
      },
      {
        "date": "2022-10-02T12:27:00.000Z",
        "voteCount": 1,
        "content": "X B.  Create IAM role then grant to user , not create \"IAM user\""
      },
      {
        "date": "2021-11-30T03:23:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-10-31T17:42:00.000Z",
        "voteCount": 1,
        "content": "C: Why most of answer is wrong ? this is not right way ."
      },
      {
        "date": "2021-10-26T12:58:00.000Z",
        "voteCount": 1,
        "content": "C . Store the credentials in AWS Secrets Manager"
      },
      {
        "date": "2021-10-23T00:10:00.000Z",
        "voteCount": 2,
        "content": "C ==&gt; for centralised credentials management with auto rotation"
      },
      {
        "date": "2021-10-22T21:02:00.000Z",
        "voteCount": 1,
        "content": "C ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-10-21T04:52:00.000Z",
        "voteCount": 1,
        "content": "C\nI cannot believe how easy the question is!"
      },
      {
        "date": "2021-10-15T12:26:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2021-10-14T14:57:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/67043-exam-aws-certified-database-specialty-topic-1-question-8/",
    "body": "A financial services company is developing a shared data service that supports different applications from throughout the company. A Database Specialist designed a solution to leverage Amazon ElastiCache for Redis with cluster mode enabled to enhance performance and scalability. The cluster is configured to listen on port 6379.<br>Which combination of steps should the Database Specialist take to secure the cache data and protect it from unauthorized access? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable in-transit and at-rest encryption on the ElastiCache cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that Amazon CloudWatch metrics are configured in the ElastiCache cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the security group for the ElastiCache cluster allows all inbound traffic from itself and inbound traffic on TCP port 6379 from trusted clients only.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy to allow the application service roles to access all ElastiCache API actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the security group for the ElastiCache clients authorize inbound TCP port 6379 and port 22 traffic from the trusted ElastiCache cluster's security group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the cluster is created with the auth-token parameter and that the parameter is used in all subsequent commands.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-14T06:28:00.000Z",
        "voteCount": 2,
        "content": "ACF are correct\n\nA refer to\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html\n\nF refers to \nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html"
      },
      {
        "date": "2023-07-04T01:40:00.000Z",
        "voteCount": 1,
        "content": "ACF is correct because those are preventive protection\nB - not preventive\nD - what about non-IAM-based connections? :)\nE - client-server architecture -&gt; always the client initiates the connection! The cluster needs SG to prevent connections from unexpected sources and on unexpected ports, instead of clients..."
      },
      {
        "date": "2023-03-28T00:06:00.000Z",
        "voteCount": 1,
        "content": "ACF is the correct answer"
      },
      {
        "date": "2023-01-17T05:37:00.000Z",
        "voteCount": 3,
        "content": "Author from the Udemy.com practice test says ACF is the correct answer."
      },
      {
        "date": "2023-01-14T04:30:00.000Z",
        "voteCount": 1,
        "content": "ACF is right choice"
      },
      {
        "date": "2022-12-13T01:39:00.000Z",
        "voteCount": 1,
        "content": "Why A, if data is encrypted, it will remain confidentials but open for manipulation as you can delete it. Encryption can give confidentiality but can't gurantee integrity."
      },
      {
        "date": "2022-11-28T12:55:00.000Z",
        "voteCount": 1,
        "content": "Elasticache for Redis 7.0 now support IAM authentication through users and roles."
      },
      {
        "date": "2022-04-30T14:50:00.000Z",
        "voteCount": 4,
        "content": "A. Enable in-transit and at-rest encryption on the ElastiCache cluster.\nx B. why CloudWatch ?\nC. Ensure the security group for the ElastiCache cluster allows all inbound traffic from itself and inbound traffic on TCP port 6379 from trusted clients only.\nx why all API? D. Create an IAM policy to allow the application service roles to access all ElastiCache API actions.\nx why 22? E. Ensure the security group for the ElastiCache clients authorize inbound TCP port 6379 and port 22 traffic from the trusted ElastiCache cluster's security group.\nF. Ensure the cluster is created with the auth-token parameter and that the parameter is used in all subsequent commands."
      },
      {
        "date": "2022-01-24T12:41:00.000Z",
        "voteCount": 2,
        "content": "ACF are the correct\nE Why do you need port 22?"
      },
      {
        "date": "2021-12-10T07:19:00.000Z",
        "voteCount": 1,
        "content": "ADF are the correct options"
      },
      {
        "date": "2021-12-10T07:12:00.000Z",
        "voteCount": 1,
        "content": "Ensure the security group for the ElastiCache cluster allows all inbound traffic from itself is only needed when  you launched your ElastiCache instance in EC2 Classic. so C is not a valid option"
      },
      {
        "date": "2021-12-23T20:23:00.000Z",
        "voteCount": 1,
        "content": "These questions are not for up to date versions. When this question was added the \nmost likely the EC2-Classic was still very much available. \n\nMy choice is ACF"
      },
      {
        "date": "2021-12-10T07:18:00.000Z",
        "voteCount": 2,
        "content": "The following is needed to protect ElastiCache\nUse multi-factor authentication (MFA) with each account.\nUse SSL/TLS to communicate with AWS resources.\nSet up API and user activity logging with AWS CloudTrail.\nUse AWS encryption solutions, along with all default security controls within AWS services.\nUse advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3."
      },
      {
        "date": "2021-12-01T09:06:00.000Z",
        "voteCount": 2,
        "content": "ACF.\nF refers to https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/25084-exam-aws-certified-database-specialty-topic-1-question-9/",
    "body": "A company is running an Amazon RDS for PostgreSQL DB instance and wants to migrate it to an Amazon Aurora PostgreSQL DB cluster. The current database is 1 TB in size. The migration needs to have minimal downtime.<br>What is the FASTEST way to accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora PostgreSQL DB cluster. Set up replication from the source RDS for PostgreSQL DB instance using AWS DMS to the target DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the pg_dump and pg_restore utilities to extract and restore the RDS for PostgreSQL DB instance to the Aurora PostgreSQL DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a database snapshot of the RDS for PostgreSQL DB instance and use this snapshot to create the Aurora PostgreSQL DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate data from the RDS for PostgreSQL DB instance to an Aurora PostgreSQL DB cluster using an Aurora Replica. Promote the replica during the cutover.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T23:08:00.000Z",
        "voteCount": 14,
        "content": "D. While C would work, the requirement is minimal downtime.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html"
      },
      {
        "date": "2023-09-14T06:42:00.000Z",
        "voteCount": 1,
        "content": "D. Migrate data from the RDS for PostgreSQL DB instance to an Aurora PostgreSQL DB cluster using an Aurora Replica. Promote the replica during the cutover. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQL.Replica"
      },
      {
        "date": "2023-07-31T06:37:00.000Z",
        "voteCount": 1,
        "content": "D\nMigrating from RDS PostgreSQL to Aurora PostgreSQL.\nYou have two options when migrating data from RDS PostgreSQL to Aurora PostgreSQL:\n- Use an RDS PostgreSQL snapshot\n- Use an Aurora Read Replica for RDS PostgreSQL\nhttps://aws.amazon.com/blogs/database/migrate-to-an-amazon-aurora-postgresql-instance-from-another-postgresql-source/"
      },
      {
        "date": "2023-07-04T01:45:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. The keyword is \"FASTEST\" ~ minimal downtime / minimal effort\n\n\"You can migrate your existing Amazon RDS MySQL databases to Amazon Aurora using Aurora Read Replica. This solution is beneficial since it's completely managed and does not involve manually configuring replication functionality to reduce downtime during migration.\"\n\"PostgreSQL follows the same process as the one described previously for MySQL for migration.\"\nhttps://docs.aws.amazon.com/whitepapers/latest/migrating-databases-to-amazon-aurora/migration-using-aurora-read-replica.html"
      },
      {
        "date": "2023-01-17T05:37:00.000Z",
        "voteCount": 1,
        "content": "Author from the Udemy.com practice test says D is the correct answer."
      },
      {
        "date": "2022-10-25T22:06:00.000Z",
        "voteCount": 1,
        "content": "D is the right option"
      },
      {
        "date": "2022-04-28T14:49:00.000Z",
        "voteCount": 2,
        "content": "C - snapshot will take time (and new transactions lost)\nD- Replica is fast"
      },
      {
        "date": "2022-03-05T17:05:00.000Z",
        "voteCount": 1,
        "content": "D is absolutely correct"
      },
      {
        "date": "2022-02-15T06:44:00.000Z",
        "voteCount": 2,
        "content": "We need the SPEEDIEST solution, however, we also need a reasonable solution; how would we deal with database changes when restoring the database when we pick B? AWS perform that job for us when choosing D."
      },
      {
        "date": "2021-12-22T14:26:00.000Z",
        "voteCount": 2,
        "content": "Keywords : seamless and speediest..\nAnswer : Option D"
      },
      {
        "date": "2021-12-07T11:37:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2021-11-05T11:11:00.000Z",
        "voteCount": 1,
        "content": "D: because in question talking about minimum downtime ."
      },
      {
        "date": "2021-11-02T21:54:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer, Read Replica is faster than snapshots"
      },
      {
        "date": "2021-11-02T16:45:00.000Z",
        "voteCount": 1,
        "content": "D ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-10-30T02:08:00.000Z",
        "voteCount": 2,
        "content": "if we say C is fastest then how to handle changes to source server while snapshot is being transferred and getting applied to target ?"
      },
      {
        "date": "2021-10-26T23:38:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2021-10-25T23:27:00.000Z",
        "voteCount": 1,
        "content": "Answer D. When you take a snapshot there will be a temporary suspension of I/O Services, now this can be seconds or longer, but our size is 1TB which means that there will be downtime, so based on that the answer is D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/25049-exam-aws-certified-database-specialty-topic-1-question-10/",
    "body": "A Database Specialist is migrating a 2 TB Amazon RDS for Oracle DB instance to an RDS for PostgreSQL DB instance using AWS DMS. The source RDS Oracle<br>DB instance is in a VPC in the us-east-1 Region. The target RDS for PostgreSQL DB instance is in a VPC in the use-west-2 Region.<br>Where should the AWS DMS replication instance be placed for the MOST optimal performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the same Region and VPC of the source DB instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the same Region and VPC as the target DB instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the same VPC and Availability Zone as the target DB instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the same VPC and Availability Zone as the source DB instance"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T05:20:00.000Z",
        "voteCount": 17,
        "content": "C. See the diagram here:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationInstance.VPC.Configurations.ScenarioVPCPeer\nIn fact, all the configurations list on above url prefer the replication instance putting into target vpc region / subnet / az."
      },
      {
        "date": "2021-12-05T01:26:00.000Z",
        "voteCount": 2,
        "content": "There is also another question related to migration between a RDS resource and a Redshift cluster where they are in different VPCs and the Redshift cluster is the target. It asks readers where should the DMS replication instance be placed and the answer is the replication instance should be placed with the same VPC as the target."
      },
      {
        "date": "2021-09-25T11:28:00.000Z",
        "voteCount": 6,
        "content": "D is correct based on :\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html\nit shows  Choose the Availability Zone where your source database is located."
      },
      {
        "date": "2021-09-26T18:31:00.000Z",
        "voteCount": 1,
        "content": "Going with D as well since it mentions in your link \"reducing load on your source database\""
      },
      {
        "date": "2021-10-02T23:41:00.000Z",
        "voteCount": 5,
        "content": "Changing my mind and going with C now. Talked with an AWS solutions architect. He said \"target DB instance\" has worked better for him in the past."
      },
      {
        "date": "2021-10-24T20:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\n\nglat mtt kar dena exam mein. All the Best !!"
      },
      {
        "date": "2024-01-12T08:32:00.000Z",
        "voteCount": 1,
        "content": "It is C - same VPC and AZ as the target."
      },
      {
        "date": "2023-09-17T01:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. In the network configuration for database migrations section on this page https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html States --&gt; When practical, we recommend that you create a DMS replication instance in the same Region as your target endpoint, and in the same VPC or subnet as your target endpoint."
      },
      {
        "date": "2023-09-14T07:02:00.000Z",
        "voteCount": 1,
        "content": "C. In the same VPC and Availability Zone as the target DB instance"
      },
      {
        "date": "2023-08-06T10:01:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/questions/QUUUL_7NsZS8OBI_GkCRSRRg/why-place-dms-replication-instance-in-target-vpc-instead-of-source-vpc"
      },
      {
        "date": "2023-07-11T13:29:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer. It is a heterogeneous migration, some data transformations need to be performed before loading in the target. That's why it should be in the source database VPC."
      },
      {
        "date": "2023-03-04T22:53:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationInstance.VPC.Configurations.ScenarioVPCPeer"
      },
      {
        "date": "2023-02-04T00:08:00.000Z",
        "voteCount": 1,
        "content": "Ans C : Generally, you\u2019d get better performance by placing primary DMS replication instance is in the same AZ as the target DB"
      },
      {
        "date": "2022-12-14T13:40:00.000Z",
        "voteCount": 2,
        "content": "Answer: C.  I'm new to this site.  Is the suggested answer typically the correct answer.  I'm really confused. Who provides the suggested answers on this site?"
      },
      {
        "date": "2023-04-24T14:15:00.000Z",
        "voteCount": 2,
        "content": "just a dummy random answer"
      },
      {
        "date": "2022-05-01T07:05:00.000Z",
        "voteCount": 3,
        "content": "C. In the same VPC and Availability Zone as the target DB instance\n(BTW, target region is implicit)\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationInstance.VPC.Configurations.ScenarioVPCPeer"
      },
      {
        "date": "2021-10-30T00:02:00.000Z",
        "voteCount": 2,
        "content": "C - \"For best results, we recommend that you locate your replication instance in the same VPC and Availability Zone as your target database, in this case Aurora MySQL.\" -"
      },
      {
        "date": "2021-10-29T17:23:00.000Z",
        "voteCount": 1,
        "content": "C: we will get performance improvement if place dms replication instance in target (VPC &amp; availability zone  same )"
      },
      {
        "date": "2021-10-28T12:39:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer here ie, the DMS replication instance should be placed in the target AZ.\n\nD would have been correct if the source data needed a lot of filtering and manipulation before transferring data to the target."
      },
      {
        "date": "2023-06-02T04:08:00.000Z",
        "voteCount": 1,
        "content": "But data will needed a lot of filtering and manipulation before transferring data to the target going from Oracle to PostgreSQL, D is the correct answer"
      },
      {
        "date": "2021-10-26T06:46:00.000Z",
        "voteCount": 1,
        "content": "C =&gt; AZ of target instance"
      },
      {
        "date": "2021-10-23T19:21:00.000Z",
        "voteCount": 1,
        "content": "C ==&gt;&gt; Correct Answer"
      },
      {
        "date": "2021-10-22T15:15:00.000Z",
        "voteCount": 1,
        "content": "It seems a tough question and need to check in real practice, not on AWS books. I read the best practice but don't see any mention where these DMSs are located.\nDMS is not an replica DB, it is like a machine that read the information from source DB and re-procedure it to target DB.\n\"AWS DMS is a managed service that runs on an Amazon EC2 instance. This service connects to the source database, reads the source data, formats the data for consumption by the target database, and loads the data into the target database.\"\nAs above, I prefer to put it close to the source DB, where it can access source DB and read data faster, then send DDL information??? to target DB to re-procedure table etc"
      },
      {
        "date": "2021-10-23T04:06:00.000Z",
        "voteCount": 1,
        "content": "Imagine that we need to do full scan of source DB to find out the change. If we put DMS at target VPC/Region, it needs to connect back to source DB and process.\nThat's I think, but need real practice so if somebody did it, he/she can prove."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/25052-exam-aws-certified-database-specialty-topic-1-question-11/",
    "body": "The Development team recently executed a database script containing several data definition language (DDL) and data manipulation language (DML) statements on an Amazon Aurora MySQL DB cluster. The release accidentally deleted thousands of rows from an important table and broke some application functionality.<br>This was discovered 4 hours after the release. Upon investigation, a Database Specialist tracked the issue to a DELETE command in the script with an incorrect<br>WHERE clause filtering the wrong set of rows.<br>The Aurora DB cluster has Backtrack enabled with an 8-hour backtrack window. The Database Administrator also took a manual snapshot of the DB cluster before the release started. The database needs to be returned to the correct state as quickly as possible to resume full application functionality. Data loss must be minimal.<br>How can the Database Specialist accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuickly rewind the DB cluster to a point in time before the release using Backtrack.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a point-in-time recovery (PITR) of the DB cluster to a time before the release and copy the deleted rows from the restored database to the original database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the DB cluster using the manual backup snapshot created before the release and change the application configuration settings to point to the new DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a clone of the DB cluster with Backtrack enabled. Rewind the cloned cluster to a point in time before the release. Copy deleted rows from the clone to the original database."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-07T16:24:00.000Z",
        "voteCount": 25,
        "content": "D is right, you can create a clone with backtrack if the database was created with backtrack which it was in this case. It's either B (pitr) or D (backtrack). Backtrack is faster. A is wrong, because if you backtrack, you lose the data that users entered for the past 4 hours. you want to clone to another area and copy the data that was lost."
      },
      {
        "date": "2022-10-02T07:06:00.000Z",
        "voteCount": 2,
        "content": "X A. May lose 4 hrs users entered\nX B. Take too long ( restored -&gt; copy delete rows to Org-DB )    \nX C. Take too long &amp; May lose 4 hrs users entered\nD. Better  a. clone is faster than restore     b. Copy delete rows to Org-DB"
      },
      {
        "date": "2022-10-02T07:10:00.000Z",
        "voteCount": 3,
        "content": "Only A, D using \"Rewind\"  which is good\nD is correct if we can rewind \"clone with Backtrack enabled\"   Anyone know ?"
      },
      {
        "date": "2023-08-24T16:33:00.000Z",
        "voteCount": 1,
        "content": "Agree with you, D is the best option:https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2023-09-01T12:57:00.000Z",
        "voteCount": 2,
        "content": "Correct my answer, it should be B. D is wrong because \"you can't backtrack a database clone to a time before that database clone was created\" from the backtrack limitation here :https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2023-05-18T16:52:00.000Z",
        "voteCount": 3,
        "content": "D its wrong, da backtrack only right after you create clone."
      },
      {
        "date": "2021-09-23T04:16:00.000Z",
        "voteCount": 24,
        "content": "A&amp;B correct.however, A cause loss data for last 4 hours but it's quickly will take just few minutes to rewind the databse to 4 hours before, \nhowever B will keep database and fix it by reinsert deleted records from database which created from restore point in time \nthis will take more time but without loss data as the question refer to\n\"Data loss must be minimal\"\nSo I guess B the correct answer"
      },
      {
        "date": "2021-10-30T12:38:00.000Z",
        "voteCount": 4,
        "content": "B is wrong because \"Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release\"\nI example: The release time was 5AM, and you found that the records are deleted at 9AM. So why we recover the database \"before 5AM\" (release time). ==&gt; we still lose 4 hours.\nThe correct way to minimize data loss is: recover as nearest as possible time such as 8:55AM) then copy the deletion data. So only 5 mins data loss. Only this minimizes data loss."
      },
      {
        "date": "2022-05-18T21:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with ChauPhan. B is wrong. A is correct."
      },
      {
        "date": "2022-09-15T20:04:00.000Z",
        "voteCount": 3,
        "content": "Option B suggest to restore the db to a point before the records were deleted, then copy the required records from this newly restored db (using PITR) to the original DB. So this way, all the other changes made on the DB are still there, while we copied the deleted rows from the PITR db. I think option B is correct. Please correct if wrong."
      },
      {
        "date": "2022-10-15T06:22:00.000Z",
        "voteCount": 1,
        "content": "PITR is restored another cluster with Product cluster. Executing PITR doesn't mean back-forward. we dont lose 4 hours. we can export and import from PITR restored cluster."
      },
      {
        "date": "2024-03-25T06:37:00.000Z",
        "voteCount": 1,
        "content": "X D : You can't backtrack a database clone to a time before that database clone was created.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2024-03-09T20:54:00.000Z",
        "voteCount": 2,
        "content": "I will go for \"A\". Here's an excerpt from the below link:\nhttps://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/\n\u2026 After that regrettable moment when all seems lost, you simply pause your application, open up the Aurora Console, select the cluster, and click Backtrack DB cluster. Then you select Backtrack and choose the point in time just before your epic fail, and click Backtrack DB cluster."
      },
      {
        "date": "2024-01-12T08:34:00.000Z",
        "voteCount": 1,
        "content": "The answer is B. \nWith A you would lose all the data gathered in the last 4 hours"
      },
      {
        "date": "2023-12-31T17:51:00.000Z",
        "voteCount": 1,
        "content": "BackTrack - If you accidentally issue a Delete statement without a Where clause (that is, delete all items) or drop a table, you can use Backtrack to quickly revert to the previous state. You can do the same thing with PITR, but it takes time to restore from an existing backup to a different cluster/instance."
      },
      {
        "date": "2023-12-31T17:54:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I noticed the window is 8 hours. We have to use PITR not to lose the data of the 4 hours. The answer is B."
      },
      {
        "date": "2023-12-04T18:22:00.000Z",
        "voteCount": 1,
        "content": "Backtracking \"rewinds\" the DB cluster to the time you specify. Backtracking is not a replacement for backing up your DB cluster so that you can restore it to a point in time. However, backtracking provides the following advantages over traditional backup and restore:\n\nYou can easily undo mistakes. If you mistakenly perform a destructive action, such as a DELETE without a WHERE clause, you can backtrack the DB cluster to a time before the destructive action with minimal interruption of service\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html#AuroraMySQL.Managing.Backtrack.Overview"
      },
      {
        "date": "2023-11-07T07:09:00.000Z",
        "voteCount": 1,
        "content": "This option combines the benefits of both Backtrack and data recovery by cloning the cluster and using Backtrack to recover lost data. After that, you can copy the missing rows back into the original database. It allows for minimal data loss.\nBased on the goal of minimizing data loss and ensuring a quick recovery, option D seems like the best choice. It leverages Backtrack, provides a way to recover the deleted data, and then copy it back into the original database. However, option B is also a valid choice if Backtrack is not an option or if the deleted data is outside the backtrack window."
      },
      {
        "date": "2023-09-09T04:09:00.000Z",
        "voteCount": 2,
        "content": "A is correct, because backtrack is the fastest way to \"[return] the database to the correct state as quickly as possible\", as the questions asks.\nThere is no mention or requirement of retaining potentially lost user input.\nAdditionally, restoring the deleted rows while keeping actual user input from the last 4 hours after the loss might infringe foreign key constraints within the database, making the data inconsistent."
      },
      {
        "date": "2023-09-01T13:01:00.000Z",
        "voteCount": 2,
        "content": "A x: It will cause 4 hours data loss after release\nB yes\nC x: It will cause 4 hours data loss after release and involve additional change\nD x: You can't backtrack a database clone to a time before that database clone was created from the backtrack limitation here :https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2023-07-14T11:09:00.000Z",
        "voteCount": 1,
        "content": "You can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2023-07-04T04:08:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C (incorrect) - dataloss!\nD (incorrect) - \"You can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created.\"\nThe correct answer is B. \n\nHowever, D could be better with a little change: \nYou should use the clone db as new production and you should rewind on the original. Copy deleted rows from original to the clone."
      },
      {
        "date": "2023-07-04T04:09:00.000Z",
        "voteCount": 1,
        "content": "Reference: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2023-06-18T07:39:00.000Z",
        "voteCount": 1,
        "content": "Option D would have been correct if it has suggested to rewind the DB cluster not the cloned cluster. Since you cannot rewind the cloned cluster before the time it was created.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html#:~:text=You%20can%27t%20backtrack%20a%20database%20clone"
      },
      {
        "date": "2023-05-31T01:27:00.000Z",
        "voteCount": 3,
        "content": "xA. Rewinding the database means restoring the deleted data, but also undoing all other changes to the database =&gt; data loss\nB. Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release and copy the deleted rows from the restored database to the original database. =&gt; This is the best course of action.\nxC. This also entails losing all of the changes made after the deletion event.\nxD. This does not work because you cannot rewind a clone to a time before the clone was created."
      },
      {
        "date": "2023-05-31T01:27:00.000Z",
        "voteCount": 1,
        "content": "xA. Rewinding the database means restoring the deleted data, but also undoing all other changes to the database =&gt; data loss\nB. Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release and copy the deleted rows from the restored database to the original database. =&gt; This is the best course of action.\nxC. This also entails losing all of the changes made after the deletion event.\nxD.  This does not work because you cannot rewind a clone to a time before the clone was created."
      },
      {
        "date": "2023-03-17T18:27:00.000Z",
        "voteCount": 1,
        "content": "D is right - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"
      },
      {
        "date": "2023-03-12T04:23:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/ &lt;--- D is the answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/25180-exam-aws-certified-database-specialty-topic-1-question-12/",
    "body": "A company is load testing its three-tier production web application deployed with an AWS CloudFormation template on AWS. The Application team is making changes to deploy additional Amazon EC2 and AWS Lambda resources to expand the load testing capacity. A Database Specialist wants to ensure that the changes made by the Application team will not change the Amazon RDS database resources already deployed.<br>Which combination of steps would allow the Database Specialist to accomplish this? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the stack drift before modifying the template",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and review a change set before applying it\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the database resources as stack outputs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine the database resources in a nested stack",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a stack policy for the database resources\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T06:57:00.000Z",
        "voteCount": 22,
        "content": "Correct Answer B and E. \nB. You need to review the change set before accepting changes.\nE. use Stack set policy to prevent updates tto resources."
      },
      {
        "date": "2021-10-07T01:08:00.000Z",
        "voteCount": 5,
        "content": "B&amp;E : https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/best-practices.html#cfn-best-practices-changesets"
      },
      {
        "date": "2023-09-27T04:06:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer B and E."
      },
      {
        "date": "2023-09-04T06:05:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      },
      {
        "date": "2022-12-14T14:44:00.000Z",
        "voteCount": 1,
        "content": "Answer BE however I don't answer the A.  Can someone explain?"
      },
      {
        "date": "2023-09-29T06:54:00.000Z",
        "voteCount": 1,
        "content": "CFT stacks drift after update but option A says to check the drift status before even making the update (need to use ChangeSet) which is impossible"
      },
      {
        "date": "2022-10-19T04:31:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      },
      {
        "date": "2022-05-06T19:59:00.000Z",
        "voteCount": 1,
        "content": "B. change set will assure that App Dev modification are not impacting resources.\nE. Policy rules will be additional to fail deployment if the resources are changed."
      },
      {
        "date": "2022-04-28T13:43:00.000Z",
        "voteCount": 2,
        "content": "https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/best-practices.html#cfn-best-practices-changesets"
      },
      {
        "date": "2022-03-07T04:34:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it). BE is correct"
      },
      {
        "date": "2022-03-05T14:12:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E are the right choices"
      },
      {
        "date": "2021-11-05T05:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: B and E"
      },
      {
        "date": "2021-11-03T11:58:00.000Z",
        "voteCount": 1,
        "content": "its BE, stack policy and changesets"
      },
      {
        "date": "2021-10-28T15:39:00.000Z",
        "voteCount": 3,
        "content": "B &amp; E are the answers.\nA is not clear and headache as we need to review the CF manually, not recommended.\nC D won't help"
      },
      {
        "date": "2021-10-24T01:30:00.000Z",
        "voteCount": 2,
        "content": "I would go with B &amp; E"
      },
      {
        "date": "2021-10-15T10:53:00.000Z",
        "voteCount": 1,
        "content": "Ans: BE"
      },
      {
        "date": "2021-10-14T18:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is BE"
      },
      {
        "date": "2021-10-13T14:25:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/25790-exam-aws-certified-database-specialty-topic-1-question-13/",
    "body": "A manufacturing company's website uses an Amazon Aurora PostgreSQL DB cluster.<br>Which configurations will result in the LEAST application downtime during a failover? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the provided read and write Aurora endpoints to establish a connection to the Aurora DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alert triggering a restore in another Availability Zone when the primary Aurora DB cluster is unreachable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit and enable Aurora DB cluster cache management in parameter groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet TCP keepalive parameters to a high value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet JDBC connection string timeout variables to a low value.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Java DNS caching timeouts to a high value."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T23:37:00.000Z",
        "voteCount": 10,
        "content": "Ans is ACE"
      },
      {
        "date": "2021-10-14T12:07:00.000Z",
        "voteCount": 3,
        "content": "Agree with ACE"
      },
      {
        "date": "2023-02-06T09:00:00.000Z",
        "voteCount": 8,
        "content": "ACE\nA: We should use read/cluster endpoints of aurora. If we use other endpoints like instance endpoint, custom endpoint, it might not work correctly after faiover.\nB: Aurora failover within region is automatic so we don't need cloudwatch alert to do anything\nC. Cluster cache management will help to warm up the cache on the new primary instance --&gt; no high load on the new primary instance\nD. F. Long dns cache or connection are enemies of failover so we should avoid that --&gt; E is correct\nSorry for my english"
      },
      {
        "date": "2023-09-27T04:10:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer A,C and E."
      },
      {
        "date": "2023-07-31T14:11:00.000Z",
        "voteCount": 2,
        "content": "ACE\nRegarding C:\nFast recovery after failover with cluster cache management for Aurora PostgreSQL.\nWith cluster cache management, you set a specific reader DB instance as the failover target. Cluster cache management ensures that the data in the designated reader's cache is kept synchronized with the data in the writer DB instance's cache. The designated reader's cache with prefilled values is known as a warm cache. If a failover occurs, the designated reader uses values in its warm cache immediately when it's promoted to the new writer DB instance. This approach provides your application much better recovery performance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2022-12-13T04:54:00.000Z",
        "voteCount": 1,
        "content": "ACE should be wright"
      },
      {
        "date": "2022-12-10T18:06:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.FastFailover.html\n\nACE based on above link"
      },
      {
        "date": "2022-05-19T17:35:00.000Z",
        "voteCount": 1,
        "content": "Question is discussing about low recovery time at failure..."
      },
      {
        "date": "2022-04-30T11:53:00.000Z",
        "voteCount": 3,
        "content": "A E is correct\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.html\nA. Use the endpoints to  cluster.\nx B cloudwatch...\nx D. Set TCP keepalive parameters to a high value. (keep it low)\nE. Set JDBC connection string timeout variables to a low value.\nx F. Set Java DNS caching timeouts to a high value. (keep it low)\n\nC is here\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html\nC. Edit and enable Aurora DB cluster cache management in parameter groups. (keeps cache in sync on replica)"
      },
      {
        "date": "2022-03-07T04:38:00.000Z",
        "voteCount": 5,
        "content": "Got this question in my exam. (i cleared it). ACE is correct"
      },
      {
        "date": "2021-11-02T11:00:00.000Z",
        "voteCount": 1,
        "content": "Ans is  ACE"
      },
      {
        "date": "2021-10-29T21:05:00.000Z",
        "voteCount": 2,
        "content": "its ACE"
      },
      {
        "date": "2021-10-21T05:33:00.000Z",
        "voteCount": 2,
        "content": "A,C,E ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-10-20T18:05:00.000Z",
        "voteCount": 2,
        "content": "A E is correct\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.html\nC is here\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2021-10-18T07:28:00.000Z",
        "voteCount": 1,
        "content": "A and E are correct. Not sure of third one.  C is for keeping the cache warm on replica so that after failover performance is not degraded. Purpose of CCM is not to reduce downtime."
      },
      {
        "date": "2021-10-14T06:13:00.000Z",
        "voteCount": 1,
        "content": "Based on the link below, the answer looks like it should be ADE. Not sure where it says C is an option to ensure a fast failover.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.html#AuroraPostgreSQL.BestPractices.FastFailover.TCPKeepalive"
      },
      {
        "date": "2021-10-20T10:34:00.000Z",
        "voteCount": 2,
        "content": "D is not correct. You should set TCP keepalive to low value, not high one.\nEnabling TCP keepalive parameters and setting them aggressively ensures that if your client is no longer able to connect to the database, then any active connections are quickly closed. This action allows the application to react appropriately, such as by picking a new host to connect to"
      },
      {
        "date": "2021-10-10T23:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is ACE"
      },
      {
        "date": "2021-10-10T14:19:00.000Z",
        "voteCount": 1,
        "content": "Ans: ACE"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/66148-exam-aws-certified-database-specialty-topic-1-question-14/",
    "body": "A company is hosting critical business data in an Amazon Redshift cluster. Due to the sensitive nature of the data, the cluster is encrypted at rest using AWS<br>KMS. As a part of disaster recovery requirements, the company needs to copy the Amazon Redshift snapshots to another Region.<br>Which steps should be taken in the AWS Management Console to meet the disaster recovery requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new KMS customer master key in the source Region. Switch to the destination Region, enable Amazon Redshift cross-Region snapshots, and use the KMS key of the source Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role with access to the KMS key. Enable Amazon Redshift cross-Region replication using the new IAM role, and use the KMS key of the source Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon Redshift cross-Region snapshots in the source Region, and create a snapshot copy grant and use a KMS key in the destination Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new KMS customer master key in the destination Region and create a new IAM role with access to the new KMS key. Enable Amazon Redshift cross-Region replication in the source Region and use the KMS key of the destination Region."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-16T14:21:00.000Z",
        "voteCount": 6,
        "content": "I think C\nIf you want to enable cross-Region snapshot copy for an AWS KMS\u2013encrypted cluster, you must configure a snapshot copy grant for a root key in the destination AWS Region\nSource-Region :  configure a cross-Region snapshot for an AWS KMS\u2013encrypted cluster\nIn Destination AWS Region : choose the AWS Region to which to copy snapshots.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot"
      },
      {
        "date": "2024-01-20T03:30:00.000Z",
        "voteCount": 1,
        "content": "C \nSee https://aws.amazon.com/blogs/big-data/migrate-your-amazon-redshift-cluster-to-another-aws-region/"
      },
      {
        "date": "2023-12-31T19:50:00.000Z",
        "voteCount": 1,
        "content": "For encrypted snapshots - configure cross region snapshots and additionally specify a snapshot copy grant which requires a KMS key"
      },
      {
        "date": "2023-11-07T07:32:00.000Z",
        "voteCount": 1,
        "content": "C. Enable Amazon Redshift cross-Region snapshots in the source Region, and create a snapshot copy grant and use a KMS key in the destination Region.\n\nHere's why this option is the correct choice:\n\nEnabling cross-Region snapshots in the source Region is necessary to initiate the snapshot copying process.\n\nCreating a snapshot copy grant allows you to define permissions and configurations for copying snapshots to the destination Region. It is an essential step in setting up snapshot replication.\n\nUsing a KMS key in the destination Region ensures that the copied snapshots are encrypted with a key specific to that Region. This maintains data security during replication."
      },
      {
        "date": "2023-09-27T04:29:00.000Z",
        "voteCount": 1,
        "content": "It*s C:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\n\nIt can't be (A) because when you read the docs it is written \"[...] 2. In the SOURCE AWS Region, enable copying of snapshots and ...\". \n\nB &amp; D is about replication and not copying a snapshot."
      },
      {
        "date": "2023-09-09T04:37:00.000Z",
        "voteCount": 1,
        "content": "The question is about cross-region snapshot copy (read carefully), not about cross-region replication, so B and D are out.\nFrom the remaining A and C, I would tend to A because the KMS key is needed from the source, the target only needs a grant on it."
      },
      {
        "date": "2023-09-04T00:01:00.000Z",
        "voteCount": 1,
        "content": "Seek \u201cCopying AWS KMS\u2013encrypted snapshots to another AWS Region\u201d from\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html\nNo need to create IAM role"
      },
      {
        "date": "2023-07-04T04:38:00.000Z",
        "voteCount": 1,
        "content": "\"To copy snapshots for AWS KMS\u2013encrypted clusters to another AWS Region, create a grant for Amazon Redshift to use a customer managed key in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region.\"\nReference: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html"
      },
      {
        "date": "2023-06-22T04:03:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      },
      {
        "date": "2023-06-22T04:02:00.000Z",
        "voteCount": 1,
        "content": "D is correct\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      },
      {
        "date": "2023-03-28T03:04:00.000Z",
        "voteCount": 1,
        "content": "because of this documentation https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\nCopying snapshots to another AWS Region article shows c is the answer"
      },
      {
        "date": "2023-04-07T17:17:00.000Z",
        "voteCount": 1,
        "content": "Why is D not correct?"
      },
      {
        "date": "2023-10-05T03:57:00.000Z",
        "voteCount": 1,
        "content": "because there is no cross region replication with redshift"
      },
      {
        "date": "2023-01-10T08:16:00.000Z",
        "voteCount": 1,
        "content": "The answer is explained here: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with-aws-kms, look for \"copying AWS KMS\u2013encrypted snapshots to another AWS Region\""
      },
      {
        "date": "2022-12-14T15:22:00.000Z",
        "voteCount": 1,
        "content": "I thought keys are region specific and one will need to be created in the destination region"
      },
      {
        "date": "2022-05-29T16:52:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html"
      },
      {
        "date": "2022-04-30T06:40:00.000Z",
        "voteCount": 1,
        "content": "-A,B,D are incorrect\n-C \nnew KMS in the destination Region\n-&gt; snapshot copy grant in the destination Region specifying the new key\n-&gt;In the source Region, configure cross-Region snapshots for the Amazon Redshift cluster specifying\n- the destination Region,\n- the snapshot copy grant,\n- and retention periods for the snapshot."
      },
      {
        "date": "2022-01-08T12:27:00.000Z",
        "voteCount": 3,
        "content": "Question is poorly written . In absence of true answer C is right. It is right because A,B,D are not correct. You definitely need snapshot copy grant in destination region but based on that region key. Answer C does not say which key."
      },
      {
        "date": "2022-02-27T12:24:00.000Z",
        "voteCount": 1,
        "content": "agree with you"
      },
      {
        "date": "2021-12-31T00:37:00.000Z",
        "voteCount": 2,
        "content": "C is correct ans"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/25069-exam-aws-certified-database-specialty-topic-1-question-15/",
    "body": "A company has a production Amazon Aurora Db cluster that serves both online transaction processing (OLTP) transactions and compute-intensive reports. The reports run for 10% of the total cluster uptime while the OLTP transactions run all the time. The company has benchmarked its workload and determined that a six- node Aurora DB cluster is appropriate for the peak workload.<br>The company is now looking at cutting costs for this DB cluster, but needs to have a sufficient number of nodes in the cluster to support the workload at different times. The workload has not changed since the previous benchmarking exercise.<br>How can a Database Specialist address these requirements with minimal user involvement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit up the DB cluster into two different clusters: one for OLTP and the other for reporting. Monitor and set up replication between the two clusters to keep data consistent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview all evaluate the peak combined workload. Ensure that utilization of the DB cluster node is at an acceptable level. Adjust the number of instances, if necessary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the stop cluster functionality to stop all the nodes of the DB cluster during times of minimal workload. The cluster can be restarted again depending on the workload at the time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up automatic scaling on the DB cluster. This will allow the number of reader nodes to adjust automatically to the reporting workload, when needed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T09:23:00.000Z",
        "voteCount": 15,
        "content": "Answer D. You dont setup a seperate cluster for reporting. Aurora does OLTP and reporting from same cluster. just use autoscaling."
      },
      {
        "date": "2024-01-12T08:41:00.000Z",
        "voteCount": 1,
        "content": "Out of the provided options, D seems to be the best."
      },
      {
        "date": "2023-09-14T19:39:00.000Z",
        "voteCount": 2,
        "content": "D. Set up Auto Scaling  --&gt; minimal user involvement\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html#Aurora.Integrating.AutoScaling.Concepts\n\n\"Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.\""
      },
      {
        "date": "2022-09-28T09:53:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-04-30T14:01:00.000Z",
        "voteCount": 1,
        "content": "Auto-scaling will scale-up/down number of read-replicas"
      },
      {
        "date": "2021-11-02T01:04:00.000Z",
        "voteCount": 1,
        "content": "D ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-11-01T22:47:00.000Z",
        "voteCount": 1,
        "content": "D =&gt; autoscaling on Aurora cluster"
      },
      {
        "date": "2021-11-01T04:54:00.000Z",
        "voteCount": 1,
        "content": "D ==&gt;&gt; Correct Answer."
      },
      {
        "date": "2021-10-24T00:08:00.000Z",
        "voteCount": 2,
        "content": "D for sure."
      },
      {
        "date": "2021-10-21T14:18:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-12T02:47:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer.  Auto-scaling will scale-up/down number of read-replicas automatically based on the workload."
      },
      {
        "date": "2021-09-29T16:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-09-29T16:02:00.000Z",
        "voteCount": 2,
        "content": "D is Correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/66661-exam-aws-certified-database-specialty-topic-1-question-16/",
    "body": "A company is running a finance application on an Amazon RDS for MySQL DB instance. The application is governed by multiple financial regulatory agencies.<br>The RDS DB instance is set up with security groups to allow access to certain Amazon EC2 servers only. AWS KMS is used for encryption at rest.<br>Which step will provide additional security?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up NACLs that allow the entire EC2 subnet to access the DB instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the master user account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a security group that blocks SSH to the DB instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up RDS to use SSL for data in transit\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-24T13:09:00.000Z",
        "voteCount": 18,
        "content": "This needs to be corrected in the question: Instead of \"AWS KMS is used to encrypt data in transit\" it should be \"AWS KMS is used to encrypt data at rest\"."
      },
      {
        "date": "2023-09-14T19:47:00.000Z",
        "voteCount": 2,
        "content": "D. Set up RDS to use SSL for data in transit\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html\n\n\"SSL/TLS connections provide a layer of security by encrypting data that moves between your client and DB instance. Optionally, your SSL/TLS connection can perform server identity verification by validating the server certificate installed on your DB instance.\""
      },
      {
        "date": "2023-05-22T10:29:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer, in AWS RDS we cannot SSH into underlying instances"
      },
      {
        "date": "2023-02-06T09:13:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect since it allows all ec2 instances can connect to db\nB is incorrect because it doesn't help security\nC is incorrect because security group doesn't have 'Deny' rule\nD is correct because the data is encrypted in transit"
      },
      {
        "date": "2022-11-24T10:07:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-04-29T10:32:00.000Z",
        "voteCount": 1,
        "content": "SSL for data in transit"
      },
      {
        "date": "2022-02-24T11:34:00.000Z",
        "voteCount": 3,
        "content": "After text correct in Q, D is good for security in transit"
      },
      {
        "date": "2021-11-25T05:59:00.000Z",
        "voteCount": 3,
        "content": "D set up ssl"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/26261-exam-aws-certified-database-specialty-topic-1-question-17/",
    "body": "A company needs a data warehouse solution that keeps data in a consistent, highly structured format. The company requires fast responses for end-user queries when looking at data from the current year, and users must have access to the full 15-year dataset, when needed. This solution also needs to handle a fluctuating number incoming queries. Storage costs for the 100 TB of data must be kept low.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage an Amazon Redshift data warehouse solution using a dense storage instance type while keeping all the data on local Amazon Redshift storage. Provision enough instances to support high demand.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent data. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum layer. Provision enough instances to support high demand.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent data. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum layer. Enable Amazon Redshift Concurrency Scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent data. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum layer. Leverage Amazon Redshift elastic resize."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T08:51:00.000Z",
        "voteCount": 14,
        "content": "C. \nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html\n\"ith the Concurrency Scaling feature, you can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance. When concurrency scaling is enabled, Amazon Redshift automatically adds additional cluster capacity when you need it to process an increase in concurrent read queries. Write operations continue as normal on your main cluster. Users always see the most current data, whether the queries run on the main cluster or on a concurrency scaling cluster. You're charged for concurrency scaling clusters only for the time they're in use. For more information about pricing, see Amazon Redshift pricing. You manage which queries are sent to the concurrency scaling cluster by configuring WLM queues. When you enable concurrency scaling for a queue, eligible queries are sent to the concurrency scaling cluster instead of waiting in line.\""
      },
      {
        "date": "2023-05-31T03:54:00.000Z",
        "voteCount": 2,
        "content": "These days, wouldn't you rather use RA3 nodes that automatically unload unused data to 23"
      },
      {
        "date": "2022-04-28T13:36:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"
      },
      {
        "date": "2022-03-21T19:38:00.000Z",
        "voteCount": 1,
        "content": "D.\nMain cluster can't be a single-node cluster."
      },
      {
        "date": "2021-12-07T08:42:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-10-21T08:59:00.000Z",
        "voteCount": 1,
        "content": "Why not D? I think D (Redshift elastic resize) is also correct."
      },
      {
        "date": "2021-10-26T22:39:00.000Z",
        "voteCount": 1,
        "content": "Elastic resize is only available for Amazon Redshift clusters that use the EC2-VPC platform. Option of \"Dense Storage\" and \"Elastic resize\" is mutually exclusive I think"
      },
      {
        "date": "2022-10-03T20:26:00.000Z",
        "voteCount": 3,
        "content": "# https://aws.amazon.com/redshift/faqs/\nQ: What is Elastic Resize and how is it different from Concurrency Scaling?\n\nElastic Resize adds or removes nodes from a single Redshift cluster within minutes to manage its query throughput. For example, an ETL workload for certain hours in a day or\nmonth-end reporting may need additional Amazon Redshift resources to complete on time. Concurrency Scaling adds additional cluster resources to increase the overall \nquery concurrency."
      },
      {
        "date": "2021-10-19T00:33:00.000Z",
        "voteCount": 1,
        "content": "C. But I would argue that you could use redshift spectrum for everything, I just don\u00b4t know if you can use it on S3 IA for the historical data, but it might work."
      },
      {
        "date": "2023-02-06T09:24:00.000Z",
        "voteCount": 1,
        "content": "redshift spectrum doesn't have fast response so C is incorrect."
      },
      {
        "date": "2021-10-15T16:24:00.000Z",
        "voteCount": 2,
        "content": "C ==&gt;&gt; Correct"
      },
      {
        "date": "2021-10-14T02:00:00.000Z",
        "voteCount": 1,
        "content": "C =&gt; S3 + local storage + concurrency scaling"
      },
      {
        "date": "2021-10-11T22:33:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2021-10-06T22:53:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-01T09:16:00.000Z",
        "voteCount": 2,
        "content": "C is the answer"
      },
      {
        "date": "2021-09-29T10:23:00.000Z",
        "voteCount": 2,
        "content": "C Here"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/26011-exam-aws-certified-database-specialty-topic-1-question-18/",
    "body": "A gaming company wants to deploy a game in multiple Regions. The company plans to save local high scores in Amazon DynamoDB tables in each Region. A<br>Database Specialist needs to design a solution to automate the deployment of the database with identical configurations in additional Regions, as needed. The solution should also automate configuration changes across all Regions.<br>Which solution would meet these requirements and deploy the DynamoDB tables?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CLI command to deploy the DynamoDB table to all the Regions and save it for future deployments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template and deploy the template to all the Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template and use a stack set to deploy the template to all the Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate DynamoDB tables using the AWS Management Console in all the Regions and create a step-by-step guide for future deployments."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T12:07:00.000Z",
        "voteCount": 11,
        "content": "C here"
      },
      {
        "date": "2022-02-27T14:13:00.000Z",
        "voteCount": 1,
        "content": "true but wording is wrong as well"
      },
      {
        "date": "2021-10-31T00:37:00.000Z",
        "voteCount": 5,
        "content": "C :  Use CloudFormation StackSets to Provision Resources Across Multiple AWS Accounts and Regions\nhttps://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/"
      },
      {
        "date": "2023-09-14T22:13:00.000Z",
        "voteCount": 2,
        "content": "C. Create an AWS CloudFormation template and use a stack set to deploy the template to all the Regions.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\n\n\"AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions.\""
      },
      {
        "date": "2023-06-22T04:28:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html"
      },
      {
        "date": "2023-03-05T03:19:00.000Z",
        "voteCount": 3,
        "content": "Stack sets are used to deploy to multiple regions."
      },
      {
        "date": "2023-03-01T08:10:00.000Z",
        "voteCount": 1,
        "content": "C is the best choice\nA &amp; D so manual"
      },
      {
        "date": "2022-12-14T18:51:00.000Z",
        "voteCount": 1,
        "content": "Stack sets to deploy to regions"
      },
      {
        "date": "2022-10-19T04:45:00.000Z",
        "voteCount": 1,
        "content": "C\n\nautomate the deployment of the database with identical configurations in additional Regions (leads us to cloud formation), as needed. The solution should also automate configuration changes across all Regions (leads us to stack set)"
      },
      {
        "date": "2022-04-30T08:56:00.000Z",
        "voteCount": 1,
        "content": "Create an AWS CloudFormation template \n-&gt; use a stack set \n-&gt;  deploy the template to all the Regions."
      },
      {
        "date": "2021-12-07T06:05:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-11-02T18:18:00.000Z",
        "voteCount": 1,
        "content": "C &gt;&gt;&gt; Absolutely agreed"
      },
      {
        "date": "2021-10-29T20:55:00.000Z",
        "voteCount": 1,
        "content": "C =&gt; stack set to deploy in multiple regions"
      },
      {
        "date": "2021-10-28T21:57:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; C"
      },
      {
        "date": "2021-10-20T07:03:00.000Z",
        "voteCount": 2,
        "content": "C. \nA stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template"
      },
      {
        "date": "2021-10-18T05:06:00.000Z",
        "voteCount": 1,
        "content": "Yup C it is."
      },
      {
        "date": "2021-10-16T15:21:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2021-10-15T00:43:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/25643-exam-aws-certified-database-specialty-topic-1-question-19/",
    "body": "A team of Database Specialists is currently investigating performance issues on an Amazon RDS for MySQL DB instance and is reviewing related metrics. The team wants to narrow the possibilities down to specific database wait events to better understand the situation.<br>How can the Database Specialists accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the option to push all database logs to Amazon CloudWatch for advanced analysis",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate appropriate Amazon CloudWatch dashboards to contain specific periods of time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon RDS Performance Insights and review the appropriate dashboard\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Enhanced Monitoring will the appropriate settings"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T02:14:00.000Z",
        "voteCount": 13,
        "content": "Yes, C its correct!"
      },
      {
        "date": "2022-04-29T12:19:00.000Z",
        "voteCount": 5,
        "content": "C. Enable Amazon RDS Performance Insights will give you metric specifically for database with the dashboard \n\nD. Enhanced monitoring is for OS info"
      },
      {
        "date": "2023-09-14T22:21:00.000Z",
        "voteCount": 3,
        "content": "C. Enable Amazon RDS Performance Insights and review the appropriate dashboard\n\n\n\nhttps://aws.amazon.com/rds/performance-insights/"
      },
      {
        "date": "2023-01-25T12:55:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.AnalyzeDBLoad.html"
      },
      {
        "date": "2023-01-14T04:56:00.000Z",
        "voteCount": 1,
        "content": "Answer - C"
      },
      {
        "date": "2022-06-21T23:47:00.000Z",
        "voteCount": 1,
        "content": "My take is B"
      },
      {
        "date": "2022-06-20T14:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.AnalyzeDBLoad.html"
      },
      {
        "date": "2023-01-25T12:55:00.000Z",
        "voteCount": 1,
        "content": "This link, confirm that it's Performance Insights.\nAnwser: C"
      },
      {
        "date": "2022-02-24T14:09:00.000Z",
        "voteCount": 2,
        "content": "Performance Insights will give you required info"
      },
      {
        "date": "2021-10-29T23:51:00.000Z",
        "voteCount": 1,
        "content": "C&gt;&gt;&gt; Enable Amazon RDS Performance Insights will give you metric specifically for database with the dashboard to navigate visually around to see database performance activities"
      },
      {
        "date": "2021-10-29T08:37:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-10-22T04:25:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer ==&gt;&gt; C"
      },
      {
        "date": "2021-10-19T23:46:00.000Z",
        "voteCount": 1,
        "content": "When you use RDS Performance Insights, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users. This way, you can identify which queries are causing issues and view the wait type and wait events associated to that query.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-db-performance/"
      },
      {
        "date": "2021-10-15T16:57:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS does not gave Fault injection - Just Aurora has this feature ... Answer is B."
      },
      {
        "date": "2021-10-15T15:25:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2021-10-11T12:16:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-11T05:59:00.000Z",
        "voteCount": 2,
        "content": "C, no doubt"
      },
      {
        "date": "2021-09-21T17:08:00.000Z",
        "voteCount": 3,
        "content": "C as well.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.Enabling.html\nhttps://aws.amazon.com/rds/performance-insights/\nhttps://aws.amazon.com/blogs/database/tuning-amazon-rds-for-mysql-with-performance-insights/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/65817-exam-aws-certified-database-specialty-topic-1-question-20/",
    "body": "A large company is using an Amazon RDS for Oracle Multi-AZ DB instance with a Java application. As a part of its disaster recovery annual testing, the company would like to simulate an Availability Zone failure and record how the application reacts during the DB instance failover activity. The company does not want to make any code changes for this activity.<br>What should the company do to achieve this in the shortest amount of time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a blue-green deployment with a complete application-level failover test",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the RDS console to reboot the DB instance by choosing the option to reboot with failover\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse RDS fault injection queries to simulate the primary node failure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a rule to the NACL to deny all traffic on the subnets associated with a single Availability Zone"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-15T02:15:00.000Z",
        "voteCount": 18,
        "content": "B is the correct option. Use the RDS fault injection query to simulate the primary instance failure - This is a trick option. You can test the fault tolerance of your Aurora DB cluster by using fault injection queries. Fault injection queries are issued as SQL commands to an Aurora instance.\n\nExam Alert:\n\nFault injection queries can be only used with Aurora DB cluster and NOT with an RDS DB cluster."
      },
      {
        "date": "2024-01-12T08:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. \nOracle does not support fault injection queries"
      },
      {
        "date": "2023-09-15T04:24:00.000Z",
        "voteCount": 3,
        "content": "B. Use the RDS console to reboot the DB instance by choosing the option to reboot with failover\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RebootInstance.html\n\n\" Rebooting with failover is beneficial when you want to simulate a failure of a DB instance for testing, or restore operations to the original AZ after a failover occurs\""
      },
      {
        "date": "2023-09-09T05:13:00.000Z",
        "voteCount": 1,
        "content": "A is irrelevant (no software-related change planned).\nB is possible, but the requirement mentions a \"simulation\", not actually doing it.\nC - fault injection is feasible only on Aurora, not on RDS for Oracle.\nD fulfills the requirement, which is to *simulate* an AZ failure (not only a failover of the database). So I'd vote for that."
      },
      {
        "date": "2023-07-04T05:11:00.000Z",
        "voteCount": 1,
        "content": "Exam Alert:\n\nFault injection queries can be only used with Aurora DB cluster and NOT with an RDS DB cluster."
      },
      {
        "date": "2023-05-01T07:34:00.000Z",
        "voteCount": 1,
        "content": "C - As question as to simulate AZ not doing it manually."
      },
      {
        "date": "2023-03-28T03:38:00.000Z",
        "voteCount": 1,
        "content": "Because the fault injection queries wiil take too much time to fail the engine"
      },
      {
        "date": "2023-02-06T09:33:00.000Z",
        "voteCount": 1,
        "content": "A is not relevant.\nB is correct because it is fast to do\nC is incorrect because it is slower than B\nD is incorrect because it affects the whole subnets with multiple resources like ec2, alb,..."
      },
      {
        "date": "2022-12-14T18:58:00.000Z",
        "voteCount": 1,
        "content": "B is the correct option."
      },
      {
        "date": "2022-04-29T11:50:00.000Z",
        "voteCount": 3,
        "content": "reboot with failover option.\n\nYou can force a crash of an Amazon Aurora instance using the ALTER SYSTEM CRASH fault injection query."
      },
      {
        "date": "2022-02-24T14:01:00.000Z",
        "voteCount": 3,
        "content": "Test for failover is done via reboot with failover"
      },
      {
        "date": "2021-12-27T02:28:00.000Z",
        "voteCount": 3,
        "content": "I prefer B.\nMy point is that you can't simulate real application behaviors. For example in JDBC connection, if the TCP socket disconnect detection is not in the java coder. Your connection is stuck while failover."
      },
      {
        "date": "2021-11-13T15:21:00.000Z",
        "voteCount": 4,
        "content": "It seems the RDS fault injection queries are supported only on Aurora :\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html\nSo, for testing Oracle, B should be the correct option."
      },
      {
        "date": "2021-11-14T06:19:00.000Z",
        "voteCount": 1,
        "content": "Yes, but even if it's Aurora instead of RDS, I think the key is that the question says they want to test how the application behaves during DB instance failover activity. in this case it is always the reboot with failover option."
      },
      {
        "date": "2021-12-08T13:41:00.000Z",
        "voteCount": 1,
        "content": "Although B is the correct answer for HA testing but RDS also support fault injection\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/disaster-recovery-resiliency.html\nhttps://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/"
      },
      {
        "date": "2023-09-09T05:16:00.000Z",
        "voteCount": 1,
        "content": "RDS for Oracle doesn't support fault injection, and on the provided links this is also not confirmed."
      },
      {
        "date": "2021-11-10T15:38:00.000Z",
        "voteCount": 3,
        "content": "Option B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/26014-exam-aws-certified-database-specialty-topic-1-question-21/",
    "body": "A company maintains several databases using Amazon RDS for MySQL and PostgreSQL. Each RDS database generates log files with retention periods set to their default values. The company has now mandated that database logs be maintained for up to 90 days in a centralized repository to facilitate real-time and after-the-fact analyses.<br>What should a Database Specialist do to meet these requirements with minimal effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to pull logs from the RDS databases and consolidate the log files in an Amazon S3 bucket. Set a lifecycle policy to expire the objects after 90 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the RDS databases to publish log to Amazon CloudWatch Logs. Change the log retention policy for each log group to expire the events after 90 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a stored procedure in each RDS database to download the logs and consolidate the log files in an Amazon S3 bucket. Set a lifecycle policy to expire the objects after 90 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to download the logs from the RDS databases and publish the logs to Amazon CloudWatch Logs. Change the log retention policy for the log group to expire the events after 90 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T08:59:00.000Z",
        "voteCount": 12,
        "content": "I'll go with B because it facilitate real-time"
      },
      {
        "date": "2021-09-27T23:13:00.000Z",
        "voteCount": 4,
        "content": "I think it's due to the question need minimal effort. A need write a lambda is not minimal effort"
      },
      {
        "date": "2022-02-27T08:23:00.000Z",
        "voteCount": 1,
        "content": "regardless of the effort, why do you want to write code? (option A).\nyou should always try to go with built-in fuctionality"
      },
      {
        "date": "2021-10-12T06:10:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.html"
      },
      {
        "date": "2023-09-15T01:20:00.000Z",
        "voteCount": 2,
        "content": "B. Modify the RDS databases to publish log to Amazon CloudWatch Logs. Change the log retention policy for each log group to expire the events after 90 days. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Procedural.UploadtoCloudWatch.html\n\n\"In an on-premises database, the database logs reside on the file system. Amazon RDS doesn't provide host access to the database logs on the file system of your DB instance. For this reason, Amazon RDS lets you export database logs to Amazon CloudWatch Logs. With CloudWatch Logs, you can perform real-time analysis of the log data. You can also store the data in highly durable storage and manage the data with the CloudWatch Logs Agent. \""
      },
      {
        "date": "2023-07-04T05:17:00.000Z",
        "voteCount": 3,
        "content": "The answer is B.\n\n\"In an on-premises database, the database logs reside on the file system. Amazon RDS doesn't provide host access to the database logs on the file system of your DB instance. For this reason, Amazon RDS lets you export database logs to Amazon CloudWatch Logs. With CloudWatch Logs, you can perform real-time analysis of the log data. You can also store the data in highly durable storage and manage the data with the CloudWatch Logs Agent.\"\nReference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Procedural.UploadtoCloudWatch.html"
      },
      {
        "date": "2023-03-28T03:41:00.000Z",
        "voteCount": 1,
        "content": "Because you need a centralized place for analysis and real time data , using s3 will require to search and view the logs 1 by 1 which is not a real time analyses"
      },
      {
        "date": "2023-03-11T13:08:00.000Z",
        "voteCount": 1,
        "content": "B.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Procedural.UploadtoCloudWatch.html\nPublish logs to CloudWatch"
      },
      {
        "date": "2022-10-16T09:03:00.000Z",
        "voteCount": 1,
        "content": "B it is"
      },
      {
        "date": "2022-06-22T00:51:00.000Z",
        "voteCount": 1,
        "content": "with the least amount of work possible ; _the ans seems to be B but \nwith have central repository to store logs for post-mortem analysis .. the Ans A seems to be correct.. \nThis is confusing question :)"
      },
      {
        "date": "2022-04-29T12:54:00.000Z",
        "voteCount": 1,
        "content": "no for B because: Logfiles are already created, and we do not want to mix that info with other log entries on CloudWatch \n\nso A:\nLambda function to pull logs from the RDS databases -&gt; consolidate the log files in an Amazon S3 bucket. Set a lifecycle policy to expire the objects after 90 days."
      },
      {
        "date": "2022-06-20T07:00:00.000Z",
        "voteCount": 2,
        "content": "Why not mix logs on CloudWatch? We can use different Log groups and we can filter logs...."
      },
      {
        "date": "2022-03-04T22:08:00.000Z",
        "voteCount": 1,
        "content": "B is the correction Option"
      },
      {
        "date": "2022-02-24T14:32:00.000Z",
        "voteCount": 1,
        "content": "B can be scripted too"
      },
      {
        "date": "2022-01-30T04:58:00.000Z",
        "voteCount": 2,
        "content": "B, best option with minimal effort and real time."
      },
      {
        "date": "2022-01-10T12:03:00.000Z",
        "voteCount": 1,
        "content": "Going with B"
      },
      {
        "date": "2021-12-16T19:53:00.000Z",
        "voteCount": 3,
        "content": "B is the right ans. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Procedural.UploadtoCloudWatch.html"
      },
      {
        "date": "2021-11-16T07:53:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2021-11-03T18:26:00.000Z",
        "voteCount": 1,
        "content": "Its a tricky question, the correct option goes down to which of the options uses minimal effort, creating a lambda function or modifying all the databases. The question doesn\u00b4t tell us how many RDS, so I would choose B"
      },
      {
        "date": "2021-11-03T15:29:00.000Z",
        "voteCount": 1,
        "content": "B is Correct"
      },
      {
        "date": "2021-11-03T08:04:00.000Z",
        "voteCount": 1,
        "content": "Vote B for minimal effort."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/25813-exam-aws-certified-database-specialty-topic-1-question-22/",
    "body": "A Database Specialist is setting up a new Amazon Aurora DB cluster with one primary instance and three Aurora Replicas for a highly intensive, business-critical application. The Aurora DB cluster has one medium-sized primary instance, one large-sized replica, and two medium sized replicas. The Database Specialist did not assign a promotion tier to the replicas.<br>In the event of a primary failure, what will occur?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAurora will promote an Aurora Replica that is of the same size as the primary instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAurora will promote an arbitrary Aurora Replica",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAurora will promote the largest-sized Aurora Replica\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAurora will not promote an Aurora Replica"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T21:39:00.000Z",
        "voteCount": 17,
        "content": "ANS: C\nPriority: If you don't select a value, the default is tier-1. This priority determines the order in which Aurora\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html\n\nMore than one Aurora Replica can share the same priority, resulting in promotion tiers. If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon RDS promotes an arbitrary replica in the same promotion tier.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance"
      },
      {
        "date": "2023-05-26T23:42:00.000Z",
        "voteCount": 1,
        "content": "C:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n\nMore than one Aurora Replica can share the same priority, resulting in promotion tiers. If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon RDS promotes an arbitrary replica in the same promotion tier."
      },
      {
        "date": "2023-03-05T03:36:00.000Z",
        "voteCount": 3,
        "content": "1. Highest priority will be promoted.\n2. Largest size will be promoted.\n3. Random replica will be promoted.\n\nSince they are the same priority, it will go to (2) Largest size will be promoted.\nIf all replicas were the same size, a random replica will be promoted."
      },
      {
        "date": "2023-11-08T20:43:00.000Z",
        "voteCount": 1,
        "content": "but he trick here is: The larger instances are reserved for specialized kinds of reporting queries. To make it unlikely for them to be promoted to the primary instance, the following example changes their promotion tier to the lowest priority."
      },
      {
        "date": "2022-04-30T19:41:00.000Z",
        "voteCount": 3,
        "content": "default tier = -1\nMore than one Aurora Replica can share the same priority, resulting in promotion tiers. If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size."
      },
      {
        "date": "2022-04-13T01:30:00.000Z",
        "voteCount": 2,
        "content": "C\n\"If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon RDS promotes an arbitrary replica in the same promotion tier.\""
      },
      {
        "date": "2021-11-02T04:38:00.000Z",
        "voteCount": 1,
        "content": "C\nIf two replicas have the same priority, then the replica that is largest in size gets promoted"
      },
      {
        "date": "2021-10-30T11:10:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; C"
      },
      {
        "date": "2021-10-29T15:41:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; C"
      },
      {
        "date": "2021-10-25T02:36:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2021-10-23T14:37:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-22T08:20:00.000Z",
        "voteCount": 4,
        "content": "C. If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon RDS promotes an arbitrary replica in the same promotion tier.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html"
      },
      {
        "date": "2021-10-17T19:27:00.000Z",
        "voteCount": 2,
        "content": "C is the right choice. Since all replicas have same default priority the largest among them will be considered for failover."
      },
      {
        "date": "2021-10-17T10:37:00.000Z",
        "voteCount": 1,
        "content": "Since priority isn't assigned, aurora will promote the largest sized replica."
      },
      {
        "date": "2021-10-11T10:39:00.000Z",
        "voteCount": 1,
        "content": "Ans is C"
      },
      {
        "date": "2021-10-09T21:40:00.000Z",
        "voteCount": 2,
        "content": "Answer: C, If you don't have priority, Aurora will pick the biggest size by default"
      },
      {
        "date": "2021-10-03T14:40:00.000Z",
        "voteCount": 4,
        "content": "C is right!"
      },
      {
        "date": "2021-09-26T21:21:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/aws/additional-failover-control-for-amazon-aurora/\nA\n\"In the event of a failover, Amazon RDS will promote the read replica that has the highest priority (the lowest numbered tier). If two or more replicas have the same priority, RDS will promote the one that is the same size as the previous primary instance.\""
      },
      {
        "date": "2021-10-02T20:23:00.000Z",
        "voteCount": 12,
        "content": "Switching to C. AWS has made this question way to hard as their own documentation isn't consistent with their priority levels. I'm going with largest first priority due to https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf\nMore than one Aurora Replica can share the same priority, resulting in promotion tiers. If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If\ntwo or more Aurora Replicas share the same priority and size, then Amazon RDS promotes an arbitrary replica in the same promotion tier.\""
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/25547-exam-aws-certified-database-specialty-topic-1-question-23/",
    "body": "A company is running its line of business application on AWS, which uses Amazon RDS for MySQL at the persistent data store. The company wants to minimize downtime when it migrates the database to Amazon Aurora.<br>Which migration method should a Database Specialist use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the RDS for MySQL DB instance and create a new Aurora DB cluster with the option to migrate snapshots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a backup of the RDS for MySQL DB instance using the mysqldump utility, create a new Aurora DB cluster, and restore the backup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Replica from the RDS for MySQL DB instance and promote the Aurora DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a clone of the RDS for MySQL DB instance and promote the Aurora DB cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T15:05:00.000Z",
        "voteCount": 12,
        "content": "Answer C. Minimal Downtime with Aurora replica. However this is not the fastest."
      },
      {
        "date": "2021-09-29T10:53:00.000Z",
        "voteCount": 13,
        "content": "Agree C.\nThis blogpost explains why C is the answer since read replicas reduce downtime.\nhttps://aws.amazon.com/blogs/database/best-practices-for-migrating-rds-for-mysql-databases-to-amazon-aurora/"
      },
      {
        "date": "2023-09-15T00:58:00.000Z",
        "voteCount": 1,
        "content": "C. Create an Aurora Replica from the RDS for MySQL DB instance and promote the Aurora DB cluster.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html"
      },
      {
        "date": "2023-05-05T10:05:00.000Z",
        "voteCount": 1,
        "content": "When migrating from RDS Mysql/Postgres to Aurora,\neasiest and most effortless way is to take an (aurora replica and then promote it to standalone).\n\nanswer is C."
      },
      {
        "date": "2023-03-28T03:49:00.000Z",
        "voteCount": 1,
        "content": "This article from aws show you that C is exactly the answer https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/"
      },
      {
        "date": "2023-03-27T08:09:00.000Z",
        "voteCount": 1,
        "content": "C.\nPlease refer below ---\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html\n\"\nYou can migrate from an RDS for MySQL DB instance by first creating an Aurora MySQL read replica of a MySQL DB instance. When the replica lag between the MySQL DB instance and the Aurora MySQL read replica is 0, you can direct your client applications to read from the Aurora read replica and then stop replication to make the Aurora MySQL read replica a standalone Aurora MySQL DB cluster for reading and writing. For details, see Migrating data from a MySQL DB instance to an Amazon Aurora MySQL DB cluster by using an Aurora read replica.\n\""
      },
      {
        "date": "2023-01-19T00:31:00.000Z",
        "voteCount": 2,
        "content": "C is the Answer as per cloud guru. Key is minimal downtime"
      },
      {
        "date": "2022-05-21T14:10:00.000Z",
        "voteCount": 1,
        "content": "C =&gt; right  as minimize downtime."
      },
      {
        "date": "2022-04-30T10:06:00.000Z",
        "voteCount": 1,
        "content": "MySQL DB instance \n-&gt; Create an Aurora Replica ( will take time)\n-&gt; promote the Aurora DB cluster (Minimal downtime)"
      },
      {
        "date": "2021-12-22T13:12:00.000Z",
        "voteCount": 1,
        "content": "Answer is option C .. Minimal downtime."
      },
      {
        "date": "2021-12-07T06:48:00.000Z",
        "voteCount": 3,
        "content": "Option C"
      },
      {
        "date": "2021-11-04T04:03:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer ==&gt;&gt; C"
      },
      {
        "date": "2021-11-01T16:12:00.000Z",
        "voteCount": 1,
        "content": "C. Create an Aurora Replica from the RDS for MySQL DB instance and promote the Aurora DB cluster.\nThis may not be the fastest but the question is \"minimize downtime\"."
      },
      {
        "date": "2021-10-24T02:41:00.000Z",
        "voteCount": 1,
        "content": "I go with C"
      },
      {
        "date": "2021-10-18T10:25:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-18T08:52:00.000Z",
        "voteCount": 2,
        "content": "The answer is A and I will prove it --- please follow with me carefully\nThere is a requirement for minimal downtime (FASTEST) --- I go with A \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQL.Import.Console \n\nAccording to that link \u201cBe prepared for migration to take a while, roughly several hours per tebibyte (TiB) of data.\u201d if you go with C. C will thus lead to a longer downtime while A will not"
      },
      {
        "date": "2021-10-19T01:57:00.000Z",
        "voteCount": 1,
        "content": "Ok, so what about data written to source RDS after snapshop but before swap to Aurora? You loose data that's unacceptable"
      },
      {
        "date": "2021-10-27T09:25:00.000Z",
        "voteCount": 1,
        "content": "You need to understand the difference between minimal downtime and Fastest. Both are not same. Answer is C."
      },
      {
        "date": "2021-10-11T12:05:00.000Z",
        "voteCount": 2,
        "content": "C because it facilitates migration to aurora with least down-time."
      },
      {
        "date": "2021-10-03T00:48:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#AuroraPostgreSQL.Migrating.RDSPostgreSQL.Replica"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/25814-exam-aws-certified-database-specialty-topic-1-question-24/",
    "body": "The Security team for a finance company was notified of an internal security breach that happened 3 weeks ago. A Database Specialist must start producing audit logs out of the production Amazon Aurora PostgreSQL cluster for the Security team to use for monitoring and alerting. The Security team is required to perform real-time alerting and monitoring outside the Aurora DB cluster and wants to have the cluster push encrypted files to the chosen solution.<br>Which approach will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse pg_audit to generate audit logs and send the logs to the Security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail to audit the DB cluster and the Security team will get data from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up database activity streams and connect the data stream from Amazon Kinesis to consumer applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on verbose logging and set up a schedule for the logs to be dumped out for the Security team."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T14:41:00.000Z",
        "voteCount": 15,
        "content": "answer is C.\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/"
      },
      {
        "date": "2021-09-26T17:31:00.000Z",
        "voteCount": 3,
        "content": "Yes, real time"
      },
      {
        "date": "2021-09-29T14:08:00.000Z",
        "voteCount": 4,
        "content": "Agree C, the link you posted nails it. Anytime the question want \"real time alerts or streams\", its almost always Kinesis streams. \n\"Database Activity Streams for Amazon Aurora with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your database and help meet compliance and regulatory requirements.\""
      },
      {
        "date": "2023-06-22T05:14:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html"
      },
      {
        "date": "2023-03-28T03:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html"
      },
      {
        "date": "2023-01-14T05:03:00.000Z",
        "voteCount": 1,
        "content": "Ans - C"
      },
      {
        "date": "2022-07-24T06:22:00.000Z",
        "voteCount": 1,
        "content": "answer is C.\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/\nDatabase Activity Streams for Amazon Aurora with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your database and help meet compliance and regulatory requirements."
      },
      {
        "date": "2022-07-12T06:44:00.000Z",
        "voteCount": 1,
        "content": "Database Activity Streams for Amazon Aurora with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your database and help meet compliance and regulatory requirements."
      },
      {
        "date": "2022-05-01T07:25:00.000Z",
        "voteCount": 2,
        "content": "I chose C over B\nx B. Use AWS CloudTrail to audit the DB cluster and the Security team will get data from Amazon S3.\n- CloudTrail is automatically enabled for all accounts, and logs events, so this is good for past activities for breach\nAmazon Aurora activity is recorded in a CloudTrail event in Event history. You can use the CloudTrail console to view the last 90 days of recorded API activity and events in an AWS Region.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/logging-using-cloudtrail.html\n\nhttps://aws.amazon.com/cloudtrail/faqs/\n\nC is for future activities\nDatabase Activity Streams can monitor and audit database activity to provide real time safeguards for your database and help meet compliance and regulatory requirements.\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/\n\nDB-&gt;Database Activity Streams-&gt;Kinesis -&gt;Security team\n\nC. Set up database activity streams and connect the data stream from Amazon Kinesis to consumer applications."
      },
      {
        "date": "2023-03-11T16:33:00.000Z",
        "voteCount": 1,
        "content": "I like your explanation -- CloudTrail is for the historical data; Kinesis is for the future data."
      },
      {
        "date": "2021-12-24T13:31:00.000Z",
        "voteCount": 1,
        "content": "Real-time alerting and monitoring &gt; Option C"
      },
      {
        "date": "2021-11-04T10:09:00.000Z",
        "voteCount": 1,
        "content": "C https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html"
      },
      {
        "date": "2021-10-31T08:57:00.000Z",
        "voteCount": 1,
        "content": "Only C meets the real-time, A, D is possible but schedule."
      },
      {
        "date": "2021-10-27T20:42:00.000Z",
        "voteCount": 1,
        "content": "C. Set up database activity streams and connect the data stream from Amazon Kinesis to consumer applications.\n\nAurora Database activity streams provide a near real-time data stream of the database activity for an Aurora DB cluster.  Database activity streams require the use of AWS KMS because the activity streams are always encrypted."
      },
      {
        "date": "2021-10-26T20:07:00.000Z",
        "voteCount": 1,
        "content": "C for sure."
      },
      {
        "date": "2021-10-20T14:37:00.000Z",
        "voteCount": 2,
        "content": "Between A and C, but would go with C after reading this:\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/"
      },
      {
        "date": "2021-10-15T04:34:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-13T02:41:00.000Z",
        "voteCount": 1,
        "content": "I agree with C. I think that Cloudtrail does not support the type of loging that would be required here, it is not the internal databsae info: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html"
      },
      {
        "date": "2021-10-10T19:50:00.000Z",
        "voteCount": 1,
        "content": "Between A &amp; C I will go with C as the logs should be used for real-time alerting and monitoring."
      },
      {
        "date": "2021-10-07T22:28:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/25405-exam-aws-certified-database-specialty-topic-1-question-25/",
    "body": "A company is using Amazon RDS for MySQL to redesign its business application. A Database Specialist has noticed that the Development team is restoring their<br>MySQL database multiple times a day when Developers make mistakes in their schema updates. The Developers sometimes need to wait hours for the restores to complete.<br>Multiple team members are working on the project, making it difficult to find the correct restore point for each mistake.<br>Which approach should the Database Specialist take to reduce downtime?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy multiple read replicas and have the team members make changes to separate replica instances",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Amazon RDS for SQL Server, take a snapshot, and restore from the snapshot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Amazon Aurora MySQL and enable the Aurora Backtrack feature\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Amazon RDS for MySQL Backtrack feature"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-10T14:13:00.000Z",
        "voteCount": 17,
        "content": "Answer is : A,Backtrack feature is only available with Aurora.\neach developer can perform DDL operations for the schema changes on the MySQL read replica once the read replica is in sync with its primary DB instance. Then the developer can promote the read replica and direct the application to use the promoted instance during the development phase. This solution isolates the schema changes done by each developer to their own promoted instance. This also avoids the problem of keeping track of the \"correct restore point\" that the team faced while using the same DB instance."
      },
      {
        "date": "2021-10-12T23:37:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-06-14T19:22:00.000Z",
        "voteCount": 1,
        "content": "Developers would still need to wait hours for the restore to complete, in case of mistakes in their schema updates. But if we try to avoid this, the other option C requires migration to Aurora! and even then \"Multiple team members are working on the project, making it difficult to find the correct restore point for each mistake.\" It is a tough call. https://serverfault.com/questions/791359/how-do-i-make-an-aws-rds-mysql-read-replica-have-a-different-schema\nthis shows how read replica can be used for writes.. with some limitations.."
      },
      {
        "date": "2023-09-10T12:30:00.000Z",
        "voteCount": 1,
        "content": "That's why C is correct: Migrate to Aurora MySQL, there you can use the BackTrack functionality which is not available in RDS MySQL."
      },
      {
        "date": "2021-09-24T00:00:00.000Z",
        "voteCount": 17,
        "content": "I don't see how it can be A. These are read replicas, so by definition you cannot make changes to them. C sounds right to me"
      },
      {
        "date": "2021-09-27T01:33:00.000Z",
        "voteCount": 3,
        "content": "yes C is the answer."
      },
      {
        "date": "2021-09-28T17:17:00.000Z",
        "voteCount": 2,
        "content": "C agree\nhttps://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/"
      },
      {
        "date": "2022-01-13T13:34:00.000Z",
        "voteCount": 3,
        "content": "Yes you can write to MYSQL readable replicas as mentioned here https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
      },
      {
        "date": "2022-10-11T18:06:00.000Z",
        "voteCount": 1,
        "content": "if using clone then will be OK  but not Replicas - Read only"
      },
      {
        "date": "2024-01-12T08:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. \nYou cannot make changes on a read replica, which makes A invalid."
      },
      {
        "date": "2023-10-05T05:38:00.000Z",
        "voteCount": 1,
        "content": "the key is 'unable to find the correct restore point', so backtrack won't as you don't know which point it needs backing to as many people making many changes, so first thing is isolate each changes by allocating their own instances"
      },
      {
        "date": "2023-09-28T03:32:00.000Z",
        "voteCount": 1,
        "content": "With RDS MySQL it's true to that you can make changes on read replica when setting read_only=0 to make read replica writable, but the use cases are recommended for indexing tables and small maintancae action because other action could break the replication. In addition the option read_only=0 works only for engines like InnoDB and with MyISAM u break the replication. In the question is nothing written about the engine type and that's why C is the right answer in my opinion."
      },
      {
        "date": "2023-09-28T23:37:00.000Z",
        "voteCount": 1,
        "content": "Mmmhh now I saw the exact same question in an Udemy Exam Course and the authors are saying it is indeed Answer A with the following explaination:\n\n\"For the given use-case, each developer can perform DDL operations for the schema changes on the MySQL read replica once the read replica is in sync with its primary DB instance. Then the developer can promote the read replica and direct the application to use the promoted instance during the development phase. This solution isolates the schema changes done by each developer to their own promoted instance. This also avoids the problem of keeping track of the \"correct restore point\" that the team faced while using the same DB instance.\n\nIf you need to make changes to the MySQL or MariaDB read replica, you must set the read_only parameter to 0 in the DB parameter group for the read replica. You can then perform all needed DDL operations, such as creating indexes, on the read replica. Actions taken on the read replica don't affect the performance of the primary DB instance.\""
      },
      {
        "date": "2023-09-01T13:50:00.000Z",
        "voteCount": 2,
        "content": "Although you can enable updates by setting the read_only parameter to 0 in the DB parameter group for the read replica, we recommend that you don't do so because it can cause problems if the read replica becomes incompatible with the source DB instance\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html#USER_MySQL.Replication.ReadReplicas.DelayReplication.ExistingReplica\nBeside, how to find the restore point with replica after mistake"
      },
      {
        "date": "2023-08-01T07:48:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\"...have the team members make changes to separate replica instances...\" - we can make MySQL read replica writable. This will destroy replication, but for our purposes it is not critical. \nC - \"Aurora Backtrack feature\" doesn't suite because of \"Multiple team members are working on the project, making it difficult to find the correct restore point for each mistake.\""
      },
      {
        "date": "2023-07-21T08:16:00.000Z",
        "voteCount": 2,
        "content": "Tricky one, is it C or A ? What is the Final answer some say A some C. Both seem plausible, but which is the better option"
      },
      {
        "date": "2023-07-04T06:11:00.000Z",
        "voteCount": 1,
        "content": "A (correct) \nB - incorrect - Migration to SQL server needs a high impact on the application code. Restoring from a snapshot is much slower than using a prepared read replica.\nC - incorrect - I think there was a good reason why the company use RDS instead of Aurora. But if you migrate to Aurora better to use the Clone feature instead of Backtrack in that case.\nD - incorrect - Backtrack is an Aurora (MySQL only) feature, not for RDS."
      },
      {
        "date": "2023-05-23T03:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is A because this involves minimum effort"
      },
      {
        "date": "2023-05-22T16:52:00.000Z",
        "voteCount": 1,
        "content": "As stated by others, PITR is not a good option since it is \"difficult to find the correct restore point for each mistake\", and PITR is also a feature only available on Aurora"
      },
      {
        "date": "2023-05-22T06:10:00.000Z",
        "voteCount": 2,
        "content": "The answer is A: Since MySQL Read replicas can be made writeable as per below documentation link. Previously, I was also thinking of C as a correct answer but this is wrong because of the statement \"Multiple team members are involved in the project, which makes it difficult to identify the right restoration point for each error\"\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#:~:text=Yes.%C2%A0You%20can%20enable%20the%20MySQL%20or%20MariaDB%20read%20replica%20to%20be%20writable."
      },
      {
        "date": "2023-03-28T04:00:00.000Z",
        "voteCount": 2,
        "content": "How is it possible to update or modify a read replica ?"
      },
      {
        "date": "2023-02-11T21:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is C: Migrate to Aurora &amp; use back track feature."
      },
      {
        "date": "2023-02-11T21:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is C: Solution is to migrate the RDS to Aurora and enable backtrack. Folks who are saying A needs to explain on how they can apply changes on Read replicas and rollback them!!??"
      },
      {
        "date": "2023-02-06T09:47:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect because 2 reasons: the first reason is read replica is read-only. The second reason is you need to modify the app to use different read replica instance endpoint for each team. If they have only one business app connects to a database, it is nightmare to connect to multiple read replica for each team\nB is incorrect because it is more expensive and can not solve the problem\nC is correct at backtrack is in-place restore\nD is incorrect because rds mysql doesn't support backtrack\nsorry for my english"
      },
      {
        "date": "2022-11-22T11:24:00.000Z",
        "voteCount": 3,
        "content": "I will go with A as well, mainly because of the wording on \"correct restore point\""
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/25406-exam-aws-certified-database-specialty-topic-1-question-26/",
    "body": "A media company is using Amazon RDS for PostgreSQL to store user data. The RDS DB instance currently has a publicly accessible setting enabled and is hosted in a public subnet. Following a recent AWS Well-Architected Framework review, a Database Specialist was given new security requirements.<br>\u2711 Only certain on-premises corporate network IPs should connect to the DB instance.<br>\u2711 Connectivity is allowed from the corporate network only.<br>Which combination of steps does the Database Specialist need to take to meet these new requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the pg_hba.conf file. Add the required corporate network IPs and remove the unwanted IPs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the associated security group. Add the required corporate network IPs and remove the unwanted IPs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the DB instance to a private subnet using AWS DMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable VPC peering between the application host running on the corporate network and the VPC associated with the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the publicly accessible setting.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the DB instance using private IPs and a VPN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BEF",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-03T18:28:00.000Z",
        "voteCount": 14,
        "content": "Answer is BEF\nDatabase does not need to be in private subnet (there is no requirement in the question) disabling public accessibility will remove public IP address associated from the instance."
      },
      {
        "date": "2021-10-25T00:20:00.000Z",
        "voteCount": 8,
        "content": "B E F are the answers.\nD is not correct because there is on-premise network, VPC peering is for AWS VPC - AWS VPC\nC is not correct. DMS is using for DB migration, not subnet modification"
      },
      {
        "date": "2021-11-03T14:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; B,E,F"
      },
      {
        "date": "2023-08-01T08:20:00.000Z",
        "voteCount": 1,
        "content": "https://kloudle.com/academy/how-to-restrict-access-to-your-publicly-accessible-rds-instance/\nAfter publicly accessible setting disable no need to move the DB instance to a private subnet.\nWe do not change the conf file settings, instead we change the security group where we configure the IP addresses from which access is required.\nVPC peering only inside AWS.\nYou need to select the third item, nothing but \"Connect to the DB instance using private IPs and a VPN\" is suitable."
      },
      {
        "date": "2023-02-19T20:33:00.000Z",
        "voteCount": 1,
        "content": "BEF for sure"
      },
      {
        "date": "2022-04-29T19:35:00.000Z",
        "voteCount": 1,
        "content": "x A. RDS you don't edit config files directly\nB. Modify the security group. Add the required corporate network IPs and remove the unwanted IPs\nx C. sunet change by DMS?\nx D. VPC peering is within AWS only\nE. disable publicly accessible\nF. .Connect to the DB instance using private IPs and a VPN."
      },
      {
        "date": "2022-07-03T05:41:00.000Z",
        "voteCount": 1,
        "content": "You cant ssh directly into rds how would you connect using private IP's?"
      },
      {
        "date": "2022-03-05T07:22:00.000Z",
        "voteCount": 2,
        "content": "1) Security Groups HAS to be done to restrict DB access to specific IPS\n2) Public accessibility has  to be removed\n3) Corp to AWS VPN has to be enabled to secure traffic"
      },
      {
        "date": "2022-02-24T19:49:00.000Z",
        "voteCount": 2,
        "content": "agree with others"
      },
      {
        "date": "2022-01-20T06:00:00.000Z",
        "voteCount": 3,
        "content": "F is incorrect. RDS uses endpoint, not IP address.\n\nI vote for BCE"
      },
      {
        "date": "2021-11-30T03:32:00.000Z",
        "voteCount": 3,
        "content": "B, E &amp; F"
      },
      {
        "date": "2021-10-24T14:39:00.000Z",
        "voteCount": 2,
        "content": "B,C and E are correct. You need to migrate the database to private subnet before you can disable the publicly accessible setting in the console."
      },
      {
        "date": "2021-10-22T01:46:00.000Z",
        "voteCount": 3,
        "content": "BEF for me"
      },
      {
        "date": "2021-10-18T21:15:00.000Z",
        "voteCount": 3,
        "content": "Ans: BEF"
      },
      {
        "date": "2021-10-18T06:23:00.000Z",
        "voteCount": 4,
        "content": "I will go with BEF.\nIdeally db should be moved to private subnet.But using DMS for that makes C a wrong choice"
      },
      {
        "date": "2021-10-17T23:19:00.000Z",
        "voteCount": 1,
        "content": "BDF\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-connectivity-instance-subnet-vpc/\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html"
      },
      {
        "date": "2021-10-27T12:28:00.000Z",
        "voteCount": 1,
        "content": "VPC peering is between AWS VPCs, not between on-primise network and AWS VPC"
      },
      {
        "date": "2021-10-15T17:03:00.000Z",
        "voteCount": 1,
        "content": "F- Connect to DB instance using VPC peering - At which point in this question does it mention the need to connect to other VPC's?  Best practices in AWS doco state that (see the Note) - - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html#USER_VPC.Hiding"
      },
      {
        "date": "2021-10-11T22:29:00.000Z",
        "voteCount": 2,
        "content": "B,C,E,F are probable answers. \nC - database needs to moved private to public subnet, however, migration can be done through snapshot instead dms. \nSo BEF would be the answers"
      },
      {
        "date": "2021-10-09T19:29:00.000Z",
        "voteCount": 1,
        "content": "BEF for me"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/25515-exam-aws-certified-database-specialty-topic-1-question-27/",
    "body": "A company is about to launch a new product, and test databases must be re-created from production data. The company runs its production databases on an<br>Amazon Aurora MySQL DB cluster. A Database Specialist needs to deploy a solution to create these test databases as quickly as possible with the least amount of administrative effort.<br>What should the Database Specialist do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore a snapshot from the production cluster into test clusters",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate logical dumps of the production cluster and restore them into new test clusters",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse database cloning to create clones of the production cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an additional read replica to the production cluster and use that node for testing"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-15T11:16:00.000Z",
        "voteCount": 17,
        "content": "C. \nhttps://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/\n\"Cloning an Aurora cluster is extremely useful if you want to assess the impact of changes to your database, or if you need to perform workload-intensive operations\u2014such as exporting data or running analytical queries, or simply if you want to use a copy of your production database in a development or testing environment. You can make multiple clones of your Aurora DB cluster. You can even create additional clones from other clones, with the constraint that the clone databases must be created in the same region as the source databases."
      },
      {
        "date": "2023-08-01T08:26:00.000Z",
        "voteCount": 1,
        "content": "When we do cloning, there is no physical overwriting of data to a new location, only meta-data is copied. Therefore, the cloning process is quite fast. However, further changes are isolated from each other in source and clone."
      },
      {
        "date": "2023-03-28T04:08:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html"
      },
      {
        "date": "2023-03-06T00:05:00.000Z",
        "voteCount": 1,
        "content": "Cloning is the best choice. It creates a new db cluster with the same data."
      },
      {
        "date": "2022-09-29T10:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\nCreating a clone is faster and more space-efficient than physically copying the data using other techniques, such as restoring a snapshot."
      },
      {
        "date": "2022-04-30T19:04:00.000Z",
        "voteCount": 2,
        "content": "C. Use database cloning to create clones of the production cluster\n\nCloning is best choice with \"copy-on-write\" protocol, database becomes available in a few mins.\n\nhttps://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/\n\"Cloning an Aurora cluster is extremely useful if you want to assess the impact of changes to your database, or if you need to perform workload-intensive operations\u2014such as exporting data or running analytical queries, or simply if you want to use a copy of your production database in a development or testing environment. You can make multiple clones of your Aurora DB cluster. You can even create additional clones from other clones, with the constraint that the clone databases must be created in the same region as the source databases."
      },
      {
        "date": "2022-02-03T10:40:00.000Z",
        "voteCount": 2,
        "content": "Cloning is best choice with \"copy-on-write\" protocol, database becomes available in a few mins."
      },
      {
        "date": "2021-11-02T19:51:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2021-11-02T13:04:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-11-02T09:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nAurora copy-on-write clones are best suited for this requirement"
      },
      {
        "date": "2021-10-23T11:26:00.000Z",
        "voteCount": 2,
        "content": "Yes, C"
      },
      {
        "date": "2021-10-12T05:26:00.000Z",
        "voteCount": 3,
        "content": "C Database Clone is quickest and the right approach. Takes secs.."
      },
      {
        "date": "2021-10-10T17:03:00.000Z",
        "voteCount": 3,
        "content": "C is correct as we have least admin effort and quck"
      },
      {
        "date": "2021-10-08T02:00:00.000Z",
        "voteCount": 1,
        "content": "D here!"
      },
      {
        "date": "2021-10-02T10:52:00.000Z",
        "voteCount": 2,
        "content": "D is neither least administrative nor will it meet the requirement. A is simplest for me"
      },
      {
        "date": "2021-10-20T09:25:00.000Z",
        "voteCount": 5,
        "content": "Restoring from snapshot is never quick.\nAnswer is C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/25516-exam-aws-certified-database-specialty-topic-1-question-28/",
    "body": "A company with branch offices in Portland, New York, and Singapore has a three-tier web application that leverages a shared database. The database runs on<br>Amazon RDS for MySQL and is hosted in the us-west-2 Region. The application has a distributed front end deployed in the us-west-2, ap-southheast-1, and us- east-2 Regions.<br>This front end is used as a dashboard for Sales Managers in each branch office to see current sales statistics. There are complaints that the dashboard performs more slowly in the Singapore location than it does in Portland or New York. A solution is needed to provide consistent performance for all users in each location.<br>Which set of actions will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the instance in the us-west-2 Region. Create a new instance from the snapshot in the ap-southeast-1 Region. Reconfigure the ap- southeast-1 front-end dashboard to access this instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS read replica in the ap-southeast-1 Region from the primary RDS DB instance in the us-west-2 Region. Reconfigure the ap-southeast-1 front- end dashboard to access this instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new RDS instance in the ap-southeast-1 Region. Use AWS DMS and change data capture (CDC) to update the new instance in the ap-southeast-1 Region. Reconfigure the ap-southeast-1 front-end dashboard to access this instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS read replica in the us-west-2 Region where the primary instance resides. Create a read replica in the ap-southeast-1 Region from the read replica located on the us-west-2 Region. Reconfigure the ap-southeast-1 front-end dashboard to access this instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T22:14:00.000Z",
        "voteCount": 20,
        "content": "Leaning to B. \nhttps://aws.amazon.com/rds/features/read-replicas/\n\"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. \""
      },
      {
        "date": "2022-05-19T02:37:00.000Z",
        "voteCount": 1,
        "content": "Agree it's B."
      },
      {
        "date": "2021-09-30T04:02:00.000Z",
        "voteCount": 6,
        "content": "If they go with A, how do they keep the databases in sync?\nB for me"
      },
      {
        "date": "2023-12-13T20:36:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-10-05T05:51:00.000Z",
        "voteCount": 1,
        "content": "apparently I need to now geography as well that Singapore is AP region"
      },
      {
        "date": "2023-11-24T22:53:00.000Z",
        "voteCount": 1,
        "content": "They assume as much."
      },
      {
        "date": "2023-09-10T12:55:00.000Z",
        "voteCount": 1,
        "content": "From the plausible options,\nA - this would make it a non-shared database, destroying the architecture.\nB - iff the dashboard is only for reading then the read replica in SGP might do."
      },
      {
        "date": "2023-03-28T04:11:00.000Z",
        "voteCount": 1,
        "content": "Because the read replica is designed for this purpose"
      },
      {
        "date": "2023-02-06T09:56:00.000Z",
        "voteCount": 3,
        "content": "A is incorrect in no data synchronization between 2 databases\nB is correct since the read replica is same region with the frontend\nC is incorrect because extra cost of DMS instance. Although it is feasible option.\nD is incorrect because of extra cost of new replica on primary region and replication lag."
      },
      {
        "date": "2022-11-28T09:49:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2022-07-05T18:50:00.000Z",
        "voteCount": 1,
        "content": "You can create read replication from a already existing replica, but the replication lag will be high. So D is operationally possible but replication lag will be high. \nSo correct answer is B."
      },
      {
        "date": "2022-06-23T05:49:00.000Z",
        "voteCount": 1,
        "content": "Answer:B"
      },
      {
        "date": "2022-04-30T12:12:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html\nYou can create replicas through Region same way as in same region\n\nCreate an RDS read replica in the ap-southeast-1 Region from the primary RDS DB instance in the us-west-2 Region. Reconfigure the ap-southeast-1 front- end dashboard to access this instance."
      },
      {
        "date": "2022-02-02T16:03:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer!!"
      },
      {
        "date": "2021-11-03T13:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct. You can create replicas through Region\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html"
      },
      {
        "date": "2021-11-01T12:36:00.000Z",
        "voteCount": 1,
        "content": "I will go with B"
      },
      {
        "date": "2021-10-31T08:24:00.000Z",
        "voteCount": 2,
        "content": "Would go with B"
      },
      {
        "date": "2021-10-30T23:21:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-10-30T04:18:00.000Z",
        "voteCount": 3,
        "content": "Not sure why you all excluded C. It is definitely a good solution to keep two databases in sync using DMS with CDC."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/67416-exam-aws-certified-database-specialty-topic-1-question-29/",
    "body": "A company wants to migrate its existing on-premises Oracle database to Amazon Aurora PostgreSQL. The migration must be completed with minimal downtime using AWS DMS. A Database Specialist must validate that the data was migrated accurately from the source to the target before the cutover. The migration must have minimal impact on the performance of the source database.<br>Which approach will MOST effectively meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to convert source Oracle database schemas to the target Aurora DB cluster. Verify the datatype of the columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the table metrics of the AWS DMS task created for migrating the data to verify the statistics for the tables being migrated and to verify that the data definition language (DDL) statements are completed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the AWS Schema Conversion Tool (AWS SCT) premigration validation and review the premigration checklist to make sure there are no issues with the conversion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS DMS data validation on the task so the AWS DMS task compares the source and target records, and reports any mismatches.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-13T08:12:00.000Z",
        "voteCount": 9,
        "content": "\"To ensure that your data was migrated accurately from the source to the target, we highly recommend that you use data validation.\" \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html\n\nAnswer is D."
      },
      {
        "date": "2023-12-31T23:04:00.000Z",
        "voteCount": 1,
        "content": "DMS supports the validation between source and target data. But SCT does not support it."
      },
      {
        "date": "2022-06-28T19:58:00.000Z",
        "voteCount": 2,
        "content": "table metrics of the AWS DMS can be verified manully for DMS task.  There is no mechanism which provided an automated for reading these metric and providing confirmation if every thing went well. If you are working 100+ tables then DMS Data Validation is the only option. \nD is best fit"
      },
      {
        "date": "2022-04-30T06:59:00.000Z",
        "voteCount": 2,
        "content": "D would run select queries on source &amp; target to compare rows, so some load\n\nB is also good candidate, rather better but it needed \"The data validation option in the DMS task has to be activated before DMS \"\nYou can find individual table metrics on the Table statistics tab for each individual task. These metrics include these numbers:\nRows loaded during the full load.\nInserts, updates, and deletes since the task started.\nDDL operations since the task started.\n\nB. Use the table metrics of the AWS DMS task created for migrating the data to verify the statistics for the tables being migrated and to verify that the data definition language (DDL) statements are completed.\n\nD. Enable AWS DMS data validation on the task so the AWS DMS task compares the source and target records, and reports any mismatches."
      },
      {
        "date": "2022-02-24T20:07:00.000Z",
        "voteCount": 3,
        "content": "D for data validation"
      },
      {
        "date": "2021-12-26T09:19:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\nDuring data validation, AWS DMS compares each row in the source with its corresponding row at the target, verifies the rows contain the same data, and reports any mismatches. To accomplish this AWS DMS issues appropriate queries to retrieve the data. Note that these queries will consume additional resources at the source and target as well as additional network resources.\n\nSo Answer should be B"
      },
      {
        "date": "2022-01-11T00:58:00.000Z",
        "voteCount": 3,
        "content": "Answer should be D:\n\nThe data validation option in the DMS task has to be activated before DMS performs what SMAZ has written. B does not mention anything about enabling it."
      },
      {
        "date": "2021-12-25T07:12:00.000Z",
        "voteCount": 2,
        "content": "'The migration should have a negligible effect on the source database's performance.'\nI believe answer should be 'B'\nTable metrics\nYou can find individual table metrics on the Table statistics tab for each individual task. These metrics include these numbers:\nRows loaded during the full load.\nInserts, updates, and deletes since the task started.\nDDL operations since the task started."
      },
      {
        "date": "2022-03-05T10:44:00.000Z",
        "voteCount": 1,
        "content": "No  datavalidation can ONLY be done using Task Validation  and it has to be  enabled before DMS tasks start and after migration is finished. Its a very common question. D is the correct choice"
      },
      {
        "date": "2021-12-09T06:54:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/26016-exam-aws-certified-database-specialty-topic-1-question-30/",
    "body": "A company is planning to close for several days. A Database Specialist needs to stop all applications along with the DB instances to ensure employees do not have access to the systems during this time. All databases are running on Amazon RDS for MySQL.<br>The Database Specialist wrote and ran a script to stop all the DB instances. When reviewing the logs, the Database Specialist found that Amazon RDS DB instances with read replicas did not stop.<br>How should the Database Specialist edit the script to fix this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the source instances before stopping their read replicas",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete each read replica before stopping its corresponding source instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the read replicas before stopping their source instances",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to stop each read replica and source instance at the same time"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T12:21:00.000Z",
        "voteCount": 19,
        "content": "B. Because You can't stop a DB instance that has a read replica, or that is a read replica.https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"
      },
      {
        "date": "2023-05-12T04:20:00.000Z",
        "voteCount": 1,
        "content": "Where does it say that you need to stop the source instance before read replicas?"
      },
      {
        "date": "2023-05-12T04:30:00.000Z",
        "voteCount": 1,
        "content": "Found it"
      },
      {
        "date": "2023-06-22T05:35:00.000Z",
        "voteCount": 1,
        "content": "You can't stop a DB instance that has a read replica, or that is a read replica.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"
      },
      {
        "date": "2023-03-28T05:08:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"
      },
      {
        "date": "2023-05-12T04:20:00.000Z",
        "voteCount": 1,
        "content": "Where does it say that you need to stop the source instance before read replicas?"
      },
      {
        "date": "2023-05-12T04:30:00.000Z",
        "voteCount": 1,
        "content": "Found it"
      },
      {
        "date": "2022-10-19T06:34:00.000Z",
        "voteCount": 2,
        "content": "B because\n\u2022 Can stop an RDS instance only if it does not have a replica\n\u2022 Cannot stop an RDS replica"
      },
      {
        "date": "2022-10-16T09:29:00.000Z",
        "voteCount": 1,
        "content": "You can't stop a DB instance that has a read replica, or that is a read replica"
      },
      {
        "date": "2022-05-21T15:43:00.000Z",
        "voteCount": 1,
        "content": "B.  as terminated all database instances"
      },
      {
        "date": "2022-04-30T12:25:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.SSL.Using.html\n\nD.  addressing the port where the RDS DB instance is listening for encrypted connections (communications connection failure =&gt; port)"
      },
      {
        "date": "2022-04-30T12:26:00.000Z",
        "voteCount": 2,
        "content": "sorry it is for next question.\nFor this one answer is B"
      },
      {
        "date": "2022-04-30T12:19:00.000Z",
        "voteCount": 3,
        "content": "B. Delete each read replica before stopping its corresponding source instance\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\n\nLimitations\nThe following are some limitations to stopping and starting a DB instance:\n\nYou can't stop a DB instance that has a read replica, or that is a read replica.\n\nYou can't stop an Amazon RDS for SQL Server DB instance in a Multi-AZ configuration.\n\nYou can't modify a stopped DB instance.\n\nYou can't delete an option group that is associated with a stopped DB instance.\n\nYou can't delete a DB parameter group that is associated with a stopped DB instance.\n\nIn a Multi-AZ configuration, the primary and secondary Availability Zones might be switched after you start the DB instance."
      },
      {
        "date": "2022-04-13T05:43:00.000Z",
        "voteCount": 2,
        "content": "\"You can't stop a DB instance that has a read replica, or that is a read replica.\""
      },
      {
        "date": "2021-11-06T01:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct\nLimitations\nThe following are some limitations to stopping and starting a DB instance:\nYou can't stop a DB instance that has a read replica, or that is a read repl"
      },
      {
        "date": "2021-11-02T07:34:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2021-10-31T20:27:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-10-22T23:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-10-13T18:29:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. The read replicas need to be deleted in order to stop the primary."
      },
      {
        "date": "2021-10-03T21:10:00.000Z",
        "voteCount": 1,
        "content": "Ans is B"
      },
      {
        "date": "2021-09-30T12:10:00.000Z",
        "voteCount": 2,
        "content": "Ans B is correct"
      },
      {
        "date": "2021-09-19T20:34:00.000Z",
        "voteCount": 3,
        "content": "I think B.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\n\"The following are some limitations to stopping and starting a DB instance:\nYou can't stop a DB instance that has a read replica, or that is a read replica.\"\nSo if you cant stop a db with a read replica, you have to delete the read replica first to then stop it???\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html#USER_MySQL.Replication.ReadReplicas.StartStop"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/67316-exam-aws-certified-database-specialty-topic-1-question-31/",
    "body": "A global digital advertising company captures browsing metadata to contextually display relevant images, pages, and links to targeted users. A single page load can generate multiple events that need to be stored individually. The maximum size of an event is 200 KB and the average size is 10 KB. Each page load must query the user's browsing history to provide targeting recommendations. The advertising company expects over 1 billion page visits per day from users in the<br>United States, Europe, Hong Kong, and India. The structure of the metadata varies depending on the event. Additionally, the browsing metadata must be written and read with very low latency to ensure a good viewing experience for the users.<br>Which database solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DocumentDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS Multi-AZ deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB global table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora Global Database"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T09:15:00.000Z",
        "voteCount": 8,
        "content": "- information structure differs according to the event =&gt; SchemaLess\nAmazon DynamoDB global table"
      },
      {
        "date": "2021-12-07T06:31:00.000Z",
        "voteCount": 7,
        "content": "Option C"
      },
      {
        "date": "2023-09-04T00:42:00.000Z",
        "voteCount": 3,
        "content": "C. Amazon DynamoDB global table\n\n\"DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. In such an environment, users expect very fast application performance. Global tables provide automatic multi-active replication to AWS Regions worldwide. They enable you to deliver low-latency data access to your users no matter where they are located.\"\n\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html"
      },
      {
        "date": "2023-08-02T00:46:00.000Z",
        "voteCount": 3,
        "content": "-The structure of the metadata varies\n-written and read with VERY LOW latency\n-average size is 10 KB\n-over 1 billion page visits per day\n-users in the United States, Europe, Hong Kong, and India\n-&gt; C. Amazon DynamoDB global table"
      },
      {
        "date": "2022-02-17T04:27:00.000Z",
        "voteCount": 1,
        "content": "C\nDynamoDB global table"
      },
      {
        "date": "2021-12-22T12:37:00.000Z",
        "voteCount": 2,
        "content": "Go with option C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/66151-exam-aws-certified-database-specialty-topic-1-question-32/",
    "body": "A Database Specialist modified an existing parameter group currently associated with a production Amazon RDS for SQL Server Multi-AZ DB instance. The change is associated with a static parameter type, which controls the number of user connections allowed on the most critical RDS SQL Server DB instance for the company. This change has been approved for a specific maintenance window to help minimize the impact on users.<br>How should the Database Specialist apply the parameter group change for the DB instance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the option to apply the change immediately",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow the preconfigured RDS maintenance window for the given DB instance to control when the change is applied",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply the change manually by rebooting the DB instance during the approved maintenance window\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReboot the secondary Multi-AZ DB instance"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-03-05T18:15:00.000Z",
        "voteCount": 7,
        "content": "C - static parameters  require a manual reboot. You can either do it immediately or during the maintenance window"
      },
      {
        "date": "2023-09-23T10:32:00.000Z",
        "voteCount": 1,
        "content": "Static parameters require a reboot. So the best option is rebooting it during the approved maintenance window."
      },
      {
        "date": "2023-08-02T00:56:00.000Z",
        "voteCount": 1,
        "content": "Static and dynamic DB parameters could be for instance or cluster.\nStatic parameters change takes effect ONLY after you manually reboot the associated DB clusters or DB instances according to documentation.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/parameter-groups-overview.html"
      },
      {
        "date": "2023-06-18T04:49:00.000Z",
        "voteCount": 1,
        "content": "C - static parameters  require a manual reboot. For minimal impact, wait until the maintenance window."
      },
      {
        "date": "2023-03-12T06:09:00.000Z",
        "voteCount": 1,
        "content": "C -\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/parameter-groups-overview.html#parameter-groups-overview.db-instance\nWhen you change a static parameter and save the DB parameter group, the parameter change takes effect after you manually reboot the associated DB instances. For static parameters, the console always uses pending-reboot for the ApplyMethod"
      },
      {
        "date": "2022-10-19T06:38:00.000Z",
        "voteCount": 1,
        "content": "C\n\nchanging static parameters in a Parameter Group requires a manual reboot, RDS will not reboot the instance during maintenance window."
      },
      {
        "date": "2022-04-30T12:07:00.000Z",
        "voteCount": 3,
        "content": "C. Apply the change manually by rebooting the DB instance during the approved maintenance window"
      },
      {
        "date": "2021-11-20T23:30:00.000Z",
        "voteCount": 2,
        "content": "Ans C \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html#USER_WorkingWithParamGroups.Modifying"
      },
      {
        "date": "2021-11-16T08:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/25605-exam-aws-certified-database-specialty-topic-1-question-33/",
    "body": "A Database Specialist is designing a new database infrastructure for a ride hailing application. The application data includes a ride tracking system that stores<br>GPS coordinates for all rides. Real-time statistics and metadata lookups must be performed with high throughput and microsecond latency. The database should be fault tolerant with minimal operational overhead and development effort.<br>Which solution meets these requirements in the MOST efficient way?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS for MySQL as the database and use Amazon ElastiCache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB as the database and use DynamoDB Accelerator\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora MySQL as the database and use Aurora's buffer cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB as the database and use Amazon API Gateway"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T19:48:00.000Z",
        "voteCount": 15,
        "content": "B. keyword here is microsecond. DAX has that"
      },
      {
        "date": "2021-09-25T06:54:00.000Z",
        "voteCount": 6,
        "content": "Agree, microsecond = DynamoDB Dax \nhttps://aws.amazon.com/dynamodb/dax/#:~:text=Amazon%20DynamoDB%20Accelerator%20(DAX)%20is,millions%20of%20requests%20per%20second.\n\"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds \u2013 even at millions of requests per second. \""
      },
      {
        "date": "2023-09-23T10:35:00.000Z",
        "voteCount": 4,
        "content": "Microsecond latency (i.e., Elastic Cache or DX).\nLittle operational overhead, DynamoDB."
      },
      {
        "date": "2023-01-11T09:37:00.000Z",
        "voteCount": 2,
        "content": "Unstructured + microsecond - Dynamo + DAX"
      },
      {
        "date": "2022-07-09T22:29:00.000Z",
        "voteCount": 2,
        "content": "Since the data lookup should be consistent and real time ,so eventual consistency will not work. DAX will not help . I think D is correct"
      },
      {
        "date": "2022-06-01T10:08:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html"
      },
      {
        "date": "2022-04-28T17:57:00.000Z",
        "voteCount": 1,
        "content": "microsecond =&gt; DAX"
      },
      {
        "date": "2021-12-14T05:17:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2021-12-10T09:28:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2021-10-29T18:01:00.000Z",
        "voteCount": 1,
        "content": "Agree with B due to latency requirement that can be full field only with DAX"
      },
      {
        "date": "2021-10-23T18:34:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-10-17T22:52:00.000Z",
        "voteCount": 4,
        "content": "B -  NoSQL is ideal here. DynamoDB+DAX provides microseconds latency"
      },
      {
        "date": "2021-10-12T23:29:00.000Z",
        "voteCount": 2,
        "content": "I will go with B. DAX facilitates the requirement stated"
      },
      {
        "date": "2021-10-05T05:36:00.000Z",
        "voteCount": 1,
        "content": "Ans B. Low latency = DynamoDB, Micro sec = DAX"
      },
      {
        "date": "2021-10-02T09:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is B ,"
      },
      {
        "date": "2021-09-30T05:42:00.000Z",
        "voteCount": 3,
        "content": "Answer is B,\nMicrosecond latency is always NoSQL\nAPI Gateway has nothing to do with requirement of this question."
      },
      {
        "date": "2021-09-21T01:37:00.000Z",
        "voteCount": 2,
        "content": "D is correct  as document says API gateway to use for minimize developer\n efforts"
      },
      {
        "date": "2021-09-20T23:03:00.000Z",
        "voteCount": 2,
        "content": "I looked at the link, and watched the video. At no point was API gateway mentioned, Dynamo was.\nI will go with B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/25720-exam-aws-certified-database-specialty-topic-1-question-34/",
    "body": "A company is using an Amazon Aurora PostgreSQL DB cluster with an xlarge primary instance master and two large Aurora Replicas for high availability and read-only workload scaling. A failover event occurs and application performance is poor for several minutes. During this time, application servers in all Availability<br>Zones are healthy and responding normally.<br>What should the company do to eliminate this application performance issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure both of the Aurora Replicas to the same instance class as the primary DB instance. Enable cache coherence on the DB cluster, set the primary DB instance failover priority to tier-0, and assign a failover priority of tier-1 to the replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function that calls the DescribeDBInstances action to establish which instance has failed, and then use the PromoteReadReplica operation to promote one Aurora Replica to be the primary DB instance. Configure an Amazon RDS event subscription to send a notification to an Amazon SNS topic to which the Lambda function is subscribed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure one Aurora Replica to have the same instance class as the primary DB instance. Implement Aurora PostgreSQL DB cluster cache management. Set the failover priority to tier-0 for the primary DB instance and one replica with the same instance class. Set the failover priority to tier-1 for the other replicas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure both Aurora Replicas to have the same instance class as the primary DB instance. Implement Aurora PostgreSQL DB cluster cache management. Set the failover priority to tier-0 for the primary DB instance and to tier-1 for the replicas."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T20:55:00.000Z",
        "voteCount": 5,
        "content": "Tier-0 is a requirement for CCM to work. Also having more than 1 read replica with Tier-0 also disables CCM - https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/aurora_ccm_status.html\n\nThis clearly eliminates Option-D"
      },
      {
        "date": "2023-08-02T01:21:00.000Z",
        "voteCount": 1,
        "content": "\"Cluster cache management is active on an Aurora PostgreSQL DB cluster when the cluster has an Aurora Reader instance configured as follows:\nThe Aurora Reader instance uses same DB instance class type and size as the cluster's Writer instance.\nThe Aurora Reader instance is configured as Tier-0 for the cluster. If the cluster has more than one Reader, this is its only Tier-0 Reader.\nSetting more than one Reader to Tier-0 disables CCM.\"\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/aurora_ccm_status.html"
      },
      {
        "date": "2023-11-25T01:43:00.000Z",
        "voteCount": 1,
        "content": "Why do we need to set promotion tier for writer instance? It is already being used and failover will occur when it fails. When writer instance fails, reader replica will be promoted to writer instance. The writer instance that failed is not going to be removed from cluster - correct?"
      },
      {
        "date": "2022-10-19T06:48:00.000Z",
        "voteCount": 4,
        "content": "C\n\nfor CCM to work\n1- you need to have one RR with the same instance class type and size as the writer instance\n2- you need to set promotion priority to 0 for both writer and RR you need\n\nthe details here https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2022-06-22T21:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct. \nhttps://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/"
      },
      {
        "date": "2022-04-29T13:24:00.000Z",
        "voteCount": 2,
        "content": "One Aurora Replica of same instance class as the primary DB\n -&gt; Aurora PostgreSQL DB cluster cache management \n-&gt; failover priority to tier-0 for the primary DB instance and one replica with the same instance class\n-&gt; failover priority to tier-1 for the other replicas"
      },
      {
        "date": "2022-03-04T22:11:00.000Z",
        "voteCount": 4,
        "content": "C is correct. Cluster Cache Management needs these three:\n- Set One Replica to have same priority as Primary\n- Set its instance class  same as Primary"
      },
      {
        "date": "2022-02-24T14:53:00.000Z",
        "voteCount": 2,
        "content": "C is economical, but priorities don't need to be set as largest replica will always be first target option for failovers"
      },
      {
        "date": "2022-02-04T11:29:00.000Z",
        "voteCount": 4,
        "content": "CCM can be set to only one replica. That's why we put it to tier-0 to guarantee the fail over to the CCM enabled replica."
      },
      {
        "date": "2022-02-01T09:12:00.000Z",
        "voteCount": 4,
        "content": "Answer is D. The replicas must be the same size as the primary when cluster cache management is used.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2022-04-29T13:27:00.000Z",
        "voteCount": 3,
        "content": "No all replicas but one where you want to fail over to, so ans is C"
      },
      {
        "date": "2021-12-01T16:31:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2022-02-01T09:09:00.000Z",
        "voteCount": 1,
        "content": "From that link - \"Cluster cache management requires that the designated reader instance have the same instance class type and size (db.r5.2xlarge or db.r5.xlarge, for example) as the writer\"\nTherefore both replicas are required to be the same size as the primary. I'm going for Answer D"
      },
      {
        "date": "2022-04-29T13:28:00.000Z",
        "voteCount": 1,
        "content": "not both, just one is good"
      },
      {
        "date": "2021-11-24T04:26:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-10-29T21:21:00.000Z",
        "voteCount": 2,
        "content": "Ans is C.  Because this doc https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/ says to set priority of primary and one replica to zero. Which is not the case with D."
      },
      {
        "date": "2021-10-27T09:24:00.000Z",
        "voteCount": 2,
        "content": "C Answer\nTier Priority 0 is the clue to eliminate D:"
      },
      {
        "date": "2021-10-27T14:00:00.000Z",
        "voteCount": 2,
        "content": "The priority of 0 has nothing to do with it since more than one replica can have the same priority. The only difference appears to be cost and C will be cheaper. \n\n\"You can customize the order in which your Aurora Replicas are promoted to the primary instance after a failure by assigning each replica a priority. Priorities range from 0 for the first priority to 15 for the last priority. If the primary instance fails, Amazon RDS promotes the Aurora Replica with the better priority to the new primary instance. You can modify the priority of an Aurora Replica at any time. Modifying the priority doesn't trigger a failover.\n\nMore than one Aurora Replica can share the same priority, resulting in promotion tiers. If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon RDS promotes an arbitrary replica in the same promotion tier. \""
      },
      {
        "date": "2023-02-06T19:30:00.000Z",
        "voteCount": 1,
        "content": "CCM requires one replica has tier 0."
      },
      {
        "date": "2021-10-26T05:05:00.000Z",
        "voteCount": 1,
        "content": "C and D look equally good to address the problem, but D would cost more as you upgrade all read replicas, not just 1. So for cost saving reasons would go with Answer C."
      },
      {
        "date": "2021-10-25T20:56:00.000Z",
        "voteCount": 1,
        "content": "C for me."
      },
      {
        "date": "2021-10-24T20:11:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-18T13:29:00.000Z",
        "voteCount": 1,
        "content": "I am torn between C &amp;D.\nPerformance issue after failover lasts only for a short duration which means it has more to do with cache management than instance sizing.  CCM definitely is the solution.\nBetween C &amp; D, I will go with C considering the fact that originally read-replicas were of lower configuration for cost-saving, so its ideal to have one read-replica matching primary instance size and others with lower configuration if cost is a concern."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/25721-exam-aws-certified-database-specialty-topic-1-question-35/",
    "body": "A company has a database monitoring solution that uses Amazon CloudWatch for its Amazon RDS for SQL Server environment. The cause of a recent spike in<br>CPU utilization was not determined using the standard metrics that were collected. The CPU spike caused the application to perform poorly, impacting users. A<br>Database Specialist needs to determine what caused the CPU spike.<br>Which combination of steps should be taken to provide more visibility into the processes and queries running during an increase in CPU load? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Events and view the incoming T-SQL statements causing the CPU to spike.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Enhanced Monitoring metrics to view CPU utilization at the RDS SQL Server DB instance level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a caching layer to help with repeated queries on the RDS SQL Server DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight to view the SQL statement being run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon RDS Performance Insights to view the database load and filter the load by waits, SQL statements, hosts, or users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T15:17:00.000Z",
        "voteCount": 11,
        "content": "B&amp;E as well\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/\n\"Several factors can cause an increase in CPU utilization. For example, user-initiated heavy workloads, analytic queries, prolonged deadlocks and lock waits, multiple concurrent transactions, long-running transactions, or other processes that utilize CPU resources.\nFirst, you can identify the source of the CPU usage by:\nUsing Enhanced Monitoring\nUsing Performance Insights\""
      },
      {
        "date": "2021-10-16T20:28:00.000Z",
        "voteCount": 7,
        "content": "B &amp; E for me too"
      },
      {
        "date": "2022-04-29T11:54:00.000Z",
        "voteCount": 3,
        "content": "you can identify the source of the CPU usage by:\nUsing Enhanced Monitoring\nUsing Performance Insights\""
      },
      {
        "date": "2022-02-24T14:03:00.000Z",
        "voteCount": 3,
        "content": "Gets good visibility"
      },
      {
        "date": "2021-10-14T12:08:00.000Z",
        "voteCount": 1,
        "content": "Ans: BE"
      },
      {
        "date": "2021-10-05T23:24:00.000Z",
        "voteCount": 2,
        "content": "Answer is B,E"
      },
      {
        "date": "2021-10-03T22:14:00.000Z",
        "voteCount": 2,
        "content": "Yes B and E here"
      },
      {
        "date": "2021-09-29T03:34:00.000Z",
        "voteCount": 4,
        "content": "B and E"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/25722-exam-aws-certified-database-specialty-topic-1-question-36/",
    "body": "A company is using Amazon with Aurora Replicas for read-only workload scaling. A Database Specialist needs to split up two read-only applications so each application always connects to a dedicated replica. The Database Specialist wants to implement load balancing and high availability for the read-only applications.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a specific instance endpoint for each replica and add the instance endpoint to each read-only application connection string.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse reader endpoints for both the read-only workload applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a reader endpoint for one read-only application and use an instance endpoint for the other read-only application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse custom endpoints for the two read-only applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-11T09:41:00.000Z",
        "voteCount": 1,
        "content": "D is answer"
      },
      {
        "date": "2022-07-18T07:18:00.000Z",
        "voteCount": 2,
        "content": "Custome end points are usually used when custom load balancing is needed ,i.e. when specific read application should connect to comstom read replicat directed by custom rules in load balancing. When all RR are of same size, so need for custom end point reader end point will do ."
      },
      {
        "date": "2022-07-18T07:18:00.000Z",
        "voteCount": 2,
        "content": "So B is correct"
      },
      {
        "date": "2022-06-24T07:07:00.000Z",
        "voteCount": 1,
        "content": "Answer:D"
      },
      {
        "date": "2022-04-30T17:43:00.000Z",
        "voteCount": 2,
        "content": "A custom endpoint for an Aurora cluster represents a set of DB instances (here replicas) that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection.\n\nwe can create like two custom endpoints: \n1. With two specific read replicas for application A, \nanother three read replicas for application B.\nHA and meet the requirements that A and B will connect to different replicas"
      },
      {
        "date": "2022-03-06T10:12:00.000Z",
        "voteCount": 2,
        "content": "Definitely (D)"
      },
      {
        "date": "2021-11-04T22:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is: D"
      },
      {
        "date": "2021-11-04T11:38:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect because it is not HA: \"Use a specific instance endpoint for each replica\"\n=&gt; if the specific instance is down, connection is lost\nB is incorrect because the demand requires 02 applications connect to different RO replicas. In case B, the traffic will be distributed among read replicas.\nC is same with A, using one endpoint or instance does not meet the HA requirement. \nD. Use custom endpoints for the two read-only applications.\nD is correct because we can create like two customer endpoints: 1. With two specific read replicas for application A, another three read replicas for application B.\nHA and meet the requirements that A and B will connect to different replicas"
      },
      {
        "date": "2021-10-31T02:21:00.000Z",
        "voteCount": 3,
        "content": "Ans is D. In order for application to connect to specific instance, and also provide HA by flipping DNS in case of one read replica goes down."
      },
      {
        "date": "2021-10-30T15:16:00.000Z",
        "voteCount": 4,
        "content": "No doubts that D is the answer."
      },
      {
        "date": "2021-10-29T18:02:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-12T05:04:00.000Z",
        "voteCount": 1,
        "content": "A. For example, your client application might require more fine-grained load balancing based on workload type. In this case, you can configure multiple clients to connect to different Aurora Replicas in a DB cluster to distribute read workloads.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Viewing"
      },
      {
        "date": "2021-10-21T22:48:00.000Z",
        "voteCount": 1,
        "content": "The reader endpoint load-balances connections to available Aurora Replicas in an Aurora DB cluster. It doesn't load-balance individual queries. If you want to load-balance each query to distribute the read workload for a DB cluster, open a new connection to the reader endpoint for each query."
      },
      {
        "date": "2021-10-10T09:38:00.000Z",
        "voteCount": 1,
        "content": "D - Custom endpoints allows distributing workload in a customized way by grouping read-replicas of various size, configuration and location"
      },
      {
        "date": "2021-10-08T13:51:00.000Z",
        "voteCount": 2,
        "content": "D, custom endpoints"
      },
      {
        "date": "2021-10-03T16:44:00.000Z",
        "voteCount": 2,
        "content": "D. Custom endpoint for custom requirements"
      },
      {
        "date": "2021-10-02T19:17:00.000Z",
        "voteCount": 1,
        "content": "the answer is D , dedicated endpoint for each application by custom endpoint \n or each application has to use reader endoint not cluster reader and this option does not exists in the answers."
      },
      {
        "date": "2021-09-28T11:00:00.000Z",
        "voteCount": 2,
        "content": "D is the answer. Dedicated replica is the key"
      },
      {
        "date": "2021-09-26T21:37:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\nB is not correct, reader endpoint of the cluster performs load balancing across all reader instances, questions say reader instances must be split up between to applications."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/26355-exam-aws-certified-database-specialty-topic-1-question-37/",
    "body": "An online gaming company is planning to launch a new game with Amazon DynamoDB as its data store. The database should be designated to support the following use cases:<br>\u2711 Update scores in real time whenever a player is playing the game.<br>\u2711 Retrieve a player's score details for a specific game session.<br>A Database Specialist decides to implement a DynamoDB table. Each player has a unique user_id and each game has a unique game_id.<br>Which choice of keys is recommended for the DynamoDB table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global secondary index with game_id as the partition key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global secondary index with user_id as the partition key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a composite primary key with game_id as the partition key and user_id as the sort key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a composite primary key with user_id as the partition key and game_id as the sort key\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T10:44:00.000Z",
        "voteCount": 19,
        "content": "I'm going with D based on the following links.\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/\n\"EA uses the user ID as the partition key and primary key (a 1:1 modeling pattern).\"\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\n\"Partition key and sort key: Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key.\""
      },
      {
        "date": "2021-10-05T15:17:00.000Z",
        "voteCount": 10,
        "content": "D is the best choice of keys for below requirements.\n\n\u2711 Update scores in real time whenever a player is playing the game.  -- user_id being partition key\n\u2711 Retrieve a player's score details for a specific game session.  -- game_id sort key"
      },
      {
        "date": "2024-01-12T22:43:00.000Z",
        "voteCount": 1,
        "content": "I go for D\n- user_ID for global score\n- user_ID (PK) + game_id (SK) for a session"
      },
      {
        "date": "2023-12-09T07:38:00.000Z",
        "voteCount": 3,
        "content": "- game_id must reflect the game session and not the actual game, because if not even user_id + game_id would not be unique. Saying that, game_id will have higher cardinality.\n- about the query... if it is to get the details of one specific player, both c and d will work, if we want to get the details of all players for a game, only c works.\n\nHaving this said, for me C) is the only answer that will work for all cases, having a higher cardinality value as partition key.\n\nBut let me say that these types of questions must be better elaborated, because at some times it dependends on the interpretation"
      },
      {
        "date": "2023-06-04T16:54:00.000Z",
        "voteCount": 1,
        "content": "this is my humble contribution to the solution:\n\n If we want to get the high score for a player for a certain game,  user_id should be our partition key and game_id should be our sort key, which is D, but to find out what the highest score ever was for a certain game, we would need game_id to be a partition key and 'score' to be the sort key, which is not in the options. \n\nI mean that cardinality can be decided according to the objective of the query but it is not given explicitly in the question."
      },
      {
        "date": "2022-12-27T12:39:00.000Z",
        "voteCount": 2,
        "content": "D.  Game_id would have lower cardinality than user_id.  Say the gaming company only had 4 games with 2m users.  You would want your partition to be by higher cardinality and your secondary with lower."
      },
      {
        "date": "2023-05-31T05:10:00.000Z",
        "voteCount": 2,
        "content": "\"each game has a unique game_id\" what are you talking about \"Game_id would have lower cardinality than user_id\"? both unique."
      },
      {
        "date": "2022-11-27T05:44:00.000Z",
        "voteCount": 3,
        "content": "I'm going with C, for queries and updates game_id and user_id are always used, in addition, game_id has more high cardinality and avoids \"hot\" partitions if used as the partition key."
      },
      {
        "date": "2022-09-30T07:50:00.000Z",
        "voteCount": 5,
        "content": "D.  There are going to be more user_ids than game_ids, hence user_id will be the higher cardinality value = the better partition key.  For those of you saying \"C\", are you telling me Electronic Arts has more unique games than they have users!!??!\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
      },
      {
        "date": "2023-06-14T20:25:00.000Z",
        "voteCount": 1,
        "content": "each game has a unique game_id.. meaning if you play same game (e.g. car racing) two times, each time you play the game, it will have a unique game id.. that is what I understand from the question.. because you can't have just one game id for car racing then how will you store data for same user id playing same game multiple times?"
      },
      {
        "date": "2022-05-23T18:07:00.000Z",
        "voteCount": 2,
        "content": "If there will be player_id it will cause of hot partitioning. So D is wrong. \nC is correct as game_id will have more high cardinality"
      },
      {
        "date": "2022-05-19T04:17:00.000Z",
        "voteCount": 3,
        "content": "C is the answer.\n\u2711 Scores should be updated in real time anytime a player is engaged in the game.  -&gt; game_id is the partition key\n\u2711 Retrieve the information of a player's score for a certain gaming session. - &gt; game_id and user_id is the primary key (i.e. user_id is the sort key)"
      },
      {
        "date": "2022-04-30T18:38:00.000Z",
        "voteCount": 2,
        "content": "use cases:\n\u2711 Scores should be updated in real time anytime a player is engaged in the game. (query based on user_id only =&gt; user_id partition key)\n\u2711 Retrieve the information of a player's score for a certain gaming session.(query based on user_id + game_id=&gt; user_id partition key, and game_id sort key)\n\nD. Create a composite primary key with user_id as the partition key and game_id as the sort key"
      },
      {
        "date": "2023-06-14T20:28:00.000Z",
        "voteCount": 1,
        "content": "Scores should be updated in real time anytime a player is engaged in the game. in this case, query will be based on user_id and game_id.. otherwise you will end up fetching all records for that user_id"
      },
      {
        "date": "2022-04-11T18:11:00.000Z",
        "voteCount": 2,
        "content": "Answer is C, partition key must have high cardinality ie. game_id as there may be multiple instances of games played by each user."
      },
      {
        "date": "2022-03-26T01:42:00.000Z",
        "voteCount": 1,
        "content": "I chose D. GSI does not support strong consistency."
      },
      {
        "date": "2021-10-26T14:04:00.000Z",
        "voteCount": 1,
        "content": "For me, both C and D can be used but C is better.\n\u2711 Update scores in real time whenever a player is playing the game.\n=&gt; Query by the game_id first, then update relevant user_id with his relevant score. \n\u2711 Retrieve a player\u05d2\u20ac\u2122s score details for a specific game session =&gt; game_id can be used to query then specify the user_id\nD can do the same as above. But for the query such as: Top score player for each game, C is better.\nD is for the query such as: This player plays how many games and the score of each."
      },
      {
        "date": "2021-10-14T10:40:00.000Z",
        "voteCount": 1,
        "content": "Why not C ? Any idea ?"
      },
      {
        "date": "2021-12-24T08:38:00.000Z",
        "voteCount": 5,
        "content": "Partition key should have high cardinality. Therefore user id is better than game id as the partition key.. Answer is D"
      },
      {
        "date": "2021-10-09T15:47:00.000Z",
        "voteCount": 2,
        "content": "I would go with A.\nReason,  Need user_id as partition key for main table and global secondary index with game_id as partition key since we want to update via user_id and also retrieve via game_id. Both these options do not exists in any of the options so A seems to be nearest correct answer."
      },
      {
        "date": "2021-12-27T09:01:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\nGameScores is identified by a partition key (UserId) and a sort key (GameTitle). A query that specified the key attributes (UserId and GameTitle) would be very efficient\nComposite primary key is the answer. For building a leader board we need global secondar index"
      },
      {
        "date": "2021-10-06T12:06:00.000Z",
        "voteCount": 3,
        "content": "Answer D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/25724-exam-aws-certified-database-specialty-topic-1-question-38/",
    "body": "A Database Specialist migrated an existing production MySQL database from on-premises to an Amazon RDS for MySQL DB instance. However, after the migration, the database needed to be encrypted at rest using AWS KMS. Due to the size of the database, reloading, the data into an encrypted database would be too time-consuming, so it is not an option.<br>How should the Database Specialist satisfy this new requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the unencrypted RDS DB instance. Create an encrypted copy of the unencrypted snapshot. Restore the encrypted snapshot copy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the RDS DB instance. Enable the AWS KMS encryption option that leverages the AWS CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore an unencrypted snapshot into a MySQL RDS DB instance that is encrypted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted read replica of the RDS DB instance. Promote it the master."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T00:33:00.000Z",
        "voteCount": 16,
        "content": "Agree with A. This blog post also says the same steps as Micky's link.\n\"However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. For more information, see Copying a Snapshot.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html"
      },
      {
        "date": "2023-08-02T01:51:00.000Z",
        "voteCount": 1,
        "content": "Encrypt your existing AWS RDS:\n\t- Take snapshot\n\t- Copy Snapshot with Enable encryption\n\t- Change unencrypted instance name\nEncrypt your existing AWS RDS:\n\t- Take snapshot\n\t- Copy Snapshot with Enable encryption\n\t- Change unencrypted instance name\n\t- Restore Snapshot (DB Instance Identifier field must contain the name of your previous database before renaming it)\nhttps://blog.theodo.com/2019/11/encrypt-existing-aws-rds-database/"
      },
      {
        "date": "2022-04-30T13:51:00.000Z",
        "voteCount": 2,
        "content": "snapshot -&gt; encrypted snapshot -&gt; Restore the encrypted snapshot copy."
      },
      {
        "date": "2023-04-08T17:09:00.000Z",
        "voteCount": 4,
        "content": "Limitations of Amazon RDS encrypted DB instances\nThe following limitations exist for Amazon RDS encrypted DB instances:\n\nYou can only encrypt an Amazon RDS DB instance when you create it, not after the DB instance is created.\n\nHowever, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. For more information, see Copying a DB snapshot.\n\nYou can't turn off encryption on an encrypted DB instance.\n\nYou can't create an encrypted snapshot of an unencrypted DB instance."
      },
      {
        "date": "2022-04-13T05:28:00.000Z",
        "voteCount": 2,
        "content": "A is an obvious choice here"
      },
      {
        "date": "2021-11-04T22:19:00.000Z",
        "voteCount": 1,
        "content": "Here source database is in on premises ? How we can access snapshot of on-premises database from AWS console ? How A is possible ?"
      },
      {
        "date": "2021-11-05T20:37:00.000Z",
        "voteCount": 2,
        "content": "its talking about the database has already been migrated to RDS, but they want to know how to encrypt it after the migration - hence a is correct"
      },
      {
        "date": "2021-11-04T18:11:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      },
      {
        "date": "2021-11-03T15:54:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-10-13T04:31:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2021-10-09T00:47:00.000Z",
        "voteCount": 1,
        "content": "I guess It should be D: As per this document it is possible to do replication from unencrypted to encrypted read replica.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/"
      },
      {
        "date": "2021-10-16T13:14:00.000Z",
        "voteCount": 1,
        "content": "This is for MariaDB!!!"
      },
      {
        "date": "2021-10-20T09:33:00.000Z",
        "voteCount": 1,
        "content": "You cannot create an encrypted Read Replica from an unencrypted DB instance."
      },
      {
        "date": "2021-09-27T02:24:00.000Z",
        "voteCount": 1,
        "content": "If you become D, do you really need to do A?\nHow do you fix data inconsistencies?\nD is possible, so I think it's best."
      },
      {
        "date": "2021-10-04T18:57:00.000Z",
        "voteCount": 2,
        "content": "To have an encrypted read-replica, primary instance must be encrypted"
      },
      {
        "date": "2021-09-22T03:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-09-20T13:17:00.000Z",
        "voteCount": 4,
        "content": "A is correct answer \nhttps://blog.theodo.com/2019/11/encrypt-existing-aws-rds-database/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/65852-exam-aws-certified-database-specialty-topic-1-question-39/",
    "body": "A Database Specialist is planning to create a read replica of an existing Amazon RDS for MySQL Multi-AZ DB instance. When using the AWS Management<br>Console to conduct this task, the Database Specialist discovers that the source RDS DB instance does not appear in the read replica source selection box, so the read replica cannot be created.<br>What is the most likely reason for this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe source DB instance has to be converted to Single-AZ first to create a read replica from it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnhanced Monitoring is not enabled on the source DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe minor MySQL version in the source DB instance does not support read replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomated backups are not enabled on the source DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-03-05T19:06:00.000Z",
        "voteCount": 6,
        "content": "(D) is correct\nIn order to create a read replica\n- Automated Backups MUST be enabled\nAdditionally for SQL Server\n- Source  MUST be Multi-AZ deployment with Always  On Availability Groups (AG)"
      },
      {
        "date": "2022-05-01T09:48:00.000Z",
        "voteCount": 5,
        "content": "D. Automated backups are not enabled on the source DB instance.\n\nIn order to create a read replica\n- Automated Backups MUST be enabled\nAdditionally for SQL Server\n- Source MUST be Multi-AZ deployment with Always On Availability Groups (AG)"
      },
      {
        "date": "2023-08-31T05:36:00.000Z",
        "voteCount": 1,
        "content": "D. Automated backups are not enabled on the source DB instance.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MultiAZDBCluster_ReadRepl.html\n\n\n\"You must turn on automatic backups on the source DB instance by setting the backup retention period to a value other than 0.\""
      },
      {
        "date": "2022-01-27T07:02:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-11-11T13:31:00.000Z",
        "voteCount": 2,
        "content": "D.\n&gt;Your source DB instance must have backup retention enabled. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_CreateDBInstanceReadReplica.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/25619-exam-aws-certified-database-specialty-topic-1-question-40/",
    "body": "A Database Specialist has migrated an on-premises Oracle database to Amazon Aurora PostgreSQL. The schema and the data have been migrated successfully.<br>The on-premises database server was also being used to run database maintenance cron jobs written in Python to perform tasks including data purging and generating data exports. The logs for these jobs show that, most of the time, the jobs completed within 5 minutes, but a few jobs took up to 10 minutes to complete. These maintenance jobs need to be set up for Aurora PostgreSQL.<br>How can the Database Specialist schedule these jobs so the setup requires minimal maintenance and provides high availability?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate cron jobs on an Amazon EC2 instance to run the maintenance jobs following the required schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the Aurora host and create cron jobs to run the maintenance jobs following the required schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Lambda functions to run the maintenance jobs and schedule them with Amazon CloudWatch Events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the maintenance job using the Amazon CloudWatch job scheduling plugin."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T23:45:00.000Z",
        "voteCount": 13,
        "content": "Answer should be C but below link confuses\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Scheduled-Rule.html\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/schedule-jobs-for-amazon-rds-and-aurora-postgresql-using-lambda-and-secrets-manager.html\n\n a job for data extraction or a job for data purging can easily be scheduled using cron. For these jobs, database credentials are typically either hard-coded or stored in a properties file. However, when you migrate to Amazon Relational Database Service (Amazon RDS) or Amazon Aurora PostgreSQL, you lose the ability to log in to the host instance to schedule cron jobs. \n\nThis pattern describes how to use AWS Lambda and AWS Secrets Manager to schedule jobs for Amazon RDS and Aurora PostgreSQL databases after migration.  \n\nit confirms that answer is C"
      },
      {
        "date": "2021-09-25T05:29:00.000Z",
        "voteCount": 3,
        "content": "I agree with C via your second link:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/schedule-jobs-for-amazon-rds-and-aurora-postgresql-using-lambda-and-secrets-manager.html"
      },
      {
        "date": "2021-09-25T22:00:00.000Z",
        "voteCount": 2,
        "content": "https://medium.com/better-programming/cron-job-patterns-in-aws-126fbf54a276"
      },
      {
        "date": "2023-08-31T05:47:00.000Z",
        "voteCount": 1,
        "content": "C. Create AWS Lambda functions to run the maintenance jobs and schedule them with Amazon CloudWatch Events."
      },
      {
        "date": "2023-08-02T02:14:00.000Z",
        "voteCount": 2,
        "content": "- You can also use CloudWatch Events to schedule automated actions that self-initiate at certain times using cron or rate expressions.\n- Lambda supports Python.\n- CloudWatch Events can call Lambda functions.\n- Less then 15 min job.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/schedule-jobs-for-amazon-rds-for-postgresql-and-aurora-postgresql-by-using-lambda-and-secrets-manager.html"
      },
      {
        "date": "2022-03-05T11:44:00.000Z",
        "voteCount": 4,
        "content": "C. Whenever its asked to create Cron Jobs Lambda vs EC2 : Its ALWAYS Lambda!!"
      },
      {
        "date": "2023-02-06T19:49:00.000Z",
        "voteCount": 2,
        "content": "It depends on how long the job run. Because lambda has 15 minutes limitation.\nIf the job needs more than 15 minutes to run, then can consider AWS Batch, ECS, EC2"
      },
      {
        "date": "2021-12-02T09:01:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2021-11-02T16:13:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer ==&gt;&gt; C"
      },
      {
        "date": "2021-11-01T09:31:00.000Z",
        "voteCount": 1,
        "content": "C is correct. \nA B is not HA.\nD, there is no such maintenance JOB for CW Events. CW Events has to trigger the other functions (Lambda/SSM/Step Function/ Batch) to do the task."
      },
      {
        "date": "2021-11-04T12:39:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C"
      },
      {
        "date": "2021-10-24T15:32:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C.  I don't think there is anything like CloudWatch job scheduling plugin. Internet search doesn't show anything like this."
      },
      {
        "date": "2021-10-12T18:27:00.000Z",
        "voteCount": 2,
        "content": "Agree with Answer C"
      },
      {
        "date": "2021-10-08T04:51:00.000Z",
        "voteCount": 2,
        "content": "Agree with Answer C"
      },
      {
        "date": "2021-10-04T05:54:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2021-10-01T22:29:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-01T07:16:00.000Z",
        "voteCount": 2,
        "content": "C is the best answer.\nEventbridge would have been a better choice to schedule lambda.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html"
      },
      {
        "date": "2021-09-25T07:30:00.000Z",
        "voteCount": 4,
        "content": "Ans is C. CloudWatch scheduling and Lambda execution and this option should be fine as long as job completes within 15 minutes."
      },
      {
        "date": "2021-09-24T21:05:00.000Z",
        "voteCount": 4,
        "content": "Answer should be C"
      },
      {
        "date": "2021-09-21T09:41:00.000Z",
        "voteCount": 2,
        "content": "The document referenced has no mention of job scheduling plugin for cloudwatch.... it is for systems manager. \nI will have to go with C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/25680-exam-aws-certified-database-specialty-topic-1-question-41/",
    "body": "A company has an Amazon RDS Multi-AZ DB instances that is 200 GB in size with an RPO of 6 hours. To meet the company's disaster recovery policies, the database backup needs to be copied into another Region. The company requires the solution to be cost-effective and operationally efficient.<br>What should a Database Specialist do to copy the database backup into a different Region?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS automated snapshots and use AWS Lambda to copy the snapshot into another Region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS automated snapshots every 6 hours and use Amazon S3 cross-Region replication to copy the snapshot into another Region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to take an Amazon RDS snapshot every 6 hours and use a second Lambda function to copy the snapshot into another Region\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cross-Region read replica for Amazon RDS in another Region and take an automated snapshot of the read replica"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T22:49:00.000Z",
        "voteCount": 27,
        "content": "Answer is C\n\nA. Use Amazon RDS automated snapshots and use AWS Lambda to copy the snapshot into another Region\nAutomated snapshots are taken once per day only, RPO is 6 hours, so not an option\n\nB. Use Amazon RDS automated snapshots every 6 hours and use Amazon S3 cross-Region replication to copy the snapshot into another Region\nYou can not take automated snapshots every 6 hours \n\nC. Create an AWS Lambda function to take an Amazon RDS snapshot every 6 hours and use a second Lambda function to copy the snapshot into another Region\nOnly possible option \n\nD. Create a cross-Region read replica for Amazon RDS in another Region and take an automated snapshot of the read replica\nNot cost-effective, replica is the most expensive DR option."
      },
      {
        "date": "2021-10-02T05:46:00.000Z",
        "voteCount": 1,
        "content": "It's hard to choose. A is correct is RDS can restore to point in time, so we don't need to do a snapshot every 6 hours."
      },
      {
        "date": "2021-10-02T16:37:00.000Z",
        "voteCount": 3,
        "content": "Agreed with C. Any options mention / use 'automated snapshot' should be dropped, so no ABD. Verified via AWS console in RDS. Change time interval for RDS automatic backup period to 0 means disable automatic backup. See:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
      },
      {
        "date": "2024-01-12T22:51:00.000Z",
        "voteCount": 1,
        "content": "Anser is C.\nD is not the most cost-effective\nA and B are not OK because automated snapshots are only once a day and you need one every 6 hours"
      },
      {
        "date": "2023-10-03T15:29:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-06-01T00:37:00.000Z",
        "voteCount": 1,
        "content": "It would be so much easier to use cross-region backup replication...\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReplicateBackups.html"
      },
      {
        "date": "2022-07-10T00:41:00.000Z",
        "voteCount": 3,
        "content": "Copying 200 Gb of snapshot every 6 hours across region will be costlier approach and ensuring lambda to  finish job of coping 200gb snapshot with 15 mins is also not viable. \nSo i think D is best choice."
      },
      {
        "date": "2022-05-22T06:18:00.000Z",
        "voteCount": 1,
        "content": "c is right ."
      },
      {
        "date": "2022-04-30T14:07:00.000Z",
        "voteCount": 1,
        "content": "x A &amp; B out: only automated snapshot per day allowed we need 4\nC. Create an AWS Lambda function to take an Amazon RDS snapshot every 6 hours and use a second Lambda function to copy the snapshot into another Region (cost effective)\nD. Create a cross-Region read replica for Amazon RDS in another Region and take an automated snapshot of the read replica (costly)"
      },
      {
        "date": "2022-03-06T11:12:00.000Z",
        "voteCount": 1,
        "content": "Definitely (C) - also a question in the pfficial sample exam discussed by Stephen Maarek and Riyaz in their Udemy course"
      },
      {
        "date": "2021-12-28T00:31:00.000Z",
        "voteCount": 1,
        "content": "System snapshot can't fulfill 6 hours requirement. You need to control it by script\nhttps://aws.amazon.com/blogs/database/%C2%AD%C2%AD%C2%ADautomating-cross-region-cross-account-snapshot-copies-with-the-snapshot-tool-for-amazon-aurora/"
      },
      {
        "date": "2021-11-06T07:32:00.000Z",
        "voteCount": 1,
        "content": "Anyone planning for exam?\nWe can share study material with each other, it would be beneficial for both. You can email me on \"awsdbguru at gmail\""
      },
      {
        "date": "2021-11-04T08:00:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is ==&gt;&gt; C. any idea how much Q we will get in real exam from Q available here? anyone is preparing for this exam and want to do group study with us, comment with mail id."
      },
      {
        "date": "2021-11-03T05:41:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\nAutomated Backup is daily, so 6 hour RPO is not possible.\nOnly C is correct."
      },
      {
        "date": "2021-10-31T11:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is C.  Reason for eliminating D is that RDS SQL SERVER does not support cross-region read replica and the question does not state which database engine used in RDS."
      },
      {
        "date": "2021-10-29T18:40:00.000Z",
        "voteCount": 2,
        "content": "Agree with C"
      },
      {
        "date": "2021-10-28T19:09:00.000Z",
        "voteCount": 1,
        "content": "I think it is C."
      },
      {
        "date": "2021-10-27T04:37:00.000Z",
        "voteCount": 2,
        "content": "Ans: D with key \"operationally efficient\".\nWith option C, it is necessary to copy 200GB of data from one region to another every 6 hours."
      },
      {
        "date": "2023-09-13T12:26:00.000Z",
        "voteCount": 1,
        "content": "With C, it would be required for a Lambda function to copy 200 GB in 15 mins (max execution time) - every 6 hours. It might be feasible given a decent throughput (cross-region!), but that's not guaranteed and there's no mention of the available throughput."
      },
      {
        "date": "2021-10-26T13:31:00.000Z",
        "voteCount": 1,
        "content": "A - wrong - I guess you cannot copy automated snapshots to another region. You must create a copy of automated snapshot in the same region first.\nB - wrong - automated snapshots are  taken once in a day which alone wouldnt meet proposed RPO of 6hrs\nC - best answer for the requirement in hand.\nD - best option for cross region DR. However having a read-replica in another region is costlier than option C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/26019-exam-aws-certified-database-specialty-topic-1-question-42/",
    "body": "An Amazon RDS EBS-optimized instance with Provisioned IOPS (PIOPS) storage is using less than half of its allocated IOPS over the course of several hours under constant load. The RDS instance exhibits multi-second read and write latency, and uses all of its maximum bandwidth for read throughput, yet the instance uses less than half of its CPU and RAM resources.<br>What should a Database Specialist do in this situation to increase performance and return latency to sub-second levels?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the DB instance storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the underlying EBS storage type to General Purpose SSD (gp2)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable EBS optimization on the DB instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the DB instance to an instance class with a higher maximum bandwidth\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T00:56:00.000Z",
        "voteCount": 13,
        "content": "I think this is D\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html\n\"If you are already using Provisioned IOPS storage, provision additional throughput capacity.\" Does D sound right?"
      },
      {
        "date": "2024-01-12T22:53:00.000Z",
        "voteCount": 1,
        "content": "Definitely D\nA will not help with the bottleneck, B and C will do things worse"
      },
      {
        "date": "2023-09-14T04:58:00.000Z",
        "voteCount": 2,
        "content": "D. Change the DB instance to an instance class with a higher maximum bandwidth\n\nthe performance issue in this case is not related to storage nor cpu nor RAM, it is related to network bandwidth."
      },
      {
        "date": "2023-08-02T12:36:00.000Z",
        "voteCount": 1,
        "content": "We need to understand the difference between throughput and IOPS.\nIOPS \u2013 Count of read/write operations per second.\nThroughput \u2013 Count of read/write bits per second (bps). This measures the amount of time it takes for a disk to read and write data. Throughput is typically the best storage metric when measuring data that needs to be streamed rapidly, such as images and video files.\nSo it's not about the size (A).\nProvisioned IOPS storage to General Purpose SSD (gp2) (B) - rather worse performance judging by data from https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nDisable EBS optimization (E) - rather, it should be activated, if there is such a thing.\nInstance class with a higher maximum bandwidth (D) - we have \"uses all of its maximum bandwidth for read throughput\" in the question. \nWe need to choose another instance class with a higher bandwidth - D."
      },
      {
        "date": "2022-12-19T08:48:00.000Z",
        "voteCount": 1,
        "content": "Correct answer I think"
      },
      {
        "date": "2022-05-01T05:51:00.000Z",
        "voteCount": 2,
        "content": "Objective is to handle maximum bandwidth for read throughput used\n\nx A. Increase the size of the DB instance storage (unrelated)\nx B. Change the underlying EBS storage type to General Purpose SSD (gp2) (will slow down)\nx C. Disable EBS optimization on the DB instance (will slow down)\nD. Change the DB instance to an instance class with a higher maximum bandwidth\n\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html"
      },
      {
        "date": "2022-04-13T01:07:00.000Z",
        "voteCount": 3,
        "content": "A. Increase the size of the DB instance storage - nonsense; instance is using EBS\nB. Change the underlying EBS storage type to General Purpose SSD (gp2) -&gt; nonsense; GP2 is slower than IO2\nC. Disable EBS optimization on the DB instance -&gt; nonsense; nothing like this exist \n\nD is the answer"
      },
      {
        "date": "2021-10-26T03:42:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; D"
      },
      {
        "date": "2021-10-14T14:27:00.000Z",
        "voteCount": 1,
        "content": "My answer is D"
      },
      {
        "date": "2021-10-12T05:10:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-10T20:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-10-09T06:38:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-07T21:49:00.000Z",
        "voteCount": 2,
        "content": "D. For high throughput."
      },
      {
        "date": "2021-10-07T03:32:00.000Z",
        "voteCount": 1,
        "content": "D here! Throughput is related to Size/Type of Instance\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html"
      },
      {
        "date": "2021-10-01T19:41:00.000Z",
        "voteCount": 1,
        "content": "Throughput is related to Size/Type of Instance\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html"
      },
      {
        "date": "2021-09-28T00:33:00.000Z",
        "voteCount": 2,
        "content": "Sorry i mean D"
      },
      {
        "date": "2021-09-26T16:13:00.000Z",
        "voteCount": 1,
        "content": "Ans B here"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/25820-exam-aws-certified-database-specialty-topic-1-question-43/",
    "body": "After restoring an Amazon RDS snapshot from 3 days ago, a company's Development team cannot connect to the restored RDS DB instance. What is the likely cause of this problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe restored DB instance does not have Enhanced Monitoring enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe production DB instance is using a custom parameter group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe restored DB instance is using the default security group\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe production DB instance is using a custom option group"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-01T18:02:00.000Z",
        "voteCount": 12,
        "content": "C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html"
      },
      {
        "date": "2021-10-02T19:15:00.000Z",
        "voteCount": 3,
        "content": "C, agree."
      },
      {
        "date": "2021-09-25T01:22:00.000Z",
        "voteCount": 5,
        "content": "C is correct"
      },
      {
        "date": "2023-10-05T07:28:00.000Z",
        "voteCount": 1,
        "content": "is it misleading us to assume that the DB connected for first three days?"
      },
      {
        "date": "2023-09-30T04:23:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-28T04:05:00.000Z",
        "voteCount": 1,
        "content": "how do we know what parameter or option groups source rds is using so assuming security group mismatch because even parameter or option group mismatch also if securrity group is correct one we can make the connections"
      },
      {
        "date": "2023-05-22T21:10:00.000Z",
        "voteCount": 1,
        "content": "It is C."
      },
      {
        "date": "2022-07-12T06:39:00.000Z",
        "voteCount": 3,
        "content": "When you restore a DB instance, the default virtual private cloud (VPC), DB subnet group, and VPC security group are associated with the restored instance, unless you choose different ones."
      },
      {
        "date": "2022-05-01T05:42:00.000Z",
        "voteCount": 1,
        "content": "C and B both have incomplete info, would go with B assuming that PROD is the source, and custom parameter group not selected or changed from default for restored one\n\nx A.  Enhanced Monitoring enabled (unrelated - for performance check)\nB. The production DB instance is using a custom parameter group (would be correct if it was the source)\nx C. The restored DB instance is using the default security group (but would not be issue if source also had default)\nx D. The production DB instance is using a custom option group (unrelated - it can specify features, called options, that are available for a particular Amazon RDS DB instance)\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/specify-parameter-groups-when-restoring-amazon-rds-backups/"
      },
      {
        "date": "2022-05-01T05:45:00.000Z",
        "voteCount": 2,
        "content": "Correction: Ans is C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html\n\nWhen you restore a DB instance, the default virtual private cloud (VPC), DB subnet group, and VPC security group are associated with the restored instance, unless you choose different ones."
      },
      {
        "date": "2022-04-13T01:08:00.000Z",
        "voteCount": 4,
        "content": "SG is most typically the cause of an connection issue"
      },
      {
        "date": "2022-01-29T10:31:00.000Z",
        "voteCount": 1,
        "content": "C, but B........\n\nParameter group considerations\n\nWe recommend that you retain the DB parameter group for any DB snapshots you create, so that you can associate your restored DB instance with the correct parameter group.\n\nThe default DB parameter group is associated with the restored instance, unless you choose a different one. No custom parameter settings are available in the default parameter group.\n\nYou can specify the parameter group when you restore the DB instance."
      },
      {
        "date": "2021-11-02T09:15:00.000Z",
        "voteCount": 3,
        "content": "Omit B, D regarding PROD DB\nA is not relevant.\nC is correct."
      },
      {
        "date": "2021-10-23T16:46:00.000Z",
        "voteCount": 2,
        "content": "C. The restored DB instance is using the default security group\n\nThis is likely the cause of the problem since security groups control the connectivity to the DB instance."
      },
      {
        "date": "2021-11-01T16:58:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/"
      },
      {
        "date": "2021-10-22T20:32:00.000Z",
        "voteCount": 4,
        "content": "Answer C"
      },
      {
        "date": "2021-10-21T08:10:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-12T06:39:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-10-10T23:11:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-10-10T18:01:00.000Z",
        "voteCount": 3,
        "content": "yes. C. SG controls the access."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/67555-exam-aws-certified-database-specialty-topic-1-question-44/",
    "body": "A gaming company has implemented a leaderboard in AWS using a Sorted Set data structure within Amazon ElastiCache for Redis. The ElastiCache cluster has been deployed with cluster mode disabled and has a replication group deployed with two additional replicas. The company is planning for a worldwide gaming event and is anticipating a higher write load than what the current cluster can handle.<br>Which method should a Database Specialist use to scale the ElastiCache cluster ahead of the upcoming event?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable cluster mode on the existing ElastiCache cluster and configure separate shards for the Sorted Set across all nodes in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the ElastiCache cluster nodes to a larger instance size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional ElastiCache cluster and load-balance traffic between the two clusters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the EXPIRE command and set a higher time to live (TTL) after each call to increment a given key."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T13:57:00.000Z",
        "voteCount": 5,
        "content": "cannot enable Cluster Mode on an existing cluster,  With cluster mode disabled it will allow only vertical scaling."
      },
      {
        "date": "2023-12-08T09:10:00.000Z",
        "voteCount": 3,
        "content": "Elasticache now supports enabling cluster mode on existing clusters.\nhttps://aws.amazon.com/about-aws/whats-new/2023/05/amazon-elasticache-redis-cluster-mode-configuration-existing-clusters/#:~:text=Amazon%20ElastiCache%20for%20Redis%20now%20supports%20enabling%20Cluster%20Mode%20configuration%20on%20existing%20clusters,-Posted%20On%3A%20May&amp;text=You%20can%20now%20update%20your,data%2C%20or%20affect%20application%20availability."
      },
      {
        "date": "2023-10-27T03:40:00.000Z",
        "voteCount": 3,
        "content": "Elasticache now supports enabling cluster mode on existing clusters. \nhttps://aws.amazon.com/about-aws/whats-new/2023/05/amazon-elasticache-redis-cluster-mode-configuration-existing-clusters/#:~:text=Amazon%20ElastiCache%20for%20Redis%20now%20supports%20enabling%20Cluster%20Mode%20configuration%20on%20existing%20clusters,-Posted%20On%3A%20May&amp;text=You%20can%20now%20update%20your,data%2C%20or%20affect%20application%20availability."
      },
      {
        "date": "2023-09-01T06:12:00.000Z",
        "voteCount": 1,
        "content": "B. Increase the size of the ElastiCache cluster nodes to a larger instance size.\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Scaling.RedisReplGrps.html"
      },
      {
        "date": "2023-07-01T07:18:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-05T13:48:00.000Z",
        "voteCount": 1,
        "content": "Option B (increasing the size of cluster nodes) can provide some level of scaling, but it may have limitations in terms of the maximum capacity it can handle. Additionally, simply increasing the node size may not fully address the anticipated higher write load during the gaming event.\nOption C: Creating an additional ElastiCache cluster and load-balancing traffic between the clusters allows for distributing the write load across multiple clusters, effectively scaling the capacity and handling increased demand. This approach provides horizontal scalability and helps mitigate the potential performance limitations of a single cluster.\n\nWhy not C?"
      },
      {
        "date": "2022-10-23T15:36:00.000Z",
        "voteCount": 2,
        "content": "B\n\nRedis (cluster mode disabled) supports scaling. You can scale read capacity by adding or deleting replica nodes, or you can scale capacity by scaling up to a larger node type."
      },
      {
        "date": "2022-06-23T06:12:00.000Z",
        "voteCount": 1,
        "content": "Answer:B"
      },
      {
        "date": "2022-03-06T11:08:00.000Z",
        "voteCount": 4,
        "content": "B is correct. You cannot enable Cluster Mode on an existing cluster"
      },
      {
        "date": "2023-12-14T16:47:00.000Z",
        "voteCount": 1,
        "content": "Cluster mode configuration can only be changed from cluster mode disabled to cluster mode enabled. Reverting this configuration is not possible.\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/modify-cluster-mode.html"
      },
      {
        "date": "2022-02-20T13:49:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html\n\nReads v. writes \u2013 If the primary load on your cluster is applications reading data, you can scale a Redis (cluster mode disabled) cluster by adding and deleting read replicas. However, there is a maximum of 5 read replicas. If the load on your cluster is write-heavy, you can benefit from the additional write endpoints of a Redis (cluster mode enabled) cluster with multiple shards."
      },
      {
        "date": "2022-02-08T01:56:00.000Z",
        "voteCount": 2,
        "content": "I think answer is B. With cluster mode disabled it will allow only vertical scaling. So going with B"
      },
      {
        "date": "2021-12-10T05:14:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer\nEnabling Cluster Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.\nhttps://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/"
      },
      {
        "date": "2022-02-07T12:20:00.000Z",
        "voteCount": 2,
        "content": "you can not enable the cluster mode on the already running Elasticache cluster. Have to provision a new cluster with the Cluster mode enabled and restore the backed up data from S3."
      },
      {
        "date": "2022-03-06T11:08:00.000Z",
        "voteCount": 2,
        "content": "Wrong. You cannot convert an EXISTING Cluster Mode Disabled cluster to Cluster Mode enabled. You need to create a new cluster with cluster mode enabled and warm cache it by loading it with the RDB file backup of the original"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/26361-exam-aws-certified-database-specialty-topic-1-question-45/",
    "body": "An ecommerce company has tasked a Database Specialist with creating a reporting dashboard that visualizes critical business metrics that will be pulled from the core production database running on Amazon Aurora. Data that is read by the dashboard should be available within 100 milliseconds of an update.<br>The Database Specialist needs to review the current configuration of the Aurora DB cluster and develop a cost-effective solution. The solution needs to accommodate the unpredictable read workload from the reporting dashboard without any impact on the write availability and performance of the DB cluster.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the serverless option in the DB cluster so it can automatically scale based on demand.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a clone of the existing DB cluster for the new Application team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate DB cluster for the new workload, refresh from the source DB cluster, and set up ongoing replication using AWS DMS change data capture (CDC).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an automatic scaling policy to the DB cluster to add Aurora Replicas to the cluster based on CPU consumption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-28T14:38:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nreplica lag &lt; 100 ms\n\nOption A would take time"
      },
      {
        "date": "2023-09-01T06:28:00.000Z",
        "voteCount": 3,
        "content": "D. Add an automatic scaling policy to the DB cluster to add Aurora Replicas to the cluster based on CPU consumption.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html"
      },
      {
        "date": "2023-06-03T10:00:00.000Z",
        "voteCount": 3,
        "content": "agree it's D...there is no 'serverless turn on ' option exist\nFor example, take a scaling policy that uses the predefined average CPU utilization metric. Such a policy can keep CPU utilization at, or close to, a specified percentage of utilization, such as 40 percent."
      },
      {
        "date": "2022-12-19T18:21:00.000Z",
        "voteCount": 1,
        "content": "C. Create a separate DB cluster for the new workload, refresh from the source DB cluster, and set up ongoing replication using AWS DMS change data capture (CDC)."
      },
      {
        "date": "2022-12-19T09:57:00.000Z",
        "voteCount": 1,
        "content": "Even though replication is asynchronous I believe it is within the 100 ms requirement.  It is also cost effective with autoscaling."
      },
      {
        "date": "2022-05-06T20:12:00.000Z",
        "voteCount": 1,
        "content": "Option A? How can you use serverless and scale in the same statement"
      },
      {
        "date": "2022-02-06T17:44:00.000Z",
        "voteCount": 3,
        "content": "Answer D\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\n\"As a result, all Aurora Replicas return the same data for query results with minimal replica lag. This lag is usually much less than 100 milliseconds after the primary instance has written an update. \""
      },
      {
        "date": "2021-11-01T09:46:00.000Z",
        "voteCount": 2,
        "content": "Option D makes most sense."
      },
      {
        "date": "2021-10-30T10:09:00.000Z",
        "voteCount": 4,
        "content": "Answer D"
      },
      {
        "date": "2021-10-26T08:49:00.000Z",
        "voteCount": 3,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-25T09:28:00.000Z",
        "voteCount": 3,
        "content": "A is a ridiculous answer. If you say A, don't bother taking the test."
      },
      {
        "date": "2021-10-26T06:09:00.000Z",
        "voteCount": 4,
        "content": "Oh, I love this your response...... Option A seems  to make the most sense to me and I will take the test\nHere is a link to help with the correct response --  https://aws.amazon.com/rds/aurora/faqs/ \n\nAmazon Aurora Serverless is an on-demand, autoscaling configuration for the MySQL-compatible and PostgreSQL-compatible editions of Amazon Aurora. An Aurora Serverless DB cluster automatically starts up, shuts down, and scales capacity up or down based on your application's needs. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. \nThe question is focused on unpredictable workloads"
      },
      {
        "date": "2022-03-05T17:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is saying \"Turn on Serverless option on the cluster\". There's no such thing."
      },
      {
        "date": "2021-10-31T00:24:00.000Z",
        "voteCount": 5,
        "content": "you can't turn on serverless option.you need to take a snapshot and restore it to Aurora Serverless. A is definitely wrong"
      },
      {
        "date": "2021-10-24T00:08:00.000Z",
        "voteCount": 1,
        "content": "Also it says that the new workload is unpredictable, and yet should have no impact on the current operations. It takes time to adjust to unpredictable workloads, so D does not solve the stated problem."
      },
      {
        "date": "2021-11-16T07:04:00.000Z",
        "voteCount": 1,
        "content": "it says read workload, not write."
      },
      {
        "date": "2021-10-21T07:41:00.000Z",
        "voteCount": 1,
        "content": "B is the best choice. A cloned Cluster will use the existing DB cluster until those items are written over, so that will have the fastest, immediate response. The best solution, of course, is to dedicate a read replica to the team and use an instance endpoint. D does not directly address the issue."
      },
      {
        "date": "2021-11-03T23:13:00.000Z",
        "voteCount": 3,
        "content": "Clone is a one time copy, it does not continuously replicate which is a requirement here."
      },
      {
        "date": "2021-10-15T12:25:00.000Z",
        "voteCount": 1,
        "content": "Why can't B be an option? Create a clone of the Aurora cluster and use the clone for data read on the Dashboard?"
      },
      {
        "date": "2021-10-19T22:40:00.000Z",
        "voteCount": 1,
        "content": "B is not an option since clone does not support read scaling of the same cluster. This question is on read scaling of Aurora cluster. Only achieved with Read replicas. Serverless as pointed out is not available as an option. Correct an D."
      },
      {
        "date": "2021-10-13T02:21:00.000Z",
        "voteCount": 2,
        "content": "Ans is D.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html#:~:text=The%20scaling%20policy%20defines%20the,CloudWatch%20metrics%20and%20target%20values."
      },
      {
        "date": "2021-10-11T12:07:00.000Z",
        "voteCount": 2,
        "content": "D is the best choice"
      },
      {
        "date": "2021-10-08T22:30:00.000Z",
        "voteCount": 3,
        "content": "Ashoks is right. There is no turn on option to Serverless, it needs migration"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/68554-exam-aws-certified-database-specialty-topic-1-question-46/",
    "body": "A retail company is about to migrate its online and mobile store to AWS. The company's CEO has strategic plans to grow the brand globally. A Database<br>Specialist has been challenged to provide predictable read and write database performance with minimal operational overhead.<br>What should the Database Specialist do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB global tables to synchronize transactions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to copy the orders table data across Regions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora Global Database to synchronize all transactions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB Streams to replicate all DynamoDB transactions and sync them"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T12:49:00.000Z",
        "voteCount": 8,
        "content": "A\nhttps://aws.amazon.com/dynamodb/features/\nWith global tables, your globally distributed applications can access data locally in the selected regions to get single-digit millisecond read and write performance. \n\nNot Aurora Global Database, as per this link: https://aws.amazon.com/rds/aurora/global-database/?nc1=h_ls .  Aurora Global Database lets you easily scale database reads across the world and place your applications close to your users."
      },
      {
        "date": "2023-12-13T20:24:00.000Z",
        "voteCount": 1,
        "content": "A. Use Amazon DynamoDB global tables to synchronize transactions"
      },
      {
        "date": "2023-10-18T16:00:00.000Z",
        "voteCount": 1,
        "content": "A is best choice https://aws.amazon.com/dynamodb/features/"
      },
      {
        "date": "2023-03-22T03:19:00.000Z",
        "voteCount": 1,
        "content": "Using Amazon DynamoDB global tables to synchronize transactions - is a viable option, but it's more suited for a NoSQL database. It can provide predictable read and write performance, but with DynamoDB, there are limitations regarding query and transactional capabilities."
      },
      {
        "date": "2023-02-06T22:27:00.000Z",
        "voteCount": 4,
        "content": "A is correct\nB is incorrect because aurora global db only allow 'write' on primary region"
      },
      {
        "date": "2022-05-01T05:57:00.000Z",
        "voteCount": 3,
        "content": "A. Use Amazon DynamoDB global tables"
      },
      {
        "date": "2022-01-12T05:34:00.000Z",
        "voteCount": 2,
        "content": "Ans: A"
      },
      {
        "date": "2021-12-24T13:26:00.000Z",
        "voteCount": 3,
        "content": "I go with option A"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/26022-exam-aws-certified-database-specialty-topic-1-question-47/",
    "body": "A company is closing one of its remote data centers. This site runs a 100 TB on-premises data warehouse solution. The company plans to use the AWS Schema<br>Conversion Tool (AWS SCT) and AWS DMS for the migration to AWS. The site network bandwidth is 500 Mbps. A Database Specialist wants to migrate the on- premises data using Amazon S3 as the data lake and Amazon Redshift as the data warehouse. This move must take place during a 2-week period when source systems are shut down for maintenance. The data should stay encrypted at rest and in transit.<br>Which approach has the least risk and the highest likelihood of a successful data transfer?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a VPN tunnel for encrypting data over the network from the data center to AWS. Leverage AWS SCT and apply the converted schema to Amazon Redshift. Once complete, start an AWS DMS task to move the data from the source to Amazon S3. Use AWS Glue to load the data from Amazon S3 to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage AWS SCT and apply the converted schema to Amazon Redshift. Start an AWS DMS task with two AWS Snowball Edge devices to copy data from on-premises to Amazon S3 with AWS KMS encryption. Use AWS DMS to finish copying data to Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage AWS SCT and apply the converted schema to Amazon Redshift. Once complete, use a fleet of 10 TB dedicated encrypted drives using the AWS Import/Export feature to copy data from on-premises to Amazon S3 with AWS KMS encryption. Use AWS Glue to load the data to Amazon redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a VPN tunnel for encrypting data over the network from the data center to AWS. Leverage a native database export feature to export the data and compress the files. Use the aws S3 cp multi-port upload command to upload these files to Amazon S3 with AWS KMS encryption. Once complete, load the data to Amazon Redshift using AWS Glue."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T10:48:00.000Z",
        "voteCount": 19,
        "content": "Ans: B\nhttps://aws.amazon.com/blogs/database/new-aws-dms-and-aws-snowball-integration-enables-mass-database-migrations-and-migrations-of-large-databases/"
      },
      {
        "date": "2021-09-21T00:53:00.000Z",
        "voteCount": 2,
        "content": "Going with B as well due to your compelling link."
      },
      {
        "date": "2021-09-29T13:14:00.000Z",
        "voteCount": 4,
        "content": "Found this link that agrees with B \nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html\n\"Large-scale data migrations can include many terabytes of information, and can be slowed by network performance and by the sheer amount of data that has to be moved. AWS Snowball Edge is an AWS service you can use to transfer data to the cloud at faster-than-network speeds using an AWS-owned appliance.\"\n\"When you use AWS SCT and an AWS Snowball Edge device, you migrate your data in two stages. First, you use the AWS SCT to process the data locally and then move that data to the AWS Snowball Edge device. You then send the device to AWS using the AWS Snowball Edge process, and then AWS automatically loads the data into an Amazon S3 bucket. Next, when the data is available on Amazon S3, you use AWS SCT to migrate the data to Amazon Redshift.\""
      },
      {
        "date": "2021-10-06T09:28:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect. \n1. You just need one snowball edge device because one device can hold up to 80 TB of data. \n2. The lastest step for B is \"Use AWS DMS to finish copying data to Amazon Redshift.\" but from the AWS docs need \"use AWS SCT to migrate the data to Amazon Redshift.\""
      },
      {
        "date": "2021-10-19T07:48:00.000Z",
        "voteCount": 9,
        "content": "There is 100 TB of data that needs to be transferred. Since a snowball can only hold 80, it makes sense that there would be a need for 2 of them. Hence B."
      },
      {
        "date": "2022-03-05T14:26:00.000Z",
        "voteCount": 2,
        "content": "Wrong. you need TWO snowball  edge devices since out of 100TB  only 80TB is usable. This is a standard question in tutotrialsdojo as well - very respectable site."
      },
      {
        "date": "2021-11-01T21:28:00.000Z",
        "voteCount": 5,
        "content": "Although snowball is a petabyte-scale data transport solution, 100TB database migration through a 500 Mbps network is impossible in 2 weeks.\nbandwidth: 500 Mbps = 62.5 MB/s\ncapacity: 100 TB = 100 * 10**6 = 100000000 MB\ntime consume: 100000000 / 62.5 MB/s = 1600000.0 s = \n1600000 / 60 / 60 / 24 = 18.5 days"
      },
      {
        "date": "2024-04-01T07:22:00.000Z",
        "voteCount": 1,
        "content": "That is why you need snowball. Network speed won't do it ."
      },
      {
        "date": "2023-11-26T23:52:00.000Z",
        "voteCount": 1,
        "content": "How is 500MBPS equal to 62.5 MB/sec?"
      },
      {
        "date": "2023-11-26T23:54:00.000Z",
        "voteCount": 1,
        "content": "Got it. Lowercase \"b\" is bits while uppercase \"B\" is bytes. 500 mbps is 500/8 MB/s."
      },
      {
        "date": "2022-05-06T20:09:00.000Z",
        "voteCount": 4,
        "content": "With the given bandwidth any other option will be completed in more than 20 days."
      },
      {
        "date": "2022-04-28T14:03:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/new-aws-dms-and-aws-snowball-integration-enables-mass-database-migrations-and-migrations-of-large-databases/"
      },
      {
        "date": "2021-12-22T14:10:00.000Z",
        "voteCount": 1,
        "content": "Option B.. With SCT data extraction agent you can extract the data as well as the schema. Snowball Edge devices are safe."
      },
      {
        "date": "2021-11-05T20:17:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2021-11-02T02:35:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-10-30T16:53:00.000Z",
        "voteCount": 1,
        "content": "B is the right choice here"
      },
      {
        "date": "2021-10-29T18:18:00.000Z",
        "voteCount": 2,
        "content": "B. Snow ball will be the solution to transfer 100TB, since existing 500Mbps bandwidth is not enough"
      },
      {
        "date": "2021-10-28T01:02:00.000Z",
        "voteCount": 1,
        "content": "For me B here"
      },
      {
        "date": "2021-10-14T09:31:00.000Z",
        "voteCount": 1,
        "content": "A. Answer: It's possible but copy 100TB via internet is not reliable\n\nB. Answer: \"Use AWS DMS to finish copying data to Amazon Redshift.\" but from the AWS docs need \"use AWS SCT to migrate the data to Amazon Redshift.\"\n\nC. Answer: Correct\n\nD. Answer: It's possible but it's missing the step AWS SCT and apply the converted schema to Amazon Redshift."
      },
      {
        "date": "2021-10-18T17:57:00.000Z",
        "voteCount": 2,
        "content": "didn't know you can ship fleets of drives to AWS."
      },
      {
        "date": "2021-10-29T17:08:00.000Z",
        "voteCount": 1,
        "content": "sorry, change my mind, B is correct."
      },
      {
        "date": "2021-10-04T16:58:00.000Z",
        "voteCount": 1,
        "content": "B is right option"
      },
      {
        "date": "2021-09-30T06:11:00.000Z",
        "voteCount": 1,
        "content": "B is correct for me"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/26418-exam-aws-certified-database-specialty-topic-1-question-48/",
    "body": "A company is looking to migrate a 1 TB Oracle database from on-premises to an Amazon Aurora PostgreSQL DB cluster. The company's Database Specialist discovered that the Oracle database is storing 100 GB of large binary objects (LOBs) across multiple tables. The Oracle database has a maximum LOB size of<br>500 MB with an average LOB size of 350 MB. The Database Specialist has chosen AWS DMS to migrate the data with the largest replication instances.<br>How should the Database Specialist optimize the database migration using AWS DMS?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single task using full LOB mode with a LOB chunk size of 500 MB to migrate the data and LOBs together",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two tasks: task1 with LOB tables using full LOB mode with a LOB chunk size of 500 MB and task2 without LOBs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two tasks: task1 with LOB tables using limited LOB mode with a maximum LOB size of 500 MB and task 2 without LOBs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single task using limited LOB mode with a maximum LOB size of 500 MB to migrate data and LOBs together"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-08T16:35:00.000Z",
        "voteCount": 17,
        "content": "C sounds correct since as per link https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS, \"AWS DMS migrates LOB data in two phases:\n1. AWS DMS creates a new row in the target table and populates the row with all data except the associated LOB value.\n2.AWS DMS updates the row in the target table with the LOB data.\"\nThis means that we would need two tasks, one per phase and use limited LOB mode for best performance."
      },
      {
        "date": "2022-05-20T00:41:00.000Z",
        "voteCount": 4,
        "content": "I think it is C. The explanation from vicks316 is irrelevant. From this link -&gt; \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html\nUsing the Max LOB size (K) option with a value greater than 63KB impacts the performance of a full load configured to run in limited LOB mode. During a full load, DMS allocates memory by multiplying the Max LOB size (k) value by the Commit rate, and the product is multiplied by the number of LOB columns. When DMS can\u2019t pre-allocate that memory, DMS starts consuming SWAP memory, and that impacts performance of a full load. So, if you experience performance issues when using limited LOB mode, consider decreasing the commit rate until you achieve an acceptable level of performance. You can also consider using inline LOB mode for supported endpoints once you understand your LOB distribution for the table."
      },
      {
        "date": "2021-10-16T17:13:00.000Z",
        "voteCount": 2,
        "content": "This doesn't mean we need two tasks.  It just explains how LOBs are copied to target in any scenario.  First all columns except LOBs then the LOB."
      },
      {
        "date": "2021-12-28T04:51:00.000Z",
        "voteCount": 14,
        "content": "Limited LOB maxsize is 100MB. This question is about LOB average 350MB. Full LOB is better option"
      },
      {
        "date": "2023-10-05T07:48:00.000Z",
        "voteCount": 1,
        "content": "the link says it not a hard limit 100MB, it is recommended."
      },
      {
        "date": "2021-12-28T04:52:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"
      },
      {
        "date": "2024-01-12T23:20:00.000Z",
        "voteCount": 1,
        "content": "I believe the right answer is C\nAs for B, 500 chunk size is way too big and will lead to many failures."
      },
      {
        "date": "2023-12-26T08:44:00.000Z",
        "voteCount": 1,
        "content": "Limited LOB with max size of 500MB makes migration fast."
      },
      {
        "date": "2023-11-27T03:47:00.000Z",
        "voteCount": 1,
        "content": "Full LOB mode has no information about chunk size.\nFull LOB mode \u2013 In full LOB mode AWS DMS migrates all LOBs from source to target regardless of size. In this configuration, AWS DMS has no information about the maximum size of LOBs to expect. Thus, LOBs are migrated one at a time, piece by piece. Full LOB mode can be quite slow.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html\n\nEither C or D - but C seems more optimal."
      },
      {
        "date": "2023-10-05T07:51:00.000Z",
        "voteCount": 1,
        "content": "the question is confusing, but the lesson here full lob -&gt; incur performance issue, limited lob - smooth, hopefully the same question doesn't repeat, if so then let it come with some additional clarity on performance, either way 500MB is huge and we expect performance issue, this is seems to be meaningless question"
      },
      {
        "date": "2023-09-28T06:20:00.000Z",
        "voteCount": 2,
        "content": "It can not be C because of  \n\"Limited LOB mode \u2013 In limited LOB mode, you set a maximum LOB size for DMS to accept. That enables DMS to pre-allocate memory and load the LOB data in bulk. LOBs that exceed the maximum LOB size are truncated, and a warning is issued to the log file. In limited LOB mode, you can gain significant performance over full LOB mode. We recommend that you use limited LOB mode whenever possible. The maximum recommended value is 102400 KB (100 MB).\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"
      },
      {
        "date": "2023-07-17T20:01:00.000Z",
        "voteCount": 1,
        "content": "To optimize for large size LOB, we should use multiple tasks =&gt; eliminate A &amp; D.\nB: we can't configure such large LOB chunk size of 500 MB. Please note that \"chunk size\" is different from \"max LOB size\".\nSo I select C but can anyone explain what benefit of task 2 without LOBs?"
      },
      {
        "date": "2023-07-12T08:17:00.000Z",
        "voteCount": 2,
        "content": "B looks to be right\nTask1 with LOB Tables:\nUse the full LOB mode in AWS DMS to ensure efficient streaming of LOB data during replication.\nSet the LOB chunk size to match the maximum LOB size in Oracle (500 MB). This ensures optimal data transfer and minimizes network overhead.\nTask2 without LOBs:\nMigrate the remaining tables that do not contain LOBs separately in a task without LOB-specific configurations."
      },
      {
        "date": "2023-06-28T04:33:00.000Z",
        "voteCount": 2,
        "content": "full lob is quite slow ! to optimize we need to use limited lob even if 500mb is not recommended size . Maybe i am super confused"
      },
      {
        "date": "2023-03-19T06:30:00.000Z",
        "voteCount": 2,
        "content": "B is correct. Option C could have been correct however, please note the word \"recommended size\" for Limited LOB mode which is 100MB. Agreed that 100 MB is not the max limit for limited LOB however, anything above 100MB in limited LOB is not recommended which makes full LOB with 2 different tasks a better option."
      },
      {
        "date": "2023-01-24T16:59:00.000Z",
        "voteCount": 3,
        "content": "The Max recommended LOB size is 100MB in limited LOB mode. So can't be C. B is correct. \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"
      },
      {
        "date": "2023-01-18T03:33:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS. Limited LOB mode migrates all LOB values up to a user-specified size limit (default is 32 KB). LOB values larger than the size limit must be manually migrated. Limited LOB mode, the default for all migration tasks, typically provides the best performance. However, ensure that the Max LOB size parameter setting is correct. Set this parameter to the largest LOB size for all your tables."
      },
      {
        "date": "2023-01-13T05:36:00.000Z",
        "voteCount": 1,
        "content": "C is the answer - https://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/"
      },
      {
        "date": "2022-12-20T04:32:00.000Z",
        "voteCount": 2,
        "content": "C. is the answer. Faster = 2 task Limited Lob and No Lob\n\nFull LOB mode \u2013 Migrate complete LOBs regardless of size. AWS DMS migrates LOBs piecewise in chunks controlled by the Max LOB Size parameter. This mode is slower than using limited LOB mode.\nLimited LOB mode \u2013 Truncate LOBs to the value specified by the Max LOB Size parameter. This mode is faster than using full LOB mode."
      },
      {
        "date": "2022-12-13T00:41:00.000Z",
        "voteCount": 2,
        "content": "C is correct, 100MB is the maximum recommended value, not a limitation.\nB is wrong, because 500MB is too huge for chunk setting, connection error would be encountered.\n\nWhen a task is configured to use full LOB mode, AWS DMS retrieves LOBs in pieces. The LOB chunk size (K) option determines the size of each piece. When setting this option, pay particular attention to the maximum packet size allowed by your network configuration. If the LOB chunk size exceeds your maximum allowed packet size, you might see disconnect errors. The recommended value for LobChunkSize is 64 kilobytes. Increasing the value for LobChunkSize above 64 kilobytes can cause task failures."
      },
      {
        "date": "2022-10-23T19:29:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS\n\nAnswer : C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/64810-exam-aws-certified-database-specialty-topic-1-question-49/",
    "body": "A Database Specialist is designing a disaster recovery strategy for a production Amazon DynamoDB table. The table uses provisioned read/write capacity mode, global secondary indexes, and time to live (TTL). The Database Specialist has restored the latest backup to a new table.<br>To prepare the new table with identical settings, which steps should be performed? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-create global secondary indexes in the new table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine IAM policies for access to the new table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine the TTL settings\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the table from the AWS Management Console or use the update-table command",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the provisioned read and write capacity"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-27T06:47:00.000Z",
        "voteCount": 16,
        "content": "I'll go B and C.\nThe following items need to be reconfigured after restoring the DynamoDB table.\n--AutoScaling policy\n--IAM policy\n--CloudWatch settings\n--Tags\n--Stream settings\n--TTL"
      },
      {
        "date": "2021-10-28T16:24:00.000Z",
        "voteCount": 3,
        "content": "You are right, it is explained here -&gt; https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html"
      },
      {
        "date": "2022-03-05T17:37:00.000Z",
        "voteCount": 6,
        "content": "The following are not restored and would need to be configured again:\n- IAM and Autoscaling Policies\n- Cloudwatch Triggers and Alarms\n- TTL and Streams\n- Tag"
      },
      {
        "date": "2023-03-28T07:11:00.000Z",
        "voteCount": 4,
        "content": "The answer is BC because this is what included in backup https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateBackup.html"
      },
      {
        "date": "2022-04-28T18:26:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CreateBackup.html\n\nYou must manually set up the following on the restored table:\nAuto scaling policies\nAWS Identity and Access Management (IAM) policies\nAmazon CloudWatch metrics and alarms\nTags\nStream settings\nTime to Live (TTL) settings"
      },
      {
        "date": "2021-12-14T05:25:00.000Z",
        "voteCount": 3,
        "content": "Option B, C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/25382-exam-aws-certified-database-specialty-topic-1-question-50/",
    "body": "A Database Specialist is creating Amazon DynamoDB tables, Amazon CloudWatch alarms, and associated infrastructure for an Application team using a development AWS account. The team wants a deployment method that will standardize the core solution components while managing environment-specific settings separately, and wants to minimize rework due to configuration errors.<br>Which process should the Database Specialist recommend to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize common and environmental-specific parameters hierarchically in the AWS Systems Manager Parameter Store, then reference the parameters dynamically from an AWS CloudFormation template. Deploy the CloudFormation stack using the environment name as a parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a parameterized AWS CloudFormation template that builds the required objects. Keep separate environment parameter files in separate Amazon S3 buckets. Provide an AWS CLI command that deploys the CloudFormation stack directly referencing the appropriate parameter bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a parameterized AWS CloudFormation template that builds the required objects. Import the template into the CloudFormation interface in the AWS Management Console. Make the required changes to the parameters and deploy the CloudFormation stack.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that builds the required objects using an AWS SDK. Set the required parameter values in a test event in the Lambda console for each environment that the Application team can modify, as needed. Deploy the infrastructure by triggering the test event in the console."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-25T05:02:00.000Z",
        "voteCount": 10,
        "content": "A. AWS Systems Manager Parameter Store"
      },
      {
        "date": "2021-10-04T11:07:00.000Z",
        "voteCount": 3,
        "content": "Is it A based off this? \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html"
      },
      {
        "date": "2021-10-11T16:18:00.000Z",
        "voteCount": 5,
        "content": "A is correct\nhttps://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/"
      },
      {
        "date": "2024-02-24T03:04:00.000Z",
        "voteCount": 1,
        "content": "C is also possible. However, A is better than C."
      },
      {
        "date": "2023-10-03T15:47:00.000Z",
        "voteCount": 1,
        "content": "The answer is A"
      },
      {
        "date": "2022-12-19T12:46:00.000Z",
        "voteCount": 3,
        "content": "Parameter store to separate the environments"
      },
      {
        "date": "2022-07-10T05:01:00.000Z",
        "voteCount": 1,
        "content": "Mapping should have been the right approach for handing environment and it related setting but that option is not there.. \nWe can still achieve setting as using parameters. \nA"
      },
      {
        "date": "2022-05-01T09:28:00.000Z",
        "voteCount": 3,
        "content": "A. Organize common and environmental-specific parameters hierarchically in the AWS Systems Manager Parameter Store, then reference the parameters dynamically from an AWS CloudFormation template. Deploy the CloudFormation stack using the environment name as a parameter."
      },
      {
        "date": "2021-11-06T00:31:00.000Z",
        "voteCount": 2,
        "content": "The only reasonable answer is A"
      },
      {
        "date": "2021-11-05T00:59:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-11-04T16:27:00.000Z",
        "voteCount": 1,
        "content": "Not sure. I will go with A"
      },
      {
        "date": "2021-10-23T13:29:00.000Z",
        "voteCount": 2,
        "content": "I would select A"
      },
      {
        "date": "2021-10-21T20:06:00.000Z",
        "voteCount": 2,
        "content": "I will go with B"
      },
      {
        "date": "2021-10-20T06:07:00.000Z",
        "voteCount": 1,
        "content": "For me is C or B because on DynamoDB probably i need different configuration (for example RCU and WCU) between Dev and Prod environment. Probably i choose C because CLI not support parameters on S3 but only the body template."
      },
      {
        "date": "2021-10-15T20:58:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/26423-exam-aws-certified-database-specialty-topic-1-question-51/",
    "body": "A company runs online transaction processing (OLTP) workloads on an Amazon RDS for PostgreSQL Multi-AZ DB instance. Tests were run on the database after work hours, which generated additional database logs. The free storage of the RDS DB instance is low due to these additional logs.<br>What should the company do to address this space constraint issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLog in to the host and run the rm $PGDATA/pg_logs/* command",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the rds.log_retention_period parameter to 1440 and wait up to 24 hours for database logs to be deleted\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a ticket with AWS Support to have the logs deleted",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the SELECT rds_rotate_error_log() stored procedure to rotate the logs"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T07:09:00.000Z",
        "voteCount": 7,
        "content": "I cant find anything for the other answers, so I'm going with B based off \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.Concepts.PostgreSQL.html\n\"To set the retention period for system logs, use the rds.log_retention_period parameter. You can find rds.log_retention_period in the DB parameter group associated with your DB instance. The unit for this parameter is minutes. For example, a setting of 1,440 retains logs for one day. The default value is 4,320 (three days).\"\n\"If storage gets too low, Aurora might delete compressed PostgreSQL logs before the retention period expires. If logs are deleted early, you get a message like the following.\nThe oldest PostgreSQL log files were deleted due to local storage constraints.\""
      },
      {
        "date": "2022-06-22T23:52:00.000Z",
        "voteCount": 1,
        "content": "Reason: DB logs (error files) that are retained for too long.\nBecause by default, PostgreSQL error log files have a retention value of 4,320 minutes (three days). Large log files can use more space because of higher workloads. You can change the retention period for system logs using the rds.log_retention_period parameter in the DB parameter group associated with your DB instance. For example, if you set the value to 1440, then logs are retained for one day\nRef: https://aws.amazon.com/premiumsupport/knowledge-center/diskfull-error-rds-postgresql/"
      },
      {
        "date": "2023-06-24T01:49:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html"
      },
      {
        "date": "2023-03-22T03:37:00.000Z",
        "voteCount": 3,
        "content": "In this scenario, the company is facing a low storage issue due to additional logs. To address this issue, they can rotate the logs to free up storage space using the stored procedure rds_rotate_error_log(). This procedure rotates the current log file and starts a new one. The rotated logs are compressed and stored in the log directory, freeing up space in the main storage.\n\nOption A is incorrect because removing log files manually is not recommended, as it may cause issues and loss of data.\n\nOption B is incorrect because changing the log retention period will not delete the existing logs immediately, and the company needs to wait for up to 24 hours for the logs to be deleted.\n\nOption C is incorrect because AWS support does not provide log deletion services."
      },
      {
        "date": "2023-11-27T19:59:00.000Z",
        "voteCount": 1,
        "content": "There is no function named rds_rotate_error_log(). It is just a distractor. Check this link:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html"
      },
      {
        "date": "2022-10-10T23:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-04-29T07:33:00.000Z",
        "voteCount": 2,
        "content": "reduce  rds.log_retention_period parameter and wait"
      },
      {
        "date": "2022-02-23T16:36:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-03T05:45:00.000Z",
        "voteCount": 3,
        "content": "B is correct Answer.\n\nTo set the retention period for system logs, use the rds.log_retention_period parameter. You can find rds.log_retention_period in the DB parameter group associated with your DB instance. The unit for this parameter is minutes. For example, a setting of 1,440 retains logs for one day. The default value is 4,320 (three days). The maximum value is 10,080 (seven days)."
      },
      {
        "date": "2021-11-01T10:58:00.000Z",
        "voteCount": 3,
        "content": "B is Correct.\n==&gt; Anyone planning for exam?\nWe can share study material with each other, it would be beneficial for both. You can email me on \"awsdbguru at gmail\""
      },
      {
        "date": "2021-10-31T15:02:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is ==&gt;&gt; B\nany idea how much Q we will get in real exam from Q available here?\nanyone is preparing for this exam and want to do group study with us, comment with mail_id."
      },
      {
        "date": "2021-10-27T06:07:00.000Z",
        "voteCount": 2,
        "content": "My choice is B"
      },
      {
        "date": "2021-10-19T03:59:00.000Z",
        "voteCount": 2,
        "content": "B here\nThe SELECT rds_rotate_error_log() does not exist on RDS"
      },
      {
        "date": "2021-10-05T09:08:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/67549-exam-aws-certified-database-specialty-topic-1-question-52/",
    "body": "A user has a non-relational key-value database. The user is looking for a fully managed AWS service that will offload the administrative burdens of operating and scaling distributed databases. The solution must be cost-effective and able to handle unpredictable application traffic.<br>What should a Database Specialist recommend for this user?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table with provisioned capacity mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DocumentDB cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table with on-demand capacity mode\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora Serverless DB cluster"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-26T15:00:00.000Z",
        "voteCount": 6,
        "content": "Key-value database -&gt; DynamoDB\nCapable of dealing with unexpected application traffic -&gt; on-demand capacity mode"
      },
      {
        "date": "2023-08-31T04:42:00.000Z",
        "voteCount": 1,
        "content": "C. Create an Amazon DynamoDB table with on-demand capacity mode\n\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand"
      },
      {
        "date": "2023-06-01T05:58:00.000Z",
        "voteCount": 1,
        "content": "\"With provisioned capacity you can also use auto scaling to automatically adjust your table\u2019s capacity based on the specified utilization rate to ensure application performance, and also to potentially reduce costs. To configure auto scaling in DynamoDB, set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage.\"\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html"
      },
      {
        "date": "2023-08-18T02:16:00.000Z",
        "voteCount": 1,
        "content": "If auto scaling was mentioned in the option then that would\u2019ve been the most cost effective solution but since it is not mentioned C seems to be more appropriate"
      },
      {
        "date": "2022-04-30T13:21:00.000Z",
        "voteCount": 1,
        "content": "C. Create an Amazon DynamoDB table with on-demand capacity mode"
      },
      {
        "date": "2021-12-10T04:25:00.000Z",
        "voteCount": 2,
        "content": "C is the answer\nA key-value database is a type of nonrelational database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier.\nOn-demand mode is a good option to create new tables with unknown workloads.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/65859-exam-aws-certified-database-specialty-topic-1-question-53/",
    "body": "A gaming company is designing a mobile gaming app that will be accessed by many users across the globe. The company wants to have replication and full support for multi-master writes. The company also wants to ensure low latency and consistent performance for app users.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB global tables for storage and enable DynamoDB automatic scaling\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora for storage and enable cross-Region Aurora Replicas",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora for storage and cache the user content with Amazon ElastiCache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Neptune for storage"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-31T05:15:00.000Z",
        "voteCount": 2,
        "content": "A. Use Amazon DynamoDB global tables for storage and enable DynamoDB automatic scaling"
      },
      {
        "date": "2023-08-31T05:24:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/dynamodb/global-tables/"
      },
      {
        "date": "2022-04-29T14:00:00.000Z",
        "voteCount": 3,
        "content": "DynamoDB global tables have read and write in all regions, so these are kind of multi master like Aurora Multi Master\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html"
      },
      {
        "date": "2021-12-20T16:00:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2021-11-26T05:48:00.000Z",
        "voteCount": 1,
        "content": "Option A. I was looking for Aurora Multi-Master or DynamoDB Global Tables, there is only one of them."
      },
      {
        "date": "2021-11-11T15:49:00.000Z",
        "voteCount": 1,
        "content": "Option A. worldwide, multi-master writes, minimal latency"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/25201-exam-aws-certified-database-specialty-topic-1-question-54/",
    "body": "A Database Specialist needs to speed up any failover that might occur on an Amazon Aurora PostgreSQL DB cluster. The Aurora DB cluster currently includes the primary instance and three Aurora Replicas.<br>How can the Database Specialist ensure that failovers occur with the least amount of downtime for the application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the TCP keepalive parameters low\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the AWS CLI failover-db-cluster command",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Enhanced Monitoring on the DB cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a database activity stream on the DB cluster"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T05:01:00.000Z",
        "voteCount": 13,
        "content": "Ans: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.html#AuroraPostgreSQL.BestPractices.FastFailover.TCPKeepalives"
      },
      {
        "date": "2021-10-17T21:44:00.000Z",
        "voteCount": 3,
        "content": "A\n\"Enabling TCP keepalive parameters and setting them aggressively ensures that if your client is no longer able to connect to the database, then any active connections are quickly closed. This action allows the application to react appropriately, such as by picking a new host to connect to.\""
      },
      {
        "date": "2021-10-17T21:53:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: A"
      },
      {
        "date": "2023-06-24T02:00:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.FastFailover.html"
      },
      {
        "date": "2022-05-01T05:32:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.BestPractices.html#AuroraPostgreSQL.BestPractices.FastFailover.TCPKeepalives\n\nYou need to set the following TCP keepalive parameters:\n\ntcp_keepalive_time controls the time, in seconds, after which a keepalive packet is sent when no data has been sent by the socket (ACKs are not considered data). We recommend the following setting:\n\ntcp_keepalive_time = 1\n\ntcp_keepalive_intvl controls the time, in seconds, between sending subsequent keepalive packets after the initial packet is sent (set using the tcp_keepalive_time parameter). We recommend the following setting:\n\ntcp_keepalive_intvl = 1\n\ntcp_keepalive_probes is the number of unacknowledged keepalive probes that occur before the application is notified. We recommend the following setting:\n\ntcp_keepalive_probes = 5\n\nThese settings should notify the application within five seconds when the database stops responding."
      },
      {
        "date": "2021-10-26T04:19:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2021-10-17T17:53:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      },
      {
        "date": "2021-10-02T07:07:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-10-02T00:02:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2021-10-01T00:44:00.000Z",
        "voteCount": 2,
        "content": "Yes. It is A"
      },
      {
        "date": "2021-09-25T16:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-09-25T12:06:00.000Z",
        "voteCount": 2,
        "content": "Ans A because we can reduce it by keeping low value of TCP among other parameter."
      },
      {
        "date": "2021-09-25T06:29:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/25202-exam-aws-certified-database-specialty-topic-1-question-55/",
    "body": "A Database Specialist needs to define a database migration strategy to migrate an on-premises Oracle database to an Amazon Aurora MySQL DB cluster. The company requires near-zero downtime for the data migration. The solution must also be cost-effective.<br>Which approach should the Database Specialist take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDump all the tables from the Oracle database into an Amazon S3 bucket using datapump (expdp). Run data transformations in AWS Glue. Load the data from the S3 bucket to the Aurora DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrder an AWS Snowball appliance and copy the Oracle backup to the Snowball appliance. Once the Snowball data is delivered to Amazon S3, create a new Aurora DB cluster. Enable the S3 integration to migrate the data directly from Amazon S3 to Amazon RDS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to help rewrite database objects to MySQL during the schema migration. Use AWS DMS to perform the full load and change data capture (CDC) tasks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Server Migration Service (AWS SMS) to import the Oracle virtual machine image as an Amazon EC2 instance. Use the Oracle Logical Dump utility to migrate the Oracle data from Amazon EC2 to an Aurora DB cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T14:43:00.000Z",
        "voteCount": 17,
        "content": "Answer C. This is heterogenous migration and requires DMS and SCT."
      },
      {
        "date": "2021-10-14T06:35:00.000Z",
        "voteCount": 6,
        "content": "Yep, C here"
      },
      {
        "date": "2021-10-14T21:49:00.000Z",
        "voteCount": 2,
        "content": "Looks correct\nhttps://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/"
      },
      {
        "date": "2023-01-27T11:22:00.000Z",
        "voteCount": 1,
        "content": "Answer C. This is heterogenous migration and requires DMS and SCT."
      },
      {
        "date": "2022-11-28T11:19:00.000Z",
        "voteCount": 2,
        "content": "Answer is C:  Use AWS Schema Conversion Tool  to convert Database and DMS for fast transfer."
      },
      {
        "date": "2022-10-24T18:57:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-on-premoracle2aurora.html"
      },
      {
        "date": "2022-04-30T19:31:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/\n\nZero down time = First SCT and then DMS full and CDC\n\nC. Use the AWS Schema Conversion Tool (AWS SCT) to help rewrite database objects to MySQL during the schema migration. Use AWS DMS to perform the full load and change data capture (CDC) tasks."
      },
      {
        "date": "2021-10-20T10:36:00.000Z",
        "voteCount": 2,
        "content": "Answer C should be the one."
      },
      {
        "date": "2021-10-18T03:15:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-16T20:55:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice"
      },
      {
        "date": "2021-10-15T04:49:00.000Z",
        "voteCount": 2,
        "content": "Yes, it is C. Zero down time = First SCT and then DMS full and CDC"
      },
      {
        "date": "2021-09-28T12:26:00.000Z",
        "voteCount": 2,
        "content": "Ans D can not be right as it is not doing schema conversion and using oracle backup for restoring. Also downtime is high."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/65887-exam-aws-certified-database-specialty-topic-1-question-56/",
    "body": "A marketing company is using Amazon DocumentDB and requires that database audit logs be enabled. A Database Specialist needs to configure monitoring so that all data definition language (DDL) statements performed are visible to the Administrator. The Database Specialist has set the audit_logs parameter to enabled in the cluster parameter group.<br>What should the Database Specialist do to automatically collect the database logs for the Administrator?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DocumentDB to export the logs to Amazon CloudWatch Logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DocumentDB to export the logs to AWS CloudTrail",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DocumentDB Events to export the logs to Amazon CloudWatch Logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to download the logs using the download-db-log-file-portion operation and store the logs in Amazon S3"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-12T05:00:00.000Z",
        "voteCount": 13,
        "content": "Option A."
      },
      {
        "date": "2022-02-04T03:15:00.000Z",
        "voteCount": 13,
        "content": "Looks like option C was added as a distractor. When the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch. Answer should be A"
      },
      {
        "date": "2024-01-13T07:07:00.000Z",
        "voteCount": 1,
        "content": "Option A\n- enable audit log\n- enable export to ClopudWatch Logs"
      },
      {
        "date": "2023-09-14T04:24:00.000Z",
        "voteCount": 1,
        "content": "A. Enable DocumentDB to export the logs to Amazon CloudWatch Logs\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\n\n\" When auditing is enabled, Amazon DocumentDB exports your cluster\u2019s auditing records (JSON documents) to Amazon CloudWatch Logs. You can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events.\""
      },
      {
        "date": "2023-08-04T03:04:00.000Z",
        "voteCount": 1,
        "content": "Amazon DocumentDB auditing supports the following event categories:\n  - Data Definition Language (DDL)\n  - Data Manipulation Language(DML)\nEnabling auditing on a cluster is a two-step process.\nStep 1. Enable the audit_logs cluster parameter (done)\nStep 2. Enable Amazon CloudWatch Logs Export\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html"
      },
      {
        "date": "2023-06-26T17:46:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html"
      },
      {
        "date": "2023-05-15T23:29:00.000Z",
        "voteCount": 2,
        "content": "The audit logs are for Document DB events.\nBut to enable it, we must enable CloudWatch Logs exports on Document DB not Document DB Events."
      },
      {
        "date": "2023-05-11T05:30:00.000Z",
        "voteCount": 2,
        "content": "Option A\n\nBelow is from AWS Documentation - \nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\n\nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch."
      },
      {
        "date": "2023-03-19T08:13:00.000Z",
        "voteCount": 1,
        "content": "A is the correct option as this is a mandatory step besides changing the value of audit_logs parameter"
      },
      {
        "date": "2023-03-19T08:11:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-03-05T22:17:00.000Z",
        "voteCount": 1,
        "content": "I don't know if this sounds like a silly question, but how about B?"
      },
      {
        "date": "2023-01-27T11:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is A with sure. When you go to the console and hit on create a cluster, you will see an option to enable exports Profile and Audit logs to cloudwatch."
      },
      {
        "date": "2022-11-22T23:07:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct. Just validated from AWS site: \nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch."
      },
      {
        "date": "2022-10-16T15:28:00.000Z",
        "voteCount": 1,
        "content": "I am leaning towards A."
      },
      {
        "date": "2022-07-10T05:59:00.000Z",
        "voteCount": 3,
        "content": "I think Answer is A \nWhen auditing is enabled, Amazon DocumentDB exports your cluster\u2019s auditing records (JSON documents) to Amazon CloudWatch Logs. You can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events."
      },
      {
        "date": "2022-07-03T04:53:00.000Z",
        "voteCount": 5,
        "content": "Answer A:\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nStep 2. Enable Amazon CloudWatch Logs Export\nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch.\n\nDo not get confused with \"Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon CloudWatch Logs.\" The above event start to get recorded when you enable auditing via enabling audit_log parameter."
      },
      {
        "date": "2022-07-03T04:39:00.000Z",
        "voteCount": 1,
        "content": "Answer A:\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nStep 2. Enable Amazon CloudWatch Logs Export\nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch.\n\nWhen creating a cluster, performing a point-in-time-restore, or restoring a snapshot, you can enable CloudWatch Logs by following these steps.\n\nDo not get confused with \"Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon CloudWatch Logs.\" The above event start to get recorded when you enable auditing via enabling audit_log parameter."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/26760-exam-aws-certified-database-specialty-topic-1-question-57/",
    "body": "A company is looking to move an on-premises IBM Db2 database running AIX on an IBM POWER7 server. Due to escalating support and maintenance costs, the company is exploring the option of moving the workload to an Amazon Aurora PostgreSQL DB cluster.<br>What is the quickest way for the company to gather data on the migration compatibility?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a logical dump from the Db2 database and restore it to an Aurora DB cluster. Identify the gaps and compatibility of the objects migrated by comparing row counts from source and target tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun AWS DMS from the Db2 database to an Aurora DB cluster. Identify the gaps and compatibility of the objects migrated by comparing the row counts from source and target tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun native PostgreSQL logical replication from the Db2 database to an Aurora DB cluster to evaluate the migration compatibility.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the AWS Schema Conversion Tool (AWS SCT) from the Db2 database to an Aurora DB cluster. Create a migration assessment report to evaluate the migration compatibility.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-02T18:23:00.000Z",
        "voteCount": 9,
        "content": "Must be D"
      },
      {
        "date": "2023-06-24T02:09:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-from-ibm-db2-on-amazon-ec2-to-aurora-postgresql-compatible-using-aws-dms-and-aws-sct.html"
      },
      {
        "date": "2022-04-30T06:50:00.000Z",
        "voteCount": 2,
        "content": "run  AWS SCT from the Db2 database to an Aurora DB cluster.\n -&gt; Create a migration assessment report to evaluate the migration compatibility."
      },
      {
        "date": "2022-02-24T20:04:00.000Z",
        "voteCount": 4,
        "content": "SCT can do the required checks"
      },
      {
        "date": "2021-10-21T06:27:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-21T01:03:00.000Z",
        "voteCount": 1,
        "content": "D -  right answer"
      },
      {
        "date": "2021-10-04T13:48:00.000Z",
        "voteCount": 3,
        "content": "ANS: D\n\n\u2022 Converts DB/DW schema from source to target (including procedures / views / secondary indexes / FK and constraints)\n\u2022 Mainly for heterogeneous DB migrations and DW migrations"
      },
      {
        "date": "2021-09-29T05:33:00.000Z",
        "voteCount": 2,
        "content": "D - SCT"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/25735-exam-aws-certified-database-specialty-topic-1-question-58/",
    "body": "An ecommerce company is using Amazon DynamoDB as the backend for its order-processing application. The steady increase in the number of orders is resulting in increased DynamoDB costs. Order verification and reporting perform many repeated GetItem functions that pull similar datasets, and this read activity is contributing to the increased costs. The company wants to control these costs without significant development efforts.<br>How should a Database Specialist address these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to migrate data from DynamoDB to Amazon DocumentDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB Streams and Amazon Kinesis Data Firehose to push the data into Amazon Redshift",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon ElastiCache for Redis in front of DynamoDB to boost read performance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB Accelerator to offload the reads\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T01:58:00.000Z",
        "voteCount": 17,
        "content": "Keywords here are \"without significant development efforts\". DAX is the answer. D"
      },
      {
        "date": "2023-09-14T04:39:00.000Z",
        "voteCount": 1,
        "content": "D. Use DynamoDB Accelerator to offload the reads\n\nhttps://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html"
      },
      {
        "date": "2023-03-19T08:25:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer. Per AWS document - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html\n\nFor read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to overprovision read capacity units. This is especially beneficial for applications that require repeated reads for individual keys."
      },
      {
        "date": "2022-11-22T23:16:00.000Z",
        "voteCount": 2,
        "content": "Option D : \nhttps://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\n\"Applications that are read-intensive, but are also cost-sensitive. With DynamoDB, you provision the number of reads per second that your application requires. If read activity increases, you can increase your tables' provisioned read throughput (at an additional cost). Or, you can offload the activity from your application to a DAX cluster, and reduce the number of read capacity units that you need to purchase otherwise.\""
      },
      {
        "date": "2022-04-29T13:53:00.000Z",
        "voteCount": 3,
        "content": "large number of repeated GetItem methods =&gt; DAX"
      },
      {
        "date": "2022-02-27T08:43:00.000Z",
        "voteCount": 1,
        "content": "D is the answer since they don't want developments"
      },
      {
        "date": "2021-12-20T15:59:00.000Z",
        "voteCount": 2,
        "content": "Option D.   Service cost is not a concern but development cost is."
      },
      {
        "date": "2021-11-24T04:39:00.000Z",
        "voteCount": 3,
        "content": "The problem is various repeated operations, a in memory DB like DAX is the best aproach."
      },
      {
        "date": "2021-11-05T03:55:00.000Z",
        "voteCount": 2,
        "content": "D is the easiest option"
      },
      {
        "date": "2021-11-03T22:15:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-11-01T17:36:00.000Z",
        "voteCount": 1,
        "content": "We need caching solution to offload reads and reduce the cost.\nDAX is the best caching solution for DynamoDB API calls"
      },
      {
        "date": "2021-10-30T18:04:00.000Z",
        "voteCount": 1,
        "content": "yes it is D"
      },
      {
        "date": "2021-10-29T21:04:00.000Z",
        "voteCount": 2,
        "content": "D here, but DAX is not cost effective way"
      },
      {
        "date": "2021-10-23T09:16:00.000Z",
        "voteCount": 1,
        "content": "B was never in play for me. Redshift is not cheap and that's not what they're asking.\nI'm surprised nobody picked C, textbook case for Caching"
      },
      {
        "date": "2021-10-26T07:49:00.000Z",
        "voteCount": 1,
        "content": "C is not easy to set up....it needs significant development efforts.\nSo, its not the ideal one choose.\nD could be the best option."
      },
      {
        "date": "2021-11-06T21:42:00.000Z",
        "voteCount": 1,
        "content": "C and D both caching solution. D is purpose built for DynamoDB, where in C you have to do deal with staleness manually.  So D is correct answer."
      },
      {
        "date": "2021-10-15T11:36:00.000Z",
        "voteCount": 2,
        "content": "Q, The company wants to control these costs\nAnswer B. I this to using Redshift and kinesis will increase the cost a lot compare to just using DAX.\nSo the answer is D"
      },
      {
        "date": "2021-10-08T03:45:00.000Z",
        "voteCount": 2,
        "content": "I think D as well. Dax helps with read intensiveness and cost effectiveness \"without significant development efforts\". \nhttps://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\n\"Applications that are read-intensive, but are also cost-sensitive. With DynamoDB, you provision the number of reads per second that your application requires. If read activity increases, you can increase your tables' provisioned read throughput (at an additional cost). Or, you can offload the activity from your application to a DAX cluster, and reduce the number of read capacity units that you need to purchase otherwise.\""
      },
      {
        "date": "2021-09-24T18:16:00.000Z",
        "voteCount": 4,
        "content": "D here"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/25738-exam-aws-certified-database-specialty-topic-1-question-59/",
    "body": "An IT consulting company wants to reduce costs when operating its development environment databases. The company's workflow creates multiple Amazon<br>Aurora MySQL DB clusters for each development group. The Aurora DB clusters are only used for 8 hours a day. The DB clusters can then be deleted at the end of the development cycle, which lasts 2 weeks.<br>Which of the following provides the MOST cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation templates. Deploy a stack with the DB cluster for each development group. Delete the stack at the end of the development cycle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Aurora DB cloning feature. Deploy a single development and test Aurora DB instance, and create clone instances for the development groups. Delete the clones at the end of the development cycle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora Replicas. From the master automatic pause compute capacity option, create replicas for each development group, and promote each replica to master. Delete the replicas at the end of the development cycle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora Serverless. Restore current Aurora snapshot and deploy to a serverless cluster for each development group. Enable the option to pause the compute capacity on the cluster and set an appropriate timeout.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-06-24T08:13:00.000Z",
        "voteCount": 10,
        "content": "It is a close call between B and D. However, not everyone talks about the actual API call that can be used for Aurora Serverless, which is called as AutoPause. More details in the link.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ScalingConfiguration.html\nSo it should be D"
      },
      {
        "date": "2021-10-24T21:40:00.000Z",
        "voteCount": 6,
        "content": "A - meets the basic requirement. But its not cost effective.\nB - Good option. Saves cost on the storage layer with copy-on-write feature\nC - Meets the requirement, bit not cost effective.\nD - Most cost effective.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html#aurora-serverless.how-it-works.pause-resume"
      },
      {
        "date": "2021-11-02T12:09:00.000Z",
        "voteCount": 1,
        "content": "Spot on"
      },
      {
        "date": "2024-01-13T07:12:00.000Z",
        "voteCount": 1,
        "content": "I go for D.\nB looks promising. However, the requirement does not state the data in the clusters is from the same source. Also, keeping the DBs up only 8 hours a day is great for Serverless with auto-pause."
      },
      {
        "date": "2023-08-07T18:00:00.000Z",
        "voteCount": 1,
        "content": "A is also viable solution."
      },
      {
        "date": "2023-06-01T06:13:00.000Z",
        "voteCount": 1,
        "content": "Definitely D since that is the only option that also allows to save on compute by stopping unneeded resources outside of the 8 hours/5 days a week they are needed"
      },
      {
        "date": "2023-05-09T18:20:00.000Z",
        "voteCount": 2,
        "content": "- 'Most Cost effective' is the Key\n- Using clone db you can save on the storage cost but still you have to pay for Compute resources which are going to be charged more than 8Hrs.\n- Aurora server less reduces the instance cost which will be the most cost saving"
      },
      {
        "date": "2022-10-14T03:53:00.000Z",
        "voteCount": 1,
        "content": "Most-Cost effective solution is the key here.  REfer to https://aws.plainenglish.io/aurora-database-clones-what-they-are-and-when-to-use-them-b82be9d60309 \n \nPricing\nNow let\u2019s talk about the pricing for creating and using Amazon Aurora database clones. We get billed per hour for the database instances provisioned as part of the clone database. The pricing is based on the DB instance class that we select for the clone. We are also charged for any addition storage that we use as we make edits to the clone database.\nOption D is full copy of the DB and definitely will cost more than Option B."
      },
      {
        "date": "2023-02-07T07:00:00.000Z",
        "voteCount": 2,
        "content": "storage cost is much lower than server cost, so option D is cheaper than B"
      },
      {
        "date": "2022-07-10T06:09:00.000Z",
        "voteCount": 4,
        "content": "Answer should be D"
      },
      {
        "date": "2022-06-21T04:34:00.000Z",
        "voteCount": 3,
        "content": "Answer:D"
      },
      {
        "date": "2022-05-20T15:31:00.000Z",
        "voteCount": 1,
        "content": "Option B is correct.\nQuestion talks about destroying the cluster after 2 weeks, With aurora you will not pay for the compute capacity but you will pay for the storage capacity. If Option D had the delete option, I could have happily choose Option D. The advantage of Option B, you are not paying additional Storage as clone use the same storage + delta changes."
      },
      {
        "date": "2022-05-06T16:37:00.000Z",
        "voteCount": 2,
        "content": "It is necessary to support operation for 8 hours."
      },
      {
        "date": "2022-04-29T18:56:00.000Z",
        "voteCount": 2,
        "content": "single dev &amp; test instanace\n-&gt; Aurora clone \n-&gt; delete clones at end of dev cycle"
      },
      {
        "date": "2022-04-15T01:38:00.000Z",
        "voteCount": 2,
        "content": "Even if u pause serverless cluster storage, doesnt address the destroyed after 2 weeks requirement. I go with B."
      },
      {
        "date": "2022-02-24T15:50:00.000Z",
        "voteCount": 4,
        "content": "Amazon Aurora now allows you to create clones between Aurora Serverless v1 and provisioned Aurora DB clusters to enable quick sharing of data."
      },
      {
        "date": "2022-01-28T05:19:00.000Z",
        "voteCount": 1,
        "content": "I vote for B:\n\nThere isn't an \"option to pause the compute capacity on the cluster and set an appropriate timeout.\" in the RDS console for Serverless."
      },
      {
        "date": "2021-12-27T07:16:00.000Z",
        "voteCount": 3,
        "content": "I prefer B. \nAurora Serverless is not compatible to all Aurora provisioned engine version. However, you can do clone with most engine version. Meanwhile, I also consider the performance while restoring snapshot to Aurora Serverless."
      },
      {
        "date": "2021-12-21T11:44:00.000Z",
        "voteCount": 1,
        "content": "It is D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/65688-exam-aws-certified-database-specialty-topic-1-question-60/",
    "body": "A company has multiple applications serving data from a secure on-premises database. The company is migrating all applications and databases to the AWS<br>Cloud. The IT Risk and Compliance department requires that auditing be enabled on all secure databases to capture all log ins, log outs, failed logins, permission changes, and database schema changes. A Database Specialist has recommended Amazon Aurora MySQL as the migration target, and leveraging the Advanced<br>Auditing feature in Aurora.<br>Which events need to be specified in the Advanced Auditing configuration to satisfy the minimum auditing requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCONNECT\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQUERY_DCL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQUERY_DDL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQUERY_DML",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTABLE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQUERY"
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "ABD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-08T20:33:00.000Z",
        "voteCount": 15,
        "content": "Answer A,B,C\nConnect - logins / DCL - authorizations (grant,revoke), DDL - schema updates"
      },
      {
        "date": "2021-11-17T23:36:00.000Z",
        "voteCount": 1,
        "content": "you are right"
      },
      {
        "date": "2022-02-27T15:04:00.000Z",
        "voteCount": 1,
        "content": "B is wrong , they are not asking for grant and revoke"
      },
      {
        "date": "2022-12-27T15:02:00.000Z",
        "voteCount": 2,
        "content": "I think they are...Grant Revoke are permission statements."
      },
      {
        "date": "2023-08-31T19:33:00.000Z",
        "voteCount": 2,
        "content": "A. CONNECT\nB. QUERY_DCL\nC. QUERY_DDL"
      },
      {
        "date": "2023-08-04T03:21:00.000Z",
        "voteCount": 3,
        "content": "- CONNECT \u2013 Logs both successful and failed connections and also disconnections. This event includes user information.\n - QUERY_DCL \u2013 Similar to the QUERY event, but returns only data control language (DCL) queries (GRANT, REVOKE, and so on).\n - QUERY_DDL \u2013 Similar to the QUERY event, but returns only data definition language (DDL) queries (CREATE, ALTER, and so on).\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html"
      },
      {
        "date": "2023-03-04T22:28:00.000Z",
        "voteCount": 1,
        "content": "The events that need to be specified in the Advanced Auditing configuration to satisfy the minimum auditing requirements are:\n\nA. CONNECT - to capture all log ins and log outs.\nB. QUERY_DDL - to capture permission changes and database schema changes.\nD. QUERY_DML - to capture data manipulation operations such as INSERT, UPDATE and DELETE."
      },
      {
        "date": "2023-08-03T07:34:00.000Z",
        "voteCount": 1,
        "content": "They're not asking about changes in data, thus DML is not needed. Instead DCL is required to track permission changes."
      },
      {
        "date": "2022-04-30T10:15:00.000Z",
        "voteCount": 4,
        "content": "A. CONNECT  (for log ins and log outs, unsuccessful logins)\nB. QUERY_DCL (for grants - authorization modifications)\nC. QUERY_DDL (for schema changes)\nx D. QUERY_DML\nx E. TABLE\nx F. QUERY"
      },
      {
        "date": "2022-04-28T13:22:00.000Z",
        "voteCount": 1,
        "content": "QUERY_DCL for authorization changes"
      },
      {
        "date": "2022-03-05T13:58:00.000Z",
        "voteCount": 3,
        "content": "ABC  - pretty straightfoward"
      },
      {
        "date": "2022-02-27T15:03:00.000Z",
        "voteCount": 1,
        "content": "ACD\nsince they need to capture data modification"
      },
      {
        "date": "2022-02-24T21:10:00.000Z",
        "voteCount": 2,
        "content": "B is necessary too"
      },
      {
        "date": "2021-12-22T13:17:00.000Z",
        "voteCount": 1,
        "content": "A,B,C are correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/67116-exam-aws-certified-database-specialty-topic-1-question-61/",
    "body": "A gaming company has recently acquired a successful iOS game, which is particularly popular during the holiday season. The company has decided to add a leaderboard to the game that uses Amazon DynamoDB. The application load is expected to ramp up over the holiday season.<br>Which solution will meet these requirements at the lowest cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamoDB Streams",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamoDB with DynamoDB Accelerator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamoDB with on-demand capacity mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamoDB with provisioned capacity mode with Auto Scaling\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-30T15:31:00.000Z",
        "voteCount": 8,
        "content": "provisioned + auto scaling\n\n1. It's not a new game, so the application's load (and its increase during Xmas) is already known.\n2. the question asks \"at the lowest possible cost\":"
      },
      {
        "date": "2022-01-11T01:44:00.000Z",
        "voteCount": 5,
        "content": "I vote for D.\n\nReason, the game is bought from another company. So it has been running for some time and the load is known. The company is able to even identify the surge during Christmas period. Provisioned capacity will be more economical as compared to on demand."
      },
      {
        "date": "2023-09-17T10:52:00.000Z",
        "voteCount": 1,
        "content": "On-Demand Capacity is the most suitable choice here.\nThe burst is expected only during holiday season, no other long-running changes are mentioned that would justify Provisioned Capacity. Also, a baseline for provisioned mode is not mentioned in the question, so there would be a reason for it and we can assume that provisioned mode has not been reasonable so far given the absence of that baseline.\nHence, On-Demand ist the preference for this use case. Once the holidays are over (~weeks), everything is expected to go back to normal."
      },
      {
        "date": "2023-09-14T05:24:00.000Z",
        "voteCount": 2,
        "content": "D. DynamoDB with provisioned capacity mode with Auto Scaling\n\nshould not be C. on-demand, \"On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience.\"\n\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"
      },
      {
        "date": "2023-09-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "I THINK C"
      },
      {
        "date": "2023-06-03T10:35:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nAuto-Scaling you are basically paying for throughput 24/7. Whereas for On-Demand Scaling you pay per request. This means for applications still in development or low traffic applications, it might be more economical to use On-Demand Scaling and not worry about provisioning throughput."
      },
      {
        "date": "2023-03-22T04:26:00.000Z",
        "voteCount": 1,
        "content": "Option D, DynamoDB with provisioned capacity mode with Auto Scaling, requires the company to provision a certain amount of read and write capacity ahead of time. While Auto Scaling can help with managing the scaling of the table based on demand, it comes with the additional cost of paying for the provisioned capacity even when it is not in use.\n\nTherefore, Option C, DynamoDB with on-demand capacity mode, is the most cost-effective option that can meet the requirements of adding a leaderboard to the game and handling the expected increase in application load over the holiday season."
      },
      {
        "date": "2023-03-19T11:56:00.000Z",
        "voteCount": 2,
        "content": "Option D is a clear winner based on this blog - https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"
      },
      {
        "date": "2023-03-04T22:30:00.000Z",
        "voteCount": 1,
        "content": "The solution that will meet these requirements at the lowest cost is DynamoDB with on-demand capacity mode. This is because with on-demand capacity mode, DynamoDB automatically scales capacity up and down based on the actual request volume and traffic patterns."
      },
      {
        "date": "2022-09-21T15:45:00.000Z",
        "voteCount": 2,
        "content": "I vote for D"
      },
      {
        "date": "2022-09-16T01:22:00.000Z",
        "voteCount": 1,
        "content": "C might be better\nhttps://dynobase.dev/dynamodb-on-demand-vs-provisioned-scaling/"
      },
      {
        "date": "2022-06-29T02:53:00.000Z",
        "voteCount": 2,
        "content": "Since this is a leader board, DAX is out of question which mostly help in read intensive workloads. \nNow when we compare on-Demand vs providioned with auto scaling .\nSince the application is already a popular application and likely to increase during chrismas that mean business is aware about the application usage pattern.\n\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\n\nProvisioned Auto Scaling can provide lower cost also in shown in the above case study."
      },
      {
        "date": "2022-06-17T16:27:00.000Z",
        "voteCount": 1,
        "content": "I will go with B.\nThe app is ok ad the new feature is the leaderboard, leaderboard needs low latency so DAX is the answer for me."
      },
      {
        "date": "2022-06-18T05:15:00.000Z",
        "voteCount": 1,
        "content": "One more thing DAX saves costs because it reduces read load in DynamoDB,"
      },
      {
        "date": "2022-04-30T07:43:00.000Z",
        "voteCount": 1,
        "content": "x A. DynamoDB Streams\nx B. DynamoDB with DynamoDB Accelerator (DAX is for fast static reads, but here data will change quickly)\nC. DynamoDB with on-demand capacity mode\nx D. DynamoDB with provisioned capacity mode with Auto Scaling ( may be good option if game in use always)"
      },
      {
        "date": "2022-02-27T13:06:00.000Z",
        "voteCount": 2,
        "content": "why mot B ? it is about leaderboard refresh"
      },
      {
        "date": "2023-10-18T11:25:00.000Z",
        "voteCount": 1,
        "content": "No, it's about a surge in writing - not reading - users. DAX no help here."
      },
      {
        "date": "2021-12-21T19:28:00.000Z",
        "voteCount": 2,
        "content": "Auto Scaling is cheaper.. If you're not expecting sudden spikes.  I vote for D"
      },
      {
        "date": "2021-12-13T08:44:00.000Z",
        "voteCount": 3,
        "content": "I'm not sure about C?\n\"On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes\"\nvs.\n'DynamoDB released auto scaling to make it easier for you to manage capacity efficiently, and auto scaling continues to help DynamoDB users lower the cost of workloads that have a predictable traffic pattern.\"\n\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\n\nTo save costs and based on the link I'd say that the answer is D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/27667-exam-aws-certified-database-specialty-topic-1-question-62/",
    "body": "A company's Security department established new requirements that state internal users must connect to an existing Amazon RDS for SQL Server DB instance using their corporate Active Directory (AD) credentials. A Database Specialist must make the modifications needed to fulfill this requirement.<br>Which combination of actions should the Database Specialist take? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable Transparent Data Encryption (TDE) on the RDS SQL Server DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the RDS SQL Server DB instance to use the directory for Windows authentication. Create appropriate new logins.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Management Console to create an AWS Managed Microsoft AD. Create a trust relationship with the corporate AD.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the RDS SQL Server DB instance, modify it to use the directory for Windows authentication, and start it again. Create appropriate new logins.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Management Console to create an AD Connector. Create a trust relationship with the corporate AD.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Managed Microsoft AD domain controller Security Group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "BCD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-02T13:54:00.000Z",
        "voteCount": 6,
        "content": "This question was asked in my exam. B,C and F seems the correct options."
      },
      {
        "date": "2023-08-04T04:45:00.000Z",
        "voteCount": 2,
        "content": "C. ...to create an AWS Managed Microsoft AD. Create a trust relationship...\nE. ... to create an AD Connector. Create a trust relationship...\nBoth option are correct.\nWe should use AWS Managed Microsoft AD when have &gt; 5000 users.\nWe should use AD Connector or Simple AD when have &lt; 5000 users.\nThe question is not complete, or one of these answers should be excluded.\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_best_practices.html"
      },
      {
        "date": "2023-06-26T18:46:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/security/how-to-enable-windows-integrated-authentication-for-rds-for-sql-server-using-on-premises-active-directory/#:~:text=The%20setup%201%20Step%201%3A%20Set%20up%20RDS,between%20your%20VPC%20domain%20and%20your%20on-premises%20domain"
      },
      {
        "date": "2023-03-04T22:34:00.000Z",
        "voteCount": 1,
        "content": "To fulfill the Security department's requirement for internal users to connect to the Amazon RDS for SQL Server DB instance using their corporate Active Directory credentials"
      },
      {
        "date": "2023-04-05T05:05:00.000Z",
        "voteCount": 1,
        "content": "D and B are conflicted, and B is the correct sequence -- modify the instance while the instance is up, let it connect to the AWS AD, then reboot the instance to make the connection valid. D -- Once you stopped the instance, you could not modify it."
      },
      {
        "date": "2023-04-05T05:06:00.000Z",
        "voteCount": 1,
        "content": "BCF is the answer"
      },
      {
        "date": "2022-06-24T07:14:00.000Z",
        "voteCount": 1,
        "content": "Answer:BCF"
      },
      {
        "date": "2022-04-30T18:23:00.000Z",
        "voteCount": 4,
        "content": "x A. Disable Transparent Data Encryption (unrelated)\nB. Modify the RDS SQL Server DB instance to use the directory for Windows authentication. Create appropriate new logins.(would need reboot)\nC. Use the AWS Management Console to create an AWS Managed Microsoft AD. Create a trust relationship with the corporate AD.\nx D. Stop the RDS SQL Server DB instance, modify it to use the directory for Windows authentication, and start it again. Create appropriate new logins. (stop-start should be at end, not first)\nx E. Use the AWS Management Console to create an AD Connector. Create a trust relationship with the corporate AD.\nF. Configure the AWS Managed Microsoft AD domain controller Security Group."
      },
      {
        "date": "2021-11-04T05:10:00.000Z",
        "voteCount": 1,
        "content": "I got this Question in exam."
      },
      {
        "date": "2021-11-01T06:38:00.000Z",
        "voteCount": 2,
        "content": "Ans: BCF"
      },
      {
        "date": "2021-10-31T11:24:00.000Z",
        "voteCount": 4,
        "content": "BCF\nC&amp;F is confirmed.\nChoosing B over D because  modifying the RDS to enable windows authentication must be done when the RDS is in available status though it will be rebooted for it to take effect."
      },
      {
        "date": "2021-10-29T02:30:00.000Z",
        "voteCount": 3,
        "content": "BCF is the answer - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html"
      },
      {
        "date": "2021-10-22T02:34:00.000Z",
        "voteCount": 4,
        "content": "BCF. \nNo restart required.  Connector is a proxy, no trust relationship can be established with it."
      },
      {
        "date": "2021-10-20T20:41:00.000Z",
        "voteCount": 1,
        "content": "Ans: BEF"
      },
      {
        "date": "2021-11-01T20:26:00.000Z",
        "voteCount": 2,
        "content": "E - Not right\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_app_compatibility.html"
      },
      {
        "date": "2021-10-19T04:06:00.000Z",
        "voteCount": 4,
        "content": "Sorry i made a mistake.\nAccording to https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html#USER_ModifyInstance.Settings ,  modifying domain or directory id parameter in AWS RDS SqlServer requires a bried outage .\n\nSo correct answers are  : C , D , F ."
      },
      {
        "date": "2021-10-17T02:54:00.000Z",
        "voteCount": 4,
        "content": "I would also vote for BCF.\nNobody doubt that C anf F are correct.\n\nRegarding my choice B ( B or D ? ).\nWe need to configure an existing DB ( stated in the question).\nAccording to https://aws.amazon.com/blogs/database/joining-your-amazon-rds-instances-across-accounts-to-a-single-shared-domain/ , there is no need to stop the RDS to join it to an AD ( step 3 )"
      },
      {
        "date": "2021-10-02T03:41:00.000Z",
        "voteCount": 3,
        "content": "CDF Here https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html"
      },
      {
        "date": "2021-09-29T07:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is BDE"
      },
      {
        "date": "2021-10-01T13:34:00.000Z",
        "voteCount": 8,
        "content": "Meant BCF\nD is not correct, you can't modify stopped RDS instance"
      },
      {
        "date": "2021-10-03T13:57:00.000Z",
        "voteCount": 1,
        "content": "Isn't B not restarting RDS?"
      },
      {
        "date": "2021-11-01T16:40:00.000Z",
        "voteCount": 1,
        "content": "E - Not right\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_app_compatibility.html"
      },
      {
        "date": "2023-04-05T04:43:00.000Z",
        "voteCount": 2,
        "content": "Note\nAmazon RDS is compatible with AWS Managed Microsoft AD only, and is not compatible with AD Connector. For more information, see the AWS Microsoft AD section in the AWS Directory Service FAQs page."
      },
      {
        "date": "2021-09-24T11:22:00.000Z",
        "voteCount": 3,
        "content": "I will go with CDF\nhttps://www.powerupcloud.com/integrate-aws-sql-server-rds-with-multiple-ad/\nhttps://www.sqlshack.com/advanced-windows-authentication-configurations-in-aws-rds-sql-server/\nhttps://aws.amazon.com/blogs/security/how-to-enable-windows-integrated-authentication-for-rds-for-sql-server-using-on-premises-active-directory/\nhttps://aws.amazon.com/blogs/aws/amazon-rds-for-sql-server-support-for-windows-authentication/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/65905-exam-aws-certified-database-specialty-topic-1-question-63/",
    "body": "A Database Specialist is performing a proof of concept with Amazon Aurora using a small instance to confirm a simple database behavior. When loading a large dataset and creating the index, the Database Specialist encounters the following error message from Aurora:<br>ERROR: cloud not write block 7507718 of temporary file: No space left on device<br>What is the cause of this error and what should the Database Specialist do to resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe scaling of Aurora storage cannot catch up with the data loading. The Database Specialist needs to modify the workload to load the data slowly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe scaling of Aurora storage cannot catch up with the data loading. The Database Specialist needs to enable Aurora storage scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe local storage used to store temporary tables is full. The Database Specialist needs to scale up the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe local storage used to store temporary tables is full. The Database Specialist needs to enable local storage scaling."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T08:53:00.000Z",
        "voteCount": 6,
        "content": "The local storage used to store temporary tables is full. \nAnd only way to increase local storage is to upgrade instance type.\nThe Database Specialist needs to scale up the instance."
      },
      {
        "date": "2023-12-01T05:02:00.000Z",
        "voteCount": 1,
        "content": "Is there something like local storage scaling? Talking about option D."
      },
      {
        "date": "2024-01-01T16:50:00.000Z",
        "voteCount": 1,
        "content": "Local storage is not defined by software, so what you can do for scaling is just scale up and changing the instance which has larger instance store. See also the link below: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html#AuroraMySQL.Managing.TempStorage"
      },
      {
        "date": "2023-09-14T00:53:00.000Z",
        "voteCount": 1,
        "content": "C. The local storage used to store temporary tables is full. The Database Specialist needs to scale up the instance. \n\n\nhttps://repost.aws/knowledge-center/postgresql-aurora-storage-issue"
      },
      {
        "date": "2023-06-26T23:01:00.000Z",
        "voteCount": 2,
        "content": "https://stackoverflow.com/questions/52610114/aurora-postgresql-engine-no-space-left-on-device"
      },
      {
        "date": "2022-01-11T02:20:00.000Z",
        "voteCount": 2,
        "content": "Answer: C"
      },
      {
        "date": "2022-02-27T13:36:00.000Z",
        "voteCount": 2,
        "content": "true , you cant change local storage  size , the only way is the change the instance type"
      },
      {
        "date": "2021-12-22T07:58:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-12-07T05:57:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2021-11-12T11:38:00.000Z",
        "voteCount": 3,
        "content": "Option C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/26777-exam-aws-certified-database-specialty-topic-1-question-64/",
    "body": "A financial company wants to store sensitive user data in an Amazon Aurora PostgreSQL DB cluster. The database will be accessed by multiple applications across the company. The company has mandated that all communications to the database be encrypted and the server identity must be validated. Any non-SSL- based connections should be disallowed access to the database.<br>Which solution addresses these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl=0 parameter in DB parameter groups. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=allow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl=1 parameter in DB parameter groups. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=disable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl=0 parameter in DB parameter groups. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=verify-ca.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl=1 parameter in DB parameter groups. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=verify-full.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T22:38:00.000Z",
        "voteCount": 10,
        "content": "ANS D is Correct!"
      },
      {
        "date": "2022-05-01T08:15:00.000Z",
        "voteCount": 6,
        "content": "- in DB parameter groups:  rds.force_ssl=1   (o=&gt;false, 1=&gt;true)\n-Download and use the Amazon RDS certificate bundle \n- configure the PostgreSQL connection string with\n sslmode=verify-full.\n\nhttps://jdbc.postgresql.org/documentation/head/ssl-client.html\nIf sslmode=verify-ca, the server is verified by checking the certificate chain up to the root certificate stored on the client.\n\nIf sslmode=verify-full, the server host name will be verified to make sure it matches the name stored in the server certificate."
      },
      {
        "date": "2022-05-20T01:09:00.000Z",
        "voteCount": 1,
        "content": "Yes answer D."
      },
      {
        "date": "2023-09-14T01:04:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\n\nrds.force_ssl=1 to force ssl in RDS and sslmode=verify-full to encrypt the connection and validate server identity."
      },
      {
        "date": "2022-12-23T01:45:00.000Z",
        "voteCount": 3,
        "content": "Ans is D\n\nrds.force_ssl=1 to force ssl in RDS and sslmode=verify-full to encrypt the connection and validate server identity."
      },
      {
        "date": "2021-11-04T10:46:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; D"
      },
      {
        "date": "2021-11-02T09:48:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-22T05:28:00.000Z",
        "voteCount": 1,
        "content": "D indeed is the right choice"
      },
      {
        "date": "2021-10-10T01:58:00.000Z",
        "voteCount": 2,
        "content": "yes. it is D"
      },
      {
        "date": "2021-09-25T12:19:00.000Z",
        "voteCount": 3,
        "content": "ANS: D\nPostgreSQL: sslrootcert=rds-cert.pem sslmode=[verify-ca | verify-full]"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/26594-exam-aws-certified-database-specialty-topic-1-question-65/",
    "body": "A company is using 5 TB Amazon RDS DB instances and needs to maintain 5 years of monthly database backups for compliance purposes. A Database<br>Administrator must provide Auditors with data within 24 hours.<br>Which solution will meet these requirements and is the MOST operationally efficient?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to run on the first day of every month to take a manual RDS snapshot. Move the snapshot to the company's Amazon S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to run on the first day of every month to take a manual RDS snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS snapshot schedule from the AWS Management Console to take a snapshot every 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to run on the first day of every month to create an automated RDS snapshot."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-11T06:10:00.000Z",
        "voteCount": 14,
        "content": "A.\nUnlike automated backups, manual snapshots aren't subject to the backup retention period. Snapshots don't expire.\n\nFor very long-term backups of MariaDB, MySQL, and PostgreSQL data, we recommend exporting snapshot data to Amazon S3. If the major version of your DB engine is no longer supported, you can't restore to that version from a snapshot.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html"
      },
      {
        "date": "2022-08-11T14:04:00.000Z",
        "voteCount": 3,
        "content": "So the keyword here is \"move to S3\". Move means copy to S3 and then delete on RDS. The question asks for \"operational efficiency\" and \"24 hours\" to report to auditor. Just leave it in RDS, it doesn't expire and can be easily share right away. B is the answer."
      },
      {
        "date": "2022-07-17T21:56:00.000Z",
        "voteCount": 10,
        "content": "There is difference between copy / share a snapshot and Export. Export to S3 option will \n\n When you export a DB snapshot, Amazon RDS extracts data from the snapshot and stores it in an Amazon S3 bucket. The data is stored in an Apache Parquet format that is compressed and consistent.\nWhere you have option to copy manual snapshot as is to different region or different AWS account. So we can not basically move the manual snapshot to S3 directly. \nB is correct."
      },
      {
        "date": "2022-07-17T22:00:00.000Z",
        "voteCount": 3,
        "content": "Plus \nUnlike automated backups, manual snapshots aren't subject to the backup retention period. Snapshots don't expire.\n\nFor very long-term backups of MariaDB, MySQL, and PostgreSQL data, we recommend exporting snapshot data to Amazon S3. If the major version of your DB engine is no longer supported, you can't restore to that version from a snapshot.\n\nHere the backup movement is only for compliance. there is no requirement to query that backup .( using parquet format query through athena or rds redshift spectrum )"
      },
      {
        "date": "2024-09-03T10:41:00.000Z",
        "voteCount": 1,
        "content": "No need to move the file it stays in aws"
      },
      {
        "date": "2024-04-24T20:35:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-13T06:50:00.000Z",
        "voteCount": 1,
        "content": "B is the most operationally efficient."
      },
      {
        "date": "2023-12-01T17:40:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/amazon-rds-snapshot-restore-and-recovery-demystified/#:~:text=Amazon%20RDS%20snapshots%20are%20stored,for%20copy%20and%20restore%20operations.\n\nManual RDS snapshots are stored in S3 anyways. So moving to S3 does not make sense."
      },
      {
        "date": "2023-10-11T07:39:00.000Z",
        "voteCount": 1,
        "content": "Keep the snapshot the in the RDS itself, no need to waste the operational efficiency by moving to and restoring from S3 unnecessarily"
      },
      {
        "date": "2023-06-08T01:14:00.000Z",
        "voteCount": 1,
        "content": "Move to S3 is additional work without efficiency"
      },
      {
        "date": "2023-06-01T07:20:00.000Z",
        "voteCount": 1,
        "content": "Actually you do not even need to take a manual snapshot. Even automated snapshots can be exported to S3. \n\nYou can export all types of DB snapshots\u2014including manual snapshots, automated system snapshots, and snapshots created by the AWS Backup service.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html"
      },
      {
        "date": "2023-03-04T22:42:00.000Z",
        "voteCount": 1,
        "content": "Option C, creating an RDS snapshot schedule from the AWS Management Console to take a snapshot every 30 days, would be the most operationally efficient solution for this scenario."
      },
      {
        "date": "2023-03-05T22:43:00.000Z",
        "voteCount": 2,
        "content": "I think the maximum retention period of automated backups is 35 days."
      },
      {
        "date": "2022-12-19T15:57:00.000Z",
        "voteCount": 1,
        "content": "This is confusing because don't allow manual snapshots end up in s3 anyway?"
      },
      {
        "date": "2022-11-21T18:48:00.000Z",
        "voteCount": 1,
        "content": "A.\nmanual snapshot has a limitation. (Each supported Region: 100)\nthis case, customer want to keep backup 5 years..\nso i think,  it can't possible to keep snapshots during 5 yeers(365*5)\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.Limits"
      },
      {
        "date": "2022-12-07T19:00:00.000Z",
        "voteCount": 2,
        "content": "my mistacke... B is the more correct.\nbecause, only need to backups for a month..\nso only need to snapshot 12*5 = 60.\nit does not reach the rds snapshot's limitation."
      },
      {
        "date": "2023-05-23T04:30:00.000Z",
        "voteCount": 1,
        "content": "Also since they asked for \"Operationally Efficient\" Option so I also think B is a better answer."
      },
      {
        "date": "2023-07-12T12:26:00.000Z",
        "voteCount": 1,
        "content": "this limit is adjustable, you can really have more than 100 snaps per region"
      },
      {
        "date": "2022-10-18T20:42:00.000Z",
        "voteCount": 2,
        "content": "A is the answer. aws recommends long term backups to be exported to s3"
      },
      {
        "date": "2022-10-16T16:17:00.000Z",
        "voteCount": 2,
        "content": "For very long-term backups of MariaDB, MySQL, and PostgreSQL data, we recommend exporting snapshot data to Amazon S3. If the major version of your DB engine is no longer supported, you can't restore to that version from a snapshot.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html"
      },
      {
        "date": "2022-09-27T18:18:00.000Z",
        "voteCount": 1,
        "content": "Option B is correct. manual snapshot won't expire."
      },
      {
        "date": "2022-06-24T07:27:00.000Z",
        "voteCount": 4,
        "content": "I don't think you can move snapshots to individual S3, so the answer is probably B."
      },
      {
        "date": "2022-05-22T07:06:00.000Z",
        "voteCount": 3,
        "content": "A and B seems right . but i choose A . Move the snapshot to the company's Amazon S3 bucket.  sound likes more right ."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/amazon/view/26778-exam-aws-certified-database-specialty-topic-1-question-66/",
    "body": "A company wants to automate the creation of secure test databases with random credentials to be stored safely for later use. The credentials should have sufficient information about each test database to initiate a connection and perform automated credential rotations. The credentials should not be logged or stored anywhere in an unencrypted form.<br>Which steps should a Database Specialist take to meet these requirements using an AWS CloudFormation template?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the database with the MasterUserName and MasterUserPassword properties set to the default values. Then, create the secret with the user name and password set to the same default values. Add a Secret Target Attachment resource with the SecretId and TargetId properties set to the Amazon Resource Names (ARNs) of the secret and the database. Finally, update the secret's password value with a randomly generated string set by the GenerateSecretString property.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Mapping property from the database Amazon Resource Name (ARN) to the secret ARN. Then, create the secret with a chosen user name and a randomly generated password set by the GenerateSecretString property. Add the database with the MasterUserName and MasterUserPassword properties set to the user name of the secret.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a resource of type AWS::SecretsManager::Secret and specify the GenerateSecretString property. Then, define the database user name in the SecureStringTemplate template. Create a resource for the database and reference the secret string for the MasterUserName and MasterUserPassword properties. Then, add a resource of type AWS::SecretsManagerSecretTargetAttachment with the SecretId and TargetId properties set to the Amazon Resource Names (ARNs) of the secret and the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the secret with a chosen user name and a randomly generated password set by the GenerateSecretString property. Add an SecretTargetAttachment resource with the SecretId property set to the Amazon Resource Name (ARN) of the secret and the TargetId property set to a parameter value matching the desired database ARN. Then, create a database with the MasterUserName and MasterUserPassword properties set to the previously created values in the secret."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T13:51:00.000Z",
        "voteCount": 11,
        "content": "Ans C is correct"
      },
      {
        "date": "2021-09-30T09:40:00.000Z",
        "voteCount": 1,
        "content": "BillyC, any idea why the AWS Database specialty exam is so hard to find on this site / why there are 404 errors?"
      },
      {
        "date": "2021-11-02T22:48:00.000Z",
        "voteCount": 1,
        "content": "New requirement, only visible if you have contributor access"
      },
      {
        "date": "2023-08-04T12:50:00.000Z",
        "voteCount": 1,
        "content": "\"Add a resource of type AWS::SecretsManager::RotationSchedule\" missing in answer C."
      },
      {
        "date": "2023-08-04T12:44:00.000Z",
        "voteCount": 1,
        "content": "https://malsouli.medium.com/aws-secrets-manager-create-and-rotate-secrets-automatically-36719faa7e4f"
      },
      {
        "date": "2022-05-01T10:29:00.000Z",
        "voteCount": 3,
        "content": "incomplete or wrong info but answer needs SecretsManager which is in C only\n\nAdd a resource of type AWS::SecretsManager::Secret \n-&gt; specify the GenerateSecretString property\n-&gt; define the database user name in the SecureStringTemplate template. \n-&gt; Create a resource for the database \n-&gt; reference the secret string for the MasterUserName and MasterUserPassword properties. \n-&gt; add a resource of type AWS::SecretsManagerSecretTargetAttachment with the SecretId and TargetId properties set to the Amazon Resource Names (ARNs) of the secret and the database."
      },
      {
        "date": "2022-03-05T18:49:00.000Z",
        "voteCount": 1,
        "content": "(C) is correct"
      },
      {
        "date": "2021-10-31T01:45:00.000Z",
        "voteCount": 4,
        "content": "yes, it is C"
      },
      {
        "date": "2021-10-01T01:19:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/amazon/view/26925-exam-aws-certified-database-specialty-topic-1-question-67/",
    "body": "A company is going to use an Amazon Aurora PostgreSQL DB cluster for an application backend. The DB cluster contains some tables with sensitive data. A<br>Database Specialist needs to control the access privileges at the table level.<br>How can the Database Specialist meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IAM database authentication and restrict access to the tables using an IAM policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the rules in a NACL to restrict outbound traffic from the Aurora DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute GRANT and REVOKE commands that restrict access to the tables containing sensitive data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine access privileges to the tables containing sensitive data in the pg_hba.conf file."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-28T05:00:00.000Z",
        "voteCount": 2,
        "content": "C. Execute GRANT and REVOKE commands that restrict access to the tables containing sensitive data.\n\nhttps://aws.amazon.com/blogs/database/managing-postgresql-users-and-roles/#:~:text=GRANT%20SELECT%20ON%20TABLE%20mytable1%2C%20mytable2%20TO%20readonly%3B"
      },
      {
        "date": "2023-08-04T12:58:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/database/managing-postgresql-users-and-roles/"
      },
      {
        "date": "2023-03-09T15:49:00.000Z",
        "voteCount": 2,
        "content": "I am leaning towards A since you do not need to issue a GRANT to revoke a permission. U use grant to grant access to a table . C is kind of confusing"
      },
      {
        "date": "2022-04-30T19:49:00.000Z",
        "voteCount": 4,
        "content": "x A. Use AWS IAM database authentication and restrict access to the tables using an IAM policy. (this is for db access)\nx B. Configure the rules in a NACL to restrict outbound traffic from the Aurora DB cluster. (This is for Network Access Control)\nC. Execute GRANT and REVOKE commands that restrict access to the tables containing sensitive data.\nx D. Define access privileges to the tables containing sensitive data in the pg_hba.conf file. (not allowed)"
      },
      {
        "date": "2023-05-31T17:25:00.000Z",
        "voteCount": 1,
        "content": "revoke dies not restrict access\nit revokes some existing grant"
      },
      {
        "date": "2021-11-01T07:58:00.000Z",
        "voteCount": 3,
        "content": "C.\nTable level means DCL"
      },
      {
        "date": "2021-10-28T10:43:00.000Z",
        "voteCount": 2,
        "content": "This is easy. I am a DBA.Table level access is GRANT,REVOKE for many database flavors including Oracle an Postgres. Answer is C."
      },
      {
        "date": "2021-10-27T05:36:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-25T17:27:00.000Z",
        "voteCount": 1,
        "content": "I see answers like A or C, but I am looking for collateral support and cannot find any in the options selected so far. I choose A, see https://aws.amazon.com/blogs/database/managing-postgresql-users-and-roles/ You create roles and then attach policies to the roles. May you support your response with a link so that we can check to confirm your reasoning? Thanks"
      },
      {
        "date": "2021-10-30T02:11:00.000Z",
        "voteCount": 2,
        "content": "in your provided link roles are db roles not IAM roles :)  so C is correct"
      },
      {
        "date": "2021-10-17T17:22:00.000Z",
        "voteCount": 3,
        "content": "It is C"
      },
      {
        "date": "2021-10-15T13:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-10-11T02:49:00.000Z",
        "voteCount": 2,
        "content": "This should be C"
      },
      {
        "date": "2021-09-23T13:54:00.000Z",
        "voteCount": 1,
        "content": "A or C.. please comments"
      },
      {
        "date": "2021-09-24T02:52:00.000Z",
        "voteCount": 7,
        "content": "Answer is C. Table level access is managed by DCL."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/amazon/view/26856-exam-aws-certified-database-specialty-topic-1-question-68/",
    "body": "A Database Specialist is working with a company to launch a new website built on Amazon Aurora with several Aurora Replicas. This new website will replace an on-premises website connected to a legacy relational database. Due to stability issues in the legacy database, the company would like to test the resiliency of<br>Aurora.<br>Which action can the Database Specialist take to test the resiliency of the Aurora DB cluster?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the DB cluster and analyze how the website responds",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora fault injection to crash the master DB instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the DB cluster endpoint to simulate a master DB instance failure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora Backtrack to crash the DB cluster"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-06T13:55:00.000Z",
        "voteCount": 7,
        "content": "B.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html"
      },
      {
        "date": "2023-12-01T19:37:00.000Z",
        "voteCount": 1,
        "content": "Cannot be A because stopping the DB cluster means application does not have a Database at all.\nCannot be C because removing DB cluster endpoint will also mean the same thing as above.\nD is just a distractor - backtrack cannot crash a cluster.\nB makes sense - it crashes one instance and allows Aurora to recover."
      },
      {
        "date": "2023-08-28T05:11:00.000Z",
        "voteCount": 2,
        "content": "B. Use Aurora fault injection to crash the master DB instance"
      },
      {
        "date": "2022-04-30T17:53:00.000Z",
        "voteCount": 4,
        "content": "B. Use Aurora fault injection to crash the master DB instance"
      },
      {
        "date": "2021-11-03T04:54:00.000Z",
        "voteCount": 3,
        "content": "Ans;: B"
      },
      {
        "date": "2021-10-21T12:22:00.000Z",
        "voteCount": 3,
        "content": "It is B"
      },
      {
        "date": "2021-10-10T02:15:00.000Z",
        "voteCount": 4,
        "content": "B with the link that is provided in the suggested answer\n\"You can test the fault tolerance of your Amazon Aurora DB cluster by using fault injection queries. Fault injection queries are issued as SQL commands to an Amazon Aurora instance and they enable you to schedule a simulated occurrence of one of the following events:\nA crash of a writer or reader DB instance\nA failure of an Aurora Replica\nA disk failure\nDisk congestion\nWhen a fault injection query specifies a crash, it forces a crash of the Aurora DB instance. The other fault injection queries result in simulations of failure events, but don't cause the event to occur. When you submit a fault injection query, you also specify an amount of time for the failure event simulation to occur for.\""
      },
      {
        "date": "2021-10-01T10:31:00.000Z",
        "voteCount": 3,
        "content": "Ans B:\n Two ways to test/simulate fault tolerance \u2022 Manual failover\n\u2022 Fault injection queries"
      },
      {
        "date": "2021-09-23T14:22:00.000Z",
        "voteCount": 1,
        "content": "will go with B"
      },
      {
        "date": "2021-09-21T15:22:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/amazon/view/26780-exam-aws-certified-database-specialty-topic-1-question-69/",
    "body": "A company just migrated to Amazon Aurora PostgreSQL from an on-premises Oracle database. After the migration, the company discovered there is a period of time every day around 3:00 PM where the response time of the application is noticeably slower. The company has narrowed down the cause of this issue to the database and not the application.<br>Which set of steps should the Database Specialist take to most efficiently find the problematic PostgreSQL query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch dashboard to show the number of connections, CPU usage, and disk space consumption. Watch these dashboards during the next slow period.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EC2 instance, and install and configure an open-source PostgreSQL monitoring tool that will run reports based on the output error logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the logging database parameter to log all the queries related to locking in the database and then check the logs after the next slow period for this information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon RDS Performance Insights on the PostgreSQL database. Use the metrics to identify any queries that are related to spikes in the graph during the next slow period.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-13T00:28:00.000Z",
        "voteCount": 2,
        "content": "D. Enable Amazon RDS Performance Insights \n\n\"Whether your database performance problem is due to database configuration or application design issues, you can quickly identify the bottleneck and see which SQL statements are contributing to it.\"\n\nhttps://aws.amazon.com/rds/performance-insights/"
      },
      {
        "date": "2022-04-29T13:14:00.000Z",
        "voteCount": 3,
        "content": "RDS Performance Insights -&gt; identify any queries that are related to spikes in the graph during the next slow period."
      },
      {
        "date": "2022-02-24T14:48:00.000Z",
        "voteCount": 2,
        "content": "Performance Insights"
      },
      {
        "date": "2022-01-31T11:18:00.000Z",
        "voteCount": 1,
        "content": "if the database is transitioned to Aurora postgresSQL - how will enabling Amazon RDS performance insight help?"
      },
      {
        "date": "2022-02-04T11:00:00.000Z",
        "voteCount": 4,
        "content": "D - \nhttps://aws.amazon.com/about-aws/whats-new/2021/10/rds-performance-insights-more-regions/\n\n\" Amazon RDS Performance Insights is a database performance tuning and monitoring feature of RDS and Aurora that helps you quickly assess the load on your database and determine when and where to take action.\""
      },
      {
        "date": "2021-11-06T15:42:00.000Z",
        "voteCount": 2,
        "content": "Answer: D"
      },
      {
        "date": "2021-10-24T04:48:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-22T08:33:00.000Z",
        "voteCount": 2,
        "content": "D is .."
      },
      {
        "date": "2021-10-21T05:41:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/database/optimizing-and-tuning-queries-in-amazon-rds-postgresql-based-on-native-and-external-tools/\nYou can monitor SQL queries that caused load, I/O waits, and the users and hosts through which the queries ran.\nAs you can see in the previous screenshot, the same SQL query has been reported as consuming the most CPUs.\nPerformance Insights supports counter metrics for RDS PostgreSQL. Counter metrics allow you to customize your Performance Insights dashboard to include up to 10 additional graphs from the available operating system and database metrics. It is helpful to identify and analyze performance issues by correlating load charts.\nPerformance counter metrics are native and non-native.\nAs you can see in the following screenshot, counter metrics are updated with tuples fetched, tuples returned, blocks latency, and blocks read."
      },
      {
        "date": "2021-10-16T12:06:00.000Z",
        "voteCount": 3,
        "content": "Going with D for now because of https://aws.amazon.com/blogs/database/optimizing-and-tuning-queries-in-amazon-rds-postgresql-based-on-native-and-external-tools/\n\"AWS recently released a feature called Amazon RDS Performance Insights, which provides an easy-to-understand dashboard for detecting performance problems in terms of load.\"\n\"AWS recently released a feature called Amazon RDS Performance Insights, which provides an easy-to-understand dashboard for detecting performance problems in terms of load.\"\n\nThe other possible answers are B &amp; C but those solutions include the possibility it is an application issue which the question says it is not."
      },
      {
        "date": "2021-10-01T00:55:00.000Z",
        "voteCount": 2,
        "content": "Ans D is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/amazon/view/25207-exam-aws-certified-database-specialty-topic-1-question-70/",
    "body": "A company has a web-based survey application that uses Amazon DynamoDB. During peak usage, when survey responses are being collected, a Database<br>Specialist sees the ProvisionedThroughputExceededException error.<br>What can the Database Specialist do to resolve this error? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the table to use Amazon DynamoDB Streams",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurchase DynamoDB reserved capacity in the affected Region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the write capacity units for the specific table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the table capacity mode to on-demand\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the table type to throughput optimized"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T18:01:00.000Z",
        "voteCount": 21,
        "content": "Ans:CD\nDynamodb doesn't have a throughput optimized table type"
      },
      {
        "date": "2021-10-16T22:14:00.000Z",
        "voteCount": 6,
        "content": "A. Change the table to use Amazon DynamoDB Streams   \t\t--&gt; Doesnt make any sense for the given problem.\nB. Purchase DynamoDB reserved capacity in the affected Region\t--&gt; reserved capacity is not going to give you anything more than you reserve\nC. Increase the write capacity units for the specific table\t--&gt; This could be an answer. But the exception is thrown when survey responses are collected which appears to be a read \t\t\t\t\t\t\t\t  operation.\nD. Change the table capacity mode to on-demand\t\t\t--&gt; This is correct answer  https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/switching.capacitymode.html\nE. Change the table type to throughput optimized\t\t--&gt; There is no such thing as throughput optimized in dynamoDB\n \nI will go with C,D"
      },
      {
        "date": "2023-08-04T13:21:00.000Z",
        "voteCount": 2,
        "content": "\"survey responses are collected\" - writing to DB, write capacity. C and D."
      },
      {
        "date": "2023-08-28T13:05:00.000Z",
        "voteCount": 1,
        "content": "C. Increase the write capacity units for the specific table\nD. Change the table capacity mode to on-demand"
      },
      {
        "date": "2022-12-20T12:14:00.000Z",
        "voteCount": 1,
        "content": "My answer"
      },
      {
        "date": "2022-07-29T02:43:00.000Z",
        "voteCount": 1,
        "content": "C and D - Any setting will be on Table level for DynamoDB"
      },
      {
        "date": "2022-06-30T06:26:00.000Z",
        "voteCount": 1,
        "content": "Ans:CD"
      },
      {
        "date": "2022-05-30T05:34:00.000Z",
        "voteCount": 1,
        "content": "C and D are not compatible. If you use on-demand mode, you don\u00b4t change read/write capacity."
      },
      {
        "date": "2022-05-12T21:20:00.000Z",
        "voteCount": 1,
        "content": "ANS : C &amp; D."
      },
      {
        "date": "2022-04-29T07:16:00.000Z",
        "voteCount": 3,
        "content": "C. Increase the write capacity units for the specific table --&gt; This could be an answer. As the exception is thrown when survey responses are collected which is write operation.\nD. Change the table capacity mode to on-demand --&gt; This is correct answer https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/switching.capacitymode.html"
      },
      {
        "date": "2022-03-06T06:42:00.000Z",
        "voteCount": 3,
        "content": "CD\n\nWhoever writes these questions should go for English grammar classes!! both options cannot go togeher which means a question like \"What is database specialist's role in this?\" makes little sense. What would have made sense is \"What are the options a specialist can try to remediate this problem?\""
      },
      {
        "date": "2023-08-04T13:25:00.000Z",
        "voteCount": 1,
        "content": "Now imagine how difficult it is to understand everything that they want from you for a non-native speaker :)"
      },
      {
        "date": "2022-03-04T12:56:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D are the absolutely correct answers."
      },
      {
        "date": "2022-02-23T16:32:00.000Z",
        "voteCount": 1,
        "content": "Correct answers"
      },
      {
        "date": "2022-02-23T14:26:00.000Z",
        "voteCount": 1,
        "content": "C,D: Valid"
      },
      {
        "date": "2021-11-07T02:57:00.000Z",
        "voteCount": 1,
        "content": "C and D are the only correct options. Although both cannot go together. \nQuestion does not clarify that both options should apply or either of it."
      },
      {
        "date": "2021-11-06T09:34:00.000Z",
        "voteCount": 3,
        "content": "Ans: CD"
      },
      {
        "date": "2021-11-04T04:34:00.000Z",
        "voteCount": 1,
        "content": "Should be C OR D, right... because either would work, but you cannot have both. On-demand does not allow you to set write capacity???\nThe only 2 answers that fit together are B and C, as both are about provisioned capacity. But B is very unlikely here, although may be a good practice overall."
      },
      {
        "date": "2021-10-06T19:50:00.000Z",
        "voteCount": 2,
        "content": "yes, C &amp; D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/amazon/view/25623-exam-aws-certified-database-specialty-topic-1-question-71/",
    "body": "A company is running a two-tier ecommerce application in one AWS account. The web server is deployed using an Amazon RDS for MySQL Multi-AZ DB instance. A Developer mistakenly deleted the database in the production environment. The database has been restored, but this resulted in hours of downtime and lost revenue.<br>Which combination of changes in existing IAM policies should a Database Specialist make to prevent an error like this from happening in the future? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant least privilege to groups, users, and roles\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow all users to restore a database from a backup that will reduce the overall downtime to restore the database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable multi-factor authentication for sensitive operations to access sensitive resources and API operations\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse policy conditions to restrict access to selective IP addresses\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AccessList Controls policy type to restrict users for database instance deletion",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail logging and Enhanced Monitoring"
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T15:57:00.000Z",
        "voteCount": 14,
        "content": "ACD are right"
      },
      {
        "date": "2024-04-02T17:22:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2022-04-29T10:46:00.000Z",
        "voteCount": 5,
        "content": "A. Least previleges\nC. Multi factor\nD. restrict access"
      },
      {
        "date": "2024-01-13T06:02:00.000Z",
        "voteCount": 1,
        "content": "I go fro ACE.\nIP address is not a reliable decision tool."
      },
      {
        "date": "2023-08-28T13:16:00.000Z",
        "voteCount": 1,
        "content": "A. Grant least privilege to groups, users, and roles \nC. Enable multi-factor authentication for sensitive operations to access sensitive resources and API operations \nD. Use policy conditions to restrict access to selective IP addresses"
      },
      {
        "date": "2023-06-25T23:31:00.000Z",
        "voteCount": 1,
        "content": "ACD are correct options"
      },
      {
        "date": "2023-03-09T00:44:00.000Z",
        "voteCount": 2,
        "content": "There's no AccessList Controls for RDS, so based on that goes for ACD"
      },
      {
        "date": "2023-03-04T22:54:00.000Z",
        "voteCount": 1,
        "content": "the correct options are A, C, and D."
      },
      {
        "date": "2022-12-20T12:21:00.000Z",
        "voteCount": 3,
        "content": "Can someone explain why E isn't plausible?  I didn't choose D because sometimes developers are need access to Prod environments and restricting their IPs doesn't mean they can't utilize another IP to do the same damage."
      },
      {
        "date": "2023-04-05T05:46:00.000Z",
        "voteCount": 1,
        "content": "E -- Access Control List -- not for RDS, IAM.\nIAM have policies to manage the privileges, but no list"
      },
      {
        "date": "2022-12-20T04:56:00.000Z",
        "voteCount": 4,
        "content": "ACE.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"rds:DeleteDBInstance\"\n            ],\n            \"Resource\": [\n                \"arn:aws:rds:::db:\"\n            ],\n            \"Effect\": \"Deny\"\n        }\n    ]\n}"
      },
      {
        "date": "2022-07-18T22:54:00.000Z",
        "voteCount": 3,
        "content": "Prevent a user from deleting a DB instance\nThe following permissions policy grants permissions to prevent a user from deleting a specific DB instance. For example, you might want to deny the ability to delete your production DB instances to any user that is not an administrator.\n\n{\n   \"Version\": \"2012-10-17\",\n   \"Statement\": [\n      {\n         \"Sid\": \"DenyDelete1\",\n         \"Effect\": \"Deny\",\n         \"Action\": \"rds:DeleteDBInstance\",\n         \"Resource\": \"arn:aws:rds:us-west-2:123456789012:db:my-mysql-instance\"\n      }\n   ]\n}\nABE-- Wording of the question not very accurate. RDS Access polict can be done this way\nbut there is nothing called as Access Control list poicy type in RDS"
      },
      {
        "date": "2022-07-18T22:55:00.000Z",
        "voteCount": 3,
        "content": "I mean ACE"
      },
      {
        "date": "2022-02-24T11:40:00.000Z",
        "voteCount": 2,
        "content": "Appropriate and RDS supported options"
      },
      {
        "date": "2022-01-28T00:51:00.000Z",
        "voteCount": 5,
        "content": "Agree with ACD \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/security_iam_id-based-policy-examples.html"
      },
      {
        "date": "2023-04-05T05:50:00.000Z",
        "voteCount": 1,
        "content": "Policy best practices ---\nRequire multi-factor authentication (MFA)\nApply least-privilege permissions\nGet started with AWS managed policies and move toward least-privilege permissions\nUse conditions in IAM policies to further restrict access"
      },
      {
        "date": "2021-11-03T22:24:00.000Z",
        "voteCount": 4,
        "content": "I got this Question in exam.\n60% questions came in actual exam from this 145 set. Bunch of new Questions.\nWe can share study material for free, You can email me on \n\"awsdbguru at gmail\""
      },
      {
        "date": "2021-11-03T06:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer ==&gt;&gt; ACD\nany idea how much Q we will get in real exam from Q available here?\nanyone is preparing for this exam and want to do group study with us, comment with mail_id."
      },
      {
        "date": "2022-09-29T04:10:00.000Z",
        "voteCount": 1,
        "content": "srivastavanitesh16@gmail.com"
      },
      {
        "date": "2021-10-23T01:19:00.000Z",
        "voteCount": 3,
        "content": "ACD are correct choices. MFA is specified in the aws docs specifically for such use case\nhttps://aws.amazon.com/blogs/database/using-iam-multifactor-authentication-with-amazon-rds/"
      },
      {
        "date": "2021-10-17T04:36:00.000Z",
        "voteCount": 4,
        "content": "I go for ACE"
      },
      {
        "date": "2021-10-19T15:48:00.000Z",
        "voteCount": 1,
        "content": "change to ACD since RDS not support access list"
      },
      {
        "date": "2021-10-15T17:45:00.000Z",
        "voteCount": 1,
        "content": "Ans: ACD"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/amazon/view/26782-exam-aws-certified-database-specialty-topic-1-question-72/",
    "body": "A company is building a new web platform where user requests trigger an AWS Lambda function that performs an insert into an Amazon Aurora MySQL DB cluster. Initial tests with less than 10 users on the new platform yielded successful execution and fast response times. However, upon more extensive tests with the actual target of 3,000 concurrent users, Lambda functions are unable to connect to the DB cluster and receive too many connections errors.<br>Which of the following will resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the my.cnf file for the DB cluster to increase max_connections",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance size of the DB cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the DB cluster to Multi-AZ",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of Aurora Replicas"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T19:56:00.000Z",
        "voteCount": 6,
        "content": "B. Increase the instance size of the DB cluster\n\nwe need more connections which depend on instance\n\nMax_connection is a formula in RDS parameter group:\nGREATEST({log(DBInstanceClassMemory/805306368)*45},{log(DBInstanceClassMemory/8187281408)*1000})\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html\nYou can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\nYou must change a larger value for the max_connections parameter in the DB parameter group, not edit my.cnf, it is not physical server hosting MySQL."
      },
      {
        "date": "2023-08-28T13:40:00.000Z",
        "voteCount": 1,
        "content": "B. Increase the instance size of the DB cluster\n\n\"You can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\"\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html"
      },
      {
        "date": "2021-12-28T03:24:00.000Z",
        "voteCount": 3,
        "content": "Max_connection is a formula in RDS parameter group: \nGREATEST({log(DBInstanceClassMemory/805306368)*45},{log(DBInstanceClassMemory/8187281408)*1000})"
      },
      {
        "date": "2021-10-22T15:02:00.000Z",
        "voteCount": 4,
        "content": "I got this Question in exam.\nAnswer: B"
      },
      {
        "date": "2021-10-17T07:44:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html\nYou can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\nYou must change a larger value for the max_connections parameter in the DB parameter group, not edit my.cnf, it is not physical server hosting MySQL."
      },
      {
        "date": "2021-10-11T22:39:00.000Z",
        "voteCount": 1,
        "content": "ANS B.  RDS doesn't allow to change config files on host so A is ruled out."
      },
      {
        "date": "2021-10-03T23:35:00.000Z",
        "voteCount": 2,
        "content": "Ans: B"
      },
      {
        "date": "2021-09-25T22:21:00.000Z",
        "voteCount": 2,
        "content": "B is the best answer"
      },
      {
        "date": "2021-09-25T15:35:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-09-24T21:45:00.000Z",
        "voteCount": 4,
        "content": "Agree with B because of https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html\n\"You can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter, up to 16,000.\"\nCould be A as well, just haven't seen good enough documentation to choose it."
      },
      {
        "date": "2021-10-20T03:52:00.000Z",
        "voteCount": 3,
        "content": "You can change a larger value for the max_connections parameter in the DB parameter group, not edit my.cnf, so A is incorrect"
      },
      {
        "date": "2021-09-19T11:53:00.000Z",
        "voteCount": 4,
        "content": "Ans B is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/amazon/view/26783-exam-aws-certified-database-specialty-topic-1-question-73/",
    "body": "A company is developing a multi-tier web application hosted on AWS using Amazon Aurora as the database. The application needs to be deployed to production and other non-production environments. A Database Specialist needs to specify different MasterUsername and MasterUserPassword properties in the AWS<br>CloudFormation templates used for automated deployment. The CloudFormation templates are version controlled in the company's code repository. The company also needs to meet compliance requirement by routinely rotating its database master password for production.<br>What is most secure solution to store the master password?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the master password in a parameter file in each environment. Reference the environment-specific parameter file in the CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the master password using an AWS KMS key. Store the encrypted master password in the CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the secretsmanager dynamic reference to retrieve the master password stored in AWS Secrets Manager and enable automatic rotation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ssm dynamic reference to retrieve the master password stored in the AWS Systems Manager Parameter Store and enable automatic rotation."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T21:32:00.000Z",
        "voteCount": 10,
        "content": "Agree with C\n\"By using the secure string support in CloudFormation with dynamic references you can better maintain your infrastructure as code. You\u2019ll be able to avoid hard coding passwords into your templates and you can keep these runtime configuration parameters separated from your code. Moreover, when properly used, secure strings will help keep your development and production code as similar as possible, while continuing to make your infrastructure code suitable for continuous deployment pipelines.\"\nhttps://aws.amazon.com/blogs/mt/using-aws-systems-manager-parameter-store-secure-string-parameters-in-aws-cloudformation-templates/\n\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/"
      },
      {
        "date": "2021-09-20T03:45:00.000Z",
        "voteCount": 7,
        "content": "C is correct"
      },
      {
        "date": "2024-02-01T01:46:00.000Z",
        "voteCount": 1,
        "content": "agree with C"
      },
      {
        "date": "2023-08-28T13:53:00.000Z",
        "voteCount": 2,
        "content": "C. Use the secretsmanager dynamic reference to retrieve the master password stored in AWS Secrets Manager and enable automatic rotation."
      },
      {
        "date": "2022-06-24T07:12:00.000Z",
        "voteCount": 1,
        "content": "Answer:C"
      },
      {
        "date": "2022-04-30T17:58:00.000Z",
        "voteCount": 2,
        "content": "C. Use the secretsmanager dynamic reference to retrieve the master password stored in AWS Secrets Manager and enable automatic rotation.\n\n\"By using the secure string support in CloudFormation with dynamic references you can better maintain your infrastructure as code. You\u2019ll be able to avoid hard coding passwords into your templates and you can keep these runtime configuration parameters separated from your code. Moreover, when properly used, secure strings will help keep your development and production code as similar as possible, while continuing to make your infrastructure code suitable for continuous deployment pipelines.\""
      },
      {
        "date": "2022-01-03T11:25:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-31T09:00:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-17T16:41:00.000Z",
        "voteCount": 1,
        "content": "C is the right answe"
      },
      {
        "date": "2021-10-13T08:55:00.000Z",
        "voteCount": 2,
        "content": "Ans - C"
      },
      {
        "date": "2021-10-12T16:15:00.000Z",
        "voteCount": 6,
        "content": "Answer id C\nSSM does not supported automatic rotation"
      },
      {
        "date": "2021-09-22T23:58:00.000Z",
        "voteCount": 1,
        "content": "Sorry D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/amazon/view/26784-exam-aws-certified-database-specialty-topic-1-question-74/",
    "body": "A company is writing a new survey application to be used with a weekly televised game show. The application will be available for 2 hours each week. The company expects to receive over 500,000 entries every week, with each survey asking 2-3 multiple choice questions of each user. A Database Specialist needs to select a platform that is highly scalable for a large number of concurrent writes to handle the anticipated volume.<br>Which AWS services should the Database Specialist consider? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Redshift",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Neptune",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Elasticsearch Service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon ElastiCache\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T17:23:00.000Z",
        "voteCount": 8,
        "content": "Ans A and E are correct"
      },
      {
        "date": "2021-11-07T00:53:00.000Z",
        "voteCount": 5,
        "content": "I got this Question in exam."
      },
      {
        "date": "2023-08-28T14:20:00.000Z",
        "voteCount": 1,
        "content": "A. Amazon DynamoDB\nE. Amazon ElastiCache"
      },
      {
        "date": "2023-05-20T02:39:00.000Z",
        "voteCount": 2,
        "content": "My answer is AE too. But my question is can DynamoDB and Elasticache be used together? Are there many scenarios of using them both in the same application?"
      },
      {
        "date": "2022-05-01T08:25:00.000Z",
        "voteCount": 4,
        "content": "A. Amazon DynamoDB (for high responses 500K in 2 hours)\nE. Amazon ElastiCache (for high static reads 500K in 2 hours for few questions + options)"
      },
      {
        "date": "2021-12-28T05:10:00.000Z",
        "voteCount": 1,
        "content": "Advantages and disadvantages of write-through\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough"
      },
      {
        "date": "2021-12-24T14:00:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/products/databases/real-time-apps-elasticache-for-redis/"
      },
      {
        "date": "2021-11-06T12:33:00.000Z",
        "voteCount": 2,
        "content": "it is asking about the use case of each DB\nC. Amazon Neptune\nIt is not relevant, AWS Neptune is Graph database\nD. Amazon Elasticsearch Service\nit is not a use case, ES is using for log store/search/metric/config info/document list/ etc\nE. Amazon ElastiCache\nIt is used to cache the data for faster query (read) performance, usually using before a database\nA, B is my choice. Redshift is dataware house, but can be used as a data lake as high concurrent write."
      },
      {
        "date": "2023-02-08T05:14:00.000Z",
        "voteCount": 1,
        "content": "redshift is not good with write-intensive use case"
      },
      {
        "date": "2022-03-05T22:11:00.000Z",
        "voteCount": 4,
        "content": "definitely not Redshift"
      },
      {
        "date": "2021-12-24T13:57:00.000Z",
        "voteCount": 1,
        "content": "Redshift is not a good idea"
      },
      {
        "date": "2021-11-04T10:30:00.000Z",
        "voteCount": 1,
        "content": "AE would make sense if not this part: \"needs to select a platform that is highly scalable for a large number of concurrent writes\".\nI think AD is a better choice here."
      },
      {
        "date": "2021-10-30T18:08:00.000Z",
        "voteCount": 1,
        "content": "Ans: AE"
      },
      {
        "date": "2021-10-28T04:34:00.000Z",
        "voteCount": 2,
        "content": "A, E for sure\nhttps://aws.amazon.com/products/databases/real-time-apps-elasticache-for-redis/"
      },
      {
        "date": "2021-10-20T23:30:00.000Z",
        "voteCount": 2,
        "content": "A. Amazon DynamoDB   \t-- ideal for this requirement\nB. Amazon Redshift   \t-- Wrong choice\nC. Amazon Neptune    \t-- Not a requirement to have graph database\nD. Amazon Elasticsearch Service  -- Not requirement for ElasticSearch\nE. Amazon ElastiCache\t\t-- The requirement is write intensive.. Not sure how Elasticache can help.\n\nA,E seem to be the best choice."
      },
      {
        "date": "2021-10-24T06:58:00.000Z",
        "voteCount": 1,
        "content": "ElastiCache store the questions and multiple answers"
      },
      {
        "date": "2021-10-07T09:35:00.000Z",
        "voteCount": 2,
        "content": "yes, A, E"
      },
      {
        "date": "2021-10-06T18:58:00.000Z",
        "voteCount": 4,
        "content": "AE \nhttps://aws.amazon.com/elasticache/\nBuilding real-time apps across versatile use cases like gaming, geospatial service, caching, session stores, or queuing, with advanced data structures, replication, and point-in-time snapshot support.\nhttps://aws.amazon.com/dynamodb/\nBuild powerful web applications that automatically scale up and down. You don't need to maintain servers, and your applications have automated high availability."
      },
      {
        "date": "2021-10-02T02:55:00.000Z",
        "voteCount": 2,
        "content": "My ans is A and D. Why not E, E is store data in memory and normally we use it as a buffer server but not the server to store the data.\nD, Amazon Elasticsearch is Highly scalable."
      },
      {
        "date": "2022-01-09T13:26:00.000Z",
        "voteCount": 1,
        "content": "Amazon OpenSearch Service (successor to Amazon Elasticsearch Service)\nmakes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more.  \n\nSeems like an overkill for a 2-3 multiple question. \n\nI vote for AE"
      },
      {
        "date": "2021-10-08T05:45:00.000Z",
        "voteCount": 1,
        "content": "My opinion is the same as yours."
      },
      {
        "date": "2021-09-30T02:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is AE"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/amazon/view/26802-exam-aws-certified-database-specialty-topic-1-question-75/",
    "body": "A company has migrated a single MySQL database to Amazon Aurora. The production data is hosted in a DB cluster in VPC_PROD, and 12 testing environments are hosted in VPC_TEST using the same AWS account. Testing results in minimal changes to the test data. The Development team wants each environment refreshed nightly so each test database contains fresh production data every day.<br>Which migration approach will be the fastest and most cost-effective to implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the master in Amazon Aurora MySQL. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the master in Amazon Aurora MySQL. Take a nightly snapshot, and restore it into 12 databases in VPC_TEST using Aurora Serverless.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the master in Amazon Aurora MySQL. Create 12 Aurora Replicas in VPC_TEST, and script the replicas to be deleted and re-created nightly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the master in Amazon Aurora MySQL using Aurora Serverless. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T03:40:00.000Z",
        "voteCount": 13,
        "content": "A. \nB dropped due to snapshot is slower (full disk dump) than clone (copy-on-write)\nC dropped due to no write on Aurora Replicas\nD dropped due to there\u2019s no option for cloning in the console."
      },
      {
        "date": "2021-10-17T23:22:00.000Z",
        "voteCount": 5,
        "content": "Cloning is not supported on Aurora Serverless  nor Cross-Region. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html"
      },
      {
        "date": "2021-10-17T23:27:00.000Z",
        "voteCount": 1,
        "content": "No, it is cross-account, not cross-region. The question mentions same account\nYou can create an Aurora provisioned clone from a provisioned Aurora DB cluster. You can create an Aurora Serverless v1 clone from an Aurora Serverless v1 DB cluster. But you can also create Aurora Serverless v1 clones from Aurora provisioned DB clusters, and you can create provisioned clones from Aurora Serverless v1 DB clusters.\n\"CROSS-ACCOUNT cloning currently doesn't support cloning Aurora Serverless v1 DB clusters\""
      },
      {
        "date": "2024-01-13T06:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nThe question does not mention variable workload, so I see no need for Serverless (D)"
      },
      {
        "date": "2023-08-05T03:12:00.000Z",
        "voteCount": 2,
        "content": "Choose between A and D.\nTo have Aurora Serverless we need to convert RDS Aurora MySQL to Aurora Serverless MySQL.\nWe can do it 3 ways:\n - Snapshot restore\n - Logical backup and restore\n - A new serverless reader (for Amazon Aurora PostgreSQL-Compatible Edition versions 13.6 and later).  Add a serverless reader, force a failover. This promotes the reader instance to a writer instance.\nhttps://repost.aws/knowledge-center/aurora-migrate-provisioned-serverless\nWe are asked for \"the fastest\" solution, so, answer A."
      },
      {
        "date": "2023-05-11T08:55:00.000Z",
        "voteCount": 1,
        "content": "D \n\nhttps://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/\n\nAurora Serverless supports fast database cloning. You only pay for additional storage if you make data changes in the cloned DB cluster.\n\nIn question it mentioned \"Testing results in minimal changes to the test data\" so there will be a minimal cost for dev env databases. Most cost-effective."
      },
      {
        "date": "2023-03-19T18:42:00.000Z",
        "voteCount": 1,
        "content": "A.\nit is possible to create an Aurora Cluster with a replica and then use it to create an Aurora Serverless cluster. Then use the Serverless cluster as the source to clone 12 DEV DB.\nNot choosing D is because D -- we can not choose Serverless as master primary DB because severless is for infrequently use"
      },
      {
        "date": "2023-02-08T05:21:00.000Z",
        "voteCount": 2,
        "content": "A is fasted but B is most cost effective\nD is wrong due to it is not recommend to use serverless db for production."
      },
      {
        "date": "2023-04-10T03:43:00.000Z",
        "voteCount": 1,
        "content": "Agree A\nAgree your point - \" not recommend to use serverless db for production\""
      },
      {
        "date": "2023-02-07T13:53:00.000Z",
        "voteCount": 2,
        "content": "D Serverless is fastest and clone is supported"
      },
      {
        "date": "2023-01-19T07:08:00.000Z",
        "voteCount": 2,
        "content": "Go for D as Aurora Serverless support clonning since June 2021 \nhttps://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/"
      },
      {
        "date": "2022-12-20T13:11:00.000Z",
        "voteCount": 3,
        "content": "I chose A over D because I don't believe serverless will be cheaper necessarily."
      },
      {
        "date": "2022-10-16T16:54:00.000Z",
        "voteCount": 2,
        "content": "aurora server less now supports cloning since june 2021. Question is abount cost effective. i am leanng towards D"
      },
      {
        "date": "2022-10-16T16:56:00.000Z",
        "voteCount": 1,
        "content": "Although Personally I wouldn't recommend Serverless for production workload with the limitations comes with aurora serverless . A is not a wrong answer either"
      },
      {
        "date": "2022-09-30T08:49:00.000Z",
        "voteCount": 3,
        "content": "Going to go with A.  Aurora Serverless is a good fit for applications that are not expected to serve traffic on a regular basis, such as development or test environments.  So in this case, moving the master to serverless seems kind of backwards."
      },
      {
        "date": "2022-07-16T08:55:00.000Z",
        "voteCount": 3,
        "content": "A. D It s not correct. Why move to serverless?"
      },
      {
        "date": "2022-05-01T07:54:00.000Z",
        "voteCount": 2,
        "content": "x A. Run the master in Amazon Aurora MySQL. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly. (right answer before June 2021 as option D's serverless did not allow clone earlier)\nx B. snapshot slow\nx C. Replica not for testing\nD. Run the master in Amazon Aurora MySQL using Aurora Serverless. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly. (right answer after 6/2021, serverless v1 supports cloning to same account)\n\nhttps://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html"
      },
      {
        "date": "2022-01-09T13:19:00.000Z",
        "voteCount": 4,
        "content": "A.\n\nAlthough the question does not mention any info about the production database. I am not convinced to move the production to Aurora Serverless, with these limitations in place:\n\n\nAurora Serverless v1 doesn't support the following features:\n    Aurora global databases\n    Aurora multi-master clusters\n    Aurora Replicas\n    AWS Identity and Access Management (IAM) database authentication\n    Backtracking in Aurora\n    Database activity streams\n    Performance Insights"
      },
      {
        "date": "2023-02-07T13:55:00.000Z",
        "voteCount": 1,
        "content": "Severless V2 has not most of that limitations"
      },
      {
        "date": "2021-12-04T09:49:00.000Z",
        "voteCount": 3,
        "content": "The best answer will be D after Jun. 21, 2021. According to https://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/,\nafter Jun. 21, 2021, Amazon Aurora allows you to create clones between Aurora Serverless v1 and provisioned Aurora DB clusters to enable quick sharing of data, i.e., you can create Aurora Serverless v1 clones from Aurora provisioned DB clusters, and you can also create provisioned clones from Aurora Serverless v1 DB clusters."
      },
      {
        "date": "2021-11-01T03:51:00.000Z",
        "voteCount": 2,
        "content": "I got this Question in exam."
      },
      {
        "date": "2021-10-29T05:07:00.000Z",
        "voteCount": 2,
        "content": "Answer: A"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/amazon/view/25628-exam-aws-certified-database-specialty-topic-1-question-76/",
    "body": "A large ecommerce company uses Amazon DynamoDB to handle the transactions on its web portal. Traffic patterns throughout the year are usually stable; however, a large event is planned. The company knows that traffic will increase by up to 10 times the normal load over the 3-day event. When sale prices are published during the event, traffic will spike rapidly.<br>How should a Database Specialist ensure DynamoDB can handle the increased traffic?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the table is always provisioned to meet peak needs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow burst capacity to handle the additional load",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet an AWS Application Auto Scaling policy for the table to handle the increase in traffic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprovision additional capacity for the known peaks and then reduce the capacity after the event\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-06-30T23:57:00.000Z",
        "voteCount": 22,
        "content": "C is the correct.\nD is not correct because in Dynamodb when you scale up the capacity your data partition will increase accorssing to your RCU and WCU, but when you scale down the partition remain unchnaged, so the per table RCU and WCU will give poor performance. \nI think Auto Scaling is the correct  way is such situation."
      },
      {
        "date": "2022-07-04T05:59:00.000Z",
        "voteCount": 7,
        "content": "correct, I'm surprised that no one talk about it. Once you add more capacity, it's really hard to reduce"
      },
      {
        "date": "2023-04-15T05:31:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-tutorial.html\nAfter completing this tutorial, you\u2019ll know how to:\n\nUse scheduled scaling to add extra capacity to meet a heavy load before it arrives, and then remove the extra capacity when it's no longer required.\n\nUse a target tracking scaling policy to scale your application based on current resource utilization.\nVote for C"
      },
      {
        "date": "2023-07-12T13:32:00.000Z",
        "voteCount": 1,
        "content": "this document regards EC2 auto scaling, not DynamoDB scaling"
      },
      {
        "date": "2023-08-13T00:44:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html and https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nWith Application Auto Scaling, you create a scaling policy for a table or a global secondary index. The scaling policy specifies whether you want to scale read capacity or write capacity (or both), and the minimum and maximum provisioned capacity unit settings for the table or index.\n\n\nAnswer: C"
      },
      {
        "date": "2021-10-01T00:48:00.000Z",
        "voteCount": 15,
        "content": "I'm going with D because we know about the increased traffic in advance because it will be due to a sale. \nBurst capacity is fine for unknown spikes up to 5 minutes. This even is for 3 days. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition-key-throughput-bursting\n\"DynamoDB provides some flexibility in your per-partition throughput provisioning by providing burst capacity. Whenever you're not fully using a partition's throughput, DynamoDB reserves a portion of that unused capacity for later bursts of throughput to handle usage spikes.\nDynamoDB currently retains up to 5 minutes (300 seconds) of unused read and write capacity. During an occasional burst of read or write activity, these extra capacity units can be consumed quickly\u2014even faster than the per-second provisioned throughput capacity that you've defined for your table.\nDynamoDB can also consume burst capacity for background maintenance and other tasks without prior notice.\nNote that these burst capacity details might change in the future.\""
      },
      {
        "date": "2024-01-13T06:12:00.000Z",
        "voteCount": 1,
        "content": "The answer is D, as we know about the event beforehand\nFurthermore, C will have an issue getting 10x performance quickly enough because of the cooldown."
      },
      {
        "date": "2024-01-02T11:29:00.000Z",
        "voteCount": 1,
        "content": "D.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual"
      },
      {
        "date": "2023-12-26T08:30:00.000Z",
        "voteCount": 1,
        "content": "scheduled auto-scaling"
      },
      {
        "date": "2023-10-23T17:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct, because prewarm:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand"
      },
      {
        "date": "2023-09-18T10:05:00.000Z",
        "voteCount": 1,
        "content": "Scheduled scaling, as one way of Application Auto Scaling, is available for DynamoDB tables and global secondary indexes. It allows to \"scale a resource one time only or on a recurring schedule\". I understand that this is what we need for the one-time event, and it's even automated (option D is not automated).\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html"
      },
      {
        "date": "2023-08-23T07:42:00.000Z",
        "voteCount": 1,
        "content": "D\nIn summary, Autoscaling requires consecutive data points where the target utilization value is being breached to scale up a DynamoDB table. For this reason Autoscaling is not recommended as a solution for dealing with spiked workloads."
      },
      {
        "date": "2023-08-06T04:31:00.000Z",
        "voteCount": 1,
        "content": "Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
      },
      {
        "date": "2023-06-02T06:11:00.000Z",
        "voteCount": 2,
        "content": "Have everyone forgot cooldown during AWS Application Auto Scaling policy? We know we need to increase by 10x for AWS Application Auto Scaling policy with cooldowns it will take time to get there."
      },
      {
        "date": "2023-11-09T07:08:00.000Z",
        "voteCount": 1,
        "content": "agree and in questo said  \"traffic will spike rapidly\" autoscaling here bad performance instead provisioned, because you know when is going to be a spike and can be prepared."
      },
      {
        "date": "2023-06-02T00:54:00.000Z",
        "voteCount": 1,
        "content": "A. This is achieved by D, but D is more precise.\nB. Does DynamoDB support burst capacity?\nC.The question is not about the application, but about DynamoDB\nD. Using provisioned capacity to meet the expected demand is one way of doing it. Using provisioned capacity with auto-scaling would also work. And of course on-demand would be an option."
      },
      {
        "date": "2023-02-12T10:01:00.000Z",
        "voteCount": 1,
        "content": "C- I will go with autoscaling. Why change 2 times config, when autoscaling is designed for that.."
      },
      {
        "date": "2023-07-12T13:33:00.000Z",
        "voteCount": 1,
        "content": "because the usage will spike rapidly, if you have pre previsioned, you dont waste time scaling"
      },
      {
        "date": "2023-02-08T05:49:00.000Z",
        "voteCount": 1,
        "content": "D is wrong because on-demand to maximum 2 times of previous peak, it can not scale to 10x.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/on-demand-table-throttling-dynamodb/"
      },
      {
        "date": "2023-01-27T18:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct. AWS always recomend to use auto scaling when you can predict usage. I now that I excpect 10x more traffic."
      },
      {
        "date": "2022-12-20T13:16:00.000Z",
        "voteCount": 1,
        "content": "I\"m going with D because auto scaling on the application doesn't mean the DB can accommodate the increased RW on the DB.  Since the peak traffic is predictable then it may be best to pre-provision ahead and reduce after sale is over.  I may be wrong though"
      },
      {
        "date": "2022-10-16T17:03:00.000Z",
        "voteCount": 1,
        "content": "I go with D. although If the answer D also includes autoscaling it would have been easy to choose D"
      },
      {
        "date": "2022-10-11T02:13:00.000Z",
        "voteCount": 1,
        "content": "C. Set an AWS Application Auto Scaling policy for the table to handle the increase in traffic"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/amazon/view/27045-exam-aws-certified-database-specialty-topic-1-question-77/",
    "body": "A Database Specialist is migrating an on-premises Microsoft SQL Server application database to Amazon RDS for PostgreSQL using AWS DMS. The application requires minimal downtime when the RDS DB instance goes live.<br>What change should the Database Specialist make to enable the migration?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the on-premises application database to act as a source for an AWS DMS full load with ongoing change data capture (CDC)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS replication instance to allow both full load and ongoing change data capture (CDC)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS task to generate full logs to allow for ongoing change data capture (CDC)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS connections to allow two-way communication to allow for ongoing change data capture (CDC)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-03T04:05:00.000Z",
        "voteCount": 10,
        "content": "Sorry, I mean A. We cannot define full load or CDC in Replication instance"
      },
      {
        "date": "2021-10-14T00:52:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.....You define full load and CDC \nwhen creating the replication task"
      },
      {
        "date": "2022-05-01T09:07:00.000Z",
        "voteCount": 5,
        "content": "C&amp;D unrelated\nA. \"AWS DMS full load with ongoing change data capture (CDC)\" (Note WITH means 1 task)\nB. allow both full load and ongoing change data capture (CDC)\n(note AND, means 2 tasks)\nThere are two types of ongoing replication tasks:\n\nFull load plus CDC \u2013 The task migrates existing data and then updates the target database based on changes to the source database.\n\nCDC only \u2013 The task migrates ongoing changes after you have data on your target database.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html"
      },
      {
        "date": "2023-06-02T01:06:00.000Z",
        "voteCount": 1,
        "content": "The answers are not very clearly stated, but A is the best option\n\nA suggests that the source instance needs to be configured for AWS DMS full load and data capture. There is some truth to that since binary logging (transaction logs) need to be enabled.\nxB suggest that it is the instance that is configured for full load and CDC, but it is really the task\nxC suggests that the task should be configured to generate full logs?\nxD configure two-way communication"
      },
      {
        "date": "2022-03-05T21:26:00.000Z",
        "voteCount": 4,
        "content": "whoever makes these questions is probably not sober - \nanswer choice (A) is super apparent to the point of silliness - what ELSE would be the source?\nanswer choice (B) says replication instance needs to be configured to allow Full Load + CDC. The INSTANCE doesnt do this. The Task it hosts does! Do they intend both to be  the one and the same?"
      },
      {
        "date": "2022-01-12T06:07:00.000Z",
        "voteCount": 2,
        "content": "Ans A: Full load and CDC are defined in a DMS Tasks."
      },
      {
        "date": "2021-11-06T22:28:00.000Z",
        "voteCount": 4,
        "content": "Question not clear.  A seems more relevant."
      },
      {
        "date": "2021-11-01T01:43:00.000Z",
        "voteCount": 2,
        "content": "\"requires minimal downtime when the RDS DB instance goes live\"\nin order to do CDC: \"you must first ensure that ARCHIVELOG MODE is on to provide information to LogMiner. AWS DMS uses LogMiner to read information from the archive logs so that AWS DMS can capture changes\"\nSo my answer is A"
      },
      {
        "date": "2021-10-29T23:10:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/sbs/chap-oracle2postgresql.steps.configureoracle.html says \"If you want to capture and apply changes (CDC), then you also need the following privileges.\"\nso A is the correct answer"
      },
      {
        "date": "2021-10-11T16:10:00.000Z",
        "voteCount": 1,
        "content": "This question is bit vague. I will go with A"
      },
      {
        "date": "2021-10-08T15:56:00.000Z",
        "voteCount": 2,
        "content": "A is the answer"
      },
      {
        "date": "2021-09-29T11:04:00.000Z",
        "voteCount": 2,
        "content": "B since Migrate existing data and replicate ongoing changes (full load + change data capture (CDC)) \u2013 To migrate data with minimal downtime, AWS DMS can migrate the existing data and replicate the data changes from the source to the target until the cutover. This migration type is best for small and medium databases that require minimal downtime, which only lasts for the duration of the cutover."
      },
      {
        "date": "2021-10-18T18:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. You need to to a DMS task that is \"full+cdc\", this has nothing to do with the DMS instance in B"
      },
      {
        "date": "2021-10-25T21:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is A replication instance can't do full and cdc task .only dms do."
      },
      {
        "date": "2022-10-10T09:46:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/sbs/chap-rdsoracle2aurora.steps.createreplicationinstance.html\n\nA DMS replication instance performs the actual data migration between source and target. The replication instance also caches the transaction logs during the migration. How much CPU and memory capacity a replication instance has influences the overall time required for the migration."
      },
      {
        "date": "2021-09-25T05:57:00.000Z",
        "voteCount": 4,
        "content": "B i think... im not sure"
      },
      {
        "date": "2021-09-19T15:36:00.000Z",
        "voteCount": 2,
        "content": "A looks fine"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/amazon/view/26311-exam-aws-certified-database-specialty-topic-1-question-78/",
    "body": "A financial company has allocated an Amazon RDS MariaDB DB instance with large storage capacity to accommodate migration efforts. Post-migration, the company purged unwanted data from the instance. The company now want to downsize storage to save money. The solution must have the least impact on production and near-zero downtime.<br>Which solution would meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the old databases and restore the snapshot with the required storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new RDS DB instance with the required storage and move the databases from the old instances to the new instance using AWS DMS\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new database using native backup and restore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new read replica and make it the primary by terminating the existing primary"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T17:35:00.000Z",
        "voteCount": 24,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-db-storage-size/\"After you create an Amazon RDS DB instance, you can't modify the allocated storage size of the DB instance to decrease the total storage space it uses. To decrease the storage size of your DB instance, create a new DB instance that has less provisioned storage size. Then, migrate your data into the new DB instance using one of the following methods:\nUse the database engine's native dump and restore method.\nNote: This method causes some downtime.\nUse AWS Database Migration Service (AWS DMS) for minimal downtime.\""
      },
      {
        "date": "2022-03-04T21:24:00.000Z",
        "voteCount": 5,
        "content": "A. Create a snapshot of the old databases and restore the snapshot with the required storage (Downtime )\nB. Create a new RDS DB instance with the required storage and move the databases from the old instances to the new instance using AWS DMS ( no downtime)\nC. Create a new database using native backup and restore (Downtime like A)\nD. Create a new read replica and make it the primary by terminating the existing primary (wont  change the storage)"
      },
      {
        "date": "2022-06-21T23:39:00.000Z",
        "voteCount": 4,
        "content": "Its not D  because For replication to operate effectively, each Read Replica should have the same amount of compute &amp; storage resources as the source DB instance."
      },
      {
        "date": "2022-03-13T04:57:00.000Z",
        "voteCount": 2,
        "content": "B. When you create a read replica you cannot set the storage size... so D is incorrect."
      },
      {
        "date": "2022-02-16T02:46:00.000Z",
        "voteCount": 1,
        "content": "B\nAlmost no downtime &gt; AWS DMS"
      },
      {
        "date": "2021-12-28T14:55:00.000Z",
        "voteCount": 1,
        "content": "Isn't A?"
      },
      {
        "date": "2022-02-27T07:16:00.000Z",
        "voteCount": 2,
        "content": "no, this will not shrink the size"
      },
      {
        "date": "2023-03-05T23:29:00.000Z",
        "voteCount": 1,
        "content": "key: 'near-zero downtime'."
      },
      {
        "date": "2021-12-01T16:11:00.000Z",
        "voteCount": 2,
        "content": "Answer is B.\nYou cannot downsize the database storage size on RDS."
      },
      {
        "date": "2021-11-13T15:29:00.000Z",
        "voteCount": 2,
        "content": "Almost no downtime &gt; AWS DMS : Option B"
      },
      {
        "date": "2021-10-31T22:54:00.000Z",
        "voteCount": 3,
        "content": "D is not correct because, You cannot set the size of the read replica, which is always same as the master."
      },
      {
        "date": "2021-10-27T10:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct. D is close, however, you don't make the read replica the primary by terminating the existing primary.\n\"Create a new read replica and make it the primary by terminating the existing primary\""
      },
      {
        "date": "2022-03-13T05:41:00.000Z",
        "voteCount": 1,
        "content": "Yes, you can make the read replica the primary just by terminating the current primary. Anyways option D is incorrect once the replica storage will be the same as the primary."
      },
      {
        "date": "2021-10-24T22:51:00.000Z",
        "voteCount": 1,
        "content": "D can be an option too.  Read replicas can have different storage size and Read replicas will have less impact on production as compared to DMS."
      },
      {
        "date": "2021-10-22T20:57:00.000Z",
        "voteCount": 1,
        "content": "B looks correct according to BillyMadaison's link. Not my initial answer though"
      },
      {
        "date": "2021-10-18T15:47:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-10-13T21:50:00.000Z",
        "voteCount": 1,
        "content": "B looks to be the correct answer"
      },
      {
        "date": "2021-10-04T13:05:00.000Z",
        "voteCount": 3,
        "content": "Yes. B is the answer\nsnapshot will retain same size. DB needs to be recreated and DMS is for near zero"
      },
      {
        "date": "2021-09-29T04:57:00.000Z",
        "voteCount": 1,
        "content": "B is corect"
      },
      {
        "date": "2021-09-27T10:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/amazon/view/26024-exam-aws-certified-database-specialty-topic-1-question-79/",
    "body": "A large financial services company requires that all data be encrypted in transit. A Developer is attempting to connect to an Amazon RDS DB instance using the company VPC for the first time with credentials provided by a Database Specialist. Other members of the Development team can connect, but this user is consistently receiving an error indicating a communications link failure. The Developer asked the Database Specialist to reset the password a number of times, but the error persists.<br>Which step should be taken to troubleshoot this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the database option group for the RDS DB instance allows ingress from the Developer machine's IP address",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the RDS DB instance's subnet group includes a public subnet to allow the Developer to connect",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the RDS DB instance has not reached its maximum connections limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the connection is using SSL and is addressing the port where the RDS DB instance is listening for encrypted connections\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T06:54:00.000Z",
        "voteCount": 8,
        "content": "D is correct"
      },
      {
        "date": "2023-09-18T11:34:00.000Z",
        "voteCount": 1,
        "content": "It's noted in the question that the connection needs to be encrypted.\nIt's also noted that the logon credentials are confirmed, they should be right.\nB is out of question, there is no such thing as a subnet group to control access.\nD is the only reasonable option."
      },
      {
        "date": "2023-09-01T05:07:00.000Z",
        "voteCount": 2,
        "content": "D. Ensure that the connection is using SSL and is addressing the port where the RDS DB instance is listening for encrypted connections"
      },
      {
        "date": "2022-06-28T17:25:00.000Z",
        "voteCount": 4,
        "content": "A lot of time this issue has happened to me, a Developer call me because is unable to connect  to Redshift or any other DB and the problem is  that they forgot to check  \"use SSL\" in DBeaver. :)\nI will go with D:"
      },
      {
        "date": "2022-06-23T05:52:00.000Z",
        "voteCount": 1,
        "content": "Ofcourse it's decided to be D"
      },
      {
        "date": "2022-05-21T15:46:00.000Z",
        "voteCount": 2,
        "content": "d. as all data in transit be encrypted"
      },
      {
        "date": "2022-04-30T12:28:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.SSL.Using.html\n\nD. addressing the port where the RDS DB instance is listening for encrypted connections (communications connection failure =&gt; port)"
      },
      {
        "date": "2022-04-13T05:41:00.000Z",
        "voteCount": 2,
        "content": "\"Other members of the Development team are able to connect\", so obviously not B"
      },
      {
        "date": "2022-01-22T03:42:00.000Z",
        "voteCount": 1,
        "content": "\"firm mandates that all data in transit be encrypted\" but \"Other members of the Development team are able to connect\", then SSL connections are ok. B is the answer that has more sence for communication error."
      },
      {
        "date": "2021-12-27T22:52:00.000Z",
        "voteCount": 1,
        "content": "\"a communications connection failure\". It's about communication failure during connection."
      },
      {
        "date": "2021-11-04T03:42:00.000Z",
        "voteCount": 1,
        "content": "Asnwer D: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.SSL.Using.html"
      },
      {
        "date": "2021-11-02T04:58:00.000Z",
        "voteCount": 1,
        "content": "D for sure."
      },
      {
        "date": "2021-11-01T17:39:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-10-31T20:26:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-30T12:39:00.000Z",
        "voteCount": 2,
        "content": "D is my choice"
      },
      {
        "date": "2021-10-25T08:49:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      },
      {
        "date": "2021-10-09T10:06:00.000Z",
        "voteCount": 1,
        "content": "D is answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/amazon/view/25210-exam-aws-certified-database-specialty-topic-1-question-80/",
    "body": "A company is running Amazon RDS for MySQL for its workloads. There is downtime when AWS operating system patches are applied during the Amazon RDS- specified maintenance window.<br>What is the MOST cost-effective action that should be taken to avoid downtime?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workloads from Amazon RDS for MySQL to Amazon DynamoDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable cross-Region read replicas and direct read traffic to them when Amazon RDS is down",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a read replica and direct read traffic to it when Amazon RDS is down",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable an Amazon RDS for MySQL Multi-AZ configuration\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T19:04:00.000Z",
        "voteCount": 14,
        "content": "Going with D for now\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\nTo minimize downtime, modify the Amazon RDS DB instance to a Multi-AZ deployment. For Multi-AZ deployments, OS maintenance is applied to the secondary instance first, then the instance fails over, and then the primary instance is updated. The downtime is during failover. For more information, see Maintenance for Multi-AZ Deployments.\nhttps://aws.amazon.com/rds/faqs/\nThe availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby. In the case of patching or DB instance class scaling, these operations occur first on the standby, prior to automatic fail over. As a result, your availability impact is limited to the time required for automatic failover to complete."
      },
      {
        "date": "2021-09-24T11:06:00.000Z",
        "voteCount": 5,
        "content": "Ans: D\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/"
      },
      {
        "date": "2023-09-18T11:39:00.000Z",
        "voteCount": 1,
        "content": "A is a diverter.\nB + C are only for reading, that doesn't help a Production database to avoid downtime.\nD is documented to help reduce downtime during OS patch cycles.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/"
      },
      {
        "date": "2023-09-01T05:18:00.000Z",
        "voteCount": 1,
        "content": "D. Enable an Amazon RDS for MySQL Multi-AZ configuration"
      },
      {
        "date": "2023-08-06T05:14:00.000Z",
        "voteCount": 1,
        "content": "For OS maintenance (\"AWS operating system patches are applied \"), OS maintenance is applied to the secondary instance first, then the instance fails over, and then the primary instance is updated. \nhttps://repost.aws/knowledge-center/rds-required-maintenance"
      },
      {
        "date": "2023-06-29T20:55:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/rds-required-maintenance"
      },
      {
        "date": "2022-05-22T07:22:00.000Z",
        "voteCount": 2,
        "content": "Obviously D is correct."
      },
      {
        "date": "2022-04-30T14:59:00.000Z",
        "voteCount": 1,
        "content": "C is the answer if the workload is read-only.\nD is the answer to do the maintenance for R+W workload with reduced outage"
      },
      {
        "date": "2021-11-03T09:08:00.000Z",
        "voteCount": 1,
        "content": "D is correct: \"Single-AZ deployments are unavailable for a few minutes. Multi-AZ deployments are unavailable for the time it takes the instance to failover (usually about 60 seconds) if the Availability Zone is affected by the maintenance. If only the secondary Availability Zone is affected, then there is no failover or downtime. \"\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/"
      },
      {
        "date": "2021-11-02T01:40:00.000Z",
        "voteCount": 1,
        "content": "D for sure."
      },
      {
        "date": "2021-10-26T10:02:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-25T11:26:00.000Z",
        "voteCount": 2,
        "content": "-Ans: D"
      },
      {
        "date": "2021-10-23T09:01:00.000Z",
        "voteCount": 4,
        "content": "A. Migrate the workloads from Amazon RDS for MySQL to Amazon DynamoDB\nDynamoDB is not appropriate to take over RDS workload.\nB. Enable cross-Region read replicas and direct read traffic to then when Amazon RDS is down\nWhat about writes ?  why do you need cross-region when you can have read-replicas in same region\nC. Enable a read replicas and direct read traffic to it when Amazon RDS is down\nWhat about write workload ?\nD. Enable an Amazon RDS for MySQL Multi-AZ configuration\nHelps in reducing the outage required for the maintenance(just a failover)\n\nC is the answer if the workload is read-only.\nD is the answer to do the maintenance for R+W workload with reduced outage"
      },
      {
        "date": "2021-10-22T05:48:00.000Z",
        "voteCount": 2,
        "content": "Yes it is D"
      },
      {
        "date": "2021-10-16T18:54:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      },
      {
        "date": "2021-10-16T11:30:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-10-04T19:52:00.000Z",
        "voteCount": 2,
        "content": "D. 100%"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/amazon/view/26312-exam-aws-certified-database-specialty-topic-1-question-81/",
    "body": "A Database Specialist must create a read replica to isolate read-only queries for an Amazon RDS for MySQL DB instance. Immediately after creating the read replica, users that query it report slow response times.<br>What could be causing these slow response times?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew volumes created from snapshots load lazily in the background\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLong-running statements on the master",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsufficient resources on the master",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOverload of a single replication thread by excessive writes on the master"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T11:09:00.000Z",
        "voteCount": 15,
        "content": "ANS: A\nsnapshot is lazy loaded If the volume is accessed where the data is not loaded, the application accessing the volume encounters a higher latency than normal while the data gets loaded"
      },
      {
        "date": "2021-11-01T11:03:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-prewarming-data-into-volumes-created-snapshots/"
      },
      {
        "date": "2022-07-06T01:43:00.000Z",
        "voteCount": 4,
        "content": "question is about RDS this link is irrevalent not need to confuse ebs and rds"
      },
      {
        "date": "2022-03-04T22:41:00.000Z",
        "voteCount": 10,
        "content": "Unlike Aurora - RDS does not have a warm pool of cache (unless its POSTGRES and you are using CCM there - but there is MYSQL). First touch penalty. Can be mitigated with doing a select * from all tables"
      },
      {
        "date": "2023-06-02T03:39:00.000Z",
        "voteCount": 2,
        "content": "An active, long-running transaction can slow the process of creating the read replica. We recommend that you wait for long-running transactions to complete before creating a read replica. If you create multiple read replicas in parallel from the same source DB instance, Amazon RDS takes only one snapshot at the start of the first create action.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Create"
      },
      {
        "date": "2022-07-11T11:39:00.000Z",
        "voteCount": 2,
        "content": "Ans: B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-high-replica-lag/\nIf the replica SQL_THREAD is the source of replication delays, then those delays could be caused by the following:\n\nLong-running queries on the primary DB instance\nInsufficient DB instance class size or storage\nParallel queries run on the primary DB instance\nBinary logs synced to the disk on the replica DB instance\nBinlog_format on the replica is set to ROW\nReplica creation lag"
      },
      {
        "date": "2022-07-17T12:46:00.000Z",
        "voteCount": 3,
        "content": "Correction: Ans: A. Lag != slow response time."
      },
      {
        "date": "2022-04-29T14:22:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-prewarming-data-into-volumes-created-snapshots/"
      },
      {
        "date": "2022-07-06T01:43:00.000Z",
        "voteCount": 4,
        "content": "question is about RDS this link is irrevalent not need to confuse ebs and rds"
      },
      {
        "date": "2021-11-26T05:50:00.000Z",
        "voteCount": 2,
        "content": "Option A."
      },
      {
        "date": "2021-10-30T17:12:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-10-23T03:22:00.000Z",
        "voteCount": 1,
        "content": "A is my choice"
      },
      {
        "date": "2021-10-22T09:42:00.000Z",
        "voteCount": 2,
        "content": "Yes, it is A"
      },
      {
        "date": "2021-10-17T10:40:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-10-16T12:31:00.000Z",
        "voteCount": 2,
        "content": "Ans A\nWhen you spin up a new replica, its EBS volume loads lazily in the background"
      },
      {
        "date": "2021-09-29T10:42:00.000Z",
        "voteCount": 3,
        "content": "Yes A is Correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/amazon/view/27047-exam-aws-certified-database-specialty-topic-1-question-82/",
    "body": "A company developed an AWS CloudFormation template used to create all new Amazon DynamoDB tables in its AWS account. The template configures provisioned throughput capacity using hard-coded values. The company wants to change the template so that the tables it creates in the future have independently configurable read and write capacity units assigned.<br>Which solution will enable this change?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd values for the rcuCount and wcuCount parameters to the Mappings section of the template. Configure DynamoDB to provision throughput capacity using the stack's mappings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd values for two Number parameters, rcuCount and wcuCount, to the template. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd values for the rcuCount and wcuCount parameters as outputs of the template. Configure DynamoDB to provision throughput capacity using the stack outputs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd values for the rcuCount and wcuCount parameters to the Mappings section of the template. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-08T15:13:00.000Z",
        "voteCount": 1,
        "content": "I will say 'B' too . the key for me to decide is the word 'number' since he wcu and rcu are numeric values.Not sure if my interpretation is correct ,but  , will go with B"
      },
      {
        "date": "2022-06-12T05:08:00.000Z",
        "voteCount": 3,
        "content": "A and D are out, parameters are useful when you know the  value for me mapping are like constant in any program language we can use them for Region, Accounts, AZ, etc and the question says \"...allocate independently variable read and write capacity...\" the word variable is key for me here.\n\nC- is out Output is for importing values into other stack.\n\nB is correct  Use the optional&nbsp;Parameters&nbsp;section to customize your templates. \nParameters enable you to input custom values to your template each time you create or update a stack. from https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\n\n\n\nD -"
      },
      {
        "date": "2022-05-21T11:40:00.000Z",
        "voteCount": 1,
        "content": "\"The organization want to modify the template in order to allocate independently variable read and write capacity units to future tables.\" this is the vital sentence in the question. Only Option is Parameter. Option B"
      },
      {
        "date": "2022-04-30T07:37:00.000Z",
        "voteCount": 1,
        "content": "Add values for two Number parameters, rcuCount and wcuCount, to the template. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters."
      },
      {
        "date": "2021-12-02T09:07:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2021-10-29T09:37:00.000Z",
        "voteCount": 1,
        "content": "I got this Question in exam."
      },
      {
        "date": "2021-10-28T15:24:00.000Z",
        "voteCount": 1,
        "content": "definetly A"
      },
      {
        "date": "2021-10-20T09:25:00.000Z",
        "voteCount": 1,
        "content": "probably B"
      },
      {
        "date": "2021-10-12T10:44:00.000Z",
        "voteCount": 3,
        "content": "Why not A? It is obvious that there are multiple tables and rcu and wcu should be defined for each of them. I think mapping is a must here."
      },
      {
        "date": "2021-10-13T03:57:00.000Z",
        "voteCount": 1,
        "content": "Mapping is not used like this way. Check https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html. We should use Parameter here so that the user could input what RCU/WCU they need when deploying a new stack."
      },
      {
        "date": "2021-10-16T20:26:00.000Z",
        "voteCount": 1,
        "content": "Input parameter and FindInMap\nYou can use an input parameter with the Fn::FindInMap function to refer to a specific value in a map. For example, suppose you have a list of regions and environment types that map to a specific AMI ID. You can select the AMI ID that your stack uses by using an input parameter (EnvironmentType). To determine the region, use the AWS::Region pseudo parameter, which gets the AWS Region in which you create the stack.\nI am not saying that B is not right answer though"
      },
      {
        "date": "2022-12-13T05:39:00.000Z",
        "voteCount": 1,
        "content": "You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html"
      },
      {
        "date": "2021-10-10T09:17:00.000Z",
        "voteCount": 3,
        "content": "Ans: B"
      },
      {
        "date": "2021-10-09T10:41:00.000Z",
        "voteCount": 1,
        "content": "B is my choice"
      },
      {
        "date": "2021-10-03T00:02:00.000Z",
        "voteCount": 3,
        "content": "yes B for dynamic and not pre-determined values"
      },
      {
        "date": "2021-10-01T20:36:00.000Z",
        "voteCount": 4,
        "content": "GOing with B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html"
      },
      {
        "date": "2021-10-01T14:55:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-09-28T04:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-09-24T21:24:00.000Z",
        "voteCount": 1,
        "content": "B or  C, please  Thoughts"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/amazon/view/26804-exam-aws-certified-database-specialty-topic-1-question-83/",
    "body": "A retail company with its main office in New York and another office in Tokyo plans to build a database solution on AWS. The company's main workload consists of a mission-critical application that updates its application data in a data store. The team at the Tokyo office is building dashboards with complex analytical queries using the application data. The dashboards will be used to make buying decisions, so they need to have access to the application data in less than 1 second.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon RDS DB instance deployed in the us-east-1 Region with a read replica instance in the ap-northeast-1 Region. Create an Amazon ElastiCache cluster in the ap-northeast-1 Region to cache application data from the replica to generate the dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon DynamoDB global table in the us-east-1 Region with replication into the ap-northeast-1 Region. Use Amazon QuickSight for displaying dashboard results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon RDS for MySQL DB instance deployed in the us-east-1 Region with a read replica instance in the ap-northeast-1 Region. Have the dashboard application read from the read replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Aurora global database. Deploy the writer instance in the us-east-1 Region and the replica in the ap-northeast-1 Region. Have the dashboard application read from the replica ap-northeast-1 Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T05:49:00.000Z",
        "voteCount": 1,
        "content": "- complex analytical queries\n- less than 1 second\n- 2 Regions\nAmazon Aurora Global Database cross-Region replication latencies below 1 second, recovery point objective (RPO) of 1 second and a recovery time objective (RTO) of less than 1 minute."
      },
      {
        "date": "2023-01-29T03:07:00.000Z",
        "voteCount": 1,
        "content": "Use an Amazon Aurora global database."
      },
      {
        "date": "2022-11-23T04:11:00.000Z",
        "voteCount": 4,
        "content": "Surely D :\nGlobal Database uses storage-based replication with typical latency of less than 1 second, using dedicated infrastructure that leaves your database fully available to serve application workloads.\nhttps://aws.amazon.com/rds/aurora/global-database/"
      },
      {
        "date": "2022-04-30T14:41:00.000Z",
        "voteCount": 1,
        "content": "Use an Amazon Aurora global database."
      },
      {
        "date": "2021-11-02T00:12:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\n\nDynamo DB ==&gt;&gt; No direct analytical queries (No joins)."
      },
      {
        "date": "2021-10-31T21:36:00.000Z",
        "voteCount": 2,
        "content": "D - Aurora Global Database - The solution needs to support complex analytical queries. Which eliminates Dynamodb from the equation."
      },
      {
        "date": "2021-10-26T09:25:00.000Z",
        "voteCount": 3,
        "content": "option B is not possible since Quicksight cannot talk with DynamoDB and hence only possible option is D"
      },
      {
        "date": "2021-10-24T03:53:00.000Z",
        "voteCount": 3,
        "content": "D final answer"
      },
      {
        "date": "2021-10-22T07:13:00.000Z",
        "voteCount": 3,
        "content": "Ans: D"
      },
      {
        "date": "2021-10-18T12:24:00.000Z",
        "voteCount": 1,
        "content": "why it's not A, I can see A and D make sense !"
      },
      {
        "date": "2021-10-26T19:58:00.000Z",
        "voteCount": 1,
        "content": "Because they have asked for 1 sec replication lag."
      },
      {
        "date": "2021-10-14T15:28:00.000Z",
        "voteCount": 4,
        "content": "I would go with D. Covers cross region 1 sec latency and complex query"
      },
      {
        "date": "2021-10-11T03:52:00.000Z",
        "voteCount": 2,
        "content": "D - \"This means that committed transactional changes from the writer are replicated globally to the Regions that you select, typically within 1 second.\" -- based on link below"
      },
      {
        "date": "2021-10-09T00:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nComplex query and analytics is key to use RDS, not DynamoDB"
      },
      {
        "date": "2021-10-09T22:19:00.000Z",
        "voteCount": 2,
        "content": "**editing: \nAnswer is for Aurora (D) - which is Relational ready for complex queries"
      },
      {
        "date": "2021-10-08T20:44:00.000Z",
        "voteCount": 3,
        "content": "D. Quicksight don't support Dynamodb"
      },
      {
        "date": "2021-10-19T15:57:00.000Z",
        "voteCount": 2,
        "content": "I dont agree with you, see https://aws.amazon.com/blogs/database/how-to-perform-advanced-analytics-and-build-visualizations-of-your-amazon-dynamodb-data-by-using-amazon-athena/"
      },
      {
        "date": "2021-12-23T20:08:00.000Z",
        "voteCount": 1,
        "content": "That blog suggests to export data from DynamoDB to S3 first and use Athena to read from S3.  This process is slow.  In this question, the dashboard requires an instant access to the application's data. \n\nAnswer is D"
      },
      {
        "date": "2021-10-06T22:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-09-26T06:12:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-09-28T15:09:00.000Z",
        "voteCount": 2,
        "content": "Changing to B\nhttps://aws.amazon.com/blogs/database/aurora-postgresql-disaster-recovery-solutions-using-amazon-aurora-global-database/"
      },
      {
        "date": "2021-10-03T16:37:00.000Z",
        "voteCount": 2,
        "content": "Sorry I meant D"
      },
      {
        "date": "2021-10-03T19:47:00.000Z",
        "voteCount": 2,
        "content": "1 sec is the key.. dyanmodb takes 2000ms for replication"
      },
      {
        "date": "2021-09-21T04:15:00.000Z",
        "voteCount": 1,
        "content": "Going with B, less than 1 second replication required."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/amazon/view/25211-exam-aws-certified-database-specialty-topic-1-question-84/",
    "body": "A company is using Amazon RDS for PostgreSQL. The Security team wants all database connection requests to be logged and retained for 180 days. The RDS for PostgreSQL DB instance is currently using the default parameter group. A Database Specialist has identified that setting the log_connections parameter to 1 will enable connections logging.<br>Which combination of steps should the Database Specialist take to meet the logging and retention requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the log_connections parameter in the default parameter group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom parameter group, update the log_connections parameter, and associate the parameter with the DB instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable publishing of database engine logs to Amazon CloudWatch Logs and set the event expiration to 180 days\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable publishing of database engine logs to an Amazon S3 bucket and set the lifecycle policy to 180 days",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the RDS PostgreSQL host and update the log_connections parameter in the postgresql.conf file"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T02:40:00.000Z",
        "voteCount": 12,
        "content": "B&amp;C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html#USER_LogAccess.PostgreSQL.log_retention_period\nTo set the retention period for system logs, use the rds.log_retention_period parameter. You can find rds.log_retention_period in the DB parameter group associated with your DB instance. The unit for this parameter is minutes. For example, a setting of 1,440 retains logs for one day. The default value is 4,320 (three days). The maximum value is 10,080 (seven days). Your instance must have enough allocated storage to contain the retained log files.\nTo retain older logs, publish them to Amazon CloudWatch Logs. For more information, see Publishing PostgreSQL Logs to CloudWatch Logs.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html#USER_LogAccess.PostgreSQL.PublishtoCloudWatchLogs"
      },
      {
        "date": "2023-09-16T16:17:00.000Z",
        "voteCount": 1,
        "content": "BC are correct.\n\nA is wrong, can not change default param group\nE is wrong, can not update postgresql.conf file on RDS, maybe can do on EC2 ?"
      },
      {
        "date": "2023-03-29T05:39:00.000Z",
        "voteCount": 1,
        "content": "you can not alter default parameter group , i read the documentation of rds and there is no integration to export logs to s3"
      },
      {
        "date": "2023-03-20T12:17:00.000Z",
        "voteCount": 1,
        "content": "Clearly B&amp;C as per this AWS URL - https://aws.amazon.com/premiumsupport/knowledge-center/track-failed-login-rds-postgresql/"
      },
      {
        "date": "2023-03-08T15:20:00.000Z",
        "voteCount": 1,
        "content": "Why not B&amp;D . There is no need to analyze the logs in Cloudwatch.They have to be retained in 180 days. Isn't storing data in S3 cheaper than storing them in Cloudwatch ?"
      },
      {
        "date": "2023-03-04T23:56:00.000Z",
        "voteCount": 1,
        "content": "To meet the logging and retention requirements, the following steps should be taken:\n\nUpdate the log_connections parameter in the default parameter group to enable connection logging.\n\nEnable publishing of database engine logs to Amazon CloudWatch Logs and set the event expiration to 180 days. This will allow the logging of all database connection requests to be retained for 180 days."
      },
      {
        "date": "2022-10-26T09:48:00.000Z",
        "voteCount": 1,
        "content": "why A,E? this is misleading there is no way you can edit postgresql.conf in RDS... correct is B&amp;C"
      },
      {
        "date": "2022-04-30T18:32:00.000Z",
        "voteCount": 2,
        "content": "B. Create a custom parameter group, update the log_connections parameter, and associate the parameter with the DB instance\n\nC. Enable publishing of database engine logs to Amazon CloudWatch Logs and set the event expiration to 180 days"
      },
      {
        "date": "2022-01-23T03:50:00.000Z",
        "voteCount": 3,
        "content": "B and C.\n\nA wrong:\nParameter groups\nEach Amazon RDS PostgreSQL instance is associated with a parameter group that contains the engine specific configurations. The engine configurations also include several parameters that control PostgreSQL logging behavior. AWS provides the parameter groups with default configuration settings to use for your instances. However, to change the default settings, you must create a clone of the default parameter group, modify it, and attach it to your instance.\n\nTo set logging parameters for a DB instance, set the parameters in a DB parameter group and associate that parameter group with the DB instance. For more information, see Working with DB parameter groups."
      },
      {
        "date": "2021-11-06T07:18:00.000Z",
        "voteCount": 1,
        "content": "B and C for sure."
      },
      {
        "date": "2021-11-05T23:22:00.000Z",
        "voteCount": 2,
        "content": "BC final answer"
      },
      {
        "date": "2021-10-30T09:26:00.000Z",
        "voteCount": 4,
        "content": "Ans: BC"
      },
      {
        "date": "2021-10-29T17:02:00.000Z",
        "voteCount": 1,
        "content": "B and C"
      },
      {
        "date": "2021-10-11T20:22:00.000Z",
        "voteCount": 2,
        "content": "Yes B&amp;C"
      },
      {
        "date": "2021-10-11T04:43:00.000Z",
        "voteCount": 1,
        "content": "BC correct answer"
      },
      {
        "date": "2021-10-04T11:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is B and C"
      },
      {
        "date": "2021-09-30T19:25:00.000Z",
        "voteCount": 2,
        "content": "agreed with B&amp;C\nexport logs to Cloudwatch Logs"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/amazon/view/26224-exam-aws-certified-database-specialty-topic-1-question-85/",
    "body": "A Database Specialist is creating a new Amazon Neptune DB cluster, and is attempting to load data from Amazon S3 into the Neptune DB cluster using the<br>Neptune bulk loader API. The Database Specialist receives the following error:<br>`Unable to connect to s3 endpoint. Provided source = s3://mybucket/graphdata/ and region = us-east-1. Please verify your<br>S3 configuration.`<br>Which combination of actions should the Database Specialist take to troubleshoot the problem? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that Amazon S3 has an IAM role granting read access to Neptune",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that an Amazon S3 VPC endpoint exists\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that a Neptune VPC endpoint exists",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that Amazon EC2 has an IAM role granting read access to Amazon S3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that Neptune has an IAM role granting read access to Amazon S3\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T15:58:00.000Z",
        "voteCount": 16,
        "content": "B &amp; E   https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html"
      },
      {
        "date": "2021-10-09T17:41:00.000Z",
        "voteCount": 8,
        "content": "BE \nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html\n\u201cAn IAM role for the Neptune DB instance to assume that has an IAM policy that allows access to the data files in the S3 bucket. The policy must grant Read and List permissions.\u201d\n\u201cAn Amazon S3 VPC endpoint. For more information, see the Creating an Amazon S3 VPC Endpoint section.\u201d"
      },
      {
        "date": "2023-09-18T12:11:00.000Z",
        "voteCount": 2,
        "content": "A. Neptune wants to read from S3, so S3 needs to provide read access.\nB. Yes, we can make use of an Amazon S3 VPC endpoint to access data.\nx C. There's nothing like a Neptune VPC endpoint.\nx D. We're not dealing with EC2 here.\nx E. Neptune wants to read from S3, not he other way around."
      },
      {
        "date": "2023-09-12T23:03:00.000Z",
        "voteCount": 1,
        "content": "B. Check that an Amazon S3 VPC endpoint exists\nE. Check that Neptune has an IAM role granting read access to Amazon S3\n\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html"
      },
      {
        "date": "2022-12-20T14:44:00.000Z",
        "voteCount": 2,
        "content": "My final answer"
      },
      {
        "date": "2022-05-01T08:33:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html\nB. Check that an Amazon S3 VPC endpoint exists\nE. Check that Neptune has an IAM role granting read access to Amazon S3\nAn IAM role for the Neptune DB instance to assume that has an IAM policy that allows access to the data files in the S3 bucket. The policy must grant Read and List permissions."
      },
      {
        "date": "2022-03-05T21:19:00.000Z",
        "voteCount": 3,
        "content": "BE are the correct answers\n- S3 VPC endpoint has to exist\n- Neptune must have the IAM role and policies with access to S3 bucket"
      },
      {
        "date": "2022-01-23T03:57:00.000Z",
        "voteCount": 2,
        "content": "B and E\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html"
      },
      {
        "date": "2021-11-06T02:25:00.000Z",
        "voteCount": 1,
        "content": "BE final answer"
      },
      {
        "date": "2021-11-04T12:52:00.000Z",
        "voteCount": 4,
        "content": "Guys this question was asked in my exam that I passed. B and E are correct."
      },
      {
        "date": "2021-10-31T16:26:00.000Z",
        "voteCount": 2,
        "content": "Ans: BE"
      },
      {
        "date": "2021-10-25T09:43:00.000Z",
        "voteCount": 1,
        "content": "send me a note ryan23680 at yahoo for new aws database questions to find the correct answers."
      },
      {
        "date": "2021-10-25T06:14:00.000Z",
        "voteCount": 1,
        "content": "Question-121\nA company's database specialist disabled TLS on an Amazon DocumentDB cluster to perform benchmarking tests. A few days after this change was implemented, a database specialist trainee accidentally deleted multiple tables. The database specialist restored the database from available snapshots. An hour after restoring the cluster. the database specialist is still unable to connect to the new cluster endpoint. What should the database specialist do to connect to the new. restored Amazon DocumentDB cluster?\nA. Change the restored cluster's parameter group to the original cluster's custom parameter group.\nB. Change the restored cluster's parameter group to the Amazon DocumentDB default parameter group.\nC. Configure the interface VPC endpoint and associate the new Amazon DocumentDB cluster.\nD. Run the synclnstances command in AWS DataSync."
      },
      {
        "date": "2021-10-27T09:27:00.000Z",
        "voteCount": 2,
        "content": "What is the answer for this? A?"
      },
      {
        "date": "2021-10-18T01:14:00.000Z",
        "voteCount": 1,
        "content": "B and E my choice"
      },
      {
        "date": "2021-10-13T02:23:00.000Z",
        "voteCount": 2,
        "content": "Ans is B&amp;E"
      },
      {
        "date": "2021-10-11T03:42:00.000Z",
        "voteCount": 2,
        "content": "Answer is BE"
      },
      {
        "date": "2021-10-08T18:01:00.000Z",
        "voteCount": 2,
        "content": "B and E here"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/amazon/view/48806-exam-aws-certified-database-specialty-topic-1-question-86/",
    "body": "A database specialist manages a critical Amazon RDS for MySQL DB instance for a company. The data stored daily could vary from .01% to 10% of the current database size. The database specialist needs to ensure that the DB instance storage grows as needed.<br>What is the MOST operationally efficient and cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure RDS Storage Auto Scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure RDS instance Auto Scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance allocated storage to meet the forecasted requirements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor the Amazon CloudWatch FreeStorageSpace metric daily and add storage as required."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-29T05:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling"
      },
      {
        "date": "2022-04-29T10:17:00.000Z",
        "voteCount": 2,
        "content": "Storage auto scaling"
      },
      {
        "date": "2022-03-12T02:34:00.000Z",
        "voteCount": 2,
        "content": "There's no Amazon RDS for MySQL DB instance autoscaling service (it's only available for Aurora). And it wouldn't solve the storage issue."
      },
      {
        "date": "2022-02-23T17:36:00.000Z",
        "voteCount": 2,
        "content": "agree with other comments"
      },
      {
        "date": "2022-01-22T07:38:00.000Z",
        "voteCount": 2,
        "content": "Cost Effective"
      },
      {
        "date": "2021-12-27T01:48:00.000Z",
        "voteCount": 3,
        "content": "Ans: A\nFrom the AWS Console, you can only see storage section with autoscaling."
      },
      {
        "date": "2021-12-01T15:39:00.000Z",
        "voteCount": 4,
        "content": "Answer is A.\nIf your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage.\n\nhttps://aws.amazon.com/about-aws/whats-new/2019/06/rds-storage-auto-scaling/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling"
      },
      {
        "date": "2021-11-02T14:52:00.000Z",
        "voteCount": 4,
        "content": "A. Configure RDS Storage Auto Scaling.\n\nhttps://aws.amazon.com/vi/about-aws/whats-new/2019/06/rds-storage-auto-scaling/\n\"operationally efficient and cost-effective\"\nRDS Storage Auto Scaling automatically scales storage capacity in response to growing database workloads, with zero downtime.\nThere is no additional cost for RDS Storage Auto Scaling."
      },
      {
        "date": "2021-10-30T16:24:00.000Z",
        "voteCount": 1,
        "content": "A is correct ans."
      },
      {
        "date": "2021-10-21T23:01:00.000Z",
        "voteCount": 1,
        "content": "Ans.A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling"
      },
      {
        "date": "2021-10-13T23:33:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      },
      {
        "date": "2021-10-03T12:25:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A. \nhttps://aws.amazon.com/vi/about-aws/whats-new/2019/06/rds-storage-auto-scaling/"
      },
      {
        "date": "2021-09-27T04:51:00.000Z",
        "voteCount": 3,
        "content": "A. Answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/amazon/view/49114-exam-aws-certified-database-specialty-topic-1-question-87/",
    "body": "A company is due for renewing its database license. The company wants to migrate its 80 TB transactional database system from on-premises to the AWS Cloud.<br>The migration should incur the least possible downtime on the downstream database applications. The company's network infrastructure has limited network bandwidth that is shared with other applications.<br>Which solution should a database specialist use for a timely migration?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a full backup of the source database to AWS Snowball Edge appliances and ship them to be loaded to Amazon S3. Use AWS DMS to migrate change data capture (CDC) data from the source database to Amazon S3. Use a second AWS DMS task to migrate all the S3 data to the target database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a full backup of the source database to AWS Snowball Edge appliances and ship them to be loaded to Amazon S3. Periodically perform incremental backups of the source database to be shipped in another Snowball Edge appliance to handle syncing change data capture (CDC) data from the source to the target database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to migrate the full load of the source database over a VPN tunnel using the internet for its primary connection. Allow AWS DMS to handle syncing change data capture (CDC) data from the source to the target database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to migrate the full load of the source database over a VPN tunnel using the internet for its primary connection. Allow AWS SCT to handle syncing change data capture (CDC) data from the source to the target database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-17T06:18:00.000Z",
        "voteCount": 11,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\nUsing Amazon S3 as a target for AWS Database Migration Service\nA"
      },
      {
        "date": "2021-10-27T10:07:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"
      },
      {
        "date": "2022-04-30T12:40:00.000Z",
        "voteCount": 5,
        "content": "-C and D out because of VPN Tunnel\n-B is out for multiple snowballs\n\nA. 80 TB = 1 Snowball -&gt; S3 -&gt; DMS -&gt; CDC -&gt; Target"
      },
      {
        "date": "2023-09-12T23:37:00.000Z",
        "voteCount": 2,
        "content": "A. 80 TB = 1 Snowball -&gt; S3 -&gt; DMS -&gt; CDC -&gt; Target"
      },
      {
        "date": "2022-11-23T04:24:00.000Z",
        "voteCount": 1,
        "content": "A for sure. 2nd snowball device not required for CDC."
      },
      {
        "date": "2022-04-13T05:38:00.000Z",
        "voteCount": 3,
        "content": "80TB -&gt; Snowball, ongoing changes rep with DMS"
      },
      {
        "date": "2022-03-05T18:30:00.000Z",
        "voteCount": 4,
        "content": "A is the correct Answer"
      },
      {
        "date": "2022-02-02T07:35:00.000Z",
        "voteCount": 3,
        "content": "I'm taking B here as the company network bandwidth is pretty much redundant for use in the migration."
      },
      {
        "date": "2021-10-23T10:22:00.000Z",
        "voteCount": 2,
        "content": "Ans: A"
      },
      {
        "date": "2021-10-17T23:17:00.000Z",
        "voteCount": 3,
        "content": "A final answer"
      },
      {
        "date": "2021-10-09T22:54:00.000Z",
        "voteCount": 2,
        "content": "You need snowball edge here. Of A and B only A makes more sense."
      },
      {
        "date": "2021-10-01T08:29:00.000Z",
        "voteCount": 1,
        "content": "A is for me."
      },
      {
        "date": "2021-09-21T01:41:00.000Z",
        "voteCount": 2,
        "content": "My answer is A ,  D is incorrect SCT  is for schemas  and doesn't support CDC"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/amazon/view/48781-exam-aws-certified-database-specialty-topic-1-question-88/",
    "body": "A database specialist is responsible for an Amazon RDS for MySQL DB instance with one read replica. The DB instance and the read replica are assigned to the default parameter group. The database team currently runs test queries against a read replica. The database team wants to create additional tables in the read replica that will only be accessible from the read replica to benefit the tests.<br>Which should the database specialist do to allow the database team to create the test tables?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContact AWS Support to disable read-only mode on the read replica. Reboot the read replica. Connect to the read replica and create the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the read_only parameter to false (read_only=0) in the default parameter group of the read replica. Perform a reboot without failover. Connect to the read replica and create the tables using the local_only MySQL option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the read_only parameter to false (read_only=0) in the default parameter group. Reboot the read replica. Connect to the read replica and create the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DB parameter group. Change the read_only parameter to false (read_only=0). Associate the read replica with the new group. Reboot the read replica. Connect to the read replica and create the tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-23T10:49:00.000Z",
        "voteCount": 7,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-read-replica/"
      },
      {
        "date": "2023-09-14T00:34:00.000Z",
        "voteCount": 2,
        "content": "D. Create new DB parameter group --&gt; change parameter value --&gt;  associate replica with new param group --&gt; reboot replica -- &gt; connect to read replica then create tables."
      },
      {
        "date": "2023-06-19T07:39:00.000Z",
        "voteCount": 1,
        "content": "Option D, as default parameter group cannot be modified."
      },
      {
        "date": "2023-02-22T13:41:00.000Z",
        "voteCount": 1,
        "content": "Any reason , its not B . https://aws.amazon.com/premiumsupport/knowledge-center/rds-read-replica/ . The replica has to be rebooted without a failover option . However, I do not understand what it mseans 'local_only MySQL'"
      },
      {
        "date": "2023-04-27T20:10:00.000Z",
        "voteCount": 3,
        "content": "wont be B becoz Default groups cannot be modified"
      },
      {
        "date": "2022-06-01T12:31:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2022-05-06T20:31:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2022-04-28T18:46:00.000Z",
        "voteCount": 4,
        "content": "read_only in default parameter group can not be changed (possible in custom parameter group)\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-read-replica/"
      },
      {
        "date": "2021-12-14T05:28:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2021-10-28T07:59:00.000Z",
        "voteCount": 1,
        "content": "This Q was asked in my exam."
      },
      {
        "date": "2021-10-25T18:36:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer due to default parameter changes are not allowed"
      },
      {
        "date": "2021-10-22T02:37:00.000Z",
        "voteCount": 1,
        "content": "D is the correct ans."
      },
      {
        "date": "2021-10-13T09:03:00.000Z",
        "voteCount": 1,
        "content": "D final answer"
      },
      {
        "date": "2021-10-12T02:04:00.000Z",
        "voteCount": 3,
        "content": "D For me"
      },
      {
        "date": "2021-10-11T23:37:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2021-10-02T16:27:00.000Z",
        "voteCount": 1,
        "content": "D is for me"
      },
      {
        "date": "2021-09-23T03:51:00.000Z",
        "voteCount": 3,
        "content": "I think D? \n\nDefault parameter group cannot be changed."
      },
      {
        "date": "2021-09-23T04:12:00.000Z",
        "voteCount": 1,
        "content": "Sorry B."
      },
      {
        "date": "2021-09-30T02:18:00.000Z",
        "voteCount": 1,
        "content": "Sorry C: - Do not think need to adjust anything like what is said in B: \n* We can edit default parameter group (no need of extra step of creating additional parameter group)"
      },
      {
        "date": "2021-10-02T00:06:00.000Z",
        "voteCount": 3,
        "content": "Sorry again, default parameter cannot be changed.\nD: answer. \n\n No way to delete the comments, otherwise it will make it easy to update."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/amazon/view/48680-exam-aws-certified-database-specialty-topic-1-question-89/",
    "body": "A company has a heterogeneous six-node production Amazon Aurora DB cluster that handles online transaction processing (OLTP) for the core business and<br>OLAP reports for the human resources department. To match compute resources to the use case, the company has decided to have the reporting workload for the human resources department be directed to two small nodes in the Aurora DB cluster, while every other workload goes to four large nodes in the same DB cluster.<br>Which option would ensure that the correct nodes are always available for the appropriate workload while meeting these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the writer endpoint for OLTP and the reader endpoint for the OLAP reporting workload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse automatic scaling for the Aurora Replica to have the appropriate number of replicas for the desired workload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate additional readers to cater to the different scenarios.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse custom endpoints to satisfy the different workloads.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T18:28:00.000Z",
        "voteCount": 8,
        "content": "D - Custom endpoints"
      },
      {
        "date": "2024-01-13T05:55:00.000Z",
        "voteCount": 1,
        "content": "D - custom endpoints can provide this flexibility"
      },
      {
        "date": "2022-11-29T06:40:00.000Z",
        "voteCount": 1,
        "content": "Use custom endpoints"
      },
      {
        "date": "2022-05-01T11:04:00.000Z",
        "voteCount": 3,
        "content": "D. Use custom endpoints to satisfy the different workloads.\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-custom-endpoints/\n\nyou can now create custom endpoints for Amazon Aurora databases. This allows you to distribute and load balance workloads across different sets of database instances in your Aurora cluster."
      },
      {
        "date": "2022-03-05T18:37:00.000Z",
        "voteCount": 1,
        "content": "What does \" heterogeneous six-node production Amazon Aurora DB cluster \" even mean? I've never heard of any Aurora infrastructure described this way!!"
      },
      {
        "date": "2022-05-20T02:22:00.000Z",
        "voteCount": 3,
        "content": "heterogeneous means the six nodes are of different size."
      },
      {
        "date": "2021-10-26T01:35:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D"
      },
      {
        "date": "2021-10-20T04:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct as per link:\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-custom-endpoints/\nou can now create custom endpoints for Amazon Aurora databases. This allows you to distribute and load balance workloads across different sets of database instances in your Aurora cluster.\n\nFor example, you may provision a set of Aurora Replicas to use an instance type with higher memory capacity in order to run an analytics workload. A custom endpoint can then help you route the analytics workload to these appropriately-configured instances, while keeping other instances in your cluster isolated from this workload. As you add or remove instances from the custom endpoint to match your workload, the endpoint helps spread the load around."
      },
      {
        "date": "2021-10-17T15:05:00.000Z",
        "voteCount": 1,
        "content": "D is right choice. Custom endpoints. OLTP can only work with master instance endpoint. OLAP reports could work with either master instance/any combination of reader endpoints"
      },
      {
        "date": "2021-10-16T07:35:00.000Z",
        "voteCount": 3,
        "content": "D final answer"
      },
      {
        "date": "2021-09-22T20:54:00.000Z",
        "voteCount": 3,
        "content": "Answer D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/amazon/view/48681-exam-aws-certified-database-specialty-topic-1-question-90/",
    "body": "Developers have requested a new Amazon Redshift cluster so they can load new third-party marketing data. The new cluster is ready and the user credentials are given to the developers. The developers indicate that their copy jobs fail with the following error message:<br>`Amazon Invalid operation: S3ServiceException:Access Denied,Status 403,Error AccessDenied.`<br>The developers need to load this data soon, so a database specialist must act quickly to solve this issue.<br>What is the MOST secure solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role with the same user name as the Amazon Redshift developer user ID. Provide the IAM role with read-only access to Amazon S3 with the assume role action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role with read-only access to the Amazon S3 bucket and include the assume role action. Modify the Amazon Redshift cluster to add the IAM role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role with read-only access to the Amazon S3 bucket with the assume role action. Add this role to the developer IAM user ID used for the copy job that ended with an error message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM user with access keys and a new role with read-only access to the Amazon S3 bucket. Add this role to the Amazon Redshift cluster. Change the copy job to use the access keys created."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T06:08:00.000Z",
        "voteCount": 6,
        "content": "Answer B"
      },
      {
        "date": "2023-12-02T07:31:00.000Z",
        "voteCount": 1,
        "content": "Why is C not correct?"
      },
      {
        "date": "2024-01-01T21:09:00.000Z",
        "voteCount": 1,
        "content": "Because the COPY from S3 will be done by Redshift Cluster"
      },
      {
        "date": "2023-11-05T15:27:00.000Z",
        "voteCount": 1,
        "content": "Why it is not D ? It seems to be a question about Unload-Copy operation. The COPY operation requires access keys and secret keys."
      },
      {
        "date": "2022-04-29T19:45:00.000Z",
        "voteCount": 4,
        "content": "B.  new IAM role with read-only access to the Amazon S3 bucket and include the assume role action. \nModify the Amazon Redshift cluster to add the IAM role.\n\n\nProvide authentication for your cluster to access Amazon S3 on your behalf to load the sample data. You provide authentication by referencing the IAM role that you created and set as the default for your cluster"
      },
      {
        "date": "2022-02-16T15:22:00.000Z",
        "voteCount": 3,
        "content": "B\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-sample-db.html\nProvide authentication for your cluster to access Amazon S3 on your behalf to load the sample data. You provide authentication by referencing the IAM role that you created and set as the default for your cluster"
      },
      {
        "date": "2021-11-30T03:36:00.000Z",
        "voteCount": 3,
        "content": "Option B"
      },
      {
        "date": "2021-11-06T22:10:00.000Z",
        "voteCount": 1,
        "content": "B. To best protect your sensitive data and safeguard your AWS access credentials, we recommend creating an IAM role and attaching it to your cluster. For more information about providing access permissions, see Permissions to access other AWS resources.\n\nIn this step, you create a new IAM role that allows Amazon Redshift to load data from Amazon S3 buckets. An IAM role is an IAM identity that you can create in your account that has specific permissions. In the next step, you attach the role to your cluster"
      },
      {
        "date": "2021-10-29T08:35:00.000Z",
        "voteCount": 2,
        "content": "anyone here already cleared the exam ?\nHow much %age Q we will get in real exam, any idea ?"
      },
      {
        "date": "2021-10-27T06:02:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-an-iam-role.html\n\"Now that you have created the new role, your next step is to attach it to your cluster. You can attach the role when you launch a new cluster or you can attach it to an existing cluster. In the next step, you attach the role to a new cluster.\""
      },
      {
        "date": "2021-10-19T18:50:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B. Good practice is to create an IAM Role with read only permission to S3 and attach this role to Redshift cluster for COPY jobs from S3 to Redshift cluster"
      },
      {
        "date": "2021-10-02T15:31:00.000Z",
        "voteCount": 1,
        "content": "B\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-an-iam-role.html"
      },
      {
        "date": "2021-10-03T09:19:00.000Z",
        "voteCount": 3,
        "content": "Role-based access control\nhttps://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html"
      },
      {
        "date": "2021-09-29T14:42:00.000Z",
        "voteCount": 1,
        "content": "C is the correct one. You cannot modify the redshift cluster to add IAM role to it. But you can add the role to the user who's trying to get access to S3"
      },
      {
        "date": "2021-09-30T15:33:00.000Z",
        "voteCount": 5,
        "content": "you can modify the redshift cluster to associal an IAM role. C is not correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/amazon/view/49602-exam-aws-certified-database-specialty-topic-1-question-91/",
    "body": "A database specialist at a large multi-national financial company is in charge of designing the disaster recovery strategy for a highly available application that is in development. The application uses an Amazon DynamoDB table as its data store. The application requires a recovery time objective (RTO) of 1 minute and a recovery point objective (RPO) of 2 minutes.<br>Which operationally efficient disaster recovery strategy should the database specialist recommend for the DynamoDB table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a DynamoDB stream that is processed by an AWS Lambda function that copies the data to a DynamoDB table in another Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a DynamoDB global table replica in another Region. Enable point-in-time recovery for both tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a DynamoDB Accelerator table in another Region. Enable point-in-time recovery for the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Backup plan and assign the DynamoDB table as a resource."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T11:06:00.000Z",
        "voteCount": 16,
        "content": "B DynamoDB global tables"
      },
      {
        "date": "2021-11-01T09:33:00.000Z",
        "voteCount": 6,
        "content": "BBBBBBBBBBB\n\nglobal tables are multi master replication enabled solution.\nit meets the requirement."
      },
      {
        "date": "2023-08-06T06:50:00.000Z",
        "voteCount": 2,
        "content": "- A single Amazon DynamoDB global table can only have one replica table per AWS Region.\n- You can enable point-in-time recovery on each replica of a global table.\nhttps://aws.amazon.com/dynamodb/global-tables/?nc1=h_ls"
      },
      {
        "date": "2022-12-20T16:07:00.000Z",
        "voteCount": 1,
        "content": "Only logical answer.  Backups take too long to restore. Global tables are synchronously updated"
      },
      {
        "date": "2022-07-02T09:24:00.000Z",
        "voteCount": 2,
        "content": "It is asking for DR soulution. My vote is for  A . If you need less RPO and RTO setup Streams and Kinesis, Lambda and  you can achive the given RPO RTO, \nRPO is 5 mins in usual automated backups .  B could be true but in global tables also RPO is 5 mins"
      },
      {
        "date": "2022-10-12T16:37:00.000Z",
        "voteCount": 2,
        "content": "B is correct. Global table is multi master &amp; it is sync up with few sec"
      },
      {
        "date": "2022-05-31T10:42:00.000Z",
        "voteCount": 2,
        "content": "Two-minute recovery point objective (RPO)."
      },
      {
        "date": "2022-04-30T09:45:00.000Z",
        "voteCount": 4,
        "content": "x A. Create a DynamoDB stream that is processed by an AWS Lambda function that copies the data to a DynamoDB table in another Region. (good if B is wrong)\nB. Use a DynamoDB global table replica in another Region. Enable point-in-time recovery for both tables. (PITR is distraction btw PITR takes 5 min recovery, but global table will auto failover to good region, right?)\nx C. DynamoDB Accelerator (its for read caching)\nx D. Create an AWS Backup plan and assign the DynamoDB table as a resource. (takes 1 hour)"
      },
      {
        "date": "2022-04-28T00:36:00.000Z",
        "voteCount": 2,
        "content": "B because of RTO and RPO limits defined. D is good solution doesn't fit limits. DynamoDB Accellerator is distractor for DR question."
      },
      {
        "date": "2022-01-22T10:56:00.000Z",
        "voteCount": 1,
        "content": "A, B and C are not DR solutions for Dynamo DB. Global Tables is multi region replication.\nD meets the objective . You can set up scheduled backups for Amazon DynamoDB using AWS Backup.. https://aws.amazon.com/blogs/database/set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/"
      },
      {
        "date": "2022-03-05T13:33:00.000Z",
        "voteCount": 5,
        "content": "to get RPO of 2  minutes you will have to create a backup plan that backs up &lt; 2 minutes!! Thats not what backup is for"
      },
      {
        "date": "2021-12-27T20:20:00.000Z",
        "voteCount": 2,
        "content": "I will go to B.\nhttps://s3.amazonaws.com/solutions-reference/multi-region-application-architecture/latest/multi-region-application-architecture.pdf"
      },
      {
        "date": "2021-11-16T05:19:00.000Z",
        "voteCount": 4,
        "content": "A is the answer. \nD is wrong. Restore takes nearly 1 hour https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html\nB is wrong. RPO is about 5 mins.\nA is true. If you need less RPO and RTO setup Streams and Kinesis, Lambda whatever"
      },
      {
        "date": "2021-11-14T04:45:00.000Z",
        "voteCount": 1,
        "content": "The question requires 2 min RPO.  Using PITR RPO is about 5 min. I think D makes more sense here"
      },
      {
        "date": "2021-11-16T05:06:00.000Z",
        "voteCount": 1,
        "content": "But restore is \"less than one hour\" aws says https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html"
      },
      {
        "date": "2021-11-03T19:58:00.000Z",
        "voteCount": 2,
        "content": "BBBBB https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html"
      },
      {
        "date": "2021-10-10T10:06:00.000Z",
        "voteCount": 4,
        "content": "B final answer"
      },
      {
        "date": "2021-10-23T05:36:00.000Z",
        "voteCount": 1,
        "content": "HI Aesthet, How are you sure about this? :) Thanks!"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/amazon/view/48683-exam-aws-certified-database-specialty-topic-1-question-92/",
    "body": "A small startup company is looking to migrate a 4 TB on-premises MySQL database to AWS using an Amazon RDS for MySQL DB instance.<br>Which strategy would allow for a successful migration with the LEAST amount of downtime?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new RDS for MySQL DB instance and configure it for access from the on-premises data center. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucket. Import the snapshot into the DB instance utilizing the MySQL utilities running on an Amazon EC2 instance. Immediately point the application to the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data center. Use the mysqldump utility to create a snapshot of the on-premises MySQL server. Copy the snapshot into the EC2 instance and restore it into the EC2 MySQL instance. Use AWS DMS to migrate data into a new RDS for MySQL DB instance. Point the application to the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data center. Use the mysqldump utility to create a snapshot of the on-premises MySQL server. Copy the snapshot into an Amazon S3 bucket and import the snapshot into a new RDS for MySQL DB instance using the MySQL utilities running on an EC2 instance. Point the application to the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new RDS for MySQL DB instance and configure it for access from the on-premises data center. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucket. Import the snapshot into the DB instance using the MySQL utilities running on an Amazon EC2 instance. Establish replication into the new DB instance using MySQL replication. Stop application access to the on-premises MySQL server and let the remaining transactions replicate over. Point the application to the DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T13:46:00.000Z",
        "voteCount": 5,
        "content": "on-premises MySQL server -&gt; mysqldump utility -&gt; snapshot -&gt; copy to S3 -&gt;MySQL Utility on EC2 -&gt; import to AWS RDS MySQL -&gt; Establish replication into the new DB instance using MySQL replication -&gt; Stop application access to the on-premises MySQL server -&gt;  let the remaining transactions replicate over. -&gt; Point the application to the DB instance."
      },
      {
        "date": "2023-08-08T13:06:00.000Z",
        "voteCount": 1,
        "content": "It says RDS so B, C (EC2 option) is out."
      },
      {
        "date": "2023-06-10T10:25:00.000Z",
        "voteCount": 1,
        "content": "I feel correct answer is B as DMS is involved"
      },
      {
        "date": "2022-12-20T16:24:00.000Z",
        "voteCount": 2,
        "content": "Was a toss between B&amp;D. DMS task threw me off but I don't see how you can get snapshot to an EC2 instance from option B"
      },
      {
        "date": "2022-11-28T10:36:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B\nI don't get why so many people chose D. How is extra copy to S3 bucket reduce downtime?"
      },
      {
        "date": "2022-12-20T16:23:00.000Z",
        "voteCount": 1,
        "content": "Installing new SQL software on an EC2 just for the sake of migration threw me off.  Getting that much data through an s3 gateway endpoint would get the data from on-prem to the VPC but directly on to an EC2 instance seems ridic to me.  So answer is D"
      },
      {
        "date": "2021-10-16T06:07:00.000Z",
        "voteCount": 3,
        "content": "D looks correct as per this liink:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.NonRDSRepl.html"
      },
      {
        "date": "2021-10-14T00:29:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDDD"
      },
      {
        "date": "2021-10-16T01:04:00.000Z",
        "voteCount": 1,
        "content": "Ignore all EC2 based answers - method of elimination -  left with D"
      },
      {
        "date": "2021-10-08T13:25:00.000Z",
        "voteCount": 2,
        "content": "D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.External.Repl.html"
      },
      {
        "date": "2021-10-04T22:45:00.000Z",
        "voteCount": 2,
        "content": "D: Ans"
      },
      {
        "date": "2021-09-23T12:15:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/amazon/view/48684-exam-aws-certified-database-specialty-topic-1-question-93/",
    "body": "A software development company is using Amazon Aurora MySQL DB clusters for several use cases, including development and reporting. These use cases place unpredictable and varying demands on the Aurora DB clusters, and can cause momentary spikes in latency. System users run ad-hoc queries sporadically throughout the week. Cost is a primary concern for the company, and a solution that does not require significant rework is needed.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new Aurora Serverless DB clusters for development and reporting, then migrate to these new DB clusters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade one of the DB clusters to a larger size, and consolidate development and reporting activities on this larger DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse existing DB clusters and stop/start the databases on a routine basis using scheduling tools.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the DB clusters to the burstable instance family."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T15:33:00.000Z",
        "voteCount": 15,
        "content": "Check https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html - t2 and t3 (8vCPU 32GB RAM) series are mostly weaker than r4/r5 (96 vCPU 768GB RAM) series. Changing from r to t series is not a good idea. Furthermore, \"unpredictable\" indicates \"Aurora Serverless\". Hence I will go with A."
      },
      {
        "date": "2022-04-29T12:29:00.000Z",
        "voteCount": 7,
        "content": "\"unexpected and variable demands, adhoc searched\" =&gt; \"Aurora Serverless\""
      },
      {
        "date": "2023-09-19T11:45:00.000Z",
        "voteCount": 1,
        "content": "Skipping B+c as distractors.\nTorn between A+D -\nD is not applicable as \"we recommend using the [burstable] T DB instance classes only for development, test, or other nonproduction servers\".\nSwitching to burstable classes is not an option here for PROD reporting.\nHence A.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.BestPractices.html#AuroraMySQL.BestPractices.T2Medium"
      },
      {
        "date": "2023-01-29T16:52:00.000Z",
        "voteCount": 4,
        "content": "Question says without significant rework. Option A would require migration. So option D is correct. \nhttps://aws.amazon.com/rds/instance-types/"
      },
      {
        "date": "2022-12-20T16:31:00.000Z",
        "voteCount": 1,
        "content": "A - Serverless will require moving existing DB although you are saving on cost.  \nD. Can respond to traffic spikes, requires very little change and the costs for the spikes won't be much because it isn't too frequent. The burst credits can be used."
      },
      {
        "date": "2022-03-13T05:20:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/rds/aurora/serverless/\n\nAmazon Aurora Serverless is a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads."
      },
      {
        "date": "2022-02-24T14:19:00.000Z",
        "voteCount": 1,
        "content": "ad-hoc searches = Serverless\n\nhttps://aws.amazon.com/rds/aurora/faqs/?nc=sn&amp;loc=6\nQ: Can I migrate an existing Aurora DB cluster to Aurora Serverless?\nYes, you can restore a snapshot taken from an existing Aurora provisioned cluster into an Aurora Serverless DB Cluster (and vice versa)."
      },
      {
        "date": "2022-02-16T09:42:00.000Z",
        "voteCount": 2,
        "content": "D \nA is good if the cluster experience no activities which is not the case"
      },
      {
        "date": "2022-01-10T11:53:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\nThe company's key concern is cost, and a solution that does not need extensive rework is required. Converting Aurora Cluster to Serverless does not require extensive rework."
      },
      {
        "date": "2021-12-26T08:31:00.000Z",
        "voteCount": 2,
        "content": "\"resulting in brief latency spikes\"  Answer is D"
      },
      {
        "date": "2021-11-13T15:54:00.000Z",
        "voteCount": 2,
        "content": "If the key concern is the cost, the right option is Aurora Serverless.. Answer is A"
      },
      {
        "date": "2021-11-05T18:09:00.000Z",
        "voteCount": 5,
        "content": "D less rework - A is too much work"
      },
      {
        "date": "2021-10-19T18:59:00.000Z",
        "voteCount": 2,
        "content": "Answer: D\n\n**does not require significant rework is needed**"
      },
      {
        "date": "2021-10-16T13:04:00.000Z",
        "voteCount": 2,
        "content": "D. no significant work."
      },
      {
        "date": "2021-10-14T06:01:00.000Z",
        "voteCount": 1,
        "content": "A. Aurora Serverless is most saving cost. You pay what you query"
      },
      {
        "date": "2021-10-07T18:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. Serverless aurora cluster can be cost effective in the varying workload and dev/test environment where services are not required 24 * 7"
      },
      {
        "date": "2021-10-05T13:52:00.000Z",
        "voteCount": 3,
        "content": "A final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/amazon/view/48808-exam-aws-certified-database-specialty-topic-1-question-94/",
    "body": "A database specialist is building a system that uses a static vendor dataset of postal codes and related territory information that is less than 1 GB in size. The dataset is loaded into the application's cache at start up. The company needs to store this data in a way that provides the lowest cost with a low application startup time.<br>Which approach will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon RDS DB instance. Shut down the instance once the data has been read.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora Serverless. Allow the service to spin resources up and down, as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB in on-demand capacity mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 and load the data from flat files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-28T05:37:00.000Z",
        "voteCount": 7,
        "content": "D, key words \" static vendor dataset\" &amp; \"lowest cost\""
      },
      {
        "date": "2024-01-13T05:16:00.000Z",
        "voteCount": 1,
        "content": "D gives the lower cost. It also meets the requirements - S3 can do tons of IOPS when needed. Most importantly, the data is loaded on startup and then used by the app in-memory, not used constantly - so there is no need for DynamoDB."
      },
      {
        "date": "2023-08-06T07:49:00.000Z",
        "voteCount": 2,
        "content": "DynamoDB in on-demand capacity mode price:\n - DynamoDB Standard table class, Read Request Units (RRU) - $0.25 per  MILLION read request units.\n - Data storage - First 25 GB stored per month is free using the DynamoDB Standard table class.\n- On-demand backup - Cold Backup Storage*\t$0.03 per GB-month (The information is static, you can do without a backup).\nhttps://aws.amazon.com/dynamodb/pricing/on-demand/\nAmazon S3 pricing:\nS3 Standard - General purpose storage for any type of data, typically used for frequently accessed data. First 50 TB / Month -\t$0.023 per GB\nhttps://aws.amazon.com/s3/pricing/\nApplication does not start so often, I would choose answer C. DynamoDB will give this information much faster than reading from S3. \nThe price of $0.25 per MILLION read request is cheaper than S3."
      },
      {
        "date": "2023-06-23T03:33:00.000Z",
        "voteCount": 2,
        "content": "the lowest cost &amp; a low application startup time -&gt; Dynamo DB with OnDemand capacity mode. \nOption D  (S3) will cause longer startup time in my opinion"
      },
      {
        "date": "2022-12-26T09:08:00.000Z",
        "voteCount": 1,
        "content": "simple option"
      },
      {
        "date": "2022-12-20T16:40:00.000Z",
        "voteCount": 2,
        "content": "Static small files.  S3 is your best bet for costs"
      },
      {
        "date": "2022-04-30T12:05:00.000Z",
        "voteCount": 4,
        "content": "x A. RDS (costly for 1 GB data)\nx B. Amazon Aurora Serverless. (costly for 1 GB data)\nx C. Use Amazon DynamoDB in on-demand capacity mode.\n (postal codes and associated territorial data that is less than 1 GB in size =&gt; key value data, \nminimizing application launch time =&gt; dynamoDB, but caching is required at application and not database)\n\nD. Use Amazon S3 and load the data from flat files.  (most cost-effective)"
      },
      {
        "date": "2021-11-07T01:53:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      },
      {
        "date": "2021-11-02T10:12:00.000Z",
        "voteCount": 1,
        "content": "Let us see in this way, 1 GB file on boot uploading in to application time &lt; (1 GB file from s3 to Application + load time). So definitely it is not D, I will go with B"
      },
      {
        "date": "2021-10-25T16:39:00.000Z",
        "voteCount": 1,
        "content": "I agree with D based on the link below:\nhttps://www.sumologic.com/insight/s3-cost-optimization/\nFor example, for 1 GB file stored on S3 with 1 TB of storage provisioned, you are billed for 1 GB only. In a lot of other services such as Amazon EC2, Amazon Elastic Block Storage (Amazon EBS) and Amazon DynamoDB you pay for provisioned capacity. For example, in the case of Amazon EBS disk you pay for the size of 1 TB of disk even if you just save 1 GB file. This makes managing S3 cost easier than many other services including Amazon EBS and Amazon EC2. On S3 there is no risk of over-provisioning and no need to manage disk utilization."
      },
      {
        "date": "2021-10-25T13:24:00.000Z",
        "voteCount": 2,
        "content": "DDDDDDDDDDD. lowest cost. \n\n--&gt; this says Application cache and NOT DB cache.\n\napplication\u05d2\u20ac\u2122s cache at start up"
      },
      {
        "date": "2021-10-24T18:57:00.000Z",
        "voteCount": 2,
        "content": "OR D is also a possibility"
      },
      {
        "date": "2021-10-22T12:58:00.000Z",
        "voteCount": 1,
        "content": "I will go with B. Will cover lowesr cost."
      },
      {
        "date": "2021-10-20T13:35:00.000Z",
        "voteCount": 2,
        "content": "C or D\nI choose D"
      },
      {
        "date": "2021-10-02T12:20:00.000Z",
        "voteCount": 1,
        "content": "C: ?\nlowest start up time asked"
      },
      {
        "date": "2021-10-16T04:08:00.000Z",
        "voteCount": 6,
        "content": "May be D:\nApplication Cache may be enough. Load from S3 straight into Application Cache"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/amazon/view/48686-exam-aws-certified-database-specialty-topic-1-question-95/",
    "body": "A database specialist needs to review and optimize an Amazon DynamoDB table that is experiencing performance issues. A thorough investigation by the database specialist reveals that the partition key is causing hot partitions, so a new partition key is created. The database specialist must effectively apply this new partition key to all existing and new data.<br>How can this solution be implemented?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to export the data from the current DynamoDB table to Amazon S3. Then use Amazon EMR again to import the data from Amazon S3 into a new DynamoDB table with the new partition key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to copy the data from the current DynamoDB table to Amazon S3. Then import the DynamoDB table to create a new DynamoDB table with the new partition key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to update the DynamoDB table and modify the partition key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to back up the DynamoDB table. Then use the restore-table-from-backup command and modify the partition key."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T12:13:00.000Z",
        "voteCount": 9,
        "content": "A. Answer\nUse EMR to copy to S3 and use EMR to create new table."
      },
      {
        "date": "2021-10-15T01:49:00.000Z",
        "voteCount": 5,
        "content": "AAAA for me as well. \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\n\nEMR to backup and restore"
      },
      {
        "date": "2023-06-05T07:03:00.000Z",
        "voteCount": 3,
        "content": "A. Use Amazon EMR to export the data from the current DynamoDB table to Amazon S3. Then use Amazon EMR again to import the data from Amazon S3 into a new DynamoDB table with the new partition key. Most Voted\nxB. DynamoDB is not a valid source for AWS DMS\nxC. The partition key cannot be modified\nxD. Although you can choose to restore a DynamoDB table without the secondary indexes, the partition key cannot be modifier during restore"
      },
      {
        "date": "2023-03-05T00:24:00.000Z",
        "voteCount": 1,
        "content": "To apply the new partition key to all existing and new data in a DynamoDB table, the AWS CLI can be used to update the table and modify the partition key"
      },
      {
        "date": "2022-12-21T10:58:00.000Z",
        "voteCount": 2,
        "content": "I believe DMS works with DynamoDB and is a simpler solution compared to EMR. Dynamodb can be a source for DMS.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html"
      },
      {
        "date": "2022-12-28T19:56:00.000Z",
        "voteCount": 1,
        "content": "Dynamodb can be a target , but not a source -- Hence A"
      },
      {
        "date": "2022-07-09T22:15:00.000Z",
        "voteCount": 5,
        "content": "Is it possible to change partition key in DynamoDB? No. Once the table is setup, you cannot modify its Key Schema. You can only provision a new table, move data there, and then remove the first table"
      },
      {
        "date": "2022-04-29T07:58:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\n\nDynamoDB to S3 is possible by EMR, AWS Glue, or AWS Data Pipeline."
      },
      {
        "date": "2022-02-23T16:49:00.000Z",
        "voteCount": 3,
        "content": "DynamoDB cannot be a source for DMS. \n\nPer -https://dynobase.com/dynamodb-keys/\nIs it possible to change partition key in DynamoDB?\nNo. Once the table is setup, you cannot modify its Key Schema. You can only provision a new table, move data there, and then remove the first table.\n\n\nPer - https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\nTo customize the process of creating backups, you can use use Amazon EMR, AWS Glue, or AWS Data Pipeline.\n\nSo, using EMR is the only valid option."
      },
      {
        "date": "2022-02-23T15:17:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://docs.aws.amazon.com/cli/latest/reference/dynamodb/restore-table-from-backup.html"
      },
      {
        "date": "2022-03-06T06:50:00.000Z",
        "voteCount": 1,
        "content": "this will recreate the same partition key"
      },
      {
        "date": "2021-10-13T10:11:00.000Z",
        "voteCount": 3,
        "content": "A, other options are not possible"
      },
      {
        "date": "2021-09-28T23:25:00.000Z",
        "voteCount": 2,
        "content": "I'll go with B. \nrestore-table-from-backup doesn't seem to be able to change partition key, not sure here"
      },
      {
        "date": "2021-11-12T04:00:00.000Z",
        "voteCount": 4,
        "content": "DynamoDB doesnt seem to be a source for DMS. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/amazon/view/48687-exam-aws-certified-database-specialty-topic-1-question-96/",
    "body": "A company is going through a security audit. The audit team has identified cleartext master user password in the AWS CloudFormation templates for Amazon<br>RDS for MySQL DB instances. The audit team has flagged this as a security risk to the database team.<br>What should a database specialist do to mitigate this risk?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange all the databases to use AWS IAM for authentication and remove all the cleartext passwords in CloudFormation templates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Secrets Manager resource to generate a random password and reference the secret in the CloudFormation template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the passwords from the CloudFormation templates so Amazon RDS prompts for the password when the database is being created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the passwords from the CloudFormation template and store them in a separate file. Replace the passwords by running CloudFormation using a sed command."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-11T11:46:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-04-29T19:18:00.000Z",
        "voteCount": 4,
        "content": "Use an AWS Secrets Manager resource to generate a random password and reference the secret in the CloudFormation template."
      },
      {
        "date": "2022-02-24T19:46:00.000Z",
        "voteCount": 3,
        "content": "secret manager to the rescue!"
      },
      {
        "date": "2022-01-31T11:19:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2021-11-30T03:28:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2021-11-03T09:53:00.000Z",
        "voteCount": 1,
        "content": "B no doubt"
      },
      {
        "date": "2021-10-30T07:42:00.000Z",
        "voteCount": 3,
        "content": "Answer: B\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/securing-passwords-in-aws-quick-starts-using-aws-secrets-manager/\nSaving a password in a clear text file is not a secure practice.\n\nToday, I want to discuss how you can store secrets in Secrets Manager via AWS CloudFormation. Then I\u2019ll show, using code examples, how to retrieve secrets."
      },
      {
        "date": "2021-10-28T19:16:00.000Z",
        "voteCount": 2,
        "content": "Even though B works, A will also work as it will generate token for login. Also there is no info if rotation of paassowrd is required. I ma not sure between A &amp; B."
      },
      {
        "date": "2022-07-29T06:51:00.000Z",
        "voteCount": 1,
        "content": "It is not necessary to state in the question that the rotation of passwords for services accounts, such as master user, it is a best practice."
      },
      {
        "date": "2021-10-06T12:50:00.000Z",
        "voteCount": 2,
        "content": "B final answer"
      },
      {
        "date": "2021-09-29T09:03:00.000Z",
        "voteCount": 4,
        "content": "B. Ans"
      },
      {
        "date": "2021-09-20T09:22:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/amazon/view/48688-exam-aws-certified-database-specialty-topic-1-question-97/",
    "body": "A company's database specialist disabled TLS on an Amazon DocumentDB cluster to perform benchmarking tests. A few days after this change was implemented, a database specialist trainee accidentally deleted multiple tables. The database specialist restored the database from available snapshots. An hour after restoring the cluster, the database specialist is still unable to connect to the new cluster endpoint.<br>What should the database specialist do to connect to the new, restored Amazon DocumentDB cluster?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the restored cluster's parameter group to the original cluster's custom parameter group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the restored cluster's parameter group to the Amazon DocumentDB default parameter group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the interface VPC endpoint and associate the new Amazon DocumentDB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the syncInstances command in AWS DataSync."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T05:04:00.000Z",
        "voteCount": 9,
        "content": "Option A"
      },
      {
        "date": "2023-08-30T23:19:00.000Z",
        "voteCount": 1,
        "content": "A. Change the restored cluster's parameter group to the original cluster's custom parameter group."
      },
      {
        "date": "2023-08-30T23:19:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/documentdb/latest/developerguide/security.encryption.ssl.html"
      },
      {
        "date": "2023-06-10T10:42:00.000Z",
        "voteCount": 1,
        "content": "A looks good, as TLS was disabled"
      },
      {
        "date": "2023-06-02T07:39:00.000Z",
        "voteCount": 3,
        "content": "There is no Amazon DocumentDB VPC endpoint, open console and check. Amazon DocumentDB belongs in a VPC and the service endpoints are created with it."
      },
      {
        "date": "2023-04-02T16:19:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT:\nIf the restored Amazon DocumentDB cluster endpoint is not accessible, the database specialist should check the cluster's security group settings to ensure that the appropriate inbound rules are configured to allow incoming connections.\n\nTherefore, none of the options presented in the question would solve the connectivity issue.\n\nA and B refer to the cluster's parameter group, which does not affect connectivity to the cluster endpoint. Parameter groups are used to configure database engine settings."
      },
      {
        "date": "2023-04-02T16:19:00.000Z",
        "voteCount": 1,
        "content": "C suggests configuring the interface VPC endpoint and associating the new Amazon DocumentDB cluster. However, this option is not relevant to the issue at hand as it relates to network connectivity within the VPC.\n\nD suggests using AWS DataSync to run the syncInstances command, which is also not relevant to this issue. DataSync is a data transfer service, and running the syncInstances command does not solve connectivity issues to an Amazon DocumentDB cluster.\n\nTherefore, the database specialist should verify the security group rules for the Amazon DocumentDB cluster to ensure that the necessary inbound rules are configured to allow incoming connections"
      },
      {
        "date": "2022-12-24T07:21:00.000Z",
        "voteCount": 4,
        "content": "Option A because context is TLS was disabled to perform benchmark testing so a new parameter group different from the default was created. As the default is to have TLS enabled, it will not be possible to connect to a new restored database with default settings so the original, previous custom parameter group should be assigned to it."
      },
      {
        "date": "2022-12-21T11:10:00.000Z",
        "voteCount": 1,
        "content": "Can someone explain why C isn't the answer? I understand why option A could be an answer but why not C?"
      },
      {
        "date": "2023-06-04T13:00:00.000Z",
        "voteCount": 1,
        "content": "There is no Amazon DocumentDB VPC endpoint, open console and check. Amazon DocumentDB belongs in a VPC and the service endpoints are created with it."
      },
      {
        "date": "2022-04-30T13:25:00.000Z",
        "voteCount": 2,
        "content": "A. Change the restored cluster's parameter group to the original cluster's custom parameter group.\n\nTrick question: You can't modify the parameter settings of the default parameter groups.\nYou can use a DB parameter group to act as a container for engine configuration values that are applied to one or more DB instances."
      },
      {
        "date": "2021-10-29T04:49:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2021-10-22T15:43:00.000Z",
        "voteCount": 4,
        "content": "A\nTrick question: You can't modify the parameter settings of the default parameter groups.\nYou can use a DB parameter group to act as a container for engine configuration values that are applied to one or more DB instances.\n\nIf you create a DB instance without specifying a DB parameter group, the DB instance uses a default DB parameter group. Each default DB parameter group contains database engine defaults and Amazon RDS system defaults. You can't modify the parameter settings of a default parameter group. Instead, you create your own parameter group where you choose your own parameter settings. Not all DB engine parameters can be changed in a parameter group that you create."
      },
      {
        "date": "2021-10-20T03:53:00.000Z",
        "voteCount": 1,
        "content": "AAAA is looking good"
      },
      {
        "date": "2021-10-16T10:07:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/amazon/view/49202-exam-aws-certified-database-specialty-topic-1-question-98/",
    "body": "A company runs a customer relationship management (CRM) system that is hosted on-premises with a MySQL database as the backend. A custom stored procedure is used to send email notifications to another system when data is inserted into a table. The company has noticed that the performance of the CRM system has decreased due to database reporting applications used by various teams. The company requires an AWS solution that would reduce maintenance, improve performance, and accommodate the email notification feature.<br>Which AWS solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MySQL running on an Amazon EC2 instance with Auto Scaling to accommodate the reporting applications. Configure a stored procedure and an AWS Lambda function that uses Amazon SES to send email notifications to the other system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora MySQL in a multi-master cluster to accommodate the reporting applications. Configure Amazon RDS event subscriptions to publish a message to an Amazon SNS topic and subscribe the other system's email address to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MySQL running on an Amazon EC2 instance with a read replica to accommodate the reporting applications. Configure Amazon SES integration to send email notifications to the other system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora MySQL with a read replica for the reporting applications. Configure a stored procedure and an AWS Lambda function to publish a message to an Amazon SNS topic. Subscribe the other system's email address to the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T03:32:00.000Z",
        "voteCount": 8,
        "content": "D. Answer\n\nReduce maintenance, hosting database on EC2 increases maintenance."
      },
      {
        "date": "2021-10-07T05:26:00.000Z",
        "voteCount": 1,
        "content": "Did you take the real test @shantest1? Are the questions here valid if you know by any chance."
      },
      {
        "date": "2023-08-30T23:38:00.000Z",
        "voteCount": 2,
        "content": "D. Use Amazon Aurora MySQL with a read replica for the reporting applications. Configure a stored procedure and an AWS Lambda function to publish a message to an Amazon SNS topic. Subscribe the other system's email address to the topic."
      },
      {
        "date": "2023-08-30T23:39:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html"
      },
      {
        "date": "2022-04-28T18:13:00.000Z",
        "voteCount": 2,
        "content": "reporting =&gt; read replica"
      },
      {
        "date": "2022-03-07T04:36:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it). D is correct"
      },
      {
        "date": "2021-12-10T09:32:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2021-11-06T19:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D"
      },
      {
        "date": "2021-11-06T09:28:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDDDD is my choice"
      },
      {
        "date": "2021-10-26T23:14:00.000Z",
        "voteCount": 2,
        "content": "D final answer"
      },
      {
        "date": "2021-10-07T13:54:00.000Z",
        "voteCount": 2,
        "content": "RDS event subscriptions do not cover \"data is inserted into a table\" - see https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_Events.Messages.html\nWe can use stored procedure to invoke Lambda function - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html\nSo I will go with B"
      },
      {
        "date": "2021-10-10T09:35:00.000Z",
        "voteCount": 1,
        "content": "Typo. I mean D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/amazon/view/48689-exam-aws-certified-database-specialty-topic-1-question-99/",
    "body": "A company needs to migrate Oracle Database Standard Edition running on an Amazon EC2 instance to an Amazon RDS for Oracle DB instance with Multi-AZ.<br>The database supports an ecommerce website that runs continuously. The company can only provide a maintenance window of up to 5 minutes.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Oracle Real Application Clusters (RAC) on the EC2 instance and the RDS DB instance. Update the connection string to point to the RAC cluster. Once the EC2 instance and RDS DB instance are in sync, fail over from Amazon EC2 to Amazon RDS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Oracle database from the EC2 instance using Oracle Data Pump and perform an import into Amazon RDS. Stop the application for the entire process. When the import is complete, change the database connection string and then restart the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS DMS with the EC2 instance as the source and the RDS DB instance as the destination. Stop the application when the replication is in sync, change the database connection string, and then restart the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS DataSync with the EC2 instance as the source and the RDS DB instance as the destination. Stop the application when the replication is in sync, change the database connection string, and then restart the application."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T06:01:00.000Z",
        "voteCount": 13,
        "content": "C is most plausible option even though it does not mention change data capture or CDC. Rest of the steps seem legit in option C. No other given alternative would ideally meet the 5 min downtime requirement."
      },
      {
        "date": "2022-03-07T04:08:00.000Z",
        "voteCount": 10,
        "content": "this question came up in my exam. I chose (C).  I passed the exam -  also fairly certain this is the right answer. (I'll add this in as many questions as possible)"
      },
      {
        "date": "2023-08-30T23:57:00.000Z",
        "voteCount": 1,
        "content": "C. Configure AWS DMS with the EC2 instance as the source and the RDS DB instance as the destination. Stop the application when the replication is in sync, change the database connection string, and then restart the application."
      },
      {
        "date": "2023-08-06T08:20:00.000Z",
        "voteCount": 1,
        "content": "According to \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-oracle-database-to-amazon-rds-for-oracle.html\nB and C could be the answer\n- AWS Database Migration Service (AWS DMS);\n- Native Oracle tools.\nBut B says \"Stop the application for the entire process\", while we have \"a maintenance window of up to 5 minutes\".\nSo, the answer is C."
      },
      {
        "date": "2022-12-21T11:27:00.000Z",
        "voteCount": 1,
        "content": "C is the most plausible"
      },
      {
        "date": "2022-04-29T10:40:00.000Z",
        "voteCount": 4,
        "content": "x B. export import would take long time\nC. EC2 -&gt; DMS -&gt; new RDS -&gt; Stop when in sync. \nChange db connections"
      },
      {
        "date": "2022-02-24T11:36:00.000Z",
        "voteCount": 2,
        "content": "C as DMS enables fastest failover"
      },
      {
        "date": "2022-02-23T16:42:00.000Z",
        "voteCount": 2,
        "content": "C: Correct \nB - Invalid as the exp/imp can take long time and there is only 5 min downtime"
      },
      {
        "date": "2021-12-27T02:01:00.000Z",
        "voteCount": 2,
        "content": "Ans:C\nOracle SE only support single thread export/import. Not possible to finish within 5 minutes."
      },
      {
        "date": "2021-11-24T13:10:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-11-12T05:07:00.000Z",
        "voteCount": 3,
        "content": "AWS Question with DMS, solution is Oracle tool, are you serious? Answer is C"
      },
      {
        "date": "2021-11-02T09:29:00.000Z",
        "voteCount": 2,
        "content": "CCCCCC -- B is a big no even for a small database would take more than 5 mins.  Stopping the application for the entire process etc"
      },
      {
        "date": "2021-11-01T22:08:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C"
      },
      {
        "date": "2021-10-28T00:34:00.000Z",
        "voteCount": 1,
        "content": "only possible choices B and C. \n\nB is wrong -- datapump import method takes longer time to migrate. it can't be done in 5 minutes - forget about it. \n\nMy choice is C."
      },
      {
        "date": "2021-10-20T05:58:00.000Z",
        "voteCount": 3,
        "content": "C final answer"
      },
      {
        "date": "2021-09-28T02:33:00.000Z",
        "voteCount": 1,
        "content": "Answer B: acle Data Pump technology enables very high-speed movement of data and metadata from one database to another. Oracle Data Pump is available only on Oracle Database 10g release 1 (10.1) and later. ... Moving Data Between Different Database Versions. Original Export and Import Versus Data Pump Export and Import."
      },
      {
        "date": "2021-10-24T15:45:00.000Z",
        "voteCount": 3,
        "content": "Option B could be correct if it didn't mention 'Stop the application for the entire process'"
      },
      {
        "date": "2021-09-27T17:35:00.000Z",
        "voteCount": 4,
        "content": "Option C is correct.\nOption B is incorrect as it says application needs to be stopped till entire migration got completed"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/amazon/view/48690-exam-aws-certified-database-specialty-topic-1-question-100/",
    "body": "A company is using Amazon Aurora PostgreSQL for the backend of its application. The system users are complaining that the responses are slow. A database specialist has determined that the queries to Aurora take longer during peak times. With the Amazon RDS Performance Insights dashboard, the load in the chart for average active sessions is often above the line that denotes maximum CPU usage and the wait state shows that most wait events are IO:XactSync.<br>What should the company do to resolve these performance issues?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Aurora Replica to scale the read traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale up the DB instance class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify applications to commit transactions in batches.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify applications to avoid conflicts by taking locks."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-19T01:15:00.000Z",
        "voteCount": 12,
        "content": "C is answer. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html. Check frank's link."
      },
      {
        "date": "2021-10-06T12:20:00.000Z",
        "voteCount": 11,
        "content": "C\nhttps://blog.dbi-services.com/aws-aurora-xactsync-batch-commit/"
      },
      {
        "date": "2024-01-01T23:54:00.000Z",
        "voteCount": 1,
        "content": "IO:XactSync with high CPU wait - means DB load exceed allocated vCPUs, so you should reduce those workloads or scale up to higher CPUs\nBut if the IO:XactSync is due to high commit, then you should modify your application to commit transactions in batches"
      },
      {
        "date": "2023-12-02T19:00:00.000Z",
        "voteCount": 1,
        "content": "Answer seems C since IO:Xactsync is about commit/rollback.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\nBatch commit seems to be the answer."
      },
      {
        "date": "2023-12-02T18:57:00.000Z",
        "voteCount": 3,
        "content": "Why not A? Add Aurora replica."
      },
      {
        "date": "2023-08-22T12:42:00.000Z",
        "voteCount": 1,
        "content": "B. Scale up the DB instance class.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html#apg-waits.xactsync.actions.scalecpu"
      },
      {
        "date": "2023-08-06T08:38:00.000Z",
        "voteCount": 2,
        "content": "According to \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\nwe should \n- Scale up the CPU;\n- Reduce the number of commits.\nThere are 2 correct answers: B and C. I don't know who to give preference to :("
      },
      {
        "date": "2023-06-23T03:51:00.000Z",
        "voteCount": 1,
        "content": "Hard choice between B&amp;C, but i select B. \n AWS recommends in documentation ( https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html ) to Scale up the CPU or Reduce the number of commits.\nModify application will require some time -&gt; therefore Scaling up will help to resolve current issues with CPU"
      },
      {
        "date": "2023-06-11T15:35:00.000Z",
        "voteCount": 1,
        "content": "By default in PostgreSQL, single-row inserts auto-commit for each row. This means that the database must wait for durability (write to storage) for every insert. To improve performance, PostgreSQL supports multi-row inserts and disabling auto-commit."
      },
      {
        "date": "2023-06-11T15:15:00.000Z",
        "voteCount": 1,
        "content": "C\nBy default in PostgreSQL, single-row inserts auto-commit for each row. This means that the database must wait for durability (write to storage) for every insert. To improve performance, PostgreSQL supports multi-row inserts and disabling auto-commit."
      },
      {
        "date": "2023-05-11T11:27:00.000Z",
        "voteCount": 1,
        "content": "B\nIf CPU would not be an issue then definitely C, since a high CPU then it effects writing to the Storage layer as well.\n\nCPU pressure\nA heavy workload might be preventing the Aurora storage daemon from getting sufficient CPU time.\n\nTo address CPU starvation issues, consider changing to an instance type with more CPU capacity"
      },
      {
        "date": "2023-06-11T15:08:00.000Z",
        "voteCount": 1,
        "content": "why CPU?\n\nIO:XactSync. PDF RSS. The IO:XactSync event occurs when the database is waiting for the Aurora storage subsystem to acknowledge the commit of a regular transaction, or the commit or rollback of a prepared transaction."
      },
      {
        "date": "2023-06-11T15:14:00.000Z",
        "voteCount": 1,
        "content": "By default in PostgreSQL, single-row inserts auto-commit for each row. This means that the database must wait for durability (write to storage) for every insert. To improve performance, PostgreSQL supports multi-row inserts and disabling auto-commit."
      },
      {
        "date": "2023-02-12T10:29:00.000Z",
        "voteCount": 2,
        "content": "After reading the comments C is the right answer"
      },
      {
        "date": "2023-01-29T17:12:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html#apg-waits.xactsync.actions.commits"
      },
      {
        "date": "2023-03-19T09:35:00.000Z",
        "voteCount": 2,
        "content": "vote for you!\nC"
      },
      {
        "date": "2022-12-24T07:38:00.000Z",
        "voteCount": 3,
        "content": "(with typos fixed)\nMy answer is B. Despite we know that CPU is related to it as well as COMMIT frequency is high, tell me honestly what would be the fastest/easiest approach here to resolve the issue? Scale up the instance (B) or Modify an application to commit in batches (C).\nSecond, this post by Franck Pachot has plenty info which basically leds to CPU issue: https://www.dbi-services.com/blog/aws-aurora-xactsync-batch-commit/"
      },
      {
        "date": "2023-01-07T03:53:00.000Z",
        "voteCount": 2,
        "content": "There is no ask about fastest/easiest approach here. so as a company the best approach to be taken here which is fixing the performance issue in the code. you can't keep increasing the hardware forever without fixing the code first!"
      },
      {
        "date": "2022-12-24T07:36:00.000Z",
        "voteCount": 1,
        "content": "My answer is B. Despite we know that CPU is related to it as well as COMMIT frequency is high, tell me honestly what would be the fastest/easiest approach here to resolve the issue? Scale up the instance (B) or Modify an application to commit in batches (C). \nSecond, this post by Frank Pachot has planty info which basically leds to CPU issue: https://www.dbi-services.com/blog/aws-aurora-xactsync-batch-commit/"
      },
      {
        "date": "2022-11-06T14:18:00.000Z",
        "voteCount": 2,
        "content": "C IS CORRECT"
      },
      {
        "date": "2022-10-26T16:27:00.000Z",
        "voteCount": 2,
        "content": "Answer : C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/amazon/view/50908-exam-aws-certified-database-specialty-topic-1-question-101/",
    "body": "A database specialist deployed an Amazon RDS DB instance in Dev-VPC1 used by their development team. Dev-VPC1 has a peering connection with Dev-VPC2 that belongs to a different development team in the same department. The networking team confirmed that the routing between VPCs is correct; however, the database engineers in Dev-VPC2 are getting a timeout connections error when trying to connect to the database in Dev-VPC1.<br>What is likely causing the timeouts?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe database is deployed in a VPC that is in a different Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe database is deployed in a VPC that is in a different Availability Zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe database is deployed with misconfigured security groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe database is deployed with the wrong client connect timeout configuration."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T13:23:00.000Z",
        "voteCount": 6,
        "content": "C it is."
      },
      {
        "date": "2021-10-08T07:10:00.000Z",
        "voteCount": 6,
        "content": "C final answer"
      },
      {
        "date": "2021-10-23T03:27:00.000Z",
        "voteCount": 5,
        "content": "\"A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html"
      },
      {
        "date": "2023-08-22T12:24:00.000Z",
        "voteCount": 1,
        "content": "C. The database is deployed with misconfigured security groups.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules-reference.html#sg-rules-db-server"
      },
      {
        "date": "2022-04-30T07:05:00.000Z",
        "voteCount": 3,
        "content": "security group needs inbound rule"
      },
      {
        "date": "2022-02-24T20:08:00.000Z",
        "voteCount": 2,
        "content": "Security groups need to be fixed"
      },
      {
        "date": "2021-10-30T02:30:00.000Z",
        "voteCount": 2,
        "content": "CCCCCC"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/amazon/view/49204-exam-aws-certified-database-specialty-topic-1-question-102/",
    "body": "A company has a production environment running on Amazon RDS for SQL Server with an in-house web application as the front end. During the last application maintenance window, new functionality was added to the web application to enhance the reporting capabilities for management. Since the update, the application is slow to respond to some reporting queries.<br>How should the company identify the source of the problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall and configure Amazon CloudWatch Application Insights for Microsoft .NET and Microsoft SQL Server. Use a CloudWatch dashboard to identify the root cause.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable RDS Performance Insights and determine which query is creating the problem. Request changes to the query to address the problem.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS X-Ray deployed with Amazon RDS to track query system traces.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a support request and work with AWS Support to identify the source of the issue."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-09T06:10:00.000Z",
        "voteCount": 7,
        "content": "B is correct\nAmazon RDS Performance Insights is a database performance tuning and monitoring feature that helps you quickly assess the load on your database, and determine when and where to take action. Performance Insights allows non-experts to detect performance problems with an easy-to-understand dashboard that visualizes database load.\nhttps://aws.amazon.com/rds/performance-insights/"
      },
      {
        "date": "2023-09-08T00:26:00.000Z",
        "voteCount": 2,
        "content": "\uff2d\uff21\uff39\uff22\uff25 \uff21"
      },
      {
        "date": "2023-08-22T12:19:00.000Z",
        "voteCount": 1,
        "content": "B. Enable RDS Performance Insights and determine which query is creating the problem. Request changes to the query to address the problem.\n\n\"Performance Insights uses lightweight data collection methods that don\u2019t impact the performance of your applications, and makes it easy to see which SQL statements are causing the load, and why. It requires no configuration or maintenance, and is currently available for Amazon Aurora (PostgreSQL- and MySQL-compatible editions), Amazon RDS for PostgreSQL, MySQL, MariaDB, SQL Server and Oracle.\"\n\"Whether your database performance problem is due to database configuration or application design issues, you can quickly identify the bottleneck and see which SQL statements are contributing to it.\"\n\nhttps://aws.amazon.com/rds/performance-insights/"
      },
      {
        "date": "2023-08-06T14:26:00.000Z",
        "voteCount": 2,
        "content": "Performance Insights to analyze database performance by waits, SQL statements, hosts, or users.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.Overview.ActiveSessions.html\nAmazon CloudWatch Application Insights for Microsoft .NET and Microsoft SQL Server allows you to monitor the health of the SQL server, general issues not related to a specific query. Metrics that are monitored for the SQL Server instance:\n - CPUUtilization\n - SQLServer:Buffer Manager cache hit ratio\n - SQLServer:Locks Number of Deadlocks/sec\n - VolumeQueueLength\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-what-is.html"
      },
      {
        "date": "2023-01-25T09:50:00.000Z",
        "voteCount": 1,
        "content": "I think it can\u00b4t be A because it's an in-house web application, saying that going for B"
      },
      {
        "date": "2022-09-29T08:58:00.000Z",
        "voteCount": 2,
        "content": "i think it should be A: as an Application, insight can provide more details to SQL server"
      },
      {
        "date": "2022-06-28T19:07:00.000Z",
        "voteCount": 3,
        "content": "I think this should be A.\nThe question no where states the slowness is due to query. it states response is slow to certain request for repoting. \n\nCloudWatch Application Insights is For common problems in .NET and SQL application stacks, such as application latency, SQL Server failed backups, memory leaks, large HTTP requests, and canceled I/O operations, it provides additional insights that point to a possible root cause and steps for resolution. Built-in integration with AWS SSM OpsCenter allows you to resolve issues by running the relevant Systems Manager Automation document. \nFirst we need to understand the reason of slow reponse. It could be application issue rather SQL issue where Performance Insight can be helpful"
      },
      {
        "date": "2022-04-30T06:46:00.000Z",
        "voteCount": 2,
        "content": "additional features to improve management reporting capabilities =&gt;  new queries needing Performance Insights to check"
      },
      {
        "date": "2022-02-24T20:03:00.000Z",
        "voteCount": 3,
        "content": "New reports = new queries needing Performance Insights to check"
      },
      {
        "date": "2021-11-14T10:11:00.000Z",
        "voteCount": 3,
        "content": "A \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html \n\" For common problems in .NET and SQL application stacks, such as application latency, SQL Server failed backups, memory leaks, large HTTP requests, and canceled I/O operations, it provides additional insights that point to a possible root cause and steps for resolution\""
      },
      {
        "date": "2021-12-21T12:59:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B.. The issue is not about the infrastructure, it is about the additional features added during with the application maintenance."
      },
      {
        "date": "2021-11-06T17:50:00.000Z",
        "voteCount": 2,
        "content": "New functionality added to update reporting capabilities.  Reports from application are  generated via queries.  These queries need to be investigated via answer which is &gt; B"
      },
      {
        "date": "2022-06-28T19:08:00.000Z",
        "voteCount": 1,
        "content": "Reports are generated by SQL but no where it is mentioned that reporting sql is slow. It says response of reporting request it slow which could due to application issue as well"
      },
      {
        "date": "2021-11-01T15:57:00.000Z",
        "voteCount": 2,
        "content": "B is the answer"
      },
      {
        "date": "2021-10-18T03:08:00.000Z",
        "voteCount": 2,
        "content": "sql server application issue. Does not say ueries. A is the answer."
      },
      {
        "date": "2021-10-25T16:19:00.000Z",
        "voteCount": 1,
        "content": "Problem started due new functionality was added to the web application to enhance the reporting capabilities for management. Since the update, the application is slow to respond to some reporting queries  so  issue is with query performance not with application that points B as correct answer"
      },
      {
        "date": "2021-10-10T05:49:00.000Z",
        "voteCount": 2,
        "content": "BBBBBBBBB"
      },
      {
        "date": "2021-09-24T07:34:00.000Z",
        "voteCount": 3,
        "content": "B final answer"
      },
      {
        "date": "2021-09-21T22:09:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2021-09-19T14:57:00.000Z",
        "voteCount": 4,
        "content": "B. Answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/amazon/view/48997-exam-aws-certified-database-specialty-topic-1-question-103/",
    "body": "An electric utility company wants to store power plant sensor data in an Amazon DynamoDB table. The utility company has over 100 power plants and each power plant has over 200 sensors that send data every 2 seconds. The sensor data includes time with milliseconds precision, a value, and a fault attribute if the sensor is malfunctioning. Power plants are identified by a globally unique identifier. Sensors are identified by a unique identifier within each power plant. A database specialist needs to design the table to support an efficient method of finding all faulty sensors within a given power plant.<br>Which schema should the database specialist use when creating the DynamoDB table to achieve the fastest query time when looking for faulty sensors?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the plant identifier as the partition key and the measurement time as the sort key. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a composite of the plant identifier and sensor identifier as the partition key. Use the measurement time as the sort key. Create a local secondary index (LSI) on the fault attribute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a composite of the plant identifier and sensor identifier as the partition key. Use the measurement time as the sort key. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the plant identifier as the partition key and the sensor identifier as the sort key. Create a local secondary index (LSI) on the fault attribute.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T11:05:00.000Z",
        "voteCount": 18,
        "content": "I am going with D. As I understand it, you can have an item as a partition key and an item as a sort key to make a composite key. However, you cannot have two items as a partition key and a third item as sort key to make a composite key."
      },
      {
        "date": "2022-10-16T21:07:00.000Z",
        "voteCount": 3,
        "content": "you can combine more than one attribute to create a partition key .\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\n\"Use composite attributes. Try to combine more than one attribute to form a unique key, if that meets your access pattern. For example, consider an orders table with customerid#productid#countrycode as the partition key and order_date as the sort key, where the symbol # is used to split different field."
      },
      {
        "date": "2023-04-28T03:13:00.000Z",
        "voteCount": 2,
        "content": "The access pattern for this question is: finding all faulty sensors within a given power plant.\n\nPartition key DOESN'T support using condition expressions - you can only use them in sort key and in filter expressions. If you combine plantID and sensorID in a partition key, how do you find ALL faulty sensors within a single power plant? You simply cannot. With such a partition key, you always have to query for a specific sensor located in a specific power plant. \n\nIf 1 of the answers would be to place pantID and sensorID in sort key, then we could consider this option.\n\nI will go with D."
      },
      {
        "date": "2022-11-19T21:36:00.000Z",
        "voteCount": 2,
        "content": "DynamoDB don't allow us do that way.  we have to merge / combine into as 1 attribute \nSo B, C are wrong"
      },
      {
        "date": "2021-09-24T20:04:00.000Z",
        "voteCount": 8,
        "content": "C. Putting the Fault attribute in a sparse index, based on https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general-sparse-indexes.html"
      },
      {
        "date": "2021-09-25T21:07:00.000Z",
        "voteCount": 1,
        "content": "Agreed C. Answer"
      },
      {
        "date": "2024-01-02T00:36:00.000Z",
        "voteCount": 1,
        "content": "If the answer is B, you need  200 partitions for each sensor."
      },
      {
        "date": "2023-10-11T08:48:00.000Z",
        "voteCount": 1,
        "content": "D is not having option to filter measurement time as an sensor could have been faulty last month but might have been fixed all the recent entries no faulty, though the question explicitly asked \"based on the recent measurement\" we need to assume it, and option D don't have this field in its solution, hence it will become a problem"
      },
      {
        "date": "2023-09-23T03:46:00.000Z",
        "voteCount": 2,
        "content": "C is correct Answer I think.\n\nDetails are below.\n\n\"Power plants are identified by a globally unique identifier. Sensors are identified by a unique identifier within each power plant. \"\n\nIt means that 'sensor_id' is not globally unique.\n\n\"A database specialist needs to design the table to support an efficient method of finding all faulty sensors within a given power plant.\"\n\nThe database specialist knows the 'plant_id' before executing the query.\n\nAs a prerequisite knowledge\n- Secondary indexes like LSI and GSI in DynamoDB are not unique.\n- The data in a LSI is organized by the same partition key as the base table, but with a different sort key."
      },
      {
        "date": "2023-09-23T03:47:00.000Z",
        "voteCount": 1,
        "content": "- A : x\n\nKey ( pk : plant_id - sk : measurement_time )\nGSI ( pk : plant_id - sk : fault ) \n\nIf the plant_id is known, it is easy to use 'fault' to find the failed sensor.\n\n1. Query for items with a specific 'plant_id' and 'fault' set to true.\n2. The 'sensor_id' can be obtained from the acquired item.\n\nIf the measurement_time is exactly the same time in plant, the data could be overwritten.\n\n- B : x\n\nKey ( pk : plant_id#sensor_id - sk : measurement_time )\nLSI ( pk : plant_id#sensor_id - sk : fault ) \n\nIf you don't know the 'sensor_id', you can't query it."
      },
      {
        "date": "2023-09-23T03:47:00.000Z",
        "voteCount": 1,
        "content": "- C : o\n\nKey ( pk : plant_id#sensor_id - sk : measurement_time )\nGSI ( pk : plant_id - sk : fault ) \n\nAs shown in A, fault sensor_id can be obtained using GSI.\n\nThe measurement_time is unique with the combination of plant_id and sensor_id.\n\n- D : x\n\nKey ( pk : plant_id - sk : sensor_id )\nLSI ( pk : plant_id - sk : fault ) \n\nThe combination of the partition key value and sort key value for each item must be unique.\n\nOnly one item per 'plant_id' and 'sensor_id' can be stored."
      },
      {
        "date": "2023-09-20T12:04:00.000Z",
        "voteCount": 1,
        "content": "x A is out: a GSI must have the same partition key as the base table.\nB. The PK consisting of partition key+sort key is unique. An LSI on the fault attribute is possible. That would satisfy the query.\nx C. a GSI must have the same partition key as the base table.\nx D. The PK consisting of partition key+sort key is not unique."
      },
      {
        "date": "2023-09-20T12:16:00.000Z",
        "voteCount": 1,
        "content": "Sorry, wrong reasoning: An LSI has the same partition key as the base table, but a different sort key. A GSi can have an altogether different partition key...\nThat means, B has a wrong LSI, C is not unique, D has a wrong LSI.\nRemains A..."
      },
      {
        "date": "2023-09-16T14:45:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer.\n\nComposite primary key: This is a combination of partition key and sort key. So, B &amp; C are ruled out. Partition key composes of one attribute only. \"Partition key: A simple primary key, composed of one attribute known as the partition key. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.\"\n\nA is wrong because no info about sensors identifier.\n\n\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
      },
      {
        "date": "2023-08-06T23:48:00.000Z",
        "voteCount": 2,
        "content": "Composite primary key: This is a combination of partition key and sort key. So, B &amp; C are wrong. \nA - no info about sensors identifier."
      },
      {
        "date": "2023-06-11T15:44:00.000Z",
        "voteCount": 1,
        "content": "looks like D\nthere is nothing about the timestamp in the task"
      },
      {
        "date": "2023-05-17T05:41:00.000Z",
        "voteCount": 4,
        "content": "A: The GSI key is not unique\nB: The Primary key is unique because measurement time is the sort key, and the LSI allows us to narrow down to the required data\nC: The primary key is not unique\nD: The primary key is not unique.\n\nThe combination of plant id and sensor id is not unique since there will be many measurements for that key. Measurement time must therefore be part of the key to make it unique."
      },
      {
        "date": "2023-04-12T16:59:00.000Z",
        "voteCount": 2,
        "content": "You can't use more than one attributes as a partition key"
      },
      {
        "date": "2023-03-29T07:05:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
      },
      {
        "date": "2023-03-20T19:31:00.000Z",
        "voteCount": 1,
        "content": "B is the only option where an index would have sensor id as well. We need faulty sensor as query output and if sensor id is included in an index then only we are going to get faulty sensor details in the output. If we notice carefully, none of the options except B have sensor id used in index and we are assuming that we will be using an index to get data faster. So, correct choice is the one which has the query attribute in the index."
      },
      {
        "date": "2023-02-08T09:18:00.000Z",
        "voteCount": 2,
        "content": "A is feasible. But we can fullfill the requirement with simpler approach (which is D)\nB is completely wrong because we can not use that LSI at all for the requirement. To use the LSI, we need to specify the plant identifier as the primary key, but the primary key now contains sensor identifier so we can not use LSI.\nC is feasible simillar to A\nD is the most simpler set up."
      },
      {
        "date": "2023-02-07T13:04:00.000Z",
        "voteCount": 1,
        "content": "Confusing question but I think it should be C"
      },
      {
        "date": "2023-01-29T18:34:00.000Z",
        "voteCount": 2,
        "content": "A,C does not make sense: there is no need to create GSI, as question says find faulty sensors by plant. So when you have plant identifier as primary key, you can find faulty sensors in that plant by using LSI.\n\nB does not make sense: Question does not mention anything finding item my measurement time. So there is no point having measurement time as sort key.\n\nD is correct."
      },
      {
        "date": "2023-01-29T13:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct Answer i think\nComposite key is a Partition key and Sort Key, and then we cannot have another Sort Key\nBut \nD works which is Partition Key Plant Identifier and Sort Key as Sensor Identified, then LSI as Fault Attribute"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/amazon/view/49205-exam-aws-certified-database-specialty-topic-1-question-104/",
    "body": "A company is releasing a new mobile game featuring a team play mode. As a group of mobile device users play together, an item containing their statuses is updated in an Amazon DynamoDB table. Periodically, the other users' devices read the latest statuses of their teammates from the table using the BatchGetltemn operation.<br>Prior to launch, some testers submitted bug reports claiming that the status data they were seeing in the game was not up-to-date. The developers are unable to replicate this issue and have asked a database specialist for a recommendation.<br>Which recommendation would resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the DynamoDB table is configured to be always consistent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the BatchGetltem operation is called with the ConsistentRead parameter set to false.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a stream on the DynamoDB table and subscribe each device to the stream to ensure all devices receive up-to-date status information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the BatchGetltem operation is called with the ConsistentRead parameter set to true.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-10T16:34:00.000Z",
        "voteCount": 15,
        "content": "D. answer"
      },
      {
        "date": "2022-06-19T00:26:00.000Z",
        "voteCount": 6,
        "content": "Answer\uff1aD\nhttps://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
      },
      {
        "date": "2023-09-01T01:25:00.000Z",
        "voteCount": 2,
        "content": "D. Ensure the BatchGetltem operation is called with the ConsistentRead parameter set to true."
      },
      {
        "date": "2023-08-07T00:49:00.000Z",
        "voteCount": 1,
        "content": "By default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html"
      },
      {
        "date": "2023-03-20T19:34:00.000Z",
        "voteCount": 2,
        "content": "D is correct\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html"
      },
      {
        "date": "2023-03-15T06:56:00.000Z",
        "voteCount": 2,
        "content": "D.\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html"
      },
      {
        "date": "2022-06-21T09:48:00.000Z",
        "voteCount": 2,
        "content": "D is correct."
      },
      {
        "date": "2022-06-18T23:52:00.000Z",
        "voteCount": 1,
        "content": "C is the answer. Questions states that \"..periodically reads\" which mean re using the BatchGetItem with consistent read (D) is not going to solve the problem since data might be already stale on the next read. However, by using Streams each device will be notified immediately."
      },
      {
        "date": "2022-08-13T03:24:00.000Z",
        "voteCount": 1,
        "content": "Consistent means getting the latest data. You are talking about the \"eventual\" case. Streams is usually used to trigger actions based on events happening on the db. D is probably the answer."
      },
      {
        "date": "2022-05-30T07:00:00.000Z",
        "voteCount": 1,
        "content": "C is the most effective. Streams always will show the last update of the item, so, all users will read the same item status.\nC would resolve the problem as well, but will double the read capacity unit and the cost will double  too\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"
      },
      {
        "date": "2022-05-30T07:01:00.000Z",
        "voteCount": 1,
        "content": "I meant \"D\" would double the RCUs."
      },
      {
        "date": "2022-04-29T09:20:00.000Z",
        "voteCount": 2,
        "content": "game's status data was out of current means query got eventual consistent data"
      },
      {
        "date": "2022-03-11T16:20:00.000Z",
        "voteCount": 4,
        "content": "By default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html"
      },
      {
        "date": "2022-02-23T15:41:00.000Z",
        "voteCount": 1,
        "content": "D, BatchGetItem  with ConsistentRead"
      },
      {
        "date": "2022-02-16T06:53:00.000Z",
        "voteCount": 1,
        "content": "D\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables."
      },
      {
        "date": "2021-12-15T05:16:00.000Z",
        "voteCount": 2,
        "content": "D\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables."
      },
      {
        "date": "2021-10-21T21:29:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDDDD"
      },
      {
        "date": "2021-10-18T12:48:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/API_BatchGetItem_v20111205.html"
      },
      {
        "date": "2021-10-16T03:38:00.000Z",
        "voteCount": 1,
        "content": "D final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/amazon/view/48811-exam-aws-certified-database-specialty-topic-1-question-105/",
    "body": "A company is running an Amazon RDS for MySQL Multi-AZ DB instance for a business-critical workload. RDS encryption for the DB instance is disabled. A recent security audit concluded that all business-critical applications must encrypt data at rest. The company has asked its database specialist to formulate a plan to accomplish this for the DB instance.<br>Which process should the database specialist recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted snapshot of the unencrypted DB instance. Copy the encrypted snapshot to Amazon S3. Restore the DB instance from the encrypted snapshot using Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new RDS for MySQL DB instance with encryption enabled. Restore the unencrypted snapshot to this DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the unencrypted DB instance. Create an encrypted copy of the snapshot. Restore the DB instance from the encrypted snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporarily shut down the unencrypted DB instance. Enable AWS KMS encryption in the AWS Management Console using an AWS managed CMK. Restart the DB instance in an encrypted state."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-12T06:16:00.000Z",
        "voteCount": 14,
        "content": "C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations"
      },
      {
        "date": "2023-08-31T00:28:00.000Z",
        "voteCount": 2,
        "content": "C. Create a snapshot of the unencrypted DB instance. Create an encrypted copy of the snapshot. Restore the DB instance from the encrypted snapshot. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations\n\n\"you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. \""
      },
      {
        "date": "2022-04-29T17:55:00.000Z",
        "voteCount": 3,
        "content": "snapshot \n-&gt; encrypted copy of the snapshot \n-&gt; Restore the DB instance from the encrypted snapshot."
      },
      {
        "date": "2022-03-04T23:22:00.000Z",
        "voteCount": 3,
        "content": "Its definitely C"
      },
      {
        "date": "2021-12-27T03:57:00.000Z",
        "voteCount": 1,
        "content": "Ans: C\nIn AWS console, you have to \"migrate\" unencrypted snapshot to encrypted one then performing database restoration."
      },
      {
        "date": "2021-11-27T13:05:00.000Z",
        "voteCount": 3,
        "content": "It is C"
      },
      {
        "date": "2021-11-26T06:44:00.000Z",
        "voteCount": 4,
        "content": "Option C"
      },
      {
        "date": "2021-11-06T05:26:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2021-10-26T08:08:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCCCC"
      },
      {
        "date": "2021-09-21T07:41:00.000Z",
        "voteCount": 2,
        "content": "C. Answer"
      },
      {
        "date": "2021-09-24T13:34:00.000Z",
        "voteCount": 2,
        "content": "A and D is incorrect\nB is for Aurora Database not for RDS Database."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/amazon/view/49000-exam-aws-certified-database-specialty-topic-1-question-106/",
    "body": "A company is migrating its on-premises database workloads to the AWS Cloud. A database specialist performing the move has chosen AWS DMS to migrate an<br>Oracle database with a large table to Amazon RDS. The database specialist notices that AWS DMS is taking significant time to migrate the data.<br>Which actions would improve the data migration speed? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple AWS DMS tasks to migrate the large table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS replication instance with Multi-AZ.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the capacity of the AWS DMS replication server.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish an AWS Direct Connect connection between the on-premises data center and AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable an Amazon RDS Multi-AZ configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable full large binary object (LOB) mode to migrate all LOB data for all large tables."
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "ACF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T13:46:00.000Z",
        "voteCount": 11,
        "content": "ACD. Answer\nSelecting A. based on https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LargeTables \nC. and D. are the only options that have to do with increasing performance, the others are irrelevant"
      },
      {
        "date": "2021-11-18T11:45:00.000Z",
        "voteCount": 3,
        "content": "it is going to take you days/weeks to establish a direct connection, if it is not from the start, it should not be an option"
      },
      {
        "date": "2021-12-01T06:40:00.000Z",
        "voteCount": 2,
        "content": "It should be not an option, but these three (ACD) are the means which fasten the migration. Other options wouldn't do that."
      },
      {
        "date": "2021-10-11T00:19:00.000Z",
        "voteCount": 6,
        "content": "Correct Answer: ACF\n\nFull LOB mode \u2013 migrates all LOB data, piecewise in chunks (you provide LOB chunk size)"
      },
      {
        "date": "2024-02-07T11:59:00.000Z",
        "voteCount": 1,
        "content": "D can't be part of the answer. It takes about a week or more to setup a direct connect connection. I will think it ACF"
      },
      {
        "date": "2023-09-16T15:32:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html\n\nB &amp; E. Multi AZ are for improving availability should a storage issue occur, not contributing to data migration speed so B&amp;E are out.  F is opposite, \"Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance.\" so F is out."
      },
      {
        "date": "2023-05-28T04:00:00.000Z",
        "voteCount": 1,
        "content": "it should be ACF\nhttps://repost.aws/knowledge-center/dms-improve-speed-lob-data"
      },
      {
        "date": "2023-05-23T23:57:00.000Z",
        "voteCount": 1,
        "content": "The answer I was searching in the comments, I will post it here.\nFor those searching for a reason to select D, and how it improves migration speed.\n\nAWS Direct Connect can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections"
      },
      {
        "date": "2023-05-17T07:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance"
      },
      {
        "date": "2023-04-12T17:12:00.000Z",
        "voteCount": 2,
        "content": "F is wrong: In full LOB mode AWS DMS migrates all LOBs from source to target regardless of size. In this configuration, AWS DMS has no information about the maximum size of LOBs to expect. Thus, LOBs are migrated one at a time, piece by piece. Full LOB mode can be quite slow. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"
      },
      {
        "date": "2023-02-25T12:06:00.000Z",
        "voteCount": 1,
        "content": "How can we break a single table into multiple DMS tasks .If the DB had multiple tables , they can be split into multiple DMS tasks . So , we cannot assume that 'large table' to mean to multiple tables. So , I will go with CDE"
      },
      {
        "date": "2023-05-17T07:06:00.000Z",
        "voteCount": 1,
        "content": "\"To do this, split the table into segments and load the segments in parallel in the same migration task.\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance"
      },
      {
        "date": "2022-10-26T17:57:00.000Z",
        "voteCount": 1,
        "content": "With Amazon RDS databases, it's a good idea to turn off backups and Multi-AZ until the cutover.\n\nSo B and E eliminated\n\nF - Full LOB mode impacts performance\n\nSo ACD right answer"
      },
      {
        "date": "2022-10-22T09:34:00.000Z",
        "voteCount": 2,
        "content": "F - Full LOB mode downgrade the performance\nFull LOB mode migrates all LOB data in your tables, regardless of size. Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance."
      },
      {
        "date": "2022-05-31T15:31:00.000Z",
        "voteCount": 5,
        "content": "\"To improve the performance when migrating a large table, break the migration into more than one task. To break the migration into multiple tasks using row filtering, use a key or a partition key\" (A)\n\nA number of factors affect the performance of your AWS DMS migration:\n- Resource availability on the source.\n- The available network throughput. (D)\n- The resource capacity of the replication server. (C)\n- The ability of the target to ingest changes.\n- The type and distribution of source data.\n- The number of objects to be migrated.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LargeTables"
      },
      {
        "date": "2022-05-10T19:45:00.000Z",
        "voteCount": 2,
        "content": "It takes much to deploy the direct connect, so D is not the correct answer.\nI choose ACF"
      },
      {
        "date": "2022-05-06T20:05:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E. Multi AZ doesn't increase speed it will add more time.\nF. LOB mode will ensure complete movement but not faster"
      },
      {
        "date": "2022-04-28T13:52:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LargeTables"
      },
      {
        "date": "2022-01-28T13:30:00.000Z",
        "voteCount": 2,
        "content": "ACF\n\nF: \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/\nIf you have LOBs that are larger than a few megabytes, then you can create a separate AWS DMS task with Full LOB mode. It's a best practice to create the separate task on a new replication instance to migrate these tables alone.  Option A stated to create multiple DMS tasks. \n\nHaving an expensive Direct Connect will not resolve this issue"
      },
      {
        "date": "2021-12-22T14:00:00.000Z",
        "voteCount": 1,
        "content": "Strange question. If you've already started the migration of a large table and observe that the DMS is slow it's too late for making such suggested changes. If you're planning to migrate a large table then go with A,C and D."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/amazon/view/49001-exam-aws-certified-database-specialty-topic-1-question-107/",
    "body": "A company is migrating a mission-critical 2-TB Oracle database from on premises to Amazon Aurora. The cost for the database migration must be kept to a minimum, and both the on-premises Oracle database and the Aurora DB cluster must remain open for write traffic until the company is ready to completely cut over to Aurora.<br>Which combination of actions should a database specialist take to accomplish this migration as quickly as possible? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to convert the source database schema. Then restore the converted schema to the target Aurora DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Oracle's Data Pump tool to export a copy of the source database schema and manually edit the schema in a text editor to make it compatible with Aurora.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS DMS task to migrate data from the Oracle database to the Aurora DB cluster. Select the migration type to replicate ongoing changes to keep the source and target databases in sync until the company is ready to move all user traffic to the Aurora DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS DMS task to migrate data from the Oracle database to the Aurora DB cluster. Once the initial load is complete, create an AWS Kinesis Data Firehose stream to perform change data capture (CDC) until the company is ready to move all user traffic to the Aurora DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue job and related resources to migrate data from the Oracle database to the Aurora DB cluster. Once the initial load is complete, create an AWS DMS task to perform change data capture (CDC) until the company is ready to move all user traffic to the Aurora DB cluster."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-07T04:19:00.000Z",
        "voteCount": 13,
        "content": "AC. Answer"
      },
      {
        "date": "2021-10-19T22:14:00.000Z",
        "voteCount": 6,
        "content": "AC  --- locked\n\nA - OK\nB  - NOT OK Text editor\nC - ok though it doesn't talk about full load\nD- NOT ok firehose\nE - not ok Glue"
      },
      {
        "date": "2021-10-20T15:04:00.000Z",
        "voteCount": 1,
        "content": "I agree with you."
      },
      {
        "date": "2023-09-16T15:42:00.000Z",
        "voteCount": 2,
        "content": "AC are correct answer\n\nSCT + DMS"
      },
      {
        "date": "2023-04-28T03:23:00.000Z",
        "voteCount": 1,
        "content": "AC. asnwer"
      },
      {
        "date": "2023-03-21T08:19:00.000Z",
        "voteCount": 1,
        "content": "A&amp;C for sure"
      },
      {
        "date": "2023-03-05T00:50:00.000Z",
        "voteCount": 1,
        "content": "AC --- locked"
      },
      {
        "date": "2022-04-30T13:16:00.000Z",
        "voteCount": 2,
        "content": "A. Use the AWS Schema Conversion Tool (AWS SCT) to convert the source database schema. Then restore the converted schema to the target Aurora DB cluster.\nx B. edit\nC. Create an AWS DMS task to migrate data from the Oracle database to the Aurora DB cluster. Select the migration type to replicate ongoing changes to keep the source and target databases in sync until the company is ready to move all user traffic to the Aurora DB cluster.\nx D. kinesis firehose\nx E. Glue job"
      },
      {
        "date": "2021-10-29T18:33:00.000Z",
        "voteCount": 2,
        "content": "a and c all day long"
      },
      {
        "date": "2021-10-28T13:37:00.000Z",
        "voteCount": 1,
        "content": "I got this Question in exam.\n60% questions came in actual exam from this 145 set. Bunch of new Questions."
      },
      {
        "date": "2021-10-18T08:37:00.000Z",
        "voteCount": 3,
        "content": "AC final answer"
      },
      {
        "date": "2021-10-08T17:37:00.000Z",
        "voteCount": 2,
        "content": "A C is the answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/amazon/view/48812-exam-aws-certified-database-specialty-topic-1-question-108/",
    "body": "A company has a 20 TB production Amazon Aurora DB cluster. The company runs a large batch job overnight to load data into the Aurora DB cluster. To ensure the company's development team has the most up-to-date data for testing, a copy of the DB cluster must be available in the shortest possible time after the batch job completes.<br>How should this be accomplished?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to schedule a manual snapshot of the DB cluster. Restore the snapshot to a new DB cluster using the AWS CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dump file from the DB cluster. Load the dump file into a new DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a job to create a clone of the DB cluster at the end of the overnight batch process.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a new daily AWS DMS task that will use cloning and change data capture (CDC) on the DB cluster to copy the data to a new DB cluster. Set up a time for the AWS DMS stream to stop when the new cluster is current."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-08T11:04:00.000Z",
        "voteCount": 14,
        "content": "C. answer\n\nAurora can do cloning."
      },
      {
        "date": "2022-04-28T18:02:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\n Creating a clone is faster and more space-efficient than physically copying the data using other techniques, such as restoring a snapshot."
      },
      {
        "date": "2023-09-16T15:55:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\nD. does not make sense, DMS doesn't use cloning."
      },
      {
        "date": "2022-04-17T05:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html"
      },
      {
        "date": "2022-01-13T19:45:00.000Z",
        "voteCount": 2,
        "content": "C - because Clone should kick-off after batch completes"
      },
      {
        "date": "2021-12-14T05:20:00.000Z",
        "voteCount": 2,
        "content": "Option C, Clone is easy and fast"
      },
      {
        "date": "2021-11-01T01:34:00.000Z",
        "voteCount": 1,
        "content": "CCCC - faster clones"
      },
      {
        "date": "2021-10-23T23:46:00.000Z",
        "voteCount": 2,
        "content": "C final answer"
      },
      {
        "date": "2021-10-18T12:47:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/amazon/view/49002-exam-aws-certified-database-specialty-topic-1-question-109/",
    "body": "A company has two separate AWS accounts: one for the business unit and another for corporate analytics. The company wants to replicate the business unit data stored in Amazon RDS for MySQL in us-east-1 to its corporate analytics Amazon Redshift environment in us-west-1. The company wants to use AWS DMS with<br>Amazon RDS as the source endpoint and Amazon Redshift as the target endpoint.<br>Which action will allow AVS DMS to perform the replication?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS replication instance in the same account and Region as Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS replication instance in the same account as Amazon Redshift and in the same Region as Amazon RDS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS replication instance in its own account and in the same Region as Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS DMS replication instance in the same account and Region as Amazon RDS."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-03T07:51:00.000Z",
        "voteCount": 17,
        "content": "Sorry I meant A. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html"
      },
      {
        "date": "2024-04-06T10:31:00.000Z",
        "voteCount": 1,
        "content": "A. not C. Ref  says it all : your Amazon Redshift cluster must be in the same account and same AWS Region as the replication instance"
      },
      {
        "date": "2023-09-08T21:02:00.000Z",
        "voteCount": 1,
        "content": "A. Configure the AWS DMS replication instance in the same account and Region as Amazon Redshift.\n\n\"The Amazon Redshift cluster must be in the same AWS account and same AWS Region as the replication instance.\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html"
      },
      {
        "date": "2023-06-11T16:50:00.000Z",
        "voteCount": 2,
        "content": "use target side"
      },
      {
        "date": "2022-12-16T06:11:00.000Z",
        "voteCount": 2,
        "content": "Ans A. \n\nThe Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance."
      },
      {
        "date": "2022-05-01T10:58:00.000Z",
        "voteCount": 4,
        "content": "A. Configure the AWS DMS replication instance in the same account and Region as Target Database"
      },
      {
        "date": "2022-01-27T19:21:00.000Z",
        "voteCount": 2,
        "content": "A is the answer. Refer : https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html"
      },
      {
        "date": "2021-11-07T09:04:00.000Z",
        "voteCount": 1,
        "content": "AAA - locked -"
      },
      {
        "date": "2021-10-28T09:08:00.000Z",
        "voteCount": 4,
        "content": "A final answer\n\"The Amazon Redshift cluster must be in the same AWS account and same AWS Region as the replication instance.\""
      },
      {
        "date": "2021-10-22T11:04:00.000Z",
        "voteCount": 1,
        "content": "A is right."
      },
      {
        "date": "2021-10-06T01:23:00.000Z",
        "voteCount": 2,
        "content": "A. Answer"
      },
      {
        "date": "2021-09-30T04:06:00.000Z",
        "voteCount": 2,
        "content": "B. Answer\nRef https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/amazon/view/48813-exam-aws-certified-database-specialty-topic-1-question-110/",
    "body": "A database specialist is managing an application in the us-west-1 Region and wants to set up disaster recovery in the us-east-1 Region. The Amazon Aurora<br>MySQL DB cluster needs an RPO of 1 minute and an RTO of 2 minutes.<br>Which approach meets these requirements with no negative performance impact?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable synchronous replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable asynchronous binlog replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Global Database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy Aurora incremental snapshots to the us-east-1 Region."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-21T11:01:00.000Z",
        "voteCount": 13,
        "content": "C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html"
      },
      {
        "date": "2023-09-08T21:18:00.000Z",
        "voteCount": 2,
        "content": "C. Create an Aurora Global Database.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html"
      },
      {
        "date": "2023-03-05T00:53:00.000Z",
        "voteCount": 2,
        "content": "Enable synchronous replication: Synchronous replication ensures that data is written to both the primary and standby DB clusters before the transaction is committed. This means that the secondary cluster is always in sync with the primary, with minimal lag time, and can be promoted to primary in case of a failure. This approach meets the RPO and RTO requirements and ensures there is no data loss or downtime."
      },
      {
        "date": "2023-12-08T21:49:00.000Z",
        "voteCount": 1,
        "content": "How about performance impact?"
      },
      {
        "date": "2022-04-30T19:12:00.000Z",
        "voteCount": 4,
        "content": "C. Create an Aurora Global Database.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html\n\nManaged planned failover RPO = 0 sec, RTO = minutes"
      },
      {
        "date": "2022-02-03T10:41:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer."
      },
      {
        "date": "2021-11-02T05:30:00.000Z",
        "voteCount": 3,
        "content": "C final answer"
      },
      {
        "date": "2021-09-20T20:09:00.000Z",
        "voteCount": 2,
        "content": "C. I believe"
      },
      {
        "date": "2021-10-01T07:36:00.000Z",
        "voteCount": 2,
        "content": "Sync replication not available cross region If I am not wrong."
      },
      {
        "date": "2022-02-08T06:31:00.000Z",
        "voteCount": 1,
        "content": "So if we want to have it in the same region, synchronous replication would be a better solution?"
      },
      {
        "date": "2022-05-19T04:27:00.000Z",
        "voteCount": 1,
        "content": "No. You can use multi-AZ read replica for single region DR."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/amazon/view/48814-exam-aws-certified-database-specialty-topic-1-question-111/",
    "body": "A gaming company is developing a new mobile game and decides to store the data for each user in Amazon DynamoDB. To make the registration process as easy as possible, users can log in with their existing Facebook or Amazon accounts. The company expects more than 10,000 users.<br>How should a database specialist implement access control with the LEAST operational effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse web identity federation on the mobile app and create individual IAM users with credentials to access DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a self-developed user management system on the mobile app that lets users access the data from DynamoDB through an API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a single IAM user on the mobile app to access DynamoDB."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T20:46:00.000Z",
        "voteCount": 11,
        "content": "A. answer"
      },
      {
        "date": "2023-10-09T06:15:00.000Z",
        "voteCount": 1,
        "content": "right infront of my salad Amazon Cognito exists"
      },
      {
        "date": "2023-09-08T21:27:00.000Z",
        "voteCount": 1,
        "content": "A. Use web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html\n\n\"If you are writing an application targeted at large numbers of users, you can optionally use web identity federation for authentication and authorization. Web identity federation removes the need for creating individual users. Instead, users can sign in to an identity provider and then obtain temporary security credentials from AWS Security Token Service (AWS STS). The app can then use these credentials to access AWS services.\n\nWeb identity federation supports the following identity providers:\nLogin with Amazon\nFacebook\nGoogle\""
      },
      {
        "date": "2022-12-16T06:16:00.000Z",
        "voteCount": 1,
        "content": "A, Web identity Fed &gt; STS &gt; IAM &gt; tempo creds"
      },
      {
        "date": "2022-06-19T00:11:00.000Z",
        "voteCount": 2,
        "content": "IAM Role is always preferred method."
      },
      {
        "date": "2022-04-29T10:29:00.000Z",
        "voteCount": 1,
        "content": "Use web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB."
      },
      {
        "date": "2022-02-24T11:28:00.000Z",
        "voteCount": 1,
        "content": "obvious"
      },
      {
        "date": "2021-11-04T06:37:00.000Z",
        "voteCount": 2,
        "content": "AAAAAAAAA"
      },
      {
        "date": "2021-11-03T01:37:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      },
      {
        "date": "2021-10-11T11:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A - least operational"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/amazon/view/48815-exam-aws-certified-database-specialty-topic-1-question-112/",
    "body": "A large retail company recently migrated its three-tier ecommerce applications to AWS. The company's backend database is hosted on Amazon Aurora<br>PostgreSQL. During peak times, users complain about longer page load times. A database specialist reviewed Amazon RDS Performance Insights and found a spike in IO:XactSync wait events. The SQL attached to the wait events are all single INSERT statements.<br>How should this issue be resolved?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to commit transactions in batches\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new Aurora Replica to the Aurora DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon ElastiCache for Redis cluster and change the application to write through.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Aurora DB cluster storage to Provisioned IOPS (PIOPS)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T00:26:00.000Z",
        "voteCount": 14,
        "content": "A. answer"
      },
      {
        "date": "2021-11-05T06:40:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html\n\"This wait most often arises when there is a very high rate of commit activity on the system. You can sometimes alleviate this wait by modifying applications to commit transactions in batches. \""
      },
      {
        "date": "2022-10-16T03:02:00.000Z",
        "voteCount": 6,
        "content": "D - There is no option to setup PIOPS for Aurora. Only RDS has it"
      },
      {
        "date": "2023-04-17T08:30:00.000Z",
        "voteCount": 2,
        "content": "Thanks for your explanation for D"
      },
      {
        "date": "2023-06-30T18:02:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\n\n    Monitor your resources\n    Scale up the CPU\n    Increase network bandwidth\n    Reduce the number of commits"
      },
      {
        "date": "2022-04-30T06:52:00.000Z",
        "voteCount": 2,
        "content": "avoid high rate of commits by using batch commit"
      },
      {
        "date": "2022-02-02T09:37:00.000Z",
        "voteCount": 6,
        "content": "Answer is A.\n\nActions\n\nWe recommend different actions depending on the causes of your wait event.\n1.-    Monitor your resources\n2.-    Scale up the CPU\n3.-    Increase network bandwidth\n4.-    Reduce the number of commits\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html"
      },
      {
        "date": "2021-12-21T13:09:00.000Z",
        "voteCount": 3,
        "content": "To reduce the number of commits, combine statements into transaction blocks."
      },
      {
        "date": "2021-11-30T23:46:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html"
      },
      {
        "date": "2021-10-13T01:40:00.000Z",
        "voteCount": 1,
        "content": "AAAAAA"
      },
      {
        "date": "2021-10-10T13:43:00.000Z",
        "voteCount": 1,
        "content": "A final answer"
      },
      {
        "date": "2021-10-03T08:14:00.000Z",
        "voteCount": 1,
        "content": "Yes, A is correct"
      },
      {
        "date": "2021-10-01T17:56:00.000Z",
        "voteCount": 1,
        "content": "Yup A is right for this use case"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/amazon/view/49004-exam-aws-certified-database-specialty-topic-1-question-113/",
    "body": "A company uses Amazon DynamoDB as the data store for its ecommerce website. The website receives little to no traffic at night, and the majority of the traffic occurs during the day. The traffic growth during peak hours is gradual and predictable on a daily basis, but it can be orders of magnitude higher than during off- peak hours.<br>The company initially provisioned capacity based on its average volume during the day without accounting for the variability in traffic patterns. However, the website is experiencing a significant amount of throttling during peak hours. The company wants to reduce the amount of throttling while minimizing costs.<br>What should a database specialist do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse reserved capacity. Set it to the capacity levels required for peak daytime throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse provisioned capacity. Set it to the capacity levels required for peak daytime throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse provisioned capacity. Create an AWS Application Auto Scaling policy to update capacity based on consumption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse on-demand capacity."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-06T02:53:00.000Z",
        "voteCount": 2,
        "content": "reduce cost - on-demand costlier\npredictable, gradual increase - auto scaling"
      },
      {
        "date": "2023-07-13T05:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct based on documentation: When you create a DynamoDB table, auto scaling is the default capacity setting, but you can also enable auto scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. \nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"
      },
      {
        "date": "2023-06-06T07:12:00.000Z",
        "voteCount": 1,
        "content": "If C had been worded differently, like \"Provisioned capacity with auto-scaling\", I might have chosen C. But \"AWS Application Auto-Scaling\" does not make sense in the DynamoDB context. Therefore I choose \"On-Demand\" which is very flexible in terms of throughput."
      },
      {
        "date": "2023-02-07T13:18:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCCCCCCC"
      },
      {
        "date": "2022-11-24T01:04:00.000Z",
        "voteCount": 1,
        "content": "C for me too..\n\nDynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto scaling uses Amazon CloudWatch to monitor a table\u2019s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity."
      },
      {
        "date": "2022-10-20T17:23:00.000Z",
        "voteCount": 2,
        "content": "no traffic at night and orders of magnitude high traffic in off peak. I will go with D"
      },
      {
        "date": "2022-07-03T05:00:00.000Z",
        "voteCount": 1,
        "content": "C: As the pattern is predictable load with mangitude of spikes this could be handled by autoscaling. As per aws autoscaling is cost saving as compared to ondemand. If the pattern was unpredictable On-demand would be good"
      },
      {
        "date": "2022-06-26T19:20:00.000Z",
        "voteCount": 1,
        "content": "but it also says \"traffic increase during peak hours is steady and predictable\" and the firm also wants to reduce the throttling \"  so it is either C or D \nI think .. \nC is more likely as traffic pattern is predictable"
      },
      {
        "date": "2022-06-26T19:00:00.000Z",
        "voteCount": 1,
        "content": "It states that there is lot of throttling though out the day and the firm wants to minimize the expenditure and cost. Provisioned is costlier and during night hours the traffic is almost nill.\nSo what ever small ( average of day ) capacity you are  provisioning will be wasted at night. \nI believe on-demand in this case is better choice."
      },
      {
        "date": "2022-05-29T13:23:00.000Z",
        "voteCount": 1,
        "content": "\"... traffic increase during peak hours is steady and predictable ...\""
      },
      {
        "date": "2022-04-29T18:48:00.000Z",
        "voteCount": 3,
        "content": "- traffic increase during peak hours is steady and predictable=&gt; provisioned\n- no uses in night. =&gt; on demand (but costly)\n\nso provision some + auto scale with a target 70% utilization set \n\nD will also work but costly"
      },
      {
        "date": "2022-03-10T07:03:00.000Z",
        "voteCount": 2,
        "content": "I go with C.\n\"To understand how DynamoDB auto scaling works, suppose that you have a table named ProductCatalog. The table is bulk-loaded with data infrequently, so it doesn't incur very much write activity. However, it does experience a high degree of read activity, which varies over time. By monitoring the Amazon CloudWatch metrics for ProductCatalog, you determine that the table requires 1,200 read capacity units (to avoid DynamoDB throttling read requests when activity is at its peak). You also determine that ProductCatalog requires 150 read capacity units at a minimum, when read traffic is at its lowest point.\n\nWithin the range of 150 to 1,200 read capacity units, you decide that a target utilization of 70 percent would be appropriate for the ProductCatalog table. Target utilization is the ratio of consumed capacity units to provisioned capacity units, expressed as a percentage. Application Auto Scaling uses its target tracking algorithm to ensure that the provisioned read capacity of ProductCatalog is adjusted as required so that utilization remains at or near 70 percent.\""
      },
      {
        "date": "2022-03-05T10:36:00.000Z",
        "voteCount": 3,
        "content": "Very tricky. Feels like (D)\nEven of  you enable Autoscaling - but you will still pay the same rate during off-peak hours. the provisioned capacity will never fall below the preconfigured one during night when theres no traffic. Plus the question WANTS to reduce expenditure - which means with OnDemand theres  barely any charge during off peak hours compared to fully charged if u use provisioned capacity using thebaseline configured RCU/WCUs."
      },
      {
        "date": "2022-04-06T16:27:00.000Z",
        "voteCount": 2,
        "content": "Remember for Provisioned with Auto-Scaling you are basically paying for throughput 24/7. Whereas for On-Demand Scaling you pay per request. This means for applications still in development or low traffic applications, it might be more economical to use On-Demand Scaling and not worry about provisioning throughput. However, at scale, this can quickly shift once you have a more consistent usage pattern.\nhttps://dynobase.dev/dynamodb-on-demand-vs-provisioned-scaling/#:~:text=Remember%20for%20Provisioned%20with%20Auto,not%20worry%20about%20provisioning%20throughput."
      },
      {
        "date": "2023-06-04T16:36:00.000Z",
        "voteCount": 1,
        "content": "The key is \"The traffic growth during peak hours is gradual and predictable on a daily basis\" provisioned with auto scaling"
      },
      {
        "date": "2022-02-27T10:33:00.000Z",
        "voteCount": 4,
        "content": "C\nProvisioned Mode\nIf you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto scaling to adjust your table\u2019s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate in order to obtain cost predictability.\n\nProvisioned mode is a good option if any of the following are true:\n\nYou have predictable application traffic.\n\nYou run applications whose traffic is consistent or ramps gradually.\n\nYou can forecast capacity requirements to control costs."
      },
      {
        "date": "2022-02-24T15:45:00.000Z",
        "voteCount": 1,
        "content": "They know peak hours\nAWS Application Auto Scaling policy is good for it."
      },
      {
        "date": "2022-01-21T06:19:00.000Z",
        "voteCount": 2,
        "content": "C. DynamoDB autoscaling saves costs comparing to on-demand (which costs a bit higher for the NoOps benefit.) \nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"
      },
      {
        "date": "2022-01-07T12:23:00.000Z",
        "voteCount": 1,
        "content": "\"traffic increase during peak hours is steady and predictable\"\nC is the answer."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/amazon/view/48817-exam-aws-certified-database-specialty-topic-1-question-114/",
    "body": "A company uses an Amazon RDS for PostgreSQL DB instance for its customer relationship management (CRM) system. New compliance requirements specify that the database must be encrypted at rest.<br>Which action will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted copy of manual snapshot of the DB instance. Restore a new DB instance from the encrypted snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance and enable encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore a DB instance from the most recent automated snapshot and enable encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted read replica of the DB instance. Promote the read replica to a standalone instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T22:02:00.000Z",
        "voteCount": 13,
        "content": "A. Answer"
      },
      {
        "date": "2021-11-28T16:48:00.000Z",
        "voteCount": 1,
        "content": "This page has detailed steps for MySQL and Maria in terms of the encryption of an unencrypted RDS instance.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/"
      },
      {
        "date": "2021-11-12T03:37:00.000Z",
        "voteCount": 5,
        "content": "From the reference: You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created.\n\nHowever, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. For more information, see Copying a snapshot."
      },
      {
        "date": "2022-01-01T05:31:00.000Z",
        "voteCount": 1,
        "content": "Don't the words \"manual snapshot\" invalidate the answer \"A\"?"
      },
      {
        "date": "2022-03-11T11:08:00.000Z",
        "voteCount": 1,
        "content": "A. Answer -&gt; Answering to my own question: No! I've just tried to restore unencrypted manual and automatic snapshots into an encrypted db instance and it isn't allowed. If you want to launch an encrypted rds instance, you need to create an encrypted copy of the unencrypted snapshot."
      },
      {
        "date": "2023-03-24T11:25:00.000Z",
        "voteCount": 1,
        "content": "Sign in to the AWS Management Console and navigate to the Amazon RDS dashboard.\nSelect the DB instance that you want to encrypt.\nClick the \"Modify\" button.\nIn the \"Encryption\" section, select the option to \"Enable encryption\".\nChoose the KMS encryption key that you want to use or create a new one.\nClick \"Continue\" and review the summary of changes.\nClick \"Modify DB instance\" to apply the changes.\nNote that the encryption process will initiate a snapshot of the DB instance, encrypt it, and restore the encrypted data from the snapshot, so there will be a brief period of downtime while the encryption process is completed."
      },
      {
        "date": "2022-06-30T22:03:00.000Z",
        "voteCount": 1,
        "content": "D. is correct. \nCreate Read Replica encrypted enable and promote standalone instance.\n\nA. The snapshot doesn't encrypred option.\nB. Unencrypted instance is not enable encrypted.\nC. Also automated snapshot is not enable encrypted."
      },
      {
        "date": "2022-06-30T06:27:00.000Z",
        "voteCount": 1,
        "content": "A. Answer"
      },
      {
        "date": "2022-06-08T20:48:00.000Z",
        "voteCount": 1,
        "content": "You can only encrypt an Amazon RDS DB instance when you create it, not after the DB instance is created.\n\nHowever, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance"
      },
      {
        "date": "2022-05-12T21:28:00.000Z",
        "voteCount": 1,
        "content": "A is correct, from the page DMS used for ongoing replication.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\nAWS DMS \u2013 You can use AWS Database Migration Service (AWS DMS) to replicate changes from the source DB to the target DB. It is important to keep the source and target DB in sync to keep downtime to a minimum. For information about setting up AWS DMS and creating tasks, see the AWS DMS documentation."
      },
      {
        "date": "2022-04-29T07:22:00.000Z",
        "voteCount": 1,
        "content": "Manual Snapshot -&gt; Create an encrypted copy -&gt; Restore a new DB instance from the encrypted snapshot."
      },
      {
        "date": "2022-02-23T16:34:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-02-23T16:33:00.000Z",
        "voteCount": 1,
        "content": "Correct option"
      },
      {
        "date": "2022-02-23T15:02:00.000Z",
        "voteCount": 1,
        "content": "A - is wrong, something has changed recently.\nI took a snapshot and tried to copy and encrypt it, it does not allows unencrypted to encrypted.\nThe best option is C, C works as is take any snapshot or manual snapshot and restore to new encrypted cluster."
      },
      {
        "date": "2021-12-17T09:51:00.000Z",
        "voteCount": 2,
        "content": "A - correct\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\nYou can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created. However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance. The pattern uses AWS Database Migration Service (AWS DMS) to migrate data and AWS Key Management Service (AWS KMS) for encryption."
      },
      {
        "date": "2021-12-06T18:16:00.000Z",
        "voteCount": 1,
        "content": "All aswer is erro, is necessary utilization DMS \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"
      },
      {
        "date": "2021-10-30T07:10:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-10-21T15:28:00.000Z",
        "voteCount": 1,
        "content": "A final answer"
      },
      {
        "date": "2021-10-03T06:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/amazon/view/48820-exam-aws-certified-database-specialty-topic-1-question-115/",
    "body": "A database specialist was alerted that a production Amazon RDS MariaDB instance with 100 GB of storage was out of space. In response, the database specialist modified the DB instance and added 50 GB of storage capacity. Three hours later, a new alert is generated due to a lack of free space on the same DB instance.<br>The database specialist decides to modify the instance immediately to increase its storage capacity by 20 GB.<br>What will happen when the modification is submitted?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe request will fail because this storage capacity is too large.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe request will succeed only if the primary instance is in active status.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe request will succeed only if CPU utilization is less than 10%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe request will fail as the most recent modification was too soon.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-12T04:23:00.000Z",
        "voteCount": 14,
        "content": "D. answer. instance can't have any more storage modifications for six hours"
      },
      {
        "date": "2022-10-31T21:36:00.000Z",
        "voteCount": 1,
        "content": "D - Storage shouldn't be extended immediately"
      },
      {
        "date": "2022-04-30T07:17:00.000Z",
        "voteCount": 4,
        "content": "needs 6 hour gap"
      },
      {
        "date": "2022-03-05T11:11:00.000Z",
        "voteCount": 4,
        "content": "Answer is D - cannot modify storage until EITHER 6 hours have passed OR the \"storage-optimization\" status is complete (instance will show \"storage-optimization\" happens after previous storage capacity has increased -it CAN take more than 6 hours)\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html"
      },
      {
        "date": "2022-02-24T20:21:00.000Z",
        "voteCount": 2,
        "content": "D\nStorage optimization can take several hours. You can't make further storage modifications for either six (6) hours or until storage optimization has completed on the instance, whichever is longer."
      },
      {
        "date": "2021-12-02T05:01:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2021-11-05T20:37:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html - either 6 hours or when the first  job completes - I would go with D too"
      },
      {
        "date": "2021-10-29T00:01:00.000Z",
        "voteCount": 2,
        "content": "D\nYou can't make further storage modifications until six (6) hours after storage optimization has completed on the instance."
      },
      {
        "date": "2021-10-19T05:15:00.000Z",
        "voteCount": 1,
        "content": "D is correct. 6 hours duration needs to pass"
      },
      {
        "date": "2021-09-19T23:14:00.000Z",
        "voteCount": 2,
        "content": "D. answer\nI think it needs to pass 6 hours to increase another storage space increase."
      },
      {
        "date": "2021-10-02T06:43:00.000Z",
        "voteCount": 2,
        "content": "Ignore the answer, that is for auto scaling, has to pass 6 hours."
      },
      {
        "date": "2021-10-02T11:38:00.000Z",
        "voteCount": 1,
        "content": "Well, that condition applies both to manual as well as auto scaling, 6 hours has to pass. So I think it is D still"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/amazon/view/49006-exam-aws-certified-database-specialty-topic-1-question-116/",
    "body": "A company uses Amazon Aurora for secure financial transactions. The data must always be encrypted at rest and in transit to meet compliance requirements.<br>Which combination of actions should a database specialist take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Replica with encryption enabled using AWS Key Management Service (AWS KMS). Then promote the replica to master.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSL/TLS to secure the in-transit connection between the financial application and the Aurora DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing Aurora DB cluster and enable encryption using an AWS Key Management Service (AWS KMS) encryption key. Apply the changes immediately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the Aurora DB cluster and encrypt the snapshot using an AWS Key Management Service (AWS KMS) encryption key. Restore the snapshot to a new DB cluster and update the financial application database endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Key Management Service (AWS KMS) to secure the in-transit connection between the financial application and the Aurora DB cluster."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-01T18:35:00.000Z",
        "voteCount": 14,
        "content": "B and D."
      },
      {
        "date": "2022-02-23T17:13:00.000Z",
        "voteCount": 6,
        "content": "Per - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html\n\nNot A as - You can't create an encrypted Aurora Replica from an unencrypted Aurora DB cluster. You can't create an unencrypted Aurora Replica from an encrypted Aurora DB cluster.\nB is good for in-transit replication\nNot C as - You can't convert an unencrypted DB cluster to an encrypted one. \nD as - You can restore an unencrypted snapshot to an encrypted Aurora DB cluster. To do this, specify a KMS key when you restore from the unencrypted snapshot.\nNot E as - KMS does not perform encryption for data in transit or in motion. If you want to encrypt data while in transit, then you would need to use a different method such as SSL.\n\nSo, B and D is correct."
      },
      {
        "date": "2023-03-24T11:33:00.000Z",
        "voteCount": 1,
        "content": "In Aurora you can encrypt at rest without copying the snapshot. So A and C for sure"
      },
      {
        "date": "2023-04-02T07:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html#Overview.Encryption.Limitations"
      },
      {
        "date": "2023-03-15T01:27:00.000Z",
        "voteCount": 1,
        "content": "I reckon is B and D"
      },
      {
        "date": "2022-05-14T03:00:00.000Z",
        "voteCount": 2,
        "content": "B and D. D is right. Take snapshot of cluster &gt; and (keyword here) &gt; enable encryption. You cannot take a snapshot and encrypt it at the same time, this where the 'and' comes into play, you can encrypt just a snapshot + you can encrypt the snapshot on restore."
      },
      {
        "date": "2022-04-29T09:06:00.000Z",
        "voteCount": 2,
        "content": "B. SSL/TLS is good for in-transit replication\nD. as - You can restore an unencrypted snapshot to an encrypted Aurora DB cluster"
      },
      {
        "date": "2022-04-29T09:07:00.000Z",
        "voteCount": 1,
        "content": "D. as - You can NOT restore an unencrypted snapshot to an encrypted Aurora DB cluster"
      },
      {
        "date": "2022-02-23T15:36:00.000Z",
        "voteCount": 3,
        "content": "B,D\nC: Wrong, you cannot modify an unencrypted to encrypted"
      },
      {
        "date": "2022-01-12T08:15:00.000Z",
        "voteCount": 4,
        "content": "A and B .. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html\n\nD is incorrect: \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_CopySnapshot.html\n\nFor Amazon Aurora DB cluster snapshots, you can't encrypt an unencrypted DB cluster snapshot when you copy the snapshot."
      },
      {
        "date": "2021-10-30T21:42:00.000Z",
        "voteCount": 1,
        "content": "B and D is correct"
      },
      {
        "date": "2021-10-25T04:02:00.000Z",
        "voteCount": 1,
        "content": "B and D are correct choice."
      },
      {
        "date": "2021-09-20T02:16:00.000Z",
        "voteCount": 5,
        "content": "BD\nB. is obvious. For D. I thought it's possible to directly restore the unencrypted snapshot into an encrypted cluster so somehow one step looks unnecessary. But A, C and E are incorrect so I pick D. by default"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/amazon/view/49605-exam-aws-certified-database-specialty-topic-1-question-117/",
    "body": "A company is running a website on Amazon EC2 instances deployed in multiple Availability Zones (AZs). The site performs a high number of repetitive reads and writes each second on an Amazon RDS for MySQL Multi-AZ DB instance with General Purpose SSD (gp2) storage. After comprehensive testing and analysis, a database specialist discovers that there is high read latency and high CPU utilization on the DB instance.<br>Which approach should the database specialist take to resolve this issue without changing the application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement sharding to distribute the load to multiple RDS for MySQL databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the same RDS for MySQL instance class with Provisioned IOPS (PIOPS) storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an RDS for MySQL read replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the RDS for MySQL database class to a bigger size and implement Provisioned IOPS (PIOPS).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-30T07:41:00.000Z",
        "voteCount": 15,
        "content": "I would go with D due to the high cpu utilisation;  not C as this would require app to use read endpoint - therefore app change required"
      },
      {
        "date": "2023-03-24T11:37:00.000Z",
        "voteCount": 1,
        "content": "The high read latency and high CPU utilization on the Amazon RDS for MySQL Multi-AZ DB instance can be addressed by using Provisioned IOPS (PIOPS) storage with the same RDS for MySQL instance class. Therefore, option B is the correct approach to resolve the issue without changing the application. By using Provisioned IOPS (PIOPS), the database can deliver predictable and consistent I/O performance, which helps improve read latency and CPU utilization."
      },
      {
        "date": "2023-04-26T08:01:00.000Z",
        "voteCount": 1,
        "content": "Multi-AZ is not for performance."
      },
      {
        "date": "2022-11-17T17:15:00.000Z",
        "voteCount": 1,
        "content": "B - doesnt incur downtime can change and improve performance but on the storage front\nC - Needs to send high load queries to replicas \nD - Chance of 60 second downtime due to change in instance class but performance improvement over CPU and storage."
      },
      {
        "date": "2022-04-30T17:30:00.000Z",
        "voteCount": 2,
        "content": "x C. Add an RDS for MySQL read replica. (Would need program to use Read End Point, else it is good solution)\nD. Modify the RDS for MySQL database class to a bigger size and implement Provisioned IOPS (PIOPS).  (because of high CPU + no change in code)"
      },
      {
        "date": "2022-03-06T10:32:00.000Z",
        "voteCount": 3,
        "content": "CPU Utilization is High - This means the Database is underprovisioned - this drives the overall strategy to remediate the Instance class.  (D) is the only option"
      },
      {
        "date": "2022-01-12T07:18:00.000Z",
        "voteCount": 3,
        "content": "\"without requiring program to be changed\" - Adding read replica will need program to be changed to add replica endpoint. D should be the answer"
      },
      {
        "date": "2021-10-25T18:24:00.000Z",
        "voteCount": 1,
        "content": "B should be the answer"
      },
      {
        "date": "2021-10-18T19:03:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer Should be D because of \"Which approach should the database specialist to take to resolve this issue without changing the application?\" WITHOUT CHANGING THE APPLICATION. Option C would need add the read-endpoints to the application."
      },
      {
        "date": "2021-10-14T02:47:00.000Z",
        "voteCount": 1,
        "content": "After comprehensive testing and analysis, a database specialist discovers that there is high read latency and high CPU utilization on the DB instance.\n=&gt; C. Add read replicas for read SQL."
      },
      {
        "date": "2021-10-12T10:17:00.000Z",
        "voteCount": 1,
        "content": "option c is out since \"The site performs a high number of repetitive reads and writes\". So mostly Option D is correct"
      },
      {
        "date": "2021-10-18T04:40:00.000Z",
        "voteCount": 2,
        "content": "After comprehensive testing and analysis, a database specialist discovers that there is high read latency and high CPU utilization on the DB instance"
      },
      {
        "date": "2021-10-09T02:07:00.000Z",
        "voteCount": 3,
        "content": "The only issue with C is that the application needs to be changed to point read traffic to read replica. This is not allowed as per the question. In that case the instance size increase will take care of the high CPU utilization. PIOPS will be bonus (As per the option D).  I will go with option D."
      },
      {
        "date": "2021-10-06T15:13:00.000Z",
        "voteCount": 3,
        "content": "Both C and D will do the job.\n\"to resolve this issue without changing the application\" - with C you have at least balance some or all reads across master and replica, add a connection string to read replica. So this makes D a correct answer (even though I prefer answer C over D if not that formulation)."
      },
      {
        "date": "2021-10-05T10:30:00.000Z",
        "voteCount": 1,
        "content": "C. The read latency and CPU util would reduce as read queries would be routed to Read replica"
      },
      {
        "date": "2021-10-02T00:35:00.000Z",
        "voteCount": 1,
        "content": "It's debatable. BCD all look plausible options."
      },
      {
        "date": "2021-10-04T09:54:00.000Z",
        "voteCount": 1,
        "content": "Changing to C. There's no indication that the IO is high."
      },
      {
        "date": "2021-10-10T00:10:00.000Z",
        "voteCount": 1,
        "content": "application needs to change if C, to read from replica"
      },
      {
        "date": "2021-09-25T00:26:00.000Z",
        "voteCount": 1,
        "content": "C. Read replica answer seems to be reasonable considering other options listed."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/amazon/view/49010-exam-aws-certified-database-specialty-topic-1-question-118/",
    "body": "A banking company recently launched an Amazon RDS for MySQL DB instance as part of a proof-of-concept project. A database specialist has configured automated database snapshots. As a part of routine testing, the database specialist noticed one day that the automated database snapshot was not created.<br>Which of the following are possible reasons why the snapshot was not created? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA copy of the RDS automated snapshot for this DB instance is in progress within the same AWS Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA copy of the RDS automated snapshot for this DB instance is in progress in a different AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe RDS maintenance window is not configured.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe RDS DB instance is in the STORAGE_FULL state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRDS event notifications have not been enabled."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-23T16:56:00.000Z",
        "voteCount": 9,
        "content": "Per - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\n\nYour DB instance must be in the AVAILABLE state for automated backups to occur. Automated backups don't occur while your DB instance is in a state other than AVAILABLE, for example STORAGE_FULL.\n\nAutomated backups don't occur while a DB snapshot copy is running in the same AWS Region for the same DB instance."
      },
      {
        "date": "2023-09-09T04:02:00.000Z",
        "voteCount": 1,
        "content": "\"Automated backups follow these rules:\nYour DB instance must be in the available state for automated backups to occur. Automated backups don't occur while your DB instance is in a state other than available, for example, storage_full.\nAutomated backups don't occur while a DB snapshot copy is running in the same AWS Region for the same database.\"\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
      },
      {
        "date": "2022-04-29T09:01:00.000Z",
        "voteCount": 2,
        "content": "A. Automated backups don't occur while a DB snapshot copy is running in the same AWS Region for the same DB instance.\n\nD.  DB instance must be in the AVAILABLE state"
      },
      {
        "date": "2022-02-27T04:26:00.000Z",
        "voteCount": 1,
        "content": "since the database is not down, that is mean storage_full is not the case"
      },
      {
        "date": "2022-02-23T15:29:00.000Z",
        "voteCount": 1,
        "content": "A,D\nAutomated backups follow these rules:\n\nYour DB instance must be in the AVAILABLE state for automated backups to occur. Automated backups don't occur while your DB instance is in a state other than AVAILABLE, for example STORAGE_FULL.\n\nAutomated backups don't occur while a DB snapshot copy is running in the same AWS Region for the same DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
      },
      {
        "date": "2021-12-14T08:55:00.000Z",
        "voteCount": 2,
        "content": "AD is the answer"
      },
      {
        "date": "2021-10-01T21:42:00.000Z",
        "voteCount": 3,
        "content": "AD is the answer"
      },
      {
        "date": "2021-09-30T20:55:00.000Z",
        "voteCount": 1,
        "content": "AD final answer"
      },
      {
        "date": "2021-09-30T06:29:00.000Z",
        "voteCount": 1,
        "content": "\u00c0D is right."
      },
      {
        "date": "2021-09-22T03:24:00.000Z",
        "voteCount": 4,
        "content": "Answer is AD"
      },
      {
        "date": "2021-09-22T22:44:00.000Z",
        "voteCount": 4,
        "content": "I concur https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
      },
      {
        "date": "2021-10-07T13:17:00.000Z",
        "voteCount": 1,
        "content": "I agree with the link above"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/amazon/view/49005-exam-aws-certified-database-specialty-topic-1-question-119/",
    "body": "An online shopping company has a large inflow of shopping requests daily. As a result, there is a consistent load on the company's Amazon RDS database. A database specialist needs to ensure the database is up and running at all times. The database specialist wants an automatic notification system for issues that may cause database downtime or for configuration changes made to the database.<br>What should the database specialist do to achieve this? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Events event to send a notification using Amazon SNS on every API call logged in AWS CloudTrail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubscribe to an RDS event subscription and configure it to use an Amazon SNS topic to send notifications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon SES to send notifications based on configured Amazon CloudWatch Events events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon CloudWatch alarms on various metrics, such as FreeStorageSpace for the RDS instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable email notifications for AWS Trusted Advisor."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T14:15:00.000Z",
        "voteCount": 18,
        "content": "B and D looks correct. With low storage instance will go out of service causing downtime. \nTricky at the same time easy question."
      },
      {
        "date": "2021-09-26T00:44:00.000Z",
        "voteCount": 6,
        "content": "BD. Answer"
      },
      {
        "date": "2022-06-21T10:54:00.000Z",
        "voteCount": 1,
        "content": "My take B and D"
      },
      {
        "date": "2022-04-29T11:19:00.000Z",
        "voteCount": 4,
        "content": "B. Subscribe to an RDS event subscription and configure it to use an Amazon SNS topic to send notifications.\n\nD. Configure Amazon CloudWatch alarms on various metrics, such as FreeStorageSpace for the RDS instance."
      },
      {
        "date": "2022-03-04T20:52:00.000Z",
        "voteCount": 1,
        "content": "BD is correct"
      },
      {
        "date": "2022-02-24T13:55:00.000Z",
        "voteCount": 1,
        "content": "Valid options"
      },
      {
        "date": "2021-10-26T12:47:00.000Z",
        "voteCount": 1,
        "content": "A, D is answer\nA -&gt; configuration changes (Cloud trail can be used to detect api calls associated with any db change)\nD -&gt; cause database downtime (CloudWatch metrics)"
      },
      {
        "date": "2022-03-04T20:51:00.000Z",
        "voteCount": 1,
        "content": "No for two reasons\n1) Getting Cloudtrail for every event\n2) You wont be alerted to Alarms on things like Low  database storage space etc"
      },
      {
        "date": "2021-11-24T13:47:00.000Z",
        "voteCount": 6,
        "content": "A is sending notifications for every single API call logged in AWS CloudTrail. You don't want to do that :)"
      },
      {
        "date": "2021-10-16T13:30:00.000Z",
        "voteCount": 1,
        "content": "DB final answer"
      },
      {
        "date": "2021-09-23T13:34:00.000Z",
        "voteCount": 2,
        "content": "Shouldn't the answer be, A &amp; B?\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html"
      },
      {
        "date": "2021-10-12T12:00:00.000Z",
        "voteCount": 5,
        "content": "A is incorrect. We don't need notification on every API call logged."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/amazon/view/49003-exam-aws-certified-database-specialty-topic-1-question-120/",
    "body": "A large company has a variety of Amazon DB clusters. Each of these clusters has various configurations that adhere to various requirements. Depending on the team and use case, these configurations can be organized into broader categories.<br>A database administrator wants to make the process of storing and modifying these parameters more systematic. The database administrator also wants to ensure that changes to individual categories of configurations are automatically applied to all instances when required.<br>Which AWS service or feature will help automate and achieve this objective?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Systems Manager Parameter Store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDB parameter group\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Config",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Secrets Manager"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-14T07:42:00.000Z",
        "voteCount": 16,
        "content": "In AWS, DB Configurations are stored in DB Parameter Group. So B"
      },
      {
        "date": "2021-09-20T07:35:00.000Z",
        "voteCount": 6,
        "content": "The answer should be C?"
      },
      {
        "date": "2024-02-06T14:54:00.000Z",
        "voteCount": 1,
        "content": "There's Aurora Global Database allows a single Amazon Aurora database to span multiple AWS regions, enabling high performance of globally distributed applications."
      },
      {
        "date": "2023-07-05T15:43:00.000Z",
        "voteCount": 2,
        "content": "Should be C - AWS Config: https://aws.amazon.com/blogs/database/enforce-configuration-policies-for-your-amazon-rds-databases-using-aws-config/"
      },
      {
        "date": "2023-02-25T13:00:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/enforce-configuration-policies-for-your-amazon-rds-databases-using-aws-config/ - I will go with C .Company has multiple DB instances."
      },
      {
        "date": "2022-07-03T13:09:00.000Z",
        "voteCount": 2,
        "content": "DB Parameter Group"
      },
      {
        "date": "2022-06-30T23:20:00.000Z",
        "voteCount": 1,
        "content": "My take is on A. \nCertain paramters are only applied to certain instances ,, so different Parameter Group for different dbs.\nOn top DBA want to make sure certain configuration are applied commanly to all instance . \nThis can only be done though parameter store   which can be implemented using Cloud Formation template. \n\nParameter Store, a capability of AWS Systems Manager, provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter."
      },
      {
        "date": "2022-07-11T22:43:00.000Z",
        "voteCount": 1,
        "content": "Parameter hierarchies will ensure that configuration is properly categorized into wider groups and users. \nThe following example uses three hierarchy levels in the name to identify the following:\n\n/Environment/Type of computer/Application/Data\n\n/Dev/DBServer/MySQL/db-string13"
      },
      {
        "date": "2022-07-11T22:45:00.000Z",
        "voteCount": 2,
        "content": "So i believe Parameter store is correct."
      },
      {
        "date": "2022-05-31T09:41:00.000Z",
        "voteCount": 2,
        "content": "B, https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html"
      },
      {
        "date": "2023-04-09T19:47:00.000Z",
        "voteCount": 2,
        "content": "A DB parameter group is a collection of database engine parameter values that can be applied to one or more Amazon RDS, Amazon Aurora, or Amazon DocumentDB instances. By creating a DB parameter group, a database administrator can define and manage a set of database engine parameter values that apply to instances in a consistent manner.\n\nWith a DB parameter group, a database administrator can modify parameter values for multiple instances at once and have the changes automatically applied to all instances associated with the parameter group. This allows for centralized management and ensures that changes are applied consistently across all instances.\n\nAWS Systems Manager Parameter Store, AWS Config, and AWS Secrets Manager are all AWS services that can be used to store and manage configuration parameters. However, these services are not specifically designed for managing database cluster configurations in a systematic manner and may not provide the same level of automation and consistency as a DB parameter group."
      },
      {
        "date": "2022-04-30T09:01:00.000Z",
        "voteCount": 2,
        "content": "Organizations create Standard Parameter Groups (for RDS) and Cluster/DB level Parameter Groups (for Aurora). These settings are applied uniformly across all instances implementing them."
      },
      {
        "date": "2022-03-07T04:33:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it). B is correct"
      },
      {
        "date": "2022-03-05T13:03:00.000Z",
        "voteCount": 4,
        "content": "Organizations create Standard Parameter Groups (for RDS) and Cluster/DB level Parameter Groups (for  Aurora).  These settings are applied uniformly across all instances implementing them.\n\nAlso For people choosing System Manager Parameter Store - the SSM does not integrate with RDS. \n\nYou can use Parameter Store parameters with other Systems Manager capabilities and AWS services to retrieve secrets and configuration data from a central store. Parameters work with Systems Manager capabilities such as Run Command, Automation, and State Manager, capabilities of AWS Systems Manager. You can also reference parameters in a number of other AWS services, including the following:\nAmazon Elastic Compute Cloud (Amazon EC2)\nAmazon Elastic Container Service (Amazon ECS)\nAWS Secrets Manager\nAWS Lambda\nAWS CloudFormation\nAWS CodeBuild\nAWS CodePipeline\nAWS CodeDeploy"
      },
      {
        "date": "2022-02-27T14:22:00.000Z",
        "voteCount": 2,
        "content": "I vote for C since the requirement is to auto implement and validation.\nB is good but it work at instance level and admin will need to manage them seperatly"
      },
      {
        "date": "2022-02-02T09:47:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\nWho should use Parameter Store? - Any AWS customer who wants to have a centralized way to manage configuration data."
      },
      {
        "date": "2021-12-22T12:17:00.000Z",
        "voteCount": 2,
        "content": "\"... changes to are automatically implemented to all instances as necessary\"..  Can you use AWS Systems Manager Parameter Store for that? I don't think so. So, the correct answer should be B (DB parameter group)"
      },
      {
        "date": "2022-03-05T12:56:00.000Z",
        "voteCount": 2,
        "content": "I  agree- unless they are talking about CLOUDFORMATION and building Clusters and storing parameters centrally - this  might make sense...I feel its PG."
      },
      {
        "date": "2021-12-09T10:40:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer\nDatabase parameters specify how the database is configured. For example, database parameters can specify the amount of resources, such as memory, to allocate to a database."
      },
      {
        "date": "2021-12-07T06:08:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2021-11-27T08:20:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/amazon/view/49606-exam-aws-certified-database-specialty-topic-1-question-121/",
    "body": "A company is developing a new web application. An AWS CloudFormation template was created as a part of the build process.<br>Recently, a change was made to an AWS::RDS::DBInstance resource in the template. The CharacterSetName property was changed to allow the application to process international text. A change set was generated using the new template, which indicated that the existing DB instance should be replaced during an upgrade.<br>What should a database specialist do to prevent data loss during the stack upgrade?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the DB instance. Modify the template to add the DBSnapshotIdentifier property with the ID of the DB snapshot. Update the stack.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the stack policy using the aws cloudformation update-stack command and the set-stack-policy command, then make the DB resource protected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the DB instance. Update the stack. Restore the database to a new instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeactivate any applications that are using the DB instance. Create a snapshot of the DB instance. Modify the template to add the DBSnapshotIdentifier property with the ID of the DB snapshot. Update the stack and reactivate the applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "Reference:<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-09T00:23:00.000Z",
        "voteCount": 6,
        "content": "D. appears to be correct based on the URL listed."
      },
      {
        "date": "2024-04-05T14:36:00.000Z",
        "voteCount": 1,
        "content": "B  :: via stack policy"
      },
      {
        "date": "2022-04-30T08:49:00.000Z",
        "voteCount": 2,
        "content": "Deactivate any applications that are using the DB instance. \n-&gt; Create a snapshot of the DB instance.\n-&gt; Modify the template to add the DBSnapshotIdentifier property with the ID of the DB snapshot. \n-&gt; Update the stack \n-&gt; and reactivate the applications."
      },
      {
        "date": "2022-03-05T12:51:00.000Z",
        "voteCount": 1,
        "content": "D seems to the right answer"
      },
      {
        "date": "2021-10-23T18:55:00.000Z",
        "voteCount": 3,
        "content": "D.\nTo preserve your data, perform the following procedure:\n1.Deactivate any applications that are using the DB instance so that there's no activity on the DB instance.\n2.Create a snapshot of the DB instance. For more information about creating DB snapshots\n3.If you want to restore your instance using a DB snapshot, modify the updated template with your DB instance changes and add the DBSnapshotIdentifier property with the ID of the DB snapshot that you want to use\n4.Update the stack."
      },
      {
        "date": "2021-10-16T22:28:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2021-10-13T07:00:00.000Z",
        "voteCount": 2,
        "content": "D final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/amazon/view/49607-exam-aws-certified-database-specialty-topic-1-question-122/",
    "body": "A company recently acquired a new business. A database specialist must migrate an unencrypted 12 TB Amazon RDS for MySQL DB instance to a new AWS account. The database specialist needs to minimize the amount of time required to migrate the database.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the source DB instance in the source account. Share the snapshot with the destination account. In the target account, create a DB instance from the snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager to share the source DB instance with the destination account. Create a DB instance in the destination account using the shared resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the DB instance. Give the destination account access to the read replica. In the destination account, create a snapshot of the shared read replica and provision a new RDS for MySQL DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse mysqldump to back up the source database. Create an RDS for MySQL DB instance in the destination account. Use the mysql command to restore the backup in the destination database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T01:01:00.000Z",
        "voteCount": 10,
        "content": "Sharing an unencrypted manual DB snapshot enables authorized AWS accounts to directly restore a DB instance from the snapshot instead of taking a copy of it and restoring from that. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html\nHowever Resource Access Manager could not share non-Aurora cluster.\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html\nHence I will go with A."
      },
      {
        "date": "2022-06-21T10:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-04-29T09:55:00.000Z",
        "voteCount": 1,
        "content": "create snapshot -&gt; share with destination account -&gt; copy -&gt; create instance from snapshot"
      },
      {
        "date": "2022-03-12T01:05:00.000Z",
        "voteCount": 1,
        "content": "A -&gt; The database professional must keep the migration time to a minimum."
      },
      {
        "date": "2022-03-04T13:01:00.000Z",
        "voteCount": 3,
        "content": "A but the answer choice is insufficient. With just this you will see a downtime. You need binlog replication for minimal downtime (or DMS)"
      },
      {
        "date": "2022-02-27T06:04:00.000Z",
        "voteCount": 1,
        "content": "answer a"
      },
      {
        "date": "2021-12-27T01:46:00.000Z",
        "voteCount": 3,
        "content": "Ans: A\nMysqldump is too slow. If mysqlpump, I might consider this."
      },
      {
        "date": "2021-11-12T04:34:00.000Z",
        "voteCount": 1,
        "content": "12 TB dump meaningless and slow, then you need to share? Answer is A."
      },
      {
        "date": "2021-10-17T23:47:00.000Z",
        "voteCount": 3,
        "content": "A and D both are both but I think A is faster than D. mysqldump for 12T data is nearly impossible."
      },
      {
        "date": "2021-10-16T07:55:00.000Z",
        "voteCount": 3,
        "content": "A final answer"
      },
      {
        "date": "2021-10-04T19:15:00.000Z",
        "voteCount": 1,
        "content": "B looks good to me. Criteria is minimize migration time. 12TB snapshot will also take time."
      },
      {
        "date": "2021-09-24T19:41:00.000Z",
        "voteCount": 2,
        "content": "A is correct even though it is missing a crucial step of copying the snapshot in the target account. Simply sharing isn't sufficient."
      },
      {
        "date": "2021-09-19T13:12:00.000Z",
        "voteCount": 4,
        "content": "A. snapshot is faster than the backup. \n\nA appears to be the answer."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/amazon/view/49014-exam-aws-certified-database-specialty-topic-1-question-123/",
    "body": "A company has applications running on Amazon EC2 instances in a private subnet with no internet connectivity. The company deployed a new application that uses Amazon DynamoDB, but the application cannot connect to the DynamoDB tables. A developer already checked that all permissions are set correctly.<br>What should a database specialist do to resolve this issue while minimizing access to external resources?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a route to an internet gateway in the subnet's route table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a route to a NAT gateway in the subnet's route table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a new security group to the EC2 instances with an outbound rule to ports 80 and 443.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC endpoint for DynamoDB and add a route to the endpoint in the subnet's route table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-03-07T04:47:00.000Z",
        "voteCount": 5,
        "content": "Got this question in my exam. (i cleared it). D is correct"
      },
      {
        "date": "2023-06-17T16:49:00.000Z",
        "voteCount": 2,
        "content": "how do u know this your answer was correct?"
      },
      {
        "date": "2021-12-23T14:14:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      },
      {
        "date": "2023-12-13T20:22:00.000Z",
        "voteCount": 1,
        "content": "D. Create a VPC endpoint for DynamoDB and add a route to the endpoint in the subnet's route table."
      },
      {
        "date": "2023-09-21T10:55:00.000Z",
        "voteCount": 2,
        "content": "\"What should a database specialist do to resolve this issue while minimizing access to external resources?\"\nx A. an internet gateway is actually maximising access to external resources - distractor.\nx B. so is a NAT gateway.\nx C. what do you want with ports 80 and 443? Distractor.\nD. Create a VPC endpoint for DynamoDB: reasonable choice fulfilling the requirement."
      },
      {
        "date": "2022-04-30T13:31:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\n\nD. Create a VPC endpoint for DynamoDB -&gt; add this endpoint in the subnet's route table."
      },
      {
        "date": "2021-10-24T00:11:00.000Z",
        "voteCount": 1,
        "content": "D. Create a VPC endpoint for DynamoDB"
      },
      {
        "date": "2021-10-20T20:06:00.000Z",
        "voteCount": 2,
        "content": "D final answer"
      },
      {
        "date": "2021-10-16T21:19:00.000Z",
        "voteCount": 1,
        "content": "agree with D"
      },
      {
        "date": "2021-10-11T16:56:00.000Z",
        "voteCount": 2,
        "content": "Yes D is right. You need a VPC endpoint in this case."
      },
      {
        "date": "2021-10-07T04:07:00.000Z",
        "voteCount": 4,
        "content": "D. Answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/amazon/view/48873-exam-aws-certified-database-specialty-topic-1-question-124/",
    "body": "The Amazon CloudWatch metric for FreeLocalStorage on an Amazon Aurora MySQL DB instance shows that the amount of local storage is below 10 MB. A database engineer must increase the local storage available in the Aurora DB instance.<br>How should the database engineer meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance to use an instance class that provides more local SSD storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Aurora DB cluster to enable automatic volume resizing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the local storage by upgrading the database engine version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance and configure the required storage volume in the configuration section."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T01:39:00.000Z",
        "voteCount": 7,
        "content": "A. answer"
      },
      {
        "date": "2021-09-22T08:41:00.000Z",
        "voteCount": 4,
        "content": "Local storage is the key here. Not the Database storage."
      },
      {
        "date": "2022-05-31T07:35:00.000Z",
        "voteCount": 1,
        "content": "A, for local storage."
      },
      {
        "date": "2022-04-30T08:17:00.000Z",
        "voteCount": 3,
        "content": "change to instance class that provides more local SSD storage.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-local-storage/\n\nLocal storage for each Aurora instance in the cluster, based on the instance class. This storage type and size is bound to the instance class, and can be changed only by moving to a larger DB instance class."
      },
      {
        "date": "2022-02-24T20:40:00.000Z",
        "voteCount": 1,
        "content": "A\nYou can increase the amount of free storage space for an instance by choosing a larger DB instance class for your instance."
      },
      {
        "date": "2021-12-07T05:51:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2021-10-07T13:52:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      },
      {
        "date": "2021-09-30T13:06:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.Monitoring.Metrics.html - A"
      },
      {
        "date": "2021-11-03T05:12:00.000Z",
        "voteCount": 2,
        "content": "\"The amount of local storage available.\n\nUnlike for other DB engines, for Aurora DB instances this metric reports the amount of storage available to each DB instance. This value depends on the DB instance class (for pricing information, see the Amazon RDS product page). You can increase the amount of free storage space for an instance by choosing a larger DB instance class for your instance.\""
      },
      {
        "date": "2021-09-26T11:45:00.000Z",
        "voteCount": 1,
        "content": "Also I could remember a question on tier-0 vs tier-1 priority on Aurora, basically asking how to reduce failover time: if both master and replica are tier-0 or both master and replica should be tier-1."
      },
      {
        "date": "2021-09-23T10:35:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/amazon/view/48822-exam-aws-certified-database-specialty-topic-1-question-125/",
    "body": "A company has an ecommerce web application with an Amazon RDS for MySQL DB instance. The marketing team has noticed some unexpected updates to the product and pricing information on the website, which is impacting sales targets. The marketing team wants a database specialist to audit future database activity to help identify how and when the changes are being made.<br>What should the database specialist do to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS event subscription to the audit event type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable auditing of CONNECT and QUERY_DML events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSSH to the DB instance and review the database logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the database logs to Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Enhanced Monitoring on the DB instance."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T22:44:00.000Z",
        "voteCount": 11,
        "content": "B and D"
      },
      {
        "date": "2021-10-25T15:43:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/database/configuring-an-audit-log-to-capture-database-activities-for-amazon-rds-for-mysql-and-amazon-aurora-with-mysql-compatibility/"
      },
      {
        "date": "2024-01-13T00:19:00.000Z",
        "voteCount": 1,
        "content": "B and D."
      },
      {
        "date": "2023-04-12T18:16:00.000Z",
        "voteCount": 3,
        "content": "A rules out because RDS event can not monitor data self change, it only monitor RDS config and parameter change, like DB snapshot,DB parameter group and so forth.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html"
      },
      {
        "date": "2023-03-24T12:01:00.000Z",
        "voteCount": 4,
        "content": "CONNECT and QUERY_DML events are part of Aurora, not RDS, so answers A and D"
      },
      {
        "date": "2023-04-29T04:21:00.000Z",
        "voteCount": 3,
        "content": "CONNECT and QUERY_DML events are  available for RDS.  Your statement is false.\n\nhttps://aws.amazon.com/blogs/database/configuring-an-audit-log-to-capture-database-activities-for-amazon-rds-for-mysql-and-amazon-aurora-with-mysql-compatibility/"
      },
      {
        "date": "2022-05-17T17:28:00.000Z",
        "voteCount": 1,
        "content": "enabling server_audit_events to auditing of CONNECT AND QUERY_DML events."
      },
      {
        "date": "2022-04-30T07:26:00.000Z",
        "voteCount": 3,
        "content": "x A. RDS event not helpful for DML\nB. Enable auditing of CONNECT and QUERY_DML events.\nx C. SSH not allowed\nD. Publish the database logs to Amazon CloudWatch Logs.\nx E. Enhanced Monitoring for performance"
      },
      {
        "date": "2022-03-05T11:42:00.000Z",
        "voteCount": 2,
        "content": "B and D is correct. You ENABLE audit logging via MARIADB_AUDIT_PLUGIN. You then modify the rds instance &gt; select the  option to export Audit Logs  &gt; this publishes it to Cloudwatch"
      },
      {
        "date": "2021-12-02T05:21:00.000Z",
        "voteCount": 2,
        "content": "Option B, D"
      },
      {
        "date": "2021-10-14T07:51:00.000Z",
        "voteCount": 2,
        "content": "BD final answer"
      },
      {
        "date": "2021-10-11T09:50:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D are correct"
      },
      {
        "date": "2021-10-10T06:16:00.000Z",
        "voteCount": 1,
        "content": "Why not A along with B?"
      },
      {
        "date": "2021-12-01T01:14:00.000Z",
        "voteCount": 3,
        "content": "RDS Events are not for this, so right answer is B&amp;D. A is Incorrect:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/amazon/view/49007-exam-aws-certified-database-specialty-topic-1-question-126/",
    "body": "A large gaming company is creating a centralized solution to store player session state for multiple online games. The workload required key-value storage with low latency and will be an equal mix of reads and writes. Data should be written into the AWS Region closest to the user across the games' geographically distributed user base. The architecture should minimize the amount of overhead required to manage the replication of data between Regions.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for MySQL with multi-Region read replicas",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora global database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for Oracle with GoldenGate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB global tables\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-14T03:34:00.000Z",
        "voteCount": 14,
        "content": "D. Answer\nKey Value pair - DynamoDB Global tables"
      },
      {
        "date": "2023-09-14T01:11:00.000Z",
        "voteCount": 1,
        "content": "D. Amazon DynamoDB global tables\n\nkey-value NoSQL, multi-region, low latency."
      },
      {
        "date": "2023-09-14T01:12:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/dynamodb/global-tables/"
      },
      {
        "date": "2022-07-30T00:03:00.000Z",
        "voteCount": 1,
        "content": "I see the answer is showing as A. How is it justified for Key value Pair. Answer would be D. Please provide reason for answers .. also that would be helpful"
      },
      {
        "date": "2023-02-10T04:39:00.000Z",
        "voteCount": 4,
        "content": "you can not trust provided answer on this site"
      },
      {
        "date": "2022-07-16T14:46:00.000Z",
        "voteCount": 1,
        "content": "There is no Aurora global database, so the answer is D"
      },
      {
        "date": "2022-08-13T03:04:00.000Z",
        "voteCount": 3,
        "content": "There is"
      },
      {
        "date": "2022-06-08T21:00:00.000Z",
        "voteCount": 1,
        "content": "D. DynamoDB for sure"
      },
      {
        "date": "2022-04-29T09:11:00.000Z",
        "voteCount": 1,
        "content": "Dynamo Global\nhttps://aws.amazon.com/dynamodb/?nc1=h_ls"
      },
      {
        "date": "2022-02-23T17:15:00.000Z",
        "voteCount": 2,
        "content": "DynamoDB Global tables is obvious for Key Value pair"
      },
      {
        "date": "2022-02-23T15:38:00.000Z",
        "voteCount": 3,
        "content": "D, low latency and key-value"
      },
      {
        "date": "2021-12-14T08:57:00.000Z",
        "voteCount": 3,
        "content": "D. Answer"
      },
      {
        "date": "2021-12-06T18:41:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/dynamodb/?nc1=h_ls\nD"
      },
      {
        "date": "2021-11-12T04:17:00.000Z",
        "voteCount": 1,
        "content": "D. Obviously not Mysql: Question: low-latency key-value storage. This is DynamoDB local writes."
      },
      {
        "date": "2021-11-03T06:13:00.000Z",
        "voteCount": 2,
        "content": "D final answer"
      },
      {
        "date": "2021-10-25T14:38:00.000Z",
        "voteCount": 2,
        "content": "Prepare on Aurora upgrades both minor and major versions. There were a few questions on the topic today at my actual test."
      },
      {
        "date": "2021-09-29T07:43:00.000Z",
        "voteCount": 3,
        "content": "Answer is D.\nB. could work, but the question specifically asks for a key-value store, hence DynamoDB"
      },
      {
        "date": "2022-10-02T11:40:00.000Z",
        "voteCount": 2,
        "content": "B not work due to \"Aurora global database\" Replic DB is ready only"
      },
      {
        "date": "2021-09-22T22:44:00.000Z",
        "voteCount": 1,
        "content": "Since the question mentions key-value and also about gaming, I think the answer should be D in that case"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/amazon/view/48823-exam-aws-certified-database-specialty-topic-1-question-127/",
    "body": "A company is running an on-premises application comprised of a web tier, an application tier, and a MySQL database tier. The database is used primarily during business hours with random activity peaks throughout the day. A database specialist needs to improve the availability and reduce the cost of the MySQL database tier as part of the company's migration to AWS.<br>Which MySQL database option would meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for MySQL with Multi-AZ",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora Serverless MySQL cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora MySQL cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for MySQL with read replica"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T10:26:00.000Z",
        "voteCount": 13,
        "content": "B. Aurora Serverless\n\ntwo requirements, reduce the cost and availability. Aurora Serverless is cheaper compared to Aurora cluster"
      },
      {
        "date": "2021-10-19T17:39:00.000Z",
        "voteCount": 1,
        "content": "Why not A ?\n\n Amazon RDS for MySQL with Multi-AZ"
      },
      {
        "date": "2021-11-26T06:21:00.000Z",
        "voteCount": 2,
        "content": "The question asks for minimal costs. Multi-AZ is super expensive, literaly double the price of single AZ"
      },
      {
        "date": "2021-10-20T12:31:00.000Z",
        "voteCount": 1,
        "content": "ok.. B looks good.\n\n***primarily during business hours with random activity peaks throughout the day***"
      },
      {
        "date": "2022-02-27T08:53:00.000Z",
        "voteCount": 3,
        "content": "A only provide HA ad not cost effective"
      },
      {
        "date": "2021-12-08T19:24:00.000Z",
        "voteCount": 6,
        "content": "B is correct\nAmazon Aurora Serverless v1 is a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\nhttps://aws.amazon.com/rds/aurora/serverless/"
      },
      {
        "date": "2023-09-16T15:04:00.000Z",
        "voteCount": 1,
        "content": "B. Aurora Serverless\n\n\"database is used primarily during business hours with random activity peaks throughout the day\" is key to select aurora serverless."
      },
      {
        "date": "2023-05-12T00:22:00.000Z",
        "voteCount": 1,
        "content": "B. Aurora Serverless"
      },
      {
        "date": "2023-01-05T21:10:00.000Z",
        "voteCount": 3,
        "content": "\"database is used primarily during business hours with random activity peaks throughout the day\" is key to select aurora serverless."
      },
      {
        "date": "2022-09-21T19:32:00.000Z",
        "voteCount": 2,
        "content": "\"MySQL database tier as part of the company's migration to AWS\" so we need to keep \"tier\" for DB-host so we can't use Serverless . \"C\" is correct"
      },
      {
        "date": "2022-04-29T17:27:00.000Z",
        "voteCount": 2,
        "content": "accessed during business hours, with occasional bursts of activity throughout the day\nincrease the availability and minimize the cost"
      },
      {
        "date": "2022-02-24T15:13:00.000Z",
        "voteCount": 2,
        "content": "B for occasional queries and minimal cost"
      },
      {
        "date": "2022-02-16T05:34:00.000Z",
        "voteCount": 1,
        "content": "B\nAurora Serverless MySQL cluster"
      },
      {
        "date": "2021-10-27T06:51:00.000Z",
        "voteCount": 1,
        "content": "Considering the use cases described below, it is assumed that there is little traffic in the case of production use.\nAlso, since V2 is currently in preview, it cannot be expected to be in production.\nConsidering these, the Aurora DB cluster is considered to be a candidate, although the cost is higher than that of serverless.\nhttps://aws.amazon.com/rds/aurora/serverless/"
      },
      {
        "date": "2021-10-05T14:34:00.000Z",
        "voteCount": 1,
        "content": "B. Aurora Serverless"
      },
      {
        "date": "2021-10-02T16:11:00.000Z",
        "voteCount": 2,
        "content": "B final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/amazon/view/48824-exam-aws-certified-database-specialty-topic-1-question-128/",
    "body": "A company wants to migrate its Microsoft SQL Server Enterprise Edition database instance from on-premises to AWS. A deep review is performed and the AWS<br>Schema Conversion Tool (AWS SCT) provides options for running this workload on Amazon RDS for SQL Server Enterprise Edition, Amazon RDS for SQL Server<br>Standard Edition, Amazon Aurora MySQL, and Amazon Aurora PostgreSQL. The company does not want to use its own SQL server license and does not want to change from Microsoft SQL Server.<br>What is the MOST cost-effective and operationally efficient solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun SQL Server Enterprise Edition on Amazon EC2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun SQL Server Standard Edition on Amazon RDS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun SQL Server Enterprise Edition on Amazon RDS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun Amazon Aurora MySQL leveraging SQL Server on Linux compatibility libraries."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-09T06:41:00.000Z",
        "voteCount": 14,
        "content": "B.\nSCT assessment says that you can use MSSQL standard edition. Since they want to stay on SQL Server, that's the cheapest option and migrating is operationally easy"
      },
      {
        "date": "2021-10-16T18:04:00.000Z",
        "voteCount": 2,
        "content": "re-read that part, Standard Edition looks suffice. \nB. Answer."
      },
      {
        "date": "2023-08-22T22:54:00.000Z",
        "voteCount": 1,
        "content": "B. Run SQL Server Standard Edition on Amazon RDS."
      },
      {
        "date": "2023-07-21T18:19:00.000Z",
        "voteCount": 1,
        "content": "They don't want to change version. And rds provides the license."
      },
      {
        "date": "2023-06-17T18:55:00.000Z",
        "voteCount": 1,
        "content": "Standard edition on RDS"
      },
      {
        "date": "2022-05-01T09:17:00.000Z",
        "voteCount": 1,
        "content": "B. Run SQL Server Standard Edition on Amazon RDS."
      },
      {
        "date": "2021-12-24T14:12:00.000Z",
        "voteCount": 3,
        "content": "OPERATIONALLY EFFECTIVE = SQL Server on RDS\nMOST COST-EFFECTIVE = Standard Edition"
      },
      {
        "date": "2021-11-04T11:59:00.000Z",
        "voteCount": 1,
        "content": "This link seems to indicate that more information is required to determine if the Enterprise instance is a candidate for downgrading to Standard.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/determine-whether-your-microsoft-sql-server-database-can-be-downgraded-from-enterprise-to-standard-edition.html"
      },
      {
        "date": "2021-10-30T06:13:00.000Z",
        "voteCount": 1,
        "content": "When Enterprise license is included in RDS price at FREE why to go to standard edition and loose all great features so Option C is correct."
      },
      {
        "date": "2021-11-01T20:21:00.000Z",
        "voteCount": 4,
        "content": "Try to calculate price Enterprise will cost you almost double for same specification \nhttps://calculator.aws/#/createCalculator/RDSSQLServer"
      },
      {
        "date": "2021-10-17T07:00:00.000Z",
        "voteCount": 3,
        "content": "B final answer"
      },
      {
        "date": "2021-10-17T01:21:00.000Z",
        "voteCount": 2,
        "content": "I will go with C as Standard Edition is not operationally efficient as Enterprise Edition. Check https://docs.microsoft.com/en-us/sql/sql-server/editions-and-components-of-sql-server-2017?view=sql-server-ver15"
      },
      {
        "date": "2021-10-03T18:20:00.000Z",
        "voteCount": 3,
        "content": "C. Run SQL Server Enterprise Edition on AWS RDS\n\nLicense is included with the RDS price\nNeed Enterprise to Enterprise edition matching.\nOn EC2 you will use your own license."
      },
      {
        "date": "2021-10-24T17:39:00.000Z",
        "voteCount": 2,
        "content": "if standard edition is sufficient for the DB why to go for Enterprise edition and pay more??\ni think B is right"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/amazon/view/48874-exam-aws-certified-database-specialty-topic-1-question-129/",
    "body": "A company's ecommerce website uses Amazon DynamoDB for purchase orders. Each order is made up of a Customer ID and an Order ID. The DynamoDB table uses the Customer ID as the partition key and the Order ID as the sort key.<br>To meet a new requirement, the company also wants the ability to query the table by using a third attribute named Invoice ID. Queries using the Invoice ID must be strongly consistent. A database specialist must provide this capability with optimal performance and minimal overhead.<br>What should the database administrator do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a global secondary index on Invoice ID to the existing table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a local secondary index on Invoice ID to the existing table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the table by using the latest snapshot while adding a local secondary index on Invoice ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the partition key and a FilterExpression parameter with a filter on Invoice ID for all queries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T03:09:00.000Z",
        "voteCount": 10,
        "content": "C. Answer"
      },
      {
        "date": "2021-11-29T17:53:00.000Z",
        "voteCount": 5,
        "content": "Answer is C ;\n- It has to be a local secondary index\n- Secondary indexes can only be created when the table is created."
      },
      {
        "date": "2023-12-09T21:25:00.000Z",
        "voteCount": 1,
        "content": "What is wrong with D?"
      },
      {
        "date": "2023-09-29T05:34:00.000Z",
        "voteCount": 1,
        "content": "C. Recreate table while adding a local secondary index\n\n- strongly consistent has to be local secondary index\n- adding a local secondary index requires recreating of the table"
      },
      {
        "date": "2023-05-12T00:48:00.000Z",
        "voteCount": 1,
        "content": "C\nDue to clause \"minimal overhead\".\n\nif you obtain 100KB of data in step 1 and filter it down to 10KB in step 2, you'll use the read capacity units for 100KB of data instead of the 10KB that was filtered down. Further, a 1MB limit is applied to all operations, regardless of the read capacity units on a table."
      },
      {
        "date": "2023-04-25T19:38:00.000Z",
        "voteCount": 4,
        "content": "A. Add a global secondary index on Invoice ID to the existing table. - GSI doesn't support strong consistency\nB. Add a local secondary index on Invoice ID to the existing table. - LSI can be added only while creating the table. LSI Partition key has to be same as the main table's partition key i.e customer id\nC. Recreate the table by using the latest snapshot while adding a local secondary index on Invoice ID. - LSI Partition key has to be same as the main table's partition key i.e customer id\nD. Use the partition key and a FilterExpression parameter with a filter on Invoice ID for all queries. filter queries can be strongly consistent. If you require strongly consistent reads, set the ConsistentRead parameter to true in the Query request.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html"
      },
      {
        "date": "2023-05-10T04:49:00.000Z",
        "voteCount": 1,
        "content": "Ans is D.\nYou are right, C is wrong"
      },
      {
        "date": "2022-04-29T20:09:00.000Z",
        "voteCount": 2,
        "content": "Queries that make use of the Invoice ID must be very consistent =&gt; recreate table, as Local secondary index can only be created while creating the Dynamodb table. \n\nglobal secondary indexes are eventual consistent only"
      },
      {
        "date": "2021-11-30T03:45:00.000Z",
        "voteCount": 3,
        "content": "Option C"
      },
      {
        "date": "2021-10-28T16:55:00.000Z",
        "voteCount": 2,
        "content": "Why not A?. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"
      },
      {
        "date": "2021-11-01T03:56:00.000Z",
        "voteCount": 2,
        "content": "According to below link \"Strongly consistent reads are not supported on global secondary indexes\" https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
      },
      {
        "date": "2021-10-24T02:59:00.000Z",
        "voteCount": 1,
        "content": "C is correct, as Local secondary index can only be created while creating the Dynamodb table.  and query needs to use third attribute on top of primary and sort key, so Local Secondary index has primary and sort key as well as the third attribute. Global secondary index can be created without primary and sort key"
      },
      {
        "date": "2021-10-21T15:21:00.000Z",
        "voteCount": 1,
        "content": "cccccc"
      },
      {
        "date": "2021-10-14T20:26:00.000Z",
        "voteCount": 2,
        "content": "C final answer"
      },
      {
        "date": "2021-10-09T19:57:00.000Z",
        "voteCount": 3,
        "content": "C is correct. Local indexes provide strong consistency and cannot be created on existing tables."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/amazon/view/49707-exam-aws-certified-database-specialty-topic-1-question-130/",
    "body": "A company wants to migrate its on-premises MySQL databases to Amazon RDS for MySQL. To comply with the company's security policy, all databases must be encrypted at rest. RDS DB instance snapshots must also be shared across various accounts to provision testing and staging environments.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS for MySQL DB instance with an AWS Key Management Service (AWS KMS) customer managed CMK. Update the key policy to include the Amazon Resource Name (ARN) of the other AWS accounts as a principal, and then allow the kms:CreateGrant action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS for MySQL DB instance with an AWS managed CMK. Create a new key policy to include the Amazon Resource Name (ARN) of the other AWS accounts as a principal, and then allow the kms:CreateGrant action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS for MySQL DB instance with an AWS owned CMK. Create a new key policy to include the administrator user name of the other AWS accounts as a principal, and then allow the kms:CreateGrant action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS for MySQL DB instance with an AWS CloudHSM key. Update the key policy to include the Amazon Resource Name (ARN) of the other AWS accounts as a principal, and then allow the kms:CreateGrant action."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T09:03:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html - A"
      },
      {
        "date": "2024-01-02T20:06:00.000Z",
        "voteCount": 1,
        "content": "- KMS Customer managed CMK: The key is stored in your account that you created, own, and manage.\n- KMS AWS managed CMK: The key is stored in your account and is managed by AWS Key Management Service.\n\nSo you should choose Customer managed CMK"
      },
      {
        "date": "2022-04-29T17:35:00.000Z",
        "voteCount": 4,
        "content": "-Need customer managed CMK for sharing (B and C are out), C is also out for administrator user\n- AWS CloudHSM key is also  AWS generated (D is out)\n- policy update to include ARN of other AWS accounts as principal\n- CreateGrant action"
      },
      {
        "date": "2022-03-13T16:41:00.000Z",
        "voteCount": 4,
        "content": "A!\n\nYou can't share a snapshot that has been encrypted using the default KMS key of the AWS account that shared the snapshot, therefore it must be encrypted with a customer managed key. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html"
      },
      {
        "date": "2022-03-07T04:24:00.000Z",
        "voteCount": 1,
        "content": "Got this question in my exam. (i cleared it). A is correct"
      },
      {
        "date": "2022-02-24T18:51:00.000Z",
        "voteCount": 2,
        "content": "A: To allow another AWS account access to a KMS key, update the key policy for the KMS key. You update it with the Amazon Resource Name (ARN) of the AWS account that you are sharing to as Principal in the KMS key policy. Then you allow the kms:CreateGrant action."
      },
      {
        "date": "2022-02-24T15:15:00.000Z",
        "voteCount": 1,
        "content": "KMS managed CMK"
      },
      {
        "date": "2021-11-27T12:53:00.000Z",
        "voteCount": 2,
        "content": "CMK is needed for data share and you just need to update the key policy."
      },
      {
        "date": "2021-10-16T05:04:00.000Z",
        "voteCount": 1,
        "content": "Agree with A"
      },
      {
        "date": "2021-10-10T11:00:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      },
      {
        "date": "2021-09-19T12:40:00.000Z",
        "voteCount": 3,
        "content": "A. Answer\nKey to the answer CMK - Customer managed Key - If I am not wrong"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/amazon/view/49008-exam-aws-certified-database-specialty-topic-1-question-131/",
    "body": "A retail company manages a web application that stores data in an Amazon DynamoDB table. The company is undergoing account consolidation efforts. A database engineer needs to migrate the DynamoDB table from the current AWS account to a new AWS account.<br>Which strategy meets these requirements with the LEAST amount of administrative work?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to crawl the data in the DynamoDB table. Create a job using an available blueprint to export the data to Amazon S3. Import the data from the S3 file to a DynamoDB table in the new account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to scan the items of the DynamoDB table in the current account and write to a file in Amazon S3. Create another Lambda function to read the S3 file and restore the items of a DynamoDB table in the new account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Data Pipeline in the current account to export the data from the DynamoDB table to a file in Amazon S3. Use Data Pipeline to import the data from the S3 file to a DynamoDB table in the new account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon DynamoDB Streams for the DynamoDB table in the current account. Create an AWS Lambda function to read from the stream and write to a file in Amazon S3. Create another Lambda function to read the S3 file and restore the items to a DynamoDB table in the new account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T04:14:00.000Z",
        "voteCount": 11,
        "content": "I think the answer is C\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/"
      },
      {
        "date": "2022-07-07T08:26:00.000Z",
        "voteCount": 4,
        "content": "For me it is A because it seems to be easier with Glue than Data Pipeline (\"with the MINIMUM amount of administrative work\")\nGLUE: https://aws.amazon.com/de/premiumsupport/knowledge-center/dynamodb-cross-account-migration/\nDATA PIPELINE: https://aws.amazon.com/de/premiumsupport/knowledge-center/data-pipeline-account-access-dynamodb-s3/"
      },
      {
        "date": "2023-10-03T22:41:00.000Z",
        "voteCount": 2,
        "content": "Create a DynamoDB table in your source account.\n    Create an Amazon Simple Storage Service (Amazon S3) bucket in the destination account.\n    Attach an AWS Identity and Access Management (IAM) policy to the Data Pipeline default roles in the source account.\n    Create an S3 bucket policy in the destination account.\n    Create and activate a pipeline in the source account.\n    Create a DynamoDB table in the destination account.\n    Restore the DynamoDB export in the destination account.\n\nhttps://aws.amazon.com/de/blogs/database/how-to-migrate-amazon-dynamodb-tables-from-one-aws-account-to-another-with-aws-data-pipeline/"
      },
      {
        "date": "2023-10-01T11:55:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C, because:\nhttps://aws.amazon.com/blogs/database/how-to-migrate-amazon-dynamodb-tables-from-one-aws-account-to-another-with-aws-data-pipeline/"
      },
      {
        "date": "2023-09-27T06:27:00.000Z",
        "voteCount": 1,
        "content": "I think it is A"
      },
      {
        "date": "2023-09-23T10:53:00.000Z",
        "voteCount": 1,
        "content": "Migrate your DynamoDB table to a different AWS account with one of these methods that suit your use case:\nhttps://repost.aws/knowledge-center/dynamodb-cross-account-migration\n\n\n- AWS Backup\n- DynamoDB import and export to Amazon Simple Storage Service (Amazon S3)\n- Amazon S3 and AWS Glue\n- Amazon EMR"
      },
      {
        "date": "2023-07-19T20:59:00.000Z",
        "voteCount": 1,
        "content": "C looks like the best option\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/"
      },
      {
        "date": "2023-06-08T19:42:00.000Z",
        "voteCount": 2,
        "content": "A. Why not?"
      },
      {
        "date": "2023-04-29T04:50:00.000Z",
        "voteCount": 1,
        "content": "You can migrate your DynamoDB tables to a different AWS account by choosing one of the following methods depending on your use case:\n\nAWS Backup\nDynamoDB import and export to Amazon Simple Storage Service (Amazon S3)\nAmazon S3 and AWS Glue\nAWS Data Pipeline\nAmazon EMR"
      },
      {
        "date": "2023-05-31T10:28:00.000Z",
        "voteCount": 2,
        "content": "Amazon S3 and AWS Glue is on your list, listed before AWS Data Pipeline, and yet you select C?"
      },
      {
        "date": "2023-03-24T12:16:00.000Z",
        "voteCount": 1,
        "content": "Configure Amazon DynamoDB Streams for the DynamoDB table in the current account. Create an AWS Lambda function to read from the stream and write to a file in Amazon S3. Create another Lambda function to read the S3 file and restore the items to a DynamoDB table in the new account.\nThis approach leverages DynamoDB Streams, which captures item-level modifications to a table, including creates, updates, and deletes. The DynamoDB Streams data can be used to replicate data in near real-time across different AWS accounts. The approach allows for minimal administrative work as it only requires the creation of two Lambda functions to read and write to S3 and DynamoDB tables, respectively. This approach also ensures that data consistency is maintained during the migration process."
      },
      {
        "date": "2023-02-26T06:03:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/ . I initially thought answer was 'A' since answer had the word 'existing blueprint' . but , I went for C based on the link pasted above. Scroll to the 'Data Pipeline' section in there ."
      },
      {
        "date": "2022-12-21T09:00:00.000Z",
        "voteCount": 2,
        "content": "DATAPIEPLINE is the correct one \"Note: The destination account can't access the DynamoDB data in S3 bucket. To work with the data, restore it to a DynamoDB table. Data Pipeline provides the easiest method to move the table with the least manual effort. However, there are fewer options for customization.\""
      },
      {
        "date": "2022-04-30T13:08:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-cross-account-migration/\n\nexport dynamoDB to S3 in other account -&gt; use Glue job (or data pipeline or EMR) to import data\n\nC. Use AWS Data Pipeline in the current account to export the data from the DynamoDB table to a file in Amazon S3. Use Data Pipeline to import the data from the S3 file to a DynamoDB table in the new account."
      },
      {
        "date": "2021-11-19T12:49:00.000Z",
        "voteCount": 2,
        "content": "Why not A? being glue serverless wouldn't it be easier to do this way?"
      },
      {
        "date": "2021-10-29T20:10:00.000Z",
        "voteCount": 2,
        "content": "Agree with C, LEAST amount of work."
      },
      {
        "date": "2021-10-26T22:56:00.000Z",
        "voteCount": 1,
        "content": "I agree with C and also that the question is a bit ambigupus"
      },
      {
        "date": "2021-10-16T10:09:00.000Z",
        "voteCount": 4,
        "content": "C\nhttps://aws.amazon.com/premiumsupport/knowledge-center/data-pipeline-account-access-dynamodb-s3/"
      },
      {
        "date": "2021-10-12T03:47:00.000Z",
        "voteCount": 2,
        "content": "A seems to be the answer. \nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-dynamo-db-cross-account.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/amazon/view/49161-exam-aws-certified-database-specialty-topic-1-question-132/",
    "body": "A company uses the Amazon DynamoDB table contractDB in us-east-1 for its contract system with the following schema: orderID (primary key) timestamp (sort key) contract (map) createdBy (string) customerEmail (string)<br>After a problem in production, the operations team has asked a database specialist to provide an IAM policy to read items from the database to debug the application. In addition, the developer is not allowed to access the value of the customerEmail field to stay compliant.<br>Which IAM policy should the database specialist use to achieve these requirements?<br>A.<br><img src=\"/assets/media/exam-media/04237/0008100001.png\" class=\"in-exam-image\"><br>B.<br><img src=\"/assets/media/exam-media/04237/0008200001.png\" class=\"in-exam-image\"><br>C.<br><img src=\"/assets/media/exam-media/04237/0008300001.png\" class=\"in-exam-image\"><br>D.<br><img src=\"/assets/media/exam-media/04237/0008400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-04T00:58:00.000Z",
        "voteCount": 9,
        "content": "A. Answer"
      },
      {
        "date": "2023-01-13T04:14:00.000Z",
        "voteCount": 1,
        "content": "A is correct as per IAM guideline"
      },
      {
        "date": "2022-05-01T10:17:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html\n\nYou use the IAM Condition element to implement a fine-grained access control policy."
      },
      {
        "date": "2021-11-03T13:55:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. you have access to all columns except CustomerEmail!"
      },
      {
        "date": "2021-10-29T23:27:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      },
      {
        "date": "2021-10-16T20:27:00.000Z",
        "voteCount": 3,
        "content": "A.\nC would be correct if using ForAnyValue instead of ForAllValues"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/amazon/view/49011-exam-aws-certified-database-specialty-topic-1-question-133/",
    "body": "A company has an application that uses an Amazon DynamoDB table to store user data. Every morning, a single-threaded process calls the DynamoDB API Scan operation to scan the entire table and generate a critical start-of-day report for management. A successful marketing campaign recently doubled the number of items in the table, and now the process takes too long to run and the report is not generated in time.<br>A database specialist needs to improve the performance of the process. The database specialist notes that, when the process is running, 15% of the table's provisioned read capacity units (RCUs) are being used.<br>What should the database specialist do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable auto scaling for the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse four threads and parallel DynamoDB API Scan operations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDouble the table's provisioned RCUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Limit and Offset parameters before every call to the API."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-16T13:47:00.000Z",
        "voteCount": 13,
        "content": "B.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan"
      },
      {
        "date": "2024-01-12T23:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-08-30T14:46:00.000Z",
        "voteCount": 1,
        "content": "B. Use four threads and parallel DynamoDB API Scan operations."
      },
      {
        "date": "2023-01-13T04:19:00.000Z",
        "voteCount": 1,
        "content": "More thread is a key"
      },
      {
        "date": "2022-07-12T07:27:00.000Z",
        "voteCount": 1,
        "content": "B make sense but this has to done programmatically by the application team to process read by multi threaded application calls"
      },
      {
        "date": "2022-04-28T14:54:00.000Z",
        "voteCount": 2,
        "content": "15% means max 6 threads possible, 4 are good"
      },
      {
        "date": "2021-10-29T10:07:00.000Z",
        "voteCount": 1,
        "content": "Every morning, a single-threaded process calls the DynamoDB API Scan operation to scan the entire table ==&gt; B"
      },
      {
        "date": "2021-10-21T07:38:00.000Z",
        "voteCount": 2,
        "content": "B final answer"
      },
      {
        "date": "2021-10-15T00:43:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/#:~:text=To%20configure%20auto%20scaling%20in,alarms%20that%20track%20consumed%20capacity.\n\nAnswer should be A?"
      },
      {
        "date": "2022-01-11T05:01:00.000Z",
        "voteCount": 2,
        "content": "15% utilization of the allocated RCUs does not required to scale the Dynamo DB."
      },
      {
        "date": "2021-12-22T14:43:00.000Z",
        "voteCount": 1,
        "content": "No, it is B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/amazon/view/49017-exam-aws-certified-database-specialty-topic-1-question-134/",
    "body": "A company is building a software as a service application. As part of the new user sign-on workflow, a Python script invokes the CreateTable operation using the<br>Amazon DynamoDB API. After the call returns, the script attempts to call PutItem.<br>Occasionally, the PutItem request fails with a ResourceNotFoundException error, which causes the workflow to fail. The development team has confirmed that the same table name is used in the two API calls.<br>How should a database specialist fix this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an allow statement for the dynamodb:PutItem action in a policy attached to the role used by the application creating the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the StreamEnabled property of the StreamSpecification parameter to true, then call PutItem.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the application to call DescribeTable periodically until the TableStatus is ACTIVE, then call PutItem.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a ConditionExpression parameter in the PutItem request."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T11:00:00.000Z",
        "voteCount": 11,
        "content": "C. Answer"
      },
      {
        "date": "2024-01-12T23:53:00.000Z",
        "voteCount": 1,
        "content": "Answer C. The error happens OCCCASIONALLY. So you need to wait for the table creation to complete."
      },
      {
        "date": "2023-08-30T16:03:00.000Z",
        "voteCount": 1,
        "content": "C. Change the application to call DescribeTable periodically until the TableStatus is ACTIVE, then call PutItem. \n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n\n\"ResourceNotFoundException\nThe operation tried to access a nonexistent table or index. The resource might not be specified correctly, or its status might not be ACTIVE.\""
      },
      {
        "date": "2023-07-05T18:10:00.000Z",
        "voteCount": 1,
        "content": "DynamoDB responds with this error when you're trying to run an operation against non-existent or not active table."
      },
      {
        "date": "2023-06-30T05:13:00.000Z",
        "voteCount": 1,
        "content": "ConditionExpression\nA condition that must be satisfied in order for a conditional PutItem operation to succeed.\n\nAn expression can contain any of the following:\n\nFunctions: attribute_exists | attribute_not_exists | attribute_type | contains | begins_with | size\n\nThese function names are case-sensitive.\n\nComparison operators: = | &lt;&gt; | &lt; | &gt; | &lt;= | &gt;= | BETWEEN | IN\n\nLogical operators: AND | OR | NOT"
      },
      {
        "date": "2023-06-30T05:12:00.000Z",
        "voteCount": 1,
        "content": "D. answer. ConditionExpression\nA condition that must be satisfied in order for a conditional PutItem operation to succeed.\n\nAn expression can contain any of the following:\n\nFunctions: attribute_exists | attribute_not_exists | attribute_type | contains | begins_with | size\n\nThese function names are case-sensitive.\n\nComparison operators: = | &lt;&gt; | &lt; | &gt; | &lt;= | &gt;= | BETWEEN | IN\n\nLogical operators: AND | OR | NOT"
      },
      {
        "date": "2023-06-01T11:44:00.000Z",
        "voteCount": 1,
        "content": "The answer is C."
      },
      {
        "date": "2022-10-20T10:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is C.\n\nResourceNotFoundException\nThe operation tried to access a nonexistent table or index. The resource might not be specified correctly, or its status might not be ACTIVE.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html"
      },
      {
        "date": "2022-07-02T23:18:00.000Z",
        "voteCount": 1,
        "content": "Condition Expression is used for PutItem DML conditions. you can specify a condition expression to determine which items should be modified. If the condition expression evaluates to true, the operation succeeds; otherwise, the operation fails.\nTo evalute if the update or new enrty should be made if the key attributes are same. \nexample : The PutItem operation overwrites an item with the same key (if it exists). If you want to avoid this, use a condition expression. This allows the write to proceed only if the item in question does not already have the same key.\nC is correct."
      },
      {
        "date": "2022-04-30T07:34:00.000Z",
        "voteCount": 1,
        "content": "TableStatus should be ACTIVE"
      },
      {
        "date": "2021-11-01T07:45:00.000Z",
        "voteCount": 3,
        "content": "C https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html"
      },
      {
        "date": "2021-10-24T01:38:00.000Z",
        "voteCount": 1,
        "content": "C final answer"
      },
      {
        "date": "2021-10-23T18:07:00.000Z",
        "voteCount": 1,
        "content": "C is the right one"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/amazon/view/49012-exam-aws-certified-database-specialty-topic-1-question-135/",
    "body": "To meet new data compliance requirements, a company needs to keep critical data durably stored and readily accessible for 7 years. Data that is more than 1 year old is considered archival data and must automatically be moved out of the Amazon Aurora MySQL DB cluster every week. On average, around 10 GB of new data is added to the database every month. A database specialist must choose the most operationally efficient solution to migrate the archival data to<br>Amazon S3.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom script that exports archival data from the DB cluster to Amazon S3 using a SQL view, then deletes the archival data from the DB cluster. Launch an Amazon EC2 instance with a weekly cron job to execute the custom script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function that exports archival data from the DB cluster to Amazon S3 using a SELECT INTO OUTFILE S3 statement, then deletes the archival data from the DB cluster. Schedule the Lambda function to run weekly using Amazon EventBridge (Amazon CloudWatch Events).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure two AWS Lambda functions: one that exports archival data from the DB cluster to Amazon S3 using the mysqldump utility, and another that deletes the archival data from the DB cluster. Schedule both Lambda functions to run weekly using Amazon EventBridge (Amazon CloudWatch Events).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to continually export the archival data from the DB cluster to Amazon S3. Configure an AWS Data Pipeline process to run weekly that executes a custom SQL script to delete the archival data from the DB cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T20:18:00.000Z",
        "voteCount": 14,
        "content": "Going for B. since SELECT INTO OUTFILE S3 is available on Aurora. \nOption C. uses mysqldump who does not dump directly to S3"
      },
      {
        "date": "2024-01-12T23:57:00.000Z",
        "voteCount": 1,
        "content": "Anser is B. The data should be readily accessible (e.g. via Athena), so mysqldump is not useful"
      },
      {
        "date": "2023-08-30T16:51:00.000Z",
        "voteCount": 1,
        "content": "B. Configure an AWS Lambda function that exports archival data from the DB cluster to Amazon S3 using a SELECT INTO OUTFILE S3 statement, then deletes the archival data from the DB cluster. Schedule the Lambda function to run weekly using Amazon EventBridge (Amazon CloudWatch Events)."
      },
      {
        "date": "2022-10-29T16:16:00.000Z",
        "voteCount": 1,
        "content": "If the amount of data to be selected is large (more than 25 GB), we recommend that you use multiple SELECT INTO OUTFILE S3 statements to save the data to Amazon S3\n\nAnswer: B"
      },
      {
        "date": "2022-07-03T01:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct approch.\nC mysqldump can not dump into S3 \nhttps://aws.amazon.com/blogs/database/best-practices-for-exporting-and-importing-data-from-amazon-aurora-mysql-to-amazon-s3/"
      },
      {
        "date": "2022-04-28T19:08:00.000Z",
        "voteCount": 1,
        "content": "B because:\n1. Lambda function max run time is 15 min\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\n2. SELECT INTO OUTFILES3 is there, and 10GB data per week sounds reasonable to finish copying within 15 min\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.html\n\nAWS DMS can copy to S3 but option D says continuilly export, while we need weekly\nhttps://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/"
      },
      {
        "date": "2022-02-15T08:13:00.000Z",
        "voteCount": 3,
        "content": "running a continually DMS job would NOT be operationally efficient, when talk about which, serverless options combined with Lambda and EventBridge would be a much better choice; considering the volume of the weekly archival, the duration would not hit Lambda timeout; however, it seems like more development would be needed for C, cause Select Into Outfile S3 directly integrates with S3. So B."
      },
      {
        "date": "2022-01-07T22:26:00.000Z",
        "voteCount": 1,
        "content": "Option A requires more effort and hence can be ruled out. Option B uses same lambda function for data migration and deletion thereafter. It doesn't work as lambda might timeout. Option C uses mysqldump which is ok but not as efficient as DMS. Option D is the correct solution in my view."
      },
      {
        "date": "2021-12-27T21:50:00.000Z",
        "voteCount": 1,
        "content": "For option D, I will consider how DMS export data export to S3 looks like and also how DMS handle \"delete\" CDC statements. With DMS option, you need additional tasks to filter data and not easy to maintain."
      },
      {
        "date": "2021-12-26T09:42:00.000Z",
        "voteCount": 1,
        "content": "I think its D\nhttps://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/"
      },
      {
        "date": "2021-12-23T11:36:00.000Z",
        "voteCount": 1,
        "content": "Lambda functions have 15mins max execution time. If the extract and delete takes longer than 15 mins using a Lambda function won't work. This limitation might rule out option B and C.    Option D will work but \"continually export the archival data\" is not a requirement. \n\nThoughts?"
      },
      {
        "date": "2022-02-06T18:07:00.000Z",
        "voteCount": 1,
        "content": "Good catch on the 15 min limit for Lambda! But in the context of the question - \" Each month, around 10 GB of fresh data is uploaded to the database.\" -  I would assume 2.5 GB weekly data volume - seems reasonable to assume that the export and delete will be done within 15 min. so B is still an option here"
      },
      {
        "date": "2021-10-19T19:43:00.000Z",
        "voteCount": 2,
        "content": "B final answer"
      },
      {
        "date": "2021-10-12T11:34:00.000Z",
        "voteCount": 4,
        "content": "B is correct.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.html"
      },
      {
        "date": "2021-10-10T08:59:00.000Z",
        "voteCount": 2,
        "content": "Option B correct"
      },
      {
        "date": "2021-09-20T22:40:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D?\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\n\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html"
      },
      {
        "date": "2021-10-28T06:59:00.000Z",
        "voteCount": 1,
        "content": "I think it should be D too. I think it provides \"most operationally efficient solution to migrate the archival data to Amazon S3 \". \nhttps://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/\nhttps://aws.amazon.com/blogs/database/replicate-data-from-amazon-aurora-to-amazon-s3-with-aws-database-migration-service/"
      },
      {
        "date": "2021-12-01T10:28:00.000Z",
        "voteCount": 1,
        "content": "Can you use AWS Data Pipeline process Custom SQL Query to delete data from RDS?"
      },
      {
        "date": "2021-12-23T11:31:00.000Z",
        "voteCount": 1,
        "content": "Yes you can but I'm not sure if using DMS is the right option"
      },
      {
        "date": "2022-10-06T19:15:00.000Z",
        "voteCount": 1,
        "content": "DMS is for DB migration tools &amp; very $$$ so we just use time by time but let it run as job tools. so Lambda is right tools for this ."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/amazon/view/48876-exam-aws-certified-database-specialty-topic-1-question-136/",
    "body": "A company developed a new application that is deployed on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances use the security group named sg-application-servers. The company needs a database to store the data from the application and decides to use an Amazon RDS for MySQL DB instance. The DB instance is deployed in a private DB subnet.<br>What is the MOST restrictive configuration for the DB instance security group?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly allow incoming traffic from the sg-application-servers security group on port 3306.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly allow incoming traffic from the sg-application-servers security group on port 443.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly allow incoming traffic from the subnet of the application servers on port 3306.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly allow incoming traffic from the subnet of the application servers on port 443."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T01:34:00.000Z",
        "voteCount": 14,
        "content": "A. Answer\n\nDatabase port 3306 and better to allow only the specific subnet instead of the entire subnet."
      },
      {
        "date": "2022-05-01T08:44:00.000Z",
        "voteCount": 1,
        "content": "Do we allow traffic from security group \nOR\nfrom resources that are assigned to the same security group ?"
      },
      {
        "date": "2023-08-23T04:47:00.000Z",
        "voteCount": 1,
        "content": "A. Only allow incoming traffic from the sg-application-servers security group on port 3306."
      },
      {
        "date": "2023-08-28T21:15:00.000Z",
        "voteCount": 1,
        "content": "Because    1) port 3306 is default port number for mySQL  2) a security group has to be explicitly assigned to an EC2 instance."
      },
      {
        "date": "2023-05-19T05:29:00.000Z",
        "voteCount": 2,
        "content": "Where on earth do these \"Correct answers\" come from?\n\nAllowing connections only from members of sg-application-servers is more restrictive than allowing traffic from the whole subnet. 3306 is probably the correct port for RDS. Therefore A"
      },
      {
        "date": "2022-11-03T16:56:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C:\nSecurity groups contains rules allowing or denying access to specified IP address and TCP Ports. Then they are associated with resources (such as ec2, rds, etc).\nThe question says that EC2 use the security group sg-application-servers. That is the sg-application-servers contains the rules that at this moment make the ec2 communication work. To allow the RDS instance talk with this ec2, is necessary to create a SG and specify the address of the ec2 instances in the ingress rules, referencing the port 3306. Or, allow the  traffic from the entire subnet at this same port.\nI've read the security group doc again and dont see nothing about grouping aws resources to reference as some kind of \"security resource group\" as the A answer say.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
      },
      {
        "date": "2023-02-10T20:07:00.000Z",
        "voteCount": 1,
        "content": "C is wrong. Because allow the whole subnets mean allow all application on that subnet --&gt; not secure.\nFYI: security group doesn't have 'Deny' rule.\nA is the answer. Security A allow traffic from Security B mean that Security A allow all resources using Security B"
      },
      {
        "date": "2022-05-01T08:46:00.000Z",
        "voteCount": 3,
        "content": "A. Only allow incoming traffic from the sg-application-servers security group on port 3306."
      },
      {
        "date": "2022-01-12T06:04:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-11-05T16:00:00.000Z",
        "voteCount": 1,
        "content": "Answer A. most restrictive approach is to allow only incoming connections from SG of EC2 instance on port 3306"
      },
      {
        "date": "2021-10-25T05:10:00.000Z",
        "voteCount": 2,
        "content": "AAAAAAAAAAAA"
      },
      {
        "date": "2021-10-03T18:21:00.000Z",
        "voteCount": 1,
        "content": "A final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/amazon/view/48845-exam-aws-certified-database-specialty-topic-1-question-137/",
    "body": "A company is moving its fraud detection application from on premises to the AWS Cloud and is using Amazon Neptune for data storage. The company has set up a 1 Gbps AWS Direct Connect connection to migrate 25 TB of fraud detection data from the on-premises data center to a Neptune DB instance. The company already has an Amazon S3 bucket and an S3 VPC endpoint, and 80% of the company's network bandwidth is available.<br>How should the company perform this data load?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS SDK with a multipart upload to transfer the data from on premises to the S3 bucket. Use the Copy command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to transfer the data from on premises to the S3 bucket. Use the Copy command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-14T16:40:00.000Z",
        "voteCount": 8,
        "content": "Answer is C. since DMS can only use databases (and S3) as sources, and the question does not specify that the on-prem data resides in a DB. In which case, Datasync is a more likely choice."
      },
      {
        "date": "2021-11-03T10:00:00.000Z",
        "voteCount": 6,
        "content": "This question was asked in my exam. I went with C."
      },
      {
        "date": "2023-09-15T19:36:00.000Z",
        "voteCount": 1,
        "content": "C. Use AWS DataSync to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.\n\nAWS DataSync is used to transfer data from On-prem to S3, no source db is mentioned. \nTo load data from S3 to AWS Neptune to use loader command\n\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
      },
      {
        "date": "2022-05-01T09:53:00.000Z",
        "voteCount": 2,
        "content": "DMS target can not be S3\n\nAWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage services.\n\nC. Use AWS DataSync to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.\n\naws DataSync is the way to transfer data from on-prem to S3, no source db is mentioned.\nOnly way to load data from S3 to AWS Neptune is using loader\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
      },
      {
        "date": "2022-10-14T06:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. But DMS target can be S3. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"
      },
      {
        "date": "2022-03-07T05:08:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it).  C is correct"
      },
      {
        "date": "2022-01-12T06:25:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B, use DMS.\n\n\nData Sync does not read from databases (https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html).\n\n\"AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, and also between AWS storage services.\""
      },
      {
        "date": "2022-01-12T06:26:00.000Z",
        "voteCount": 1,
        "content": "Sorry .. Answer is C. \n\nMy error, the questions says move fraud data, nothing is mentioned about database."
      },
      {
        "date": "2021-11-06T15:04:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer. \naws DataSync is the way to transfer data from on-prem to S3, no source db is mentioned. \nOnly way to load data from S3 to AWS Neptune is using loader\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
      },
      {
        "date": "2021-11-04T10:02:00.000Z",
        "voteCount": 1,
        "content": "C is the answer\nIf on-premise DB was mentioned I would choose B"
      },
      {
        "date": "2021-11-01T20:57:00.000Z",
        "voteCount": 1,
        "content": "C makes sense"
      },
      {
        "date": "2021-10-25T20:46:00.000Z",
        "voteCount": 1,
        "content": "Between B and C that are the only plausible options C seems more legit for the given scenario."
      },
      {
        "date": "2021-10-03T23:10:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B I guess"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/amazon/view/49013-exam-aws-certified-database-specialty-topic-1-question-138/",
    "body": "A company migrated one of its business-critical database workloads to an Amazon Aurora Multi-AZ DB cluster. The company requires a very low RTO and needs to improve the application recovery time after database failovers.<br>Which approach meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the max_connections parameter to 16,000 in the instance-level parameter group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the client connection timeout to 300 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS Proxy database proxy and update client connections to point to the proxy endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the query cache at the instance level."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-29T15:53:00.000Z",
        "voteCount": 15,
        "content": "C looks to be the winner. \nAmazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66% and database credentials, authentication, and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM)."
      },
      {
        "date": "2021-09-22T04:22:00.000Z",
        "voteCount": 6,
        "content": "Can the answer be C?\n\nhttps://aws.amazon.com/rds/proxy/"
      },
      {
        "date": "2021-10-11T23:43:00.000Z",
        "voteCount": 1,
        "content": "I'm sorry, you're correct. Answer is C."
      },
      {
        "date": "2023-09-12T18:20:00.000Z",
        "voteCount": 1,
        "content": "C. Amazon RDS Proxy database proxy\n\n\"Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66% and database credentials, authentication, and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM).\"\n\nhttps://aws.amazon.com/rds/proxy/"
      },
      {
        "date": "2022-05-06T20:19:00.000Z",
        "voteCount": 1,
        "content": "When your primary instance is down ABD won't work."
      },
      {
        "date": "2022-04-28T17:53:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/rds/proxy/"
      },
      {
        "date": "2022-03-05T17:10:00.000Z",
        "voteCount": 2,
        "content": "Definitely C. Proxies maintain a pool of client connections that respond to failovers and actually IMPROVE failover times by 66% https://aws.amazon.com/rds/proxy/"
      },
      {
        "date": "2022-02-27T15:26:00.000Z",
        "voteCount": 1,
        "content": "i vote for C,\nthe key is \"application recovery time after database failovers\""
      },
      {
        "date": "2021-11-04T06:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct. A D is not relevant, B even makes failover time slower."
      },
      {
        "date": "2021-11-03T03:27:00.000Z",
        "voteCount": 2,
        "content": "C final answer"
      },
      {
        "date": "2021-10-10T09:20:00.000Z",
        "voteCount": 1,
        "content": "B. Answer"
      },
      {
        "date": "2021-10-15T04:01:00.000Z",
        "voteCount": 1,
        "content": "Oops. Answer is C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/amazon/view/48844-exam-aws-certified-database-specialty-topic-1-question-139/",
    "body": "A company is using an Amazon RDS for MySQL DB instance for its internal applications. A security audit shows that the DB instance is not encrypted at rest. The company's application team needs to encrypt the DB instance.<br>What should the team do to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the DB instance and modify it to enable encryption. Apply this setting immediately without waiting for the next scheduled RDS maintenance window.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the DB instance and create an encrypted snapshot. Restore the encrypted snapshot to a new encrypted DB instance. Delete the original DB instance, and update the applications to point to the new encrypted DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the DB instance and create a snapshot. Copy the snapshot into another encrypted snapshot. Restore the encrypted snapshot to a new encrypted DB instance. Delete the original DB instance, and update the applications to point to the new encrypted DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted read replica of the DB instance. Promote the read replica to master. Delete the original DB instance, and update the applications to point to the new encrypted DB instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-16T21:57:00.000Z",
        "voteCount": 8,
        "content": "C. Answer"
      },
      {
        "date": "2021-10-13T08:15:00.000Z",
        "voteCount": 5,
        "content": "C. Answer"
      },
      {
        "date": "2023-09-12T19:58:00.000Z",
        "voteCount": 2,
        "content": "C is correct. You can only enable encryption during copy of the snapshot or when you create DB instance ."
      },
      {
        "date": "2022-10-29T04:50:00.000Z",
        "voteCount": 2,
        "content": "C is the right choice. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html"
      },
      {
        "date": "2022-05-01T05:27:00.000Z",
        "voteCount": 3,
        "content": "C. Stop the DB instance and create a snapshot. Copy the snapshot into another encrypted snapshot. Restore the encrypted snapshot to a new encrypted DB instance. Delete the original DB instance, and update the applications to point to the new encrypted DB instance."
      },
      {
        "date": "2022-04-13T01:13:00.000Z",
        "voteCount": 4,
        "content": "C is the way. You can only enable encryption during copy operation of the snapshot."
      },
      {
        "date": "2021-11-04T17:19:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-21T22:37:00.000Z",
        "voteCount": 1,
        "content": "C final answer"
      },
      {
        "date": "2021-10-17T03:56:00.000Z",
        "voteCount": 1,
        "content": "Deleting the old database was kind of an unnecessary step added there."
      },
      {
        "date": "2021-10-19T04:24:00.000Z",
        "voteCount": 1,
        "content": "Eh, maybe required for compliance I suppose but C it is."
      },
      {
        "date": "2021-10-04T16:07:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/amazon/view/48843-exam-aws-certified-database-specialty-topic-1-question-140/",
    "body": "A database specialist must create nightly backups of an Amazon DynamoDB table in a mission-critical workload as part of a disaster recovery strategy.<br>Which backup methodology should the database specialist use to MINIMIZE management overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS CLI on an Amazon EC2 instance. Write a CLI command that creates a backup of the DynamoDB table. Create a scheduled job or task that runs the command on a nightly basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that creates a backup of the DynamoDB table. Create an Amazon CloudWatch Events rule that runs the Lambda function on a nightly basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a backup plan using AWS Backup, specify a backup frequency of every 24 hours, and give the plan a nightly backup window.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure DynamoDB backup and restore for an on-demand backup frequency of every 24 hours."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-25T20:30:00.000Z",
        "voteCount": 2,
        "content": "c is correct"
      },
      {
        "date": "2022-08-13T06:06:00.000Z",
        "voteCount": 3,
        "content": "The key word here is \"minimum overhead\". Using AWS Backup would insure that.\nRef: https://aws.amazon.com/blogs/database/set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/"
      },
      {
        "date": "2022-06-27T17:49:00.000Z",
        "voteCount": 2,
        "content": "C is correct, If you go in the Dynamodb console and click scheduled backup it just takes you to aws backup to create a plan. Since it's nightly scheduled backups that's needed then aws back. If they wanted ondemand manual backup then Ondemand in Dynamodb"
      },
      {
        "date": "2022-06-21T10:26:00.000Z",
        "voteCount": 1,
        "content": "C is correct as it is using backup service. D option can only take manula backup."
      },
      {
        "date": "2022-04-29T10:02:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html"
      },
      {
        "date": "2022-04-29T10:03:00.000Z",
        "voteCount": 1,
        "content": "correction: Ans is C\n\nuse AWS backup service"
      },
      {
        "date": "2022-03-12T01:14:00.000Z",
        "voteCount": 3,
        "content": "C is better than D, as the latter introduces administration overhead: Amazon DynamoDB supports stand-alone on-demand backup and restores features. You can create table backups using the console, the AWS Command Line Interface (AWS CLI), or the DynamoDB API.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html"
      },
      {
        "date": "2022-02-27T06:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is C.\ni'm not sure why they are providing the wrong answers for most of the questions"
      },
      {
        "date": "2022-02-23T17:34:00.000Z",
        "voteCount": 1,
        "content": "agree with other comments"
      },
      {
        "date": "2022-02-16T07:30:00.000Z",
        "voteCount": 1,
        "content": "C\nIf you don't want to create scheduling scripts and cleanup jobs, you can use AWS Backup to create backup plans with schedules and retention policies for your DynamoDB tables. AWS Backup runs the backups and deletes them when they expire. For more information, see the AWS Backup Developer Guide."
      },
      {
        "date": "2022-02-06T00:57:00.000Z",
        "voteCount": 1,
        "content": "Why not D"
      },
      {
        "date": "2022-02-04T09:54:00.000Z",
        "voteCount": 3,
        "content": "Answer is C\nTo be compliance with disaster recovery it\u00b4s necessary to use AWS Backup: (link bellow) To create backup copies across AWS accounts and Regions and for other advanced features, you should use AWS Backup.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html"
      },
      {
        "date": "2022-01-22T07:36:00.000Z",
        "voteCount": 1,
        "content": "C seems more approprite, as it has more control"
      },
      {
        "date": "2021-12-15T05:53:00.000Z",
        "voteCount": 1,
        "content": "Ans is C more control"
      },
      {
        "date": "2021-12-06T19:00:00.000Z",
        "voteCount": 1,
        "content": "C and D do the same thing, more D reduce overhead.\nanswer is D"
      },
      {
        "date": "2021-12-01T15:34:00.000Z",
        "voteCount": 2,
        "content": "Answer is C.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CreateBackup.html#:~:text=If%20you%20don%27t%20want%20to%20create%20scheduling%20scripts%20and%20cleanup%20jobs%2C%20you%20can%20use%20AWS%20Backup%20to%20create%20backup%20plans%20with%20schedules%20and%20retention%20policies%20for%20your%20DynamoDB%20tables.%20AWS%20Backup%20runs%20the%20backups%20and%20deletes%20them%20when%20they%20expire.%20For%20more%20information%2C%20see%20the%20AWS%20Backup%20Developer%20Guide."
      },
      {
        "date": "2021-10-22T11:17:00.000Z",
        "voteCount": 3,
        "content": "Answer is C, as per the following link:\nIf you don't want to create scheduling scripts and cleanup jobs, you can use AWS Backup to create backup plans with schedules and retention policies for your DynamoDB tables. AWS Backup runs the backups and deletes them when they expire\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html"
      },
      {
        "date": "2021-10-21T22:40:00.000Z",
        "voteCount": 3,
        "content": "C final answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/amazon/view/78766-exam-aws-certified-database-specialty-topic-1-question-141/",
    "body": "A company is using a Single-AZ Amazon RDS for MySQL DB instance for development. The DB instance is experiencing slow performance when queries run.<br>Amazon CloudWatch metrics indicate that the instance requires more I/O capacity.<br>Which actions can a database specialist perform to resolve this issue? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestart the application tool used to run queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange to a database instance class with higher throughput.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert from Single-AZ to Multi-AZ.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the I/O parameter in Amazon RDS Enhanced Monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert from General Purpose to Provisioned IOPS (PIOPS).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T11:55:00.000Z",
        "voteCount": 3,
        "content": "My choices..."
      },
      {
        "date": "2022-11-01T09:47:00.000Z",
        "voteCount": 1,
        "content": "The I/O capacity of the instance is based on the instance storage type and size. \n\nSo Answer is  B &amp; E"
      },
      {
        "date": "2022-09-03T05:27:00.000Z",
        "voteCount": 2,
        "content": "D.  Increase the I/O parameter in Amazon RDS Enhanced Monitoring.  - not the right answer. Just enabling enhanced monitoring is not going to help the performance.   \n\ncorrect Answer: B &amp; E"
      },
      {
        "date": "2022-08-31T13:31:00.000Z",
        "voteCount": 3,
        "content": "B &amp; E\n\nhttps://aws.amazon.com/blogs/database/best-storage-practices-for-running-production-workloads-on-hosted-databases-with-amazon-rds-or-amazon-ec2/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/amazon/view/48841-exam-aws-certified-database-specialty-topic-1-question-142/",
    "body": "A company has an AWS CloudFormation template written in JSON that is used to launch new Amazon RDS for MySQL DB instances. The security team has asked a database specialist to ensure that the master password is automatically rotated every 30 days for all new DB instances that are launched using the template.<br>What is the MOST operationally efficient solution to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the password in an Amazon S3 object. Encrypt the S3 object with an AWS KMS key. Set the KMS key to be rotated every 30 days by setting the EnableKeyRotation property to true. Use a CloudFormation custom resource to read the S3 object to extract the password.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to rotate the secret. Modify the CloudFormation template to add an AWS::SecretsManager::RotationSchedule resource. Configure the RotationLambdaARN value and, for the RotationRules property, set the AutomaticallyAfterDays parameter to 30.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation template to use the AWS KMS key as the database password. Configure an Amazon EventBridge rule to invoke the KMS API to rotate the key every 30 days by setting the ScheduleExpression parameter to ***/30***.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate the Amazon RDS for MySQL DB instances with AWS IAM and centrally manage the master database user password."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T00:29:00.000Z",
        "voteCount": 12,
        "content": "B. Answer"
      },
      {
        "date": "2022-12-17T19:32:00.000Z",
        "voteCount": 2,
        "content": "B is the answer. AWS secret supports rotation"
      },
      {
        "date": "2022-05-29T01:55:00.000Z",
        "voteCount": 1,
        "content": "B. Answer"
      },
      {
        "date": "2022-04-29T11:28:00.000Z",
        "voteCount": 2,
        "content": "Secrete Manager -&gt; Lambda to rotate secret -&gt; modify Cloud formation to add rotation schedule\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html"
      },
      {
        "date": "2022-03-12T23:44:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html"
      },
      {
        "date": "2022-03-04T12:55:00.000Z",
        "voteCount": 4,
        "content": "(B) is the correct answer. (Who is making the official chosen answers? Almost all of  them are wrong - the community ones are the correct ones)"
      },
      {
        "date": "2022-02-24T13:56:00.000Z",
        "voteCount": 1,
        "content": "Lambda with Secrets Manager works perfectly"
      },
      {
        "date": "2022-02-23T18:08:00.000Z",
        "voteCount": 1,
        "content": "B - Lambda with secretsManager."
      },
      {
        "date": "2021-11-06T16:33:00.000Z",
        "voteCount": 2,
        "content": "B final answer"
      },
      {
        "date": "2021-10-11T00:11:00.000Z",
        "voteCount": 2,
        "content": "This question was asked in my exam. B is correct."
      },
      {
        "date": "2021-09-28T22:43:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't the answer be B?"
      },
      {
        "date": "2021-09-30T05:18:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/amazon/view/48840-exam-aws-certified-database-specialty-topic-1-question-143/",
    "body": "A startup company is building a new application to allow users to visualize their on-premises and cloud networking components. The company expects billions of components to be stored and requires responses in milliseconds. The application should be able to identify:<br>\u2711 The networks and routes affected if a particular component fails.<br>\u2711 The networks that have redundant routes between them.<br>\u2711 The networks that do not have redundant routes between them.<br>\u2711 The fastest path between two networks.<br>Which database engine meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora MySQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Neptune\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon ElastiCache for Redis",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T07:06:00.000Z",
        "voteCount": 6,
        "content": "Answer is B"
      },
      {
        "date": "2024-04-03T23:05:00.000Z",
        "voteCount": 1,
        "content": "The use cases show that its a graph db"
      },
      {
        "date": "2023-06-01T13:59:00.000Z",
        "voteCount": 1,
        "content": "Graphs = Neptune (B!)"
      },
      {
        "date": "2022-05-01T11:00:00.000Z",
        "voteCount": 1,
        "content": "B. Amazon Neptune"
      },
      {
        "date": "2021-10-28T15:28:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBBBBB\n\nrelationship based store is Nepture."
      },
      {
        "date": "2021-10-27T08:53:00.000Z",
        "voteCount": 4,
        "content": "B final answer"
      },
      {
        "date": "2021-10-17T02:23:00.000Z",
        "voteCount": 3,
        "content": "This question was asked in my exam. Looks like aws do not have many variety of questions on Neptune."
      },
      {
        "date": "2021-10-12T08:57:00.000Z",
        "voteCount": 2,
        "content": "B. answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/amazon/view/48839-exam-aws-certified-database-specialty-topic-1-question-144/",
    "body": "An online retail company is planning a multi-day flash sale that must support processing of up to 5,000 orders per second. The number of orders and exact schedule for the sale will vary each day. During the sale, approximately 10,000 concurrent users will look at the deals before buying items. Outside of the sale, the traffic volume is very low. The acceptable performance for read/write queries should be under 25 ms. Order items are about 2 KB in size and have a unique identifier. The company requires the most cost-effective solution that will automatically scale and is highly available.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB with on-demand capacity mode\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora with one writer node and an Aurora Replica with the parallel query feature enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB with provisioned capacity mode with 5,000 write capacity units (WCUs) and 10,000 read capacity units (RCUs)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora with one writer node and two cross-Region Aurora Replicas"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T08:56:00.000Z",
        "voteCount": 14,
        "content": "I'll go for A.\nI think C. is a trap; if you are writing 5000 items at 2KB/second then you need 10000 WCU and 5000 RCU (assuming eventually consistent reads). C. has it in reverse."
      },
      {
        "date": "2021-10-12T16:46:00.000Z",
        "voteCount": 3,
        "content": "And C. does not automatically scale as in the requirement"
      },
      {
        "date": "2021-10-20T09:47:00.000Z",
        "voteCount": 1,
        "content": "A seems to be correct \nI think so as well,  c is in reverse"
      },
      {
        "date": "2021-10-21T11:08:00.000Z",
        "voteCount": 2,
        "content": "For 5000 RCU you are assuming strongly consistent reads. If it's an eventual consistent read you only need 2500 RCU"
      },
      {
        "date": "2023-10-11T09:08:00.000Z",
        "voteCount": 1,
        "content": "the next closest answer would be DynamoDB with Auto scaling but that it not part of the answers hence on-demand is the best answer"
      },
      {
        "date": "2023-08-07T04:31:00.000Z",
        "voteCount": 1,
        "content": "With 2KB size for 5,000 orders per second we need 10 000 WCU (1 table write/sec = 1 WCU with blocks of 1KB). C suggest 5,000 write capacity units (WCUs), not enough."
      },
      {
        "date": "2022-10-16T22:43:00.000Z",
        "voteCount": 1,
        "content": "1250 RCUs can read 10,000 KB. A is correct"
      },
      {
        "date": "2022-10-16T22:43:00.000Z",
        "voteCount": 1,
        "content": "1250 RCUs can read 10,000 KB if its eventual consistent read. A is correct"
      },
      {
        "date": "2022-10-18T22:11:00.000Z",
        "voteCount": 2,
        "content": "2500 RCU for reads"
      },
      {
        "date": "2022-05-17T03:07:00.000Z",
        "voteCount": 1,
        "content": "I'll go for A. C is not scalable &amp; cost affective."
      },
      {
        "date": "2022-04-29T20:15:00.000Z",
        "voteCount": 1,
        "content": "25 milliseconds\nOutside of the sale, the amount of traffic is really minimal\nThe business seeks the most cost-effective solution possible that is both highly accessible and scales automatically."
      },
      {
        "date": "2021-12-21T12:18:00.000Z",
        "voteCount": 3,
        "content": "It is A"
      },
      {
        "date": "2021-11-30T03:51:00.000Z",
        "voteCount": 4,
        "content": "Option A"
      },
      {
        "date": "2021-11-06T22:12:00.000Z",
        "voteCount": 3,
        "content": "he number of orders and exact schedule for the sale will vary each day. During the sale, approximately 10,000 concurrent users will look at the deals before buying items. Outside of the sale, the traffic volume is very low\n==&gt; Setting provisioning DynamoDB fix read 5000/write 10000 with will waste the resource when the traffic is low. It is not cost-effective.\nI go with A"
      },
      {
        "date": "2021-11-06T14:03:00.000Z",
        "voteCount": 2,
        "content": "A final answer"
      },
      {
        "date": "2021-10-25T16:09:00.000Z",
        "voteCount": 3,
        "content": "c does not mention auto scaling. in addition, 10000 WCU are needed. I will go with A"
      },
      {
        "date": "2021-10-23T12:35:00.000Z",
        "voteCount": 1,
        "content": "C seems correct\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
      },
      {
        "date": "2021-11-01T13:21:00.000Z",
        "voteCount": 1,
        "content": "Correction. A is the answer"
      },
      {
        "date": "2021-09-22T21:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/amazon/view/48838-exam-aws-certified-database-specialty-topic-1-question-145/",
    "body": "A ride-hailing application uses an Amazon RDS for MySQL DB instance as persistent storage for bookings. This application is very popular and the company expects a tenfold increase in the user base in next few months. The application experiences more traffic during the morning and evening hours.<br>This application has two parts:<br>\u2711 An in-house booking component that accepts online bookings that directly correspond to simultaneous requests from users.<br>\u2711 A third-party customer relationship management (CRM) component used by customer care representatives. The CRM uses queries to access booking data.<br>A database specialist needs to design a cost-effective database solution to handle this workload.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ElastiCache for Redis to accept the bookings. Associate an AWS Lambda function to capture changes and push the booking data to the RDS for MySQL DB instance used by the CRM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB to accept the bookings. Enable DynamoDB Streams and associate an AWS Lambda function to capture changes and push the booking data to an Amazon SQS queue. This triggers another Lambda function that pulls data from Amazon SQS and writes it to the RDS for MySQL DB instance used by the CRM.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ElastiCache for Redis to accept the bookings. Associate an AWS Lambda function to capture changes and push the booking data to an Amazon Redshift database used by the CRM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB to accept the bookings. Enable DynamoDB Streams and associate an AWS Lambda function to capture changes and push the booking data to Amazon Athena, which is used by the CRM."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-12T13:33:00.000Z",
        "voteCount": 11,
        "content": "In A \"AWS Lambda function to capture changes\" capture changes to what? ElastiCache? The main use of ElastiCache is to cache frequently read data. Also \"the company expects a tenfold increase in the user base\" and \"correspond to simultaneous requests from users\" suggest DynamoDB imo. Since we already have MySQL RDS I prefer answer B"
      },
      {
        "date": "2021-10-15T15:31:00.000Z",
        "voteCount": 4,
        "content": "Actually, It is D. based on the increasing the overall, we cannot go for B."
      },
      {
        "date": "2021-10-24T00:08:00.000Z",
        "voteCount": 4,
        "content": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.\nIt is DB query service serverless for data stored on S3. It is itself not a DB storage"
      },
      {
        "date": "2023-05-26T16:43:00.000Z",
        "voteCount": 1,
        "content": "Athena can read from DynamoDB... This kind of answers show that you guys are only studying for the cert without any actual experience. Setting up a whole new RDS clusters just so some CRM can read from it will cost your company thousands of dollars per month."
      },
      {
        "date": "2023-09-30T09:56:00.000Z",
        "voteCount": 1,
        "content": "Athena is more cost-effective than RDS.\nI prefer D."
      },
      {
        "date": "2021-10-01T21:57:00.000Z",
        "voteCount": 7,
        "content": "This question wasn't asked but in summary a lot of questions exactly from this 145 set questions were asked. I passed with a score of 794. Bunch of new questions on Aurora upgrades and maintenance windows, MS SQL server and Oracle migrations. All the best \ud83d\udc4d"
      },
      {
        "date": "2022-02-27T08:15:00.000Z",
        "voteCount": 1,
        "content": "you are trying to mention score to give gradability to your answer, you only get pass or fail but not the score"
      },
      {
        "date": "2021-10-30T13:46:00.000Z",
        "voteCount": 2,
        "content": "I gave exam today. Only 60% questions came in actual exam from this 145 set."
      },
      {
        "date": "2021-11-12T19:14:00.000Z",
        "voteCount": 1,
        "content": "Do you have the 145 questions.I could see only 112 question in the site"
      },
      {
        "date": "2023-06-08T00:02:00.000Z",
        "voteCount": 1,
        "content": "Although I do not understant why one should use one Lambda function to read from the DunamoDB stream, write to an SQS queue and then another Lambda to read from SQS and write to RDS... B is what makes most sense. \n\nBut I would have considered using only one Lambda function and no SQS."
      },
      {
        "date": "2023-04-23T17:28:00.000Z",
        "voteCount": 1,
        "content": "I go with Answer D.  \nFor booking we need a persistent DB which is a dynamoDB but querying the data through Athena is more cost effective way."
      },
      {
        "date": "2022-12-23T13:57:00.000Z",
        "voteCount": 1,
        "content": "Chose this option because this architecture definitely needs a SQS component being that it's a ride hailing application.  Only makes sense"
      },
      {
        "date": "2022-04-29T12:41:00.000Z",
        "voteCount": 1,
        "content": "no use of Elasticache + Redis ( A, C)\nAthena is not DB (D)"
      },
      {
        "date": "2022-03-13T05:36:00.000Z",
        "voteCount": 4,
        "content": "- Redis is an in-memory data store, therefore not suitable for bookings persistence. A and C are incorrect.\n- Athena is an interactive query service, not a database. D is incorrect."
      },
      {
        "date": "2022-02-27T08:13:00.000Z",
        "voteCount": 1,
        "content": "B is the answer , we need the data in mysql"
      },
      {
        "date": "2022-02-24T14:25:00.000Z",
        "voteCount": 2,
        "content": "CRM needs data in MySQL"
      },
      {
        "date": "2022-02-16T09:51:00.000Z",
        "voteCount": 2,
        "content": "i go with B\nyou need to save data to MySQL \nA is not persistence"
      },
      {
        "date": "2021-12-04T05:22:00.000Z",
        "voteCount": 1,
        "content": "ElastiCache, either MemcaheD or Redis, is used for data frequently read. And Amazon Athena is for querying data stored in Amazon S3. Considering all the features provided by the services, Option B is the best option."
      },
      {
        "date": "2021-11-24T17:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-11-24T04:01:00.000Z",
        "voteCount": 4,
        "content": "It\u00b4s B"
      },
      {
        "date": "2021-11-03T08:40:00.000Z",
        "voteCount": 3,
        "content": "should be A"
      },
      {
        "date": "2021-11-02T03:45:00.000Z",
        "voteCount": 1,
        "content": "@guru_ji ? only 60% ?  , hm... you can send me email"
      },
      {
        "date": "2021-10-30T02:25:00.000Z",
        "voteCount": 1,
        "content": "I gave exam today. Only 60% questions came in actual exam."
      },
      {
        "date": "2021-10-28T08:32:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/amazon/view/64693-exam-aws-certified-database-specialty-topic-1-question-146/",
    "body": "An online advertising website uses an Amazon DynamoDB table with on-demand capacity mode as its data store. The website also has a DynamoDB Accelerator<br>(DAX) cluster in the same VPC as its web application server. The application needs to perform infrequent writes and many strongly consistent reads from the data store by querying the DAX cluster.<br>During a performance audit, a systems administrator notices that the application can look up items by using the DAX cluster. However, the QueryCacheHits metric for the DAX cluster consistently shows 0 while the QueryCacheMisses metric continuously keeps growing in Amazon CloudWatch.<br>What is the MOST likely reason for this occurrence?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA VPC endpoint was not added to access DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStrongly consistent reads are always passed through DAX to DynamoDB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamoDB is scaling due to a burst in traffic, resulting in degraded performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA VPC endpoint was not added to access CloudWatch."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-23T11:52:00.000Z",
        "voteCount": 6,
        "content": "B is correct.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\n\n\"If the request specifies strongly consistent reads, DAX passes the request through to DynamoDB. The results from DynamoDB are not cached in DAX. Instead, they are simply returned to the application.\""
      },
      {
        "date": "2023-09-11T21:22:00.000Z",
        "voteCount": 1,
        "content": "B. Strongly consistent reads are always passed through DAX to DynamoDB."
      },
      {
        "date": "2022-12-23T15:47:00.000Z",
        "voteCount": 1,
        "content": "My thoughts - if strongly consistency reads are requested it will pass through DAX to the DB."
      },
      {
        "date": "2022-04-28T19:27:00.000Z",
        "voteCount": 3,
        "content": "strong consistency =&gt; bypass DAX"
      },
      {
        "date": "2022-03-05T17:58:00.000Z",
        "voteCount": 2,
        "content": "Strongly  Consistent Reads  are  considered PASS THROUGH and will never update DAX\nInterestingly - TransactionGetItems  is also a PASS THROUGH but TransactionWriteItems is not!!"
      },
      {
        "date": "2022-02-25T12:02:00.000Z",
        "voteCount": 3,
        "content": "B:\nIf the request specifies eventually consistent reads (the default behavior), it tries to read the item from DAX:\n\nIf DAX has the item available (a cache hit), DAX returns the item to the application without accessing DynamoDB.\nIf DAX does not have the item available (a cache miss), DAX passes the request through to DynamoDB. When it receives the response from DynamoDB, DAX returns the results to the application. But it also writes the results to the cache on the primary node.\n\n\nIf the request specifies strongly consistent reads, DAX passes the request through to DynamoDB. The results from DynamoDB are not cached in DAX. Instead, they are simply returned to the application."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/amazon/view/64692-exam-aws-certified-database-specialty-topic-1-question-147/",
    "body": "A financial company recently launched a portfolio management solution. The backend of the application is powered by Amazon Aurora with MySQL compatibility.<br>The company requires an RTO of 5 minutes and an RPO of 5 minutes. A database specialist must configure an efficient disaster recovery solution with minimal replication lag.<br>Which approach should the database specialist take to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Database Migration Service (AWS DMS) and create a replica in a different AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Aurora global database and add a different AWS Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a binlog and create a replica in a different AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a cross-Region read replica."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-27T04:49:00.000Z",
        "voteCount": 1,
        "content": "Option D could be also correct, but according to documentation:\n\"The promotion process takes a few minutes to complete. When you promote a read replica, replication is stopped and the DB instances are rebooted. When the reboot is complete, the read replica is available as a new DB cluster.\"\nTherefore it does not suit the condition, that RTO schould be less than 5 minutes"
      },
      {
        "date": "2023-04-23T17:38:00.000Z",
        "voteCount": 2,
        "content": "B\nAurora Global cluster RPO typically &lt; 1Mins and RTO depends on how soon we detach the other region cluster from Global cluster and promote to primary. With simple Lambda/jenkins job this can be easily achievable within &lt; 5 Mins."
      },
      {
        "date": "2023-01-28T23:31:00.000Z",
        "voteCount": 1,
        "content": "Verfied."
      },
      {
        "date": "2022-06-22T00:03:00.000Z",
        "voteCount": 1,
        "content": "B Aurora Global Tables. ( As an alternative to cross-Region read replicas, you can scale read operations with minimal lag time by using an Aurora global database.)"
      },
      {
        "date": "2022-04-29T12:22:00.000Z",
        "voteCount": 1,
        "content": "5 min RTO 5 min RPO\n Aurora GLOBAL RPO 1second and RTO 1 minute"
      },
      {
        "date": "2022-03-13T05:13:00.000Z",
        "voteCount": 4,
        "content": "Amazon Aurora global databases span multiple AWS Regions, enabling low latency global reads and providing fast recovery from the rare outage that might affect an entire AWS Region. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\nhttps://www.amazonaws.cn/en/rds/aurora/faqs/"
      },
      {
        "date": "2022-03-04T21:28:00.000Z",
        "voteCount": 4,
        "content": "question is vague. what does \"The firm demands a response time of five minutes and a response time of five minutes. \" mean? I assume they mean RTO and RPO. If 5 minutes is the same for both then Aurora GLOBAL will suffice (RPO  1second and RTO 1 minute)"
      },
      {
        "date": "2022-02-24T14:13:00.000Z",
        "voteCount": 1,
        "content": "Efficient is key - takes our global DBs\nRead-replicas are efficient and low-latency"
      },
      {
        "date": "2022-04-12T05:08:00.000Z",
        "voteCount": 2,
        "content": "Global DB uses storage replication  = low replication latency, Answer is B"
      },
      {
        "date": "2022-02-16T09:24:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2021-12-12T00:05:00.000Z",
        "voteCount": 4,
        "content": "B for me\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html"
      },
      {
        "date": "2021-12-09T08:34:00.000Z",
        "voteCount": 2,
        "content": "B for me\nhttps://aws.amazon.com/blogs/database/how-to-choose-the-best-disaster-recovery-option-for-your-amazon-aurora-mysql-cluster/"
      },
      {
        "date": "2021-11-13T15:48:00.000Z",
        "voteCount": 1,
        "content": "If cost is not a concern, global database is a better option than read replica."
      },
      {
        "date": "2021-11-02T12:45:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is B\n\nhttps://aws.amazon.com/about-aws/whats-new/2019/11/aurora-supports-in-place-conversion-to-global-database/\n\nhttps://aws.amazon.com/blogs/database/how-to-choose-the-best-disaster-recovery-option-for-your-amazon-aurora-mysql-cluster/\n\n\"Aurora Global Database provides the lowest consistent RTO and RPO option while requiring the least management overhead.\""
      },
      {
        "date": "2021-11-11T12:45:00.000Z",
        "voteCount": 3,
        "content": "I think you are right, the other options would work but B seems to be the faster"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/amazon/view/65860-exam-aws-certified-database-specialty-topic-1-question-148/",
    "body": "A company hosts an internal file-sharing application running on Amazon EC2 instances in VPC_A. This application is backed by an Amazon ElastiCache cluster, which is in VPC_B and peered with VPC_A. The company migrates its application instances from VPC_A to VPC_B. Logs indicate that the file-sharing application no longer can connect to the ElastiCache cluster.<br>What should a database specialist do to resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second security group on the EC2 instances. Add an outbound rule to allow traffic from the ElastiCache cluster security group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the ElastiCache security group. Add an interface VPC endpoint to enable the EC2 instances to connect to the ElastiCache cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the ElastiCache security group by adding outbound rules that allow traffic to VPC_B's CIDR blocks from the ElastiCache cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the ElastiCache security group by adding an inbound rule that allows traffic from the EC2 instances' security group to the ElastiCache cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-07T16:20:00.000Z",
        "voteCount": 1,
        "content": "Why not B? https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-privatelink.html"
      },
      {
        "date": "2022-04-29T14:32:00.000Z",
        "voteCount": 3,
        "content": "You can update security groups to reference peer VPC group\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html"
      },
      {
        "date": "2022-03-04T22:53:00.000Z",
        "voteCount": 3,
        "content": "Has to be D"
      },
      {
        "date": "2022-02-24T15:12:00.000Z",
        "voteCount": 2,
        "content": "peering is best option"
      },
      {
        "date": "2022-01-29T13:08:00.000Z",
        "voteCount": 1,
        "content": "Why not B?"
      },
      {
        "date": "2021-12-08T18:48:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-11-27T12:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-11-11T16:04:00.000Z",
        "voteCount": 4,
        "content": "Option D."
      },
      {
        "date": "2021-11-26T06:05:00.000Z",
        "voteCount": 1,
        "content": "I don\u00b4t know if its possible.. Since SGs are attached to the VPC, can you use it to reference on another VPC?"
      },
      {
        "date": "2021-12-08T18:45:00.000Z",
        "voteCount": 1,
        "content": "You can update security groups to reference peer VPC group\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html"
      },
      {
        "date": "2021-11-13T18:28:00.000Z",
        "voteCount": 2,
        "content": "I think you are right"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/amazon/view/65970-exam-aws-certified-database-specialty-topic-1-question-149/",
    "body": "A database specialist must load 25 GB of data files from a company's on-premises storage to an Amazon Neptune database.<br>Which approach to load the data is FASTEST?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to Amazon S3 and use the Loader command to load the data from Amazon S3 into the Neptune database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a utility to read the data from the on-premises storage and run INSERT statements in a loop to load the data into the Neptune database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to load the data directly from the on-premises storage into the Neptune database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to load the data directly from the on-premises storage into the Neptune database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-20T14:50:00.000Z",
        "voteCount": 8,
        "content": "they point to the right place but the answer is wrong, should be A:\n1.Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n2. Create an IAM role with Read and List access to the bucket.\n3. Create an Amazon S3 VPC endpoint.\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n5. The Neptune DB instance assumes the IAM role to load the data from the bucket."
      },
      {
        "date": "2023-06-30T20:52:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/datasync/\n\nD is not correct as DataSync cannot write into the Neptune directly!"
      },
      {
        "date": "2023-05-19T07:06:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
      },
      {
        "date": "2023-03-14T11:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2023-02-25T10:59:00.000Z",
        "voteCount": 4,
        "content": "Option D, on the other hand, allows for direct transfer of data from the on-premises storage to Neptune using AWS DataSync, which can be a more efficient and cost-effective option. Additionally, DataSync can handle large data sets and can automatically handle network interruptions and failures during the data transfer process.\n\nTherefore, if speed is a top priority, option D is likely to be the fastest approach for loading the 25 GB of data files to the Neptune database."
      },
      {
        "date": "2023-05-19T07:11:00.000Z",
        "voteCount": 2,
        "content": "The problem with that is that AWS DataSync cannot \"load data into Neptune\". So the solution of uploading data to S3 and then using the Neptune Loader to ingest the data is still the closest we get..."
      },
      {
        "date": "2022-04-30T19:15:00.000Z",
        "voteCount": 2,
        "content": "A. Upload the data to Amazon S3 and use the Loader command to load the data from Amazon S3 into the Neptune database.\n\n1.Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n2. Create an IAM role with Read and List access to the bucket.\n3. Create an Amazon S3 VPC endpoint.\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n5. The Neptune DB instance assumes the IAM role to load the data from the bucket."
      },
      {
        "date": "2022-03-07T05:03:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it). A is correct"
      },
      {
        "date": "2021-11-13T09:28:00.000Z",
        "voteCount": 2,
        "content": "Answer A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/amazon/view/65935-exam-aws-certified-database-specialty-topic-1-question-150/",
    "body": "A finance company needs to make sure that its MySQL database backups are available for the most recent 90 days. All of the MySQL databases are hosted on<br>Amazon RDS for MySQL DB instances. A database specialist must implement a solution that meets the backup retention requirement with the least possible development effort.<br>Which approach should the database specialist take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to build a backup plan for the required retention period. Assign the DB instances to the backup plan.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instances to enable the automated backup option. Select the required backup retention period.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate a daily cron job on an Amazon EC2 instance to create MySQL dumps, transfer to Amazon S3, and implement an S3 Lifecycle policy to meet the retention requirement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to schedule a daily manual snapshot of the DB instances. Delete snapshots that exceed the retention requirement."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-27T01:54:00.000Z",
        "voteCount": 5,
        "content": "Ans: A\nAuto Backup is 35 Max. Creating Backup plan and Create on-demand backup are the solution."
      },
      {
        "date": "2023-10-25T21:59:00.000Z",
        "voteCount": 2,
        "content": "not A because, AWS backup has a 35 day retention policy"
      },
      {
        "date": "2024-01-02T21:47:00.000Z",
        "voteCount": 1,
        "content": "No. The answer is A. \nAuto backup is actually 35 days as maximum. You should manually backup for 90 days. Then you can two choices to control the manual backup (or snapshot) which you can define longer retention period than 35 days of RDS automated backup.\n- use AWS Backup so that you make a schedule to create a manual snapshot\n- use the Lambda Function to make a schedule to create a manual snapshot\nOf course, AWS Backup is easier than implements of Lambda."
      },
      {
        "date": "2023-09-11T23:09:00.000Z",
        "voteCount": 2,
        "content": "A. Use AWS Backup to build a backup plan for the required retention period. Assign the DB instances to the backup plan."
      },
      {
        "date": "2023-08-07T14:32:00.000Z",
        "voteCount": 1,
        "content": "Can't believe so many votes for A. AWS backup also has a 35 day retention policy and the same page references snapshots. A is 100% incorrect\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/point-in-time-recovery.html"
      },
      {
        "date": "2023-09-11T23:08:00.000Z",
        "voteCount": 3,
        "content": "AWS Backup can retain snapshots between 1 day and 100 years (or indefinitely, if you do not enter a retention period), and continuous backups between 1 and 35 days."
      },
      {
        "date": "2023-05-26T04:05:00.000Z",
        "voteCount": 2,
        "content": "Not Selecting D Since it is more development effort."
      },
      {
        "date": "2023-04-24T07:32:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2022-07-16T20:25:00.000Z",
        "voteCount": 2,
        "content": "max limit for automated backups is 35 days hene manual snapshot is solution\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupRetention"
      },
      {
        "date": "2022-07-09T22:42:00.000Z",
        "voteCount": 1,
        "content": "You can set the backup retention period when you create a DB instance. If you don't set the backup retention period, the default backup retention period is one day if you create the DB instance using the Amazon RDS API or the AWS CLI. The default backup retention period is seven days if you create the DB instance using the console.\n\nAfter you create a DB instance, you can modify the backup retention period. You can set the backup retention period to between 0 and 35 days. Setting the backup retention period to 0 disables automated backups. Manual snapshot limits (100 per Region) do not apply to automated backups.\nmax limit for automated backups is 35 days hen e manual snapshot is solution \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupRetention"
      },
      {
        "date": "2022-06-20T17:46:00.000Z",
        "voteCount": 2,
        "content": "RDS autobackup's retention period is max 35days"
      },
      {
        "date": "2022-04-29T10:21:00.000Z",
        "voteCount": 4,
        "content": "AWS Backup service + retention period set"
      },
      {
        "date": "2022-02-23T17:37:00.000Z",
        "voteCount": 2,
        "content": "Agree with other comments"
      },
      {
        "date": "2021-12-15T06:05:00.000Z",
        "voteCount": 1,
        "content": "A for me - AWS Backup Service solution"
      },
      {
        "date": "2021-12-09T07:58:00.000Z",
        "voteCount": 1,
        "content": "A for me\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\t\t\t\t\t\t\n\t\t\t\t\t\t\nB is wrong , because Automated backups with unsupported MySQL storage engines\t."
      },
      {
        "date": "2021-11-30T02:43:00.000Z",
        "voteCount": 2,
        "content": "Automatic backups are only for max 35 days"
      },
      {
        "date": "2021-11-13T05:14:00.000Z",
        "voteCount": 2,
        "content": "A https://aws.amazon.com/getting-started/hands-on/amazon-rds-backup-restore-using-aws-backup/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/amazon/view/65971-exam-aws-certified-database-specialty-topic-1-question-151/",
    "body": "An online advertising company uses an Amazon DynamoDb table as its data store. The table has Amazon DynamoDB Streams enabled and has a global secondary index on one of the keys. The table is encrypted using an AWS Key Management Service (AWS KMS) customer managed key.<br>The company has decided to expand its operations globally and wants to replicate the database in a different AWS Region by using DynamoDB global tables.<br>Upon review, an administrator notices the following:<br>\u2711 No role with the dynamodb: CreateGlobalTable permission exists in the account.<br>\u2711 An empty table with the same name exists in the new Region where replication is desired.<br>\u2711 A global secondary index with the same partition key but a different sort key exists in the new Region where replication is desired.<br>Which configurations will block the creation of a global table or the creation of a replica in the new Region? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA global secondary index with the same partition key but a different sort key exists in the new Region where replication is desired.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn empty table with the same name exists in the Region where replication is desired.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo role with the dynamodb:CreateGlobalTable permission exists in the account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamoDB Streams is enabled for the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table is encrypted using a KMS customer managed key."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-03-06T06:32:00.000Z",
        "voteCount": 10,
        "content": "BC is the correct Answer.\nA is NOT an issue. I've tested this by creating a different table name in a destination region with TWO GSIs one with the exact same name and indexes and one with a different name and index than the original table. GSIs are table specific. The Key constraint is a table with the same name should NOT exist and permissions to access should be there"
      },
      {
        "date": "2022-05-01T10:07:00.000Z",
        "voteCount": 7,
        "content": "A. A global secondary index with the same partition key but a different sort key exists in the new Region where replication is desired.\nIf global secondary indexes are specified, then the following conditions must also be met:\nThe global secondary indexes must have the same name.\nThe global secondary indexes must have the same hash key and sort key (if present).\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateGlobalTable.html\n\nThe table must have the same name as all of the other replicas. (means B is out)\nC. No role with the dynamodb:CreateGlobalTable permission exists in the account.\nTo create and maintain global tables in DynamoDB, you must have the dynamodb:CreateGlobalTable permission to access each of the following:\nThe replica table that you want to add.\nEach existing replica that's already part of the global table.\nThe global table itself.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/gt_IAM.html"
      },
      {
        "date": "2022-05-20T01:39:00.000Z",
        "voteCount": 1,
        "content": "Yes, A and C."
      },
      {
        "date": "2023-09-30T10:30:00.000Z",
        "voteCount": 1,
        "content": "BC for me.\nA is not right. As long as same index_name don't exist &amp; it will create new 2nd index.\nB. it can't create a new table if the table with the same name is existed."
      },
      {
        "date": "2023-12-15T23:42:00.000Z",
        "voteCount": 1,
        "content": "B is not an issue. \n\"None of the new or existing replica tables in the global table can contain any data.\"\nfrom: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html"
      },
      {
        "date": "2024-02-26T02:45:00.000Z",
        "voteCount": 1,
        "content": "AC\n\n\"Requirements for adding a new replica table\"\nIf you want to add a new replica table to a global table, each of the following conditions must be true:\nThe table must have the same partition key as all of the other replicas.\nThe table must have the same write capacity management settings specified.\nThe table must have the same name as all of the other replicas.\nThe table must have DynamoDB Streams enabled, with the stream containing both the new and the old images of the item.\nNone of the new or existing replica tables in the global table can contain any data.\n\nIf global secondary indexes are specified, the following conditions must also be met:\nThe global secondary indexes must have the same name.\nThe global secondary indexes must have the same partition key and sort key (if present).\n\nhttps://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html"
      },
      {
        "date": "2024-01-15T10:03:00.000Z",
        "voteCount": 1,
        "content": "AC\nAn empty table is not a problem"
      },
      {
        "date": "2023-11-22T22:02:00.000Z",
        "voteCount": 1,
        "content": "To create and maintain global tables in DynamoDB, you must have the dynamodb:CreateGlobalTable permission to access each of the following:\n\nThe global secondary indexes must have the same partition key and sort key (if present).\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html#globaltables_reqs_bestpractices.requirements"
      },
      {
        "date": "2023-10-09T08:03:00.000Z",
        "voteCount": 1,
        "content": "According doc: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html#globaltables_reqs_bestpractices.requirements\n\n1. The global secondary indexes must have the same partition key and sort key (if present). . Thus, A\n\nB: Doc: None of the new or existing replica tables in the global table can contain any data.\nThus, empty table with same name is OK"
      },
      {
        "date": "2023-07-07T06:07:00.000Z",
        "voteCount": 1,
        "content": "BC: GSI allow distinct combination of partition and sort key so A is not a problem. D and E are not an issue."
      },
      {
        "date": "2023-05-09T16:41:00.000Z",
        "voteCount": 1,
        "content": "BC for me"
      },
      {
        "date": "2023-03-06T08:32:00.000Z",
        "voteCount": 1,
        "content": "Why not B &amp; E . The CMK are region specific. There is no mention that there the DB specialst has the CMK for the new region --https://aws.amazon.com/about-aws/whats-new/2020/11/encrypt-your-amazon-dynamodb-global-tables-by-using-your-own-encryption-keys/"
      },
      {
        "date": "2023-09-24T07:22:00.000Z",
        "voteCount": 1,
        "content": "A CMK is not a hinderance, but a prerequisite for expanding into the new region.\nAn Amazon owned key would not be possible because it cannot be shared across regions."
      },
      {
        "date": "2022-10-29T16:27:00.000Z",
        "voteCount": 1,
        "content": "Anser AB\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html\nRequirements for adding a new replica table\nIf you want to add a new replica table to a global table, each of the following conditions must be true:\n\nThe table must have the same partition key as all of the other replicas.\n\nThe table must have the same write capacity management settings specified.\n\nThe table must have the same name as all of the other replicas.\n\nThe table must have DynamoDB Streams enabled, with the stream containing both the new and the old images of the item.\n\nNone of the new or existing replica tables in the global table can contain any data.\n\nIf global secondary indexes are specified, the following conditions must also be met:\n\nThe global secondary indexes must have the same name.\n\nThe global secondary indexes must have the same partition key and sort key (if present)."
      },
      {
        "date": "2022-11-29T19:23:00.000Z",
        "voteCount": 2,
        "content": "B is wrong as indicated \"None of the new or existing replica tables in the global table can contain any data.\" so \"An empty table with the same name exists in the new Region where replication is desired\" should be acceptable."
      },
      {
        "date": "2022-09-27T12:16:00.000Z",
        "voteCount": 1,
        "content": "A. No issue : as long as same index_name don't exist &amp; it will create new 2nd index"
      },
      {
        "date": "2022-10-09T20:22:00.000Z",
        "voteCount": 1,
        "content": "Question asked : \"creating a global table\"  so \"A\"  not related ( about indexes etc )"
      },
      {
        "date": "2022-02-04T11:42:00.000Z",
        "voteCount": 4,
        "content": "B and C\nC: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/gt_IAM.html\nTo create and maintain global tables in DynamoDB, you must have the dynamodb:CreateGlobalTable permission to access.\nB: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html\nThe console checks to ensure that a table with the same name doesn't exist in the selected Region. If a table with the same name does exist, you must delete the existing table before you can create a new replica table in that Region."
      },
      {
        "date": "2023-05-22T00:11:00.000Z",
        "voteCount": 1,
        "content": "Not only does the console check it, but the AWS CLI fails to enable replication if there is a pre-existing table in the replica region.\n\naws dynamodb update-table --table-name my-table --cli-input-json  \\\n'{\n  \"ReplicaUpdates\":\n  [\n    {\n      \"Create\": {\n        \"RegionName\": \"eu-central-1\"\n      }\n    }\n  ]\n}' \\\n--region=eu-west-3\n\nAn error occurred (ValidationException) when calling the UpdateTable operation:\nFailed to create a the new replica of table with name: \u2018my-table\u2019 because one o\n more replicas already existed as tables."
      },
      {
        "date": "2021-12-28T06:14:00.000Z",
        "voteCount": 2,
        "content": "1. No role with DynamoDB, but Account with CreateGlobalTable permiession, so no issues to create Global table.\n2. Change Sort key when replicate a table -  don't think so\n3. Name exists in another region -  Error pop-up."
      },
      {
        "date": "2021-12-27T21:12:00.000Z",
        "voteCount": 1,
        "content": "BC!!!!!"
      },
      {
        "date": "2021-12-25T17:37:00.000Z",
        "voteCount": 2,
        "content": "B,C\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html\nFrom the Available replication Regions dropdown, choose US West (Oregon).\n\nThe console checks to ensure that a table with the same name doesn't exist in the selected Region. If a table with the same name does exist, you must delete the existing table before you can create a new replica table in that Region."
      },
      {
        "date": "2021-11-18T04:56:00.000Z",
        "voteCount": 3,
        "content": "Should be A and C"
      },
      {
        "date": "2021-11-21T18:48:00.000Z",
        "voteCount": 2,
        "content": "You are right"
      },
      {
        "date": "2021-11-13T11:09:00.000Z",
        "voteCount": 1,
        "content": "Options A and B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/amazon/view/66017-exam-aws-certified-database-specialty-topic-1-question-152/",
    "body": "A large automobile company is migrating the database of a critical financial application to Amazon DynamoDB. The company's risk and compliance policy requires that every change in the database be recorded as a log entry for audits. The system is anticipating more than 500,000 log entries each minute. Log entries should be stored in batches of at least 100,000 records in each file in Apache Parquet format.<br>How should a database specialist implement these requirements with DynamoDB?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon DynamoDB Streams on the table. Create an AWS Lambda function triggered by the stream. Write the log entries to an Amazon S3 object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a backup plan in AWS Backup to back up the DynamoDB table once a day. Create an AWS Lambda function that restores the backup in another table and compares both tables for changes. Generate the log entries and write them to an Amazon S3 object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail logs on the table. Create an AWS Lambda function that reads the log files once an hour and filters DynamoDB API actions. Write the filtered log files to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon DynamoDB Streams on the table. Create an AWS Lambda function triggered by the stream. Write the log entries to an Amazon Kinesis Data Firehose delivery stream with buffering and Amazon S3 as the destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-24T14:29:00.000Z",
        "voteCount": 7,
        "content": "https://aws.amazon.com/blogs/big-data/streaming-amazon-dynamodb-data-into-a-centralized-data-lake/\n- Kinesis Data Firehose \u2013 Kinesis Data Firehose helps to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3 and other destinations. It\u2019s a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, which minimizes the amount of storage used and increases security."
      },
      {
        "date": "2023-09-24T07:28:00.000Z",
        "voteCount": 1,
        "content": "B+C are distractors (\"once an hour\", \"once a day\" for 500000 entries *per minute* cannot work).\nA is difficult to achieve since a Lambda function is limited to 15 mins and there is no buffering. D provides this."
      },
      {
        "date": "2023-05-09T16:40:00.000Z",
        "voteCount": 1,
        "content": "D makes sense to me."
      },
      {
        "date": "2022-12-17T20:00:00.000Z",
        "voteCount": 1,
        "content": "D make sense."
      },
      {
        "date": "2022-04-29T12:47:00.000Z",
        "voteCount": 1,
        "content": "DynamoDB Streams -&gt; Lambda function -&gt;log entries  Kinesis Data Firehose delivery stream with buffering -&gt; S3"
      },
      {
        "date": "2022-01-18T14:58:00.000Z",
        "voteCount": 1,
        "content": "Should be A as per https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html"
      },
      {
        "date": "2022-02-16T09:58:00.000Z",
        "voteCount": 3,
        "content": "with A you cant batch them as per the requirements ,with D you can"
      },
      {
        "date": "2022-07-09T21:14:00.000Z",
        "voteCount": 2,
        "content": "You can write lambda to start writing to new S3 object ( file ) after it has writter 10000 records to it. Option A is more reasonable ."
      },
      {
        "date": "2021-12-01T23:25:00.000Z",
        "voteCount": 4,
        "content": "DDDDDDDD"
      },
      {
        "date": "2021-11-24T17:42:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2021-11-24T04:07:00.000Z",
        "voteCount": 3,
        "content": "Option D"
      },
      {
        "date": "2021-11-14T06:47:00.000Z",
        "voteCount": 2,
        "content": "Option D."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/amazon/view/66079-exam-aws-certified-database-specialty-topic-1-question-153/",
    "body": "A company released a mobile game that quickly grew to 10 million daily active users in North America. The game's backend is hosted on AWS and makes extensive use of an Amazon DynamoDB table that is configured with a TTL attribute.<br>When an item is added or updated, its TTL is set to the current epoch time plus 600 seconds. The game logic relies on old data being purged so that it can calculate rewards points accurately. Occasionally, items are read from the table that are several hours past their TTL expiry.<br>How should a database specialist fix this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a client library that supports the TTL functionality for DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude a query filter expression to ignore items with an expired TTL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the ConsistentRead parameter to true when querying the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a local secondary index on the TTL attribute."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-15T08:27:00.000Z",
        "voteCount": 8,
        "content": "Answer is B https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html"
      },
      {
        "date": "2022-10-22T09:16:00.000Z",
        "voteCount": 2,
        "content": "B\nItems that have expired, but haven\u2019t yet been deleted by TTL, still appear in reads, queries, and scans. If you do not want expired items in the result set, you must filter them out. To do this, use a filter expression that returns only items where the Time to Live expiration value is greater than the current time in epoch format. For more information, see Filter expressions for scan."
      },
      {
        "date": "2022-04-29T19:17:00.000Z",
        "voteCount": 1,
        "content": "TTL expires recs, but delete process may run later"
      },
      {
        "date": "2022-02-24T19:45:00.000Z",
        "voteCount": 2,
        "content": "async deletes via tel"
      },
      {
        "date": "2021-11-30T03:25:00.000Z",
        "voteCount": 2,
        "content": "option B"
      },
      {
        "date": "2021-11-29T17:17:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/amazon/view/66501-exam-aws-certified-database-specialty-topic-1-question-154/",
    "body": "A development team at an international gaming company is experimenting with Amazon DynamoDB to store in-game events for three mobile games. The most popular game hosts a maximum of 500,000 concurrent users, and the least popular game hosts a maximum of 10,000 concurrent users. The average size of an event is 20 KB, and the average user session produces one event each second. Each event is tagged with a time in milliseconds and a globally unique identifier.<br>The lead developer created a single DynamoDB table for the events with the following schema:<br>\u2711 Partition key: game name<br>\u2711 Sort key: event identifier<br>\u2711 Local secondary index: player identifier<br>\u2711 Event time<br>The tests were successful in a small-scale development environment. However, when deployed to production, new events stopped being added to the table and the logs show DynamoDB failures with the ItemCollectionSizeLimitExceededException error code.<br>Which design change should a database specialist recommend to the development team?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the player identifier as the partition key. Use the event time as the sort key. Add a global secondary index with the game name as the partition key and the event time as the sort key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two tables. Use the game name as the partition key in both tables. Use the event time as the sort key for the first table. Use the player identifier as the sort key for the second table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the sort key with a compound value consisting of the player identifier collated with the event time, separated by a dash. Add a local secondary index with the player identifier as the sort key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one table for each game. Use the player identifier as the partition key. Use the event time as the sort key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T20:29:00.000Z",
        "voteCount": 7,
        "content": "ItemCollectionSizeLimitExceededException\nMessage: Collection size exceeded.\nFor a table with a local secondary index, a group of items with the same partition key value has exceeded the maximum size limit of 10 GB. For more information on item collections, see Item Collections.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html\n\nProblem is too much data in partition cause issues with LocalSecondaryIndex.\none game_id had 500K users with 20K data per event = 10GB/event\n\nSolution is separate table for game and  partition by player_id\nEach table will have max 500000 partitions\neach partition gets 3600 events per hour * 20KB/events = 72 MB... so it can store data for 10GB/72 MB = 13 hours data before ItemCollectionSizeLimitExceededException for option D design. For game I guess TTL would be set to avoid it"
      },
      {
        "date": "2022-09-05T08:29:00.000Z",
        "voteCount": 3,
        "content": "Are you asking the developers to rewrite the application as changing from 1 table to 3 tables ?"
      },
      {
        "date": "2023-03-28T15:43:00.000Z",
        "voteCount": 1,
        "content": "should not be that difficult because the original code is possible with \"...from table X where game_id=?\" and table X could be a dynamic parameter -- maybe the programer even don't need to change the code."
      },
      {
        "date": "2023-09-24T07:39:00.000Z",
        "voteCount": 1,
        "content": "The partition key was obviously chosen wrong (game name has low cardinality as there are only 3 distinct games). This leads to overloading their partitions.\nThe solution and best practice is to choose a partition key with high cardinality --&gt; user name.\n\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
      },
      {
        "date": "2023-04-04T18:45:00.000Z",
        "voteCount": 1,
        "content": "I' will go with D, for A you can consider GSI as table, if you still use game name as partition key, guess it will raise the same exception"
      },
      {
        "date": "2022-12-24T13:01:00.000Z",
        "voteCount": 1,
        "content": "Chose D because the partition key is to have high cardinality.  Option A and C are out of the question because it is inferring to modify the table which is not possible on the fly.  Option B also didn't seem to feasible"
      },
      {
        "date": "2022-12-02T09:57:00.000Z",
        "voteCount": 1,
        "content": "B and C is out of question. Between A and D, problem with A is the global index with game name as the partition key. This will create hot partitions since one game has over 500000 concurrent users. So better to have 3 tables, with players are partition key."
      },
      {
        "date": "2022-03-07T05:04:00.000Z",
        "voteCount": 1,
        "content": "Got this question in my exam. (i cleared it). But this is one of those questions I dont know if I got it right...I chose (D)"
      },
      {
        "date": "2022-03-06T08:56:00.000Z",
        "voteCount": 1,
        "content": "I feel its  A\nLSI is the problem here with Low cardinality. But remember the question is asking about doing queries using game name. So you do need some form of index for that and GSI will solve it. This isnt being addressed in (D)"
      },
      {
        "date": "2022-03-06T09:53:00.000Z",
        "voteCount": 1,
        "content": "Actually GSI will also have a problem with very low cardinality  as game name is a terrible choice for partition key. I will go with (D) - damn this is a tough one"
      },
      {
        "date": "2022-02-04T12:06:00.000Z",
        "voteCount": 2,
        "content": "I vote for D\nAn item collection is too large. This exception is only returned for tables that have one or more local secondary indexes.\nhttps://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/DynamoDB/Types/ItemCollectionSizeLimitExceededException.html"
      },
      {
        "date": "2022-03-06T08:36:00.000Z",
        "voteCount": 1,
        "content": "Is the Global Secondary  Index having a very low cardinality also an issue?"
      },
      {
        "date": "2022-12-23T04:42:00.000Z",
        "voteCount": 1,
        "content": "No, 10GB limit does not apply to GSI. \n\nThe maximum size of any item collection for a table which has one or more local secondary indexes is 10 GB. This does not apply to item collections in tables without local secondary indexes, and also does not apply to item collections in global secondary indexes. Only tables that have one or more local secondary indexes are affected."
      },
      {
        "date": "2022-12-23T04:44:00.000Z",
        "voteCount": 1,
        "content": "Hence \"A\" should be a right ans too. With A, we do not need to change the application like \"D\". Hence my vote will be to \"A\""
      },
      {
        "date": "2022-01-13T10:41:00.000Z",
        "voteCount": 2,
        "content": "I think D is correct"
      },
      {
        "date": "2021-12-24T12:33:00.000Z",
        "voteCount": 3,
        "content": "I'd go with option D"
      },
      {
        "date": "2021-11-28T03:12:00.000Z",
        "voteCount": 3,
        "content": "I think A make sense to me as the document mention: \"The maximum size of any item collection is 10 GB. This limit does not apply to tables without local secondary indexes; only tables that have one or more local secondary indexes are affected\", avoid to use local secondary index will be the better solution."
      },
      {
        "date": "2021-12-24T12:21:00.000Z",
        "voteCount": 2,
        "content": "Why not option D?"
      },
      {
        "date": "2021-11-21T14:11:00.000Z",
        "voteCount": 2,
        "content": "C Makes sense to me"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/amazon/view/65689-exam-aws-certified-database-specialty-topic-1-question-155/",
    "body": "An ecommerce company recently migrated one of its SQL Server databases to an Amazon RDS for SQL Server Enterprise Edition DB instance. The company expects a spike in read traffic due to an upcoming sale. A database specialist must create a read replica of the DB instance to serve the anticipated read traffic.<br>Which actions should the database specialist take before creating the read replica? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify a potential downtime window and stop the application calls to the source DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that automatic backups are enabled for the source DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the source DB instance is a Multi-AZ deployment with Always ON Availability Groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the source DB instance is a Multi-AZ deployment with SQL Server Database Mirroring (DBM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the read replica parameter group setting and set the value to 1."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-08T20:41:00.000Z",
        "voteCount": 6,
        "content": "Ans B,C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2021-12-22T13:43:00.000Z",
        "voteCount": 5,
        "content": "B, C \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2023-07-23T12:18:00.000Z",
        "voteCount": 1,
        "content": "B. Ensure that automatic backups are enabled for the source DB instance: Before creating a read replica, it is essential to have automatic backups enabled on the source DB instance. This ensures that the read replica can be created from a recent snapshot of the source DB instance.\n\nC. Ensure that the source DB instance is a Multi-AZ deployment with Always ON Availability Groups: To create a read replica for a SQL Server DB instance, the source DB instance must be configured as a Multi-AZ deployment with Always ON Availability Groups. This ensures high availability and data synchronization between the primary and standby replicas."
      },
      {
        "date": "2023-06-08T04:53:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2023-05-09T16:50:00.000Z",
        "voteCount": 1,
        "content": "BC is right"
      },
      {
        "date": "2022-12-26T09:54:00.000Z",
        "voteCount": 1,
        "content": "Ans B, C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html\n\nBefore a DB instance can serve as a source instance for replication, you must enable automatic backups on the source DB instance. To do so, you set the backup retention period to a value other than 0. The source DB instance must be a Multi-AZ deployment with Always On Availability Groups (AGs). Setting this type of deployment also enforces that automatic backups are enabled."
      },
      {
        "date": "2022-12-24T13:24:00.000Z",
        "voteCount": 1,
        "content": "Makes the most sense to me"
      },
      {
        "date": "2022-04-28T13:28:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2022-04-17T04:56:00.000Z",
        "voteCount": 3,
        "content": "Before a DB instance can serve as a source instance for replication, you must enable automatic backups on the source DB instance. To do so, you set the backup retention period to a value other than 0. The source DB instance must be a Multi-AZ deployment with Always On Availability Groups (AGs). Setting this type of deployment also enforces that automatic backups are enabled."
      },
      {
        "date": "2021-12-18T20:05:00.000Z",
        "voteCount": 2,
        "content": "B &amp; C\nBefore a DB instance can serve as a source instance for replication, you must enable automatic backups on the source DB instance. To do so, you set the backup retention period to a value other than 0. The source DB instance must be a Multi-AZ deployment with Always On Availability Groups (AGs). Setting this type of deployment also enforces that automatic backups are enabled."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/amazon/view/66370-exam-aws-certified-database-specialty-topic-1-question-156/",
    "body": "A company is running a two-tier ecommerce application in one AWS account. The application is backed by an Amazon RDS for MySQL Multi-AZ DB instance. A developer mistakenly deleted the DB instance in the production environment. The company restores the database, but this event results in hours of downtime and lost revenue.<br>Which combination of changes would minimize the risk of this mistake occurring in the future? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant least privilege to groups, IAM users, and roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow all users to restore a database from a backup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable deletion protection on existing production DB instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an ACL policy to restrict users from DB instance deletion.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail logging and Enhanced Monitoring."
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-03-04T22:51:00.000Z",
        "voteCount": 7,
        "content": "ACL is used ONLY WITH S3. AC are absolutely correct but the question seems flawed. Enhanced Monitoring WONT help with a developer from deleting the instance"
      },
      {
        "date": "2023-11-07T04:58:00.000Z",
        "voteCount": 1,
        "content": "The answers should be:\n- Grant least privilege to groups, users, and roles\n- Allow all users to restore a database from a backup that will reduce the overall downtime to restore the database\n- Enable multi-factor authentication for sensitive operations to access sensitive resources and API operations\n- Use policy conditions to restrict access to selective IP addresses\n- Use AccessList Controls policy type to restrict users for database instance deletion\n- Enable AWS CloudTrail logging and Enhanced Monitoring"
      },
      {
        "date": "2023-09-09T00:08:00.000Z",
        "voteCount": 1,
        "content": "\uff37\uff28\uff39 \uff24\uff1f"
      },
      {
        "date": "2023-06-08T04:56:00.000Z",
        "voteCount": 1,
        "content": "B and E are not preventive measures. A, C, D are."
      },
      {
        "date": "2022-12-02T10:03:00.000Z",
        "voteCount": 3,
        "content": "ACL is not supported with RDS\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/security_iam_service-with-iam.html\n\nAccess control lists (ACLs) in Amazon RDS\nSupports access control lists (ACLs) - No"
      },
      {
        "date": "2022-04-29T14:27:00.000Z",
        "voteCount": 3,
        "content": "Least Priv\nDelete Protection\nAccess control"
      },
      {
        "date": "2022-03-13T12:17:00.000Z",
        "voteCount": 1,
        "content": "There's no 3rd correct option."
      },
      {
        "date": "2022-02-24T15:03:00.000Z",
        "voteCount": 3,
        "content": "AC\nD with IAM not ACL"
      },
      {
        "date": "2021-12-23T23:21:00.000Z",
        "voteCount": 4,
        "content": "There are only 2 right answers here.. question or answer need to be corrected ..A nad C only are right"
      },
      {
        "date": "2021-12-16T20:52:00.000Z",
        "voteCount": 1,
        "content": "A, C &amp; D.\nC - ACL is the IAM policy, preventing users\n from deleting a specific DB instance."
      },
      {
        "date": "2021-12-20T16:16:00.000Z",
        "voteCount": 1,
        "content": "Do you mean there is a typo in option D? Should it be \"Use an IAM policy to ... \" ?"
      },
      {
        "date": "2021-12-08T18:34:00.000Z",
        "voteCount": 1,
        "content": "ACE is the correct answer.\nYou can't use ACL policy to restrict users. You can only use IAM policy to restrict users"
      },
      {
        "date": "2021-12-20T16:14:00.000Z",
        "voteCount": 2,
        "content": "How can you use AWS CloudTrail logging and Enhanced Monitoring to restrict users from DB instance deletion?"
      },
      {
        "date": "2023-10-31T00:51:00.000Z",
        "voteCount": 1,
        "content": "Minimize the risk, not restrict users..."
      },
      {
        "date": "2023-05-09T16:55:00.000Z",
        "voteCount": 1,
        "content": "You are right, ACL is not for user level access control."
      },
      {
        "date": "2021-11-19T19:49:00.000Z",
        "voteCount": 2,
        "content": "The answer is ACD"
      },
      {
        "date": "2021-11-27T12:45:00.000Z",
        "voteCount": 3,
        "content": "A &amp; C is clear but how can you use an ACL policy to restrict users from DB instance deletion?"
      },
      {
        "date": "2021-12-20T16:14:00.000Z",
        "voteCount": 1,
        "content": "The only use case for ACL I can think of is to block traffic from everywhere except the App server. I'm not sure if that make sense to restrict users from DB instance deletion. On the other hand enabling enhanced monitoring won't help either.."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/amazon/view/65738-exam-aws-certified-database-specialty-topic-1-question-157/",
    "body": "A financial services company uses Amazon RDS for Oracle with Transparent Data Encryption (TDE). The company is required to encrypt its data at rest at all times. The key required to decrypt the data has to be highly available, and access to the key must be limited. As a regulatory requirement, the company must have the ability to rotate the encryption key on demand. The company must be able to make the key unusable if any potential security breaches are spotted. The company also needs to accomplish these tasks with minimum overhead.<br>What should the database administrator use to set up the encryption to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS CloudHSM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Key Management Service (AWS KMS) with an AWS managed key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Key Management Service (AWS KMS) with server-side encryption",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Key Management Service (AWS KMS) CMK with customer-provided material\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-03-11T17:06:00.000Z",
        "voteCount": 6,
        "content": "Key rotation \u2713 \nKey deletion \u2713 \nMinimal overhead \u2713"
      },
      {
        "date": "2023-09-14T06:06:00.000Z",
        "voteCount": 2,
        "content": "D. AWS Key Management Service (AWS KMS) CMK with customer-provided material \n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt\n\n\"The KMS keys that you create are customer managed keys. Customer managed keys are KMS keys in your AWS account that you create, own, and manage. You have full control over these KMS keys, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the KMS keys, and scheduling the KMS keys for deletion. \""
      },
      {
        "date": "2023-07-11T11:59:00.000Z",
        "voteCount": 1,
        "content": "Could be A: https://aws.amazon.com/blogs/security/aws-cloudhsm-is-now-integrated-with-amazon-rds-for-oracle-and-provides-enhanced-management-tools/\nHesitant as I cannot figure out how to rotate the keys in CloudHSM"
      },
      {
        "date": "2023-09-24T07:46:00.000Z",
        "voteCount": 1,
        "content": "Overhead for CloudHSM is exorbitant. This is not an option."
      },
      {
        "date": "2022-12-24T13:40:00.000Z",
        "voteCount": 1,
        "content": "It was a toss between C and D however SSE provides less overhead and maintenance than D.  I could be wrong."
      },
      {
        "date": "2022-11-01T12:36:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt\n\nCustomer managed keys are KMS keys in your AWS account that you create, own, and manage. You have full control over these KMS keys, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the KMS keys, and scheduling the KMS keys for deletion.\n\nAnswer D"
      },
      {
        "date": "2022-08-13T03:58:00.000Z",
        "voteCount": 3,
        "content": "For those mentioning HSM: \"You cannot use an Oracle instance in Amazon Relational Database Service (Amazon RDS) to integrate with AWS CloudHSM. You must install Oracle Database on an Amazon EC2 instance.\" Ref: https://docs.aws.amazon.com/cloudhsm/latest/userguide/oracle-tde.html\nThis eliminates answer A.\nThe key words that would make me choose answer D is \"Rotate encryption key on demand\". Only CMK allows to do that. AWS managed key are automatically rotated every 1 year and the organization can't change that."
      },
      {
        "date": "2022-07-09T22:24:00.000Z",
        "voteCount": 1,
        "content": "Could be A \n\nEnable transparent data encryption (TDE) for Oracle databases\nCopy Plain Link[] l : t - s[] t - s - l\nSome versions of Oracle's database software offer a feature called Transparent Data Encryption (TDE). With TDE, the database software encrypts data before storing it on disk. The data in the database's table columns or tablespaces is encrypted with a table key or tablespace key. These keys are encrypted with the TDE master encryption key. You can store the TDE master encryption key in the HSMs in your AWS CloudHSM cluster, which provides additional security.\nhttps://docs.aws.amazon.com/cloudhsm/latest/userguide/use-cases.html#transparent-data-encryption"
      },
      {
        "date": "2022-06-21T09:54:00.000Z",
        "voteCount": 1,
        "content": "Could be A because Cloud HSM is recently added to Oracle RDS \nhttps://aws.amazon.com/blogs/security/aws-cloudhsm-is-now-integrated-with-amazon-rds-for-oracle-and-provides-enhanced-management-tools/"
      },
      {
        "date": "2023-03-25T07:29:00.000Z",
        "voteCount": 1,
        "content": "Sachin, please read the update on Nov 24 2021 for the same blog post (https://aws.amazon.com/blogs/security/aws-cloudhsm-is-now-integrated-with-amazon-rds-for-oracle-and-provides-enhanced-management-tools/). AWS CloudHSM Classic used to support RDS Oracle however, AWS CloudHSM Classic has been discontinued and replaced by AWS CloudHSM which supports HSM with EC2 only not RDS. First paragraph of this blog clearly says it - November 24, 2021: This blog post announced a feature of AWS CloudHSM Classic which integrated with Amazon RDS for Oracle to provide customers with an easy integration for Transparent Data Encryption (TDE). The AWS CloudHSM team have since released AWS CloudHSM, and this feature is no longer available. For updated options, please see out this blog post: https://aws.amazon.com/blogs/security/architecting-for-database-encryption-on-aws/."
      },
      {
        "date": "2022-06-21T09:51:00.000Z",
        "voteCount": 2,
        "content": "Will go with D"
      },
      {
        "date": "2022-04-29T09:23:00.000Z",
        "voteCount": 3,
        "content": "KMS CMK"
      },
      {
        "date": "2022-03-02T06:25:00.000Z",
        "voteCount": 1,
        "content": "This one A for me:\nhttps://docs.aws.amazon.com/cloudhsm/latest/userguide/use-cases.html"
      },
      {
        "date": "2022-03-15T04:41:00.000Z",
        "voteCount": 2,
        "content": "You can't use cloudHSM with RDS, database has to be on ec2. D is the correct answer."
      },
      {
        "date": "2022-06-14T07:03:00.000Z",
        "voteCount": 2,
        "content": "Sure? https://aws.amazon.com/de/blogs/security/aws-cloudhsm-is-now-integrated-with-amazon-rds-for-oracle-and-provides-enhanced-management-tools/"
      },
      {
        "date": "2022-02-23T17:22:00.000Z",
        "voteCount": 3,
        "content": "Per - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\nYou must manage different keys for each encryption method."
      },
      {
        "date": "2022-02-23T15:43:00.000Z",
        "voteCount": 2,
        "content": "D - CMK"
      },
      {
        "date": "2021-11-15T01:13:00.000Z",
        "voteCount": 3,
        "content": "Look at rotation: Customer managed have on demand. Question asks for it https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/aws-managed-and-customer-managed-cmks.html"
      },
      {
        "date": "2021-11-13T00:07:00.000Z",
        "voteCount": 3,
        "content": "D. https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/aws-managed-and-customer-managed-cmks.html"
      },
      {
        "date": "2021-11-09T15:52:00.000Z",
        "voteCount": 4,
        "content": "D. AWS Key Management Service (AWS KMS) CMK with customer-provided material"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/amazon/view/66993-exam-aws-certified-database-specialty-topic-1-question-158/",
    "body": "A company is setting up a new Amazon RDS for SQL Server DB instance. The company wants to enable SQL Server auditing on the database.<br>Which combination of steps should a database specialist take to meet this requirement? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service-linked role for Amazon RDS that grants permissions for Amazon RDS to store audit logs on Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a parameter group to configure an IAM role and an Amazon S3 bucket for audit log storage. Associate the parameter group with the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable Multi-AZ on the DB instance, and then enable auditing. Enable Multi-AZ after auditing is enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable automated backup on the DB instance, and then enable auditing. Enable automated backup after auditing is enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an options group to configure an IAM role and an Amazon S3 bucket for audit log storage. Associate the options group with the DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T07:13:00.000Z",
        "voteCount": 4,
        "content": "A. service-linked role that grants permissions to RDS to store audit logs on S3.\nx B. parameter group (it is for db params, options is for features )\nx C. Disable \nx D. Disable \nE.  options group to configure an IAM role and an S3 bucket for audit log storage. Associate the options group with the DB instance."
      },
      {
        "date": "2022-03-07T04:29:00.000Z",
        "voteCount": 1,
        "content": "Got this question in my exam. (i cleared it). AE is correct"
      },
      {
        "date": "2023-06-19T14:43:00.000Z",
        "voteCount": 6,
        "content": "how do you know that you answered this particular one correctly and why did you return after that to exam materials?"
      },
      {
        "date": "2023-10-04T03:45:00.000Z",
        "voteCount": 1,
        "content": "Maybe he failed. That's why he went over the questions again?"
      },
      {
        "date": "2022-03-05T11:08:00.000Z",
        "voteCount": 3,
        "content": "A&amp;E are correct"
      },
      {
        "date": "2021-12-09T07:18:00.000Z",
        "voteCount": 2,
        "content": "A &amp; E is correct \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/security_iam_service-with-iam.html"
      },
      {
        "date": "2021-12-07T20:13:00.000Z",
        "voteCount": 2,
        "content": "yeah A &amp; E"
      },
      {
        "date": "2021-11-30T11:50:00.000Z",
        "voteCount": 3,
        "content": "I think its A and E\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.Audit.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/amazon/view/67696-exam-aws-certified-database-specialty-topic-1-question-159/",
    "body": "A database specialist is creating an AWS CloudFormation stack. The database specialist wants to prevent accidental deletion of an Amazon RDS<br>ProductionDatabase resource in the stack.<br>Which solution will meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stack policy to prevent updates. Include \u05d2\u20acEffect\u05d2\u20ac : \u05d2\u20acProductionDatabase\u05d2\u20ac and \u05d2\u20acResource\u05d2\u20ac : \u05d2\u20acDeny\u05d2\u20ac in the policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation stack in XML format. Set xAttribute as false.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS DB instance without the DeletionPolicy attribute. Disable termination protection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stack policy to prevent updates. Include \u05d2\u20acEffect\u05d2\u20ac : \u05d2\u20acDeny\u05d2\u20ac and \u05d2\u20acResource\u05d2\u20ac : \u05d2\u20acProductionDatabase\u05d2\u20ac in the policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-11T23:06:00.000Z",
        "voteCount": 7,
        "content": "The answer is D. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html\n\"When you set a stack policy, all resources are protected by default. To allow updates on all resources, we add an Allow statement that allows all actions on all resources. Although the Allow statement specifies all resources, the explicit Deny statement overrides it for the resource with the ProductionDatabase logical ID. This Deny statement prevents all update actions, such as replacement or deletion, on the ProductionDatabase resource.\""
      },
      {
        "date": "2023-09-14T05:41:00.000Z",
        "voteCount": 1,
        "content": "D. Create a stack policy to prevent updates.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html#:~:text=The%20following%20example%20stack%20policy%20prevents%20updates%20to%20the%20ProductionDatabase%20resource%3A"
      },
      {
        "date": "2023-01-26T04:20:00.000Z",
        "voteCount": 1,
        "content": "Can someone please explain what's the difference option A and D? I see DENY in both the options."
      },
      {
        "date": "2023-03-31T05:43:00.000Z",
        "voteCount": 1,
        "content": "effect only takes two values: allow or deny."
      },
      {
        "date": "2022-04-30T12:33:00.000Z",
        "voteCount": 2,
        "content": "stack policy \nEffect: DENY\nResource: &lt;ARN of the Production Database&gt;"
      },
      {
        "date": "2022-03-07T04:46:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam. (i cleared it). D is correct"
      },
      {
        "date": "2022-03-05T18:28:00.000Z",
        "voteCount": 1,
        "content": "Can barely read the answers but the correct answer is (D)\nEffect: DENY\nResource: &lt;ARN of the Production Database&gt;"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/amazon/view/65891-exam-aws-certified-database-specialty-topic-1-question-160/",
    "body": "An ecommerce company migrates an on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility). After the migration, a database specialist realizes that encryption at rest has not been turned on for the Amazon DocumentDB cluster.<br>What should the database specialist do to enable encryption at rest for the Amazon DocumentDB cluster?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the Amazon DocumentDB cluster. Restore the unencrypted snapshot as a new cluster while specifying the encryption option, and provide an AWS Key Management Service (AWS KMS) key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable encryption for the Amazon DocumentDB cluster on the AWS Management Console. Reboot the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Amazon DocumentDB cluster by using the modify-db-cluster command with the --storage-encrypted parameter set to true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new encrypted instance to the Amazon DocumentDB cluster, and then delete an unencrypted instance from the cluster. Repeat until all instances are encrypted."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-12T06:00:00.000Z",
        "voteCount": 16,
        "content": "I'll go A.\n\"Limitations for Amazon DocumentDB Encrypted Clusters \nYou can enable or disable encryption at rest for an Amazon DocumentDB cluster only at the time that it is created, not after the cluster has been created. However, you can create an encrypted copy of an unencrypted cluster by creating a snapshot of the unencrypted cluster, and then restoring the unencrypted snapshot as a new cluster while specifying the encryption at rest option. \"\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html"
      },
      {
        "date": "2021-11-14T01:14:00.000Z",
        "voteCount": 6,
        "content": "you are absolutely right, about 80% of the answers on this exam are wrong, if it wasn't for the comment sections it would be useless."
      },
      {
        "date": "2023-09-16T19:58:00.000Z",
        "voteCount": 1,
        "content": "A is correct\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html#encryption-at-rest-limits\n\n\"You can enable or disable encryption at rest for an Amazon DocumentDB cluster only at the time that it is created, not after the cluster has been created. However, you can create an encrypted copy of an unencrypted cluster by creating a snapshot of the unencrypted cluster, and then restoring the unencrypted snapshot as a new cluster while specifying the encryption at rest option.\""
      },
      {
        "date": "2022-12-24T15:36:00.000Z",
        "voteCount": 2,
        "content": "went with this choice"
      },
      {
        "date": "2022-04-29T18:10:00.000Z",
        "voteCount": 2,
        "content": "for DocumentDB only\nsnapshot of the unencrypted cluster\n-&gt; restoring the unencrypted snapshot as a new cluster while specifying the encryption at rest option."
      },
      {
        "date": "2022-02-24T15:25:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-11-27T13:15:00.000Z",
        "voteCount": 3,
        "content": "\"... you can create an encrypted copy of an unencrypted cluster by creating a snapshot of the unencrypted cluster, and then restoring the unencrypted snapshot as a new cluster while specifying the encryption at rest option\""
      },
      {
        "date": "2021-11-26T06:51:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/amazon/view/66266-exam-aws-certified-database-specialty-topic-1-question-161/",
    "body": "A company that analyzes the stock market has two offices: one in the us-east-1 Region and another in the eu-west-2 Region. The company wants to implement an AWS database solution that can provide fast and accurate updates.<br>The office in eu-west-2 has dashboards with complex analytical queries to display the data. The company will use these dashboards to make buying decisions, so the dashboards must have access to the application data in less than 1 second.<br>Which solution meets these requirements and provides the MOST up-to-date dashboard?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS DB instance in us-east-1 with a read replica instance in eu-west-2. Create an Amazon ElastiCache cluster in eu-west-2 to cache data from the read replica to generate the dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon DynamoDB global table in us-east-1 with replication into eu-west-2. Use multi-active replication to ensure that updates are quickly propagated to eu-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Aurora global database. Deploy the primary DB cluster in us-east-1. Deploy the secondary DB cluster in eu-west-2. Configure the dashboard application to read from the secondary cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS for MySQL DB instance in us-east-1 with a read replica instance in eu-west-2. Configure the dashboard application to read from the read replica."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-09T11:21:00.000Z",
        "voteCount": 6,
        "content": "C is correct\nAmazon Aurora global databases span multiple AWS Regions, enabling low latency global reads and providing fast recovery from the rare outage that might affect an entire AWS Region. An Aurora global database has a primary DB cluster in one Region, and up to five secondary DB clusters in different Regions.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html"
      },
      {
        "date": "2023-02-13T07:37:00.000Z",
        "voteCount": 3,
        "content": "Answer is C: \nAdvanced analytics = RDS \nFastest = Aurora Global tables"
      },
      {
        "date": "2022-12-24T15:54:00.000Z",
        "voteCount": 1,
        "content": "logical choice"
      },
      {
        "date": "2022-12-17T20:58:00.000Z",
        "voteCount": 1,
        "content": "Aurora global DB it is. Letter C."
      },
      {
        "date": "2022-04-30T09:34:00.000Z",
        "voteCount": 1,
        "content": "advanced analytical queries =&gt; RDS\n&lt; 1 sec =&gt; Aurora Global\n\n Use an Amazon Aurora global database. Deploy the primary DB cluster in us-east-1. Deploy the secondary DB cluster in eu-west-2. Configure the dashboard application to read from the secondary cluster."
      },
      {
        "date": "2022-01-21T18:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. \nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions.\nGlobal Database uses storage-based replication with typical latency of less than 1 second, using dedicated infrastructure that leaves your database fully available to serve application workloads.\nhttps://aws.amazon.com/rds/aurora/global-database/"
      },
      {
        "date": "2022-01-21T18:59:00.000Z",
        "voteCount": 4,
        "content": "A is wrong. Read Replica can only be deployed in the same region. ElasticCache only cache data for app, but not ensure replication speed between primary node and replica.\nB. Multi-active replication data in all Regions is eventually consistent. This means replica data in other region may not be up to date.\nC is correct. Use an Amazon Aurora global database for replication cross region under 1 second.\nD. Same as A"
      },
      {
        "date": "2022-05-29T07:56:00.000Z",
        "voteCount": 1,
        "content": "Great explanation, I asked myself why not B and you are right, \"eventually consistent\" Thanks. Now is clear."
      },
      {
        "date": "2023-04-09T03:36:00.000Z",
        "voteCount": 1,
        "content": "Agree. I thought, why not choose B?"
      },
      {
        "date": "2023-11-10T01:22:00.000Z",
        "voteCount": 1,
        "content": "Complex analytical queries as well"
      },
      {
        "date": "2023-04-09T04:55:00.000Z",
        "voteCount": 1,
        "content": "Since Nov 16, 2022. AWS support across region replicas.\nI understand that we didn't choose A because of the performance, and RTO could not reach the requirement.\n\n https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-sql-server-cross-region-read-replica/#:~:text=Amazon%20Relational%20Database%20Service%20(Amazon,a%20new%20standalone%20production%20database."
      },
      {
        "date": "2021-12-22T13:04:00.000Z",
        "voteCount": 2,
        "content": "Aurora global db is the right option"
      },
      {
        "date": "2021-11-27T08:42:00.000Z",
        "voteCount": 2,
        "content": "C - i will go with aurora global database as replication needs to be less than a second"
      },
      {
        "date": "2021-11-17T18:02:00.000Z",
        "voteCount": 1,
        "content": "Does anyone have some insight on why this choice? I am not totally sold on it"
      },
      {
        "date": "2021-12-13T10:30:00.000Z",
        "voteCount": 5,
        "content": "\"advanced analytical queries\" rules out DynamoDB.  To decide between Aurora Global Database and RDS with Read Replcia in another Region see this FAQ\n\nhttps://aws.amazon.com/rds/aurora/faqs/#High_Availability_and_Replication\n\"Physical replication, called Amazon Aurora Global Database, uses dedicated infrastructure that leaves your databases entirely available to serve your application, and can replicate up to five secondary regions with typical latency of under a second.\"\n\nRDS cross-Region replication lag can be several seconds to several minutes depending on workload on the source.\nhttps://aws.amazon.com/blogs/database/best-practices-for-amazon-rds-for-postgresql-cross-region-read-replicas/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/amazon/view/65925-exam-aws-certified-database-specialty-topic-1-question-162/",
    "body": "A company is running its customer feedback application on Amazon Aurora MySQL. The company runs a report every day to extract customer feedback, and a team reads the feedback to determine if the customer comments are positive or negative. It sometimes takes days before the company can contact unhappy customers and take corrective measures. The company wants to use machine learning to automate this workflow.<br>Which solution meets this requirement with the LEAST amount of effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Aurora MySQL database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Use Amazon Comprehend to run sentiment analysis on the exported files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Aurora MySQL database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Use Amazon SageMaker to run sentiment analysis on the exported files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Aurora native integration with Amazon Comprehend. Use SQL functions to extract sentiment analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Aurora native integration with Amazon SageMaker. Use SQL functions to extract sentiment analysis."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-13T02:33:00.000Z",
        "voteCount": 3,
        "content": "\"When you run an ML query, Aurora calls Amazon SageMaker for a wide variety of ML algorithms or Amazon Comprehend for sentiment analysis\".\nRef: https://aws.amazon.com/getting-started/hands-on/sentiment-analysis-amazon-aurora-ml-integration/"
      },
      {
        "date": "2022-04-29T08:57:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.html\n\nAurora machine learning uses a highly optimized integration between the Aurora database and the AWS machine learning (ML) services SageMaker and Amazon Comprehend."
      },
      {
        "date": "2022-02-23T16:51:00.000Z",
        "voteCount": 2,
        "content": "Comprehend is a simple ML solution and takes the least effort"
      },
      {
        "date": "2021-12-15T03:52:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2021-12-09T07:35:00.000Z",
        "voteCount": 1,
        "content": "C for me\nFor details about using Aurora and Amazon Comprehend together, see&nbsp;Using Amazon Comprehend for sentiment detection.\nAurora machine learning uses a highly optimized integration between the Aurora database and the AWS machine learning (ML) services SageMaker and Amazon Comprehend."
      },
      {
        "date": "2021-11-17T12:42:00.000Z",
        "voteCount": 1,
        "content": "Ans C. All answers are available. The Minimum amount of effort is the key."
      },
      {
        "date": "2021-11-13T11:14:00.000Z",
        "voteCount": 1,
        "content": "Answer A. Comprehend is a simple ML solution and takes the least effort"
      },
      {
        "date": "2021-11-12T23:29:00.000Z",
        "voteCount": 1,
        "content": "Ans C   https://www.stackovercloud.com/2019/11/27/new-for-amazon-aurora-use-machine-learning-directly-from-your-databases/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/amazon/view/68497-exam-aws-certified-database-specialty-topic-1-question-163/",
    "body": "A bank plans to use an Amazon RDS for MySQL DB instance. The database should support read-intensive traffic with very few repeated queries.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon ElastiCache cluster. Use a write-through strategy to populate the cache.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon ElastiCache cluster. Use a lazy loading strategy to populate the cache.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the DB instance to Multi-AZ with a standby instance in another AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the DB instance. Use the read replica to distribute the read traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-31T09:05:00.000Z",
        "voteCount": 8,
        "content": "few repeated Queries. in this case elastic cache will have to reach to the database to get the information which defeats the purpose of Elasticache"
      },
      {
        "date": "2022-12-17T21:04:00.000Z",
        "voteCount": 1,
        "content": "D. keyword \"few repeated Queries\" caching wont help."
      },
      {
        "date": "2022-04-30T17:36:00.000Z",
        "voteCount": 2,
        "content": "D. Create a read replica of the DB instance. Use the read replica to distribute the read traffic."
      },
      {
        "date": "2022-04-13T05:03:00.000Z",
        "voteCount": 2,
        "content": "very bad wording used in this question ; because of \"with extremely few repeated queries\" which basically means \"with a lot of different queries\" Elasticache won't help"
      },
      {
        "date": "2022-03-06T11:00:00.000Z",
        "voteCount": 2,
        "content": "Sorry I take  it back completely. Im an idiot. \"Very few repeated read  queries\" as in most queries are  unique. Elasticache wont help here. I will change my option to (D)"
      },
      {
        "date": "2022-03-06T10:44:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is (B) Another question right here in ExamTopics\n\nFor its application database tier, a corporation uses Amazon RDS MySQL instances, and for its web tier, Apache Tomcat servers. Repeated read requests make up the majority of database queries from web apps.\nWhich AWS service would benefit from the addition of an in-memory store for repeated read queries?\n\nA. Amazon RDS Multi-AZ\nB. Amazon SQS\nC. Amazon ElastiCache &lt;&lt;&lt; (Official Answer)\nD. Amazon RDS read replica"
      },
      {
        "date": "2022-03-06T10:40:00.000Z",
        "voteCount": 1,
        "content": "what does  this even mean? \"...with extremely few repeated queries.\" -&gt; English grammar is aborrant in these questions and they're  artificially creating phrases they think makese sense!!\n\nBut if they  are TRYING to ask prevent the repeated queries  from hitting the database- then Elasticache with lazy loading will do the trick the first  query will take a hit to the database but the repeated queries will be returned from the cache\n\nFeels like (B)"
      },
      {
        "date": "2023-04-17T16:41:00.000Z",
        "voteCount": 1,
        "content": "Replica is for read intensive -- D"
      },
      {
        "date": "2022-01-26T13:02:00.000Z",
        "voteCount": 3,
        "content": "B\n\nElasticache is the best solution to \"handle high-volume read requests with extremely few repeated queries.\"  \n\nNote that it is extremely few repeated queries. As per this link, https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html, the lazy loading strategy fits this requirement"
      },
      {
        "date": "2022-02-13T09:35:00.000Z",
        "voteCount": 1,
        "content": "go D\nkeywork  \"use query\""
      },
      {
        "date": "2021-12-25T14:40:00.000Z",
        "voteCount": 1,
        "content": "'with extremely few repeated queries.' I believe its D"
      },
      {
        "date": "2021-12-23T21:55:00.000Z",
        "voteCount": 2,
        "content": "Is it D?"
      },
      {
        "date": "2021-12-26T18:26:00.000Z",
        "voteCount": 1,
        "content": "Yes, the answer is D .. extremely few repeated queries -&gt;  read replica"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/amazon/view/65816-exam-aws-certified-database-specialty-topic-1-question-164/",
    "body": "A database specialist has a fleet of Amazon RDS DB instances that use the default DB parameter group. The database specialist needs to associate a custom parameter group with some of the DB instances.<br>After the database specialist makes this change, when will the instances be assigned to this new parameter group?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstantaneously after the change is made to the parameter group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the next scheduled maintenance window of the DB instances",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the DB instances are manually rebooted\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWithin 24 hours after the change is made to the parameter group"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-29T11:39:00.000Z",
        "voteCount": 8,
        "content": "When you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is rebooted. However, if you modify dynamic parameters in the newly associated DB parameter group, these changes are applied immediately without a reboot."
      },
      {
        "date": "2022-02-24T13:59:00.000Z",
        "voteCount": 3,
        "content": "Manual reboot is necessary"
      },
      {
        "date": "2022-02-16T08:21:00.000Z",
        "voteCount": 3,
        "content": "C\nWhen you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is rebooted. However, if you modify dynamic parameters in the newly associated DB parameter group, these changes are applied immediately without a reboot. For more information about changing the DB parameter group, see Modifying an Amazon RDS DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html"
      },
      {
        "date": "2021-11-13T15:10:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2021-11-10T15:28:00.000Z",
        "voteCount": 2,
        "content": "Option C. When you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is rebooted."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/amazon/view/66015-exam-aws-certified-database-specialty-topic-1-question-165/",
    "body": "A company is planning on migrating a 500-GB database from Oracle to Amazon Aurora PostgreSQL using the AWS Schema Conversion Tool (AWS SCT) and<br>AWS DMS. The database does not have any stored procedures to migrate but has some tables that are large or partitioned. The application is critical for business so a migration with minimal downtime is preferred.<br>Which combination of steps should a database specialist take to accelerate the migration process? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS SCT data extraction agent to migrate the schema from Oracle to Aurora PostgreSQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the large tables, change the setting for the maximum number of tables to load in parallel and perform a full load using AWS DMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the large tables, create a table settings rule with a parallel load option in AWS DMS, then perform a full load using DMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to set up change data capture (CDC) for continuous replication until the cutover date.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS SCT to convert the schema from Oracle to Aurora PostgreSQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to convert the schema from Oracle to Aurora PostgreSQL and for continuous replication."
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-29T18:05:00.000Z",
        "voteCount": 8,
        "content": "C, D, E ( does not allow to select 3 options)\n\nE. use SCT to convert schema\n-&gt; C. Create a table setting rule with parallel load option, load using DMS\n-&gt; D. Continue DMS till cutover date"
      },
      {
        "date": "2022-12-24T16:12:00.000Z",
        "voteCount": 3,
        "content": "CDE are the answers"
      },
      {
        "date": "2022-12-19T04:28:00.000Z",
        "voteCount": 1,
        "content": "DE is the correct"
      },
      {
        "date": "2022-11-29T11:33:00.000Z",
        "voteCount": 2,
        "content": "CDE correct answer"
      },
      {
        "date": "2022-03-04T23:28:00.000Z",
        "voteCount": 3,
        "content": "D and E are obviously correct.  \nBetween B and C  -&gt; C is correct  - the aws recommendation is to load Large tables via partitioning. That means creating filters to load data in parallel. The rest of the smaller tables can use the MaxFullLoadSubTasks settings to  a higher value (default is 8)\n\nAn example is if you have a schema that contains 90 tables, 1 of which is 100GB while rest are small tables Create multiple tasks that break up the 100GB based on primary key  Create another task to load the smaller tables with \"MaxFullLoadSubtasks\" settings to 20"
      },
      {
        "date": "2022-02-24T15:24:00.000Z",
        "voteCount": 1,
        "content": "CDE are valid options"
      },
      {
        "date": "2022-01-20T04:53:00.000Z",
        "voteCount": 1,
        "content": "I vote for CDE"
      },
      {
        "date": "2021-12-27T04:09:00.000Z",
        "voteCount": 3,
        "content": "I prefer CDE\nFor option C, https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Tablesettings.html#CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Tablesettings.ParallelLoad"
      },
      {
        "date": "2021-11-27T13:10:00.000Z",
        "voteCount": 1,
        "content": "CDE correct options"
      },
      {
        "date": "2021-11-15T13:34:00.000Z",
        "voteCount": 3,
        "content": "I Think CDE"
      },
      {
        "date": "2021-11-14T06:12:00.000Z",
        "voteCount": 1,
        "content": "ACD :-https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-data-from-an-on-premises-oracle-database-to-aurora-postgresql.html"
      },
      {
        "date": "2021-11-15T07:28:00.000Z",
        "voteCount": 3,
        "content": "A: wrong: AWS SCT is not data extraction tool. CDE."
      },
      {
        "date": "2021-11-27T13:11:00.000Z",
        "voteCount": 1,
        "content": "Yes, CDE are the correct options."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/amazon/view/67108-exam-aws-certified-database-specialty-topic-1-question-166/",
    "body": "A company is migrating an IBM Informix database to a Multi-AZ deployment of Amazon RDS for SQL Server with Always On Availability Groups (AGs). SQL<br>Server Agent jobs on the Always On AG listener run at 5-minute intervals to synchronize data between the Informix database and the SQL Server database. Users experience hours of stale data after a successful failover to the secondary node with minimal latency.<br>What should a database specialist do to ensure that users see recent data after a failover?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet TTL to less than 30 seconds for cached DNS values on the Always On AG listener.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBreak up large transactions into multiple smaller transactions that complete in less than 5 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the databases on the secondary node to read-only mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the SQL Server Agent jobs on the secondary node from a script when the secondary node takes over after a failure."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-09T04:06:00.000Z",
        "voteCount": 13,
        "content": "A is the correct answer\nThe question is how to refresh stale data"
      },
      {
        "date": "2023-09-02T04:27:00.000Z",
        "voteCount": 1,
        "content": "AAAAAAA"
      },
      {
        "date": "2023-08-28T11:50:00.000Z",
        "voteCount": 1,
        "content": "A is wrong, MultiSubnetfailover is not the issue here."
      },
      {
        "date": "2022-12-17T21:21:00.000Z",
        "voteCount": 3,
        "content": "A. correct\nD. is obsolete. Amazon RDS for SQL Server now supports SQL Server Agent job replication\nPosted On: Apr 7, 2022\n\nAmazon RDS for SQL Server now supports SQL Server Agent job replication. With this new feature, SQL Server Agent jobs created, modified, or deleted on the primary instance will be automatically synchronized to the secondary instance in a Multi-AZ configuration."
      },
      {
        "date": "2022-12-07T16:13:00.000Z",
        "voteCount": 4,
        "content": "A\nSQL Agent jobs can be replicated by turning on SQL Server Agent job replication\n\nEXECUTE msdb.dbo.rds_set_system_database_sync_objects @object_types = 'SQLAgentJob';\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.Agent.html"
      },
      {
        "date": "2022-11-01T13:51:00.000Z",
        "voteCount": 2,
        "content": "Question prepared before the Job replication feature it seems, so now Answer D is no longer valid, and it should be A.\n\nhttps://aws.amazon.com/about-aws/whats-new/2022/04/amazon-rds-sql-server-sql-agent-job-replication/"
      },
      {
        "date": "2022-07-03T04:52:00.000Z",
        "voteCount": 1,
        "content": "C is wrong due to\n\nYou can't configure the secondary DB instance to accept database read activity.\nLink: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\nSection:\nMicrosoft SQL Server Multi-AZ deployment limitations, notes, and recommendations"
      },
      {
        "date": "2022-07-03T04:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer\n\nC is wrong due to \nYou can't configure the secondary DB instance to accept database read activity.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\nMicrosoft SQL Server Multi-AZ deployment limitations, notes, and recommendations:"
      },
      {
        "date": "2022-06-26T18:41:00.000Z",
        "voteCount": 1,
        "content": "D is the answer as JOB replication needs to be enabled explicitly."
      },
      {
        "date": "2022-06-26T11:27:00.000Z",
        "voteCount": 3,
        "content": "It's about refreshing stale data. @Jusfunda's comment has the correct link"
      },
      {
        "date": "2022-05-30T17:55:00.000Z",
        "voteCount": 3,
        "content": "Answer is A according to this link.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/sql-server-ec2-best-practices/configure-availability-groups.html#:~:text=Set%20HostRecordTTL%20to%2060%20or%20less%20when%20using%20Always%20On%20availability%20groups"
      },
      {
        "date": "2022-04-29T18:40:00.000Z",
        "voteCount": 1,
        "content": "If you have SQL Server Agent jobs, recreate them on the secondary.\n Create the jobs first in the original primary, then fail over, and create the same jobs in the new primary."
      },
      {
        "date": "2021-12-27T06:46:00.000Z",
        "voteCount": 3,
        "content": "As grek mentioned URL."
      },
      {
        "date": "2021-12-21T11:19:00.000Z",
        "voteCount": 3,
        "content": "I think the correct answer is D.. The SQL Server Agent jobs are used to synchronize data between the Informix and SQL Server databases and without those being created on secondary node the data won't be synchronized."
      },
      {
        "date": "2022-02-27T10:23:00.000Z",
        "voteCount": 1,
        "content": "answer is D\nA is Wrong because it is more than 30 seconds , AG does not replicate agent on databases"
      },
      {
        "date": "2021-12-02T07:25:00.000Z",
        "voteCount": 3,
        "content": "I think D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\n\nIf you have SQL Server Agent jobs, recreate them on the secondary. You do so because these jobs are stored in the msdb database, and you can't replicate this database by using Database Mirroring (DBM) or Always On Availability Groups (AGs). Create the jobs first in the original primary, then fail over, and create the same jobs in the new primary."
      },
      {
        "date": "2022-06-26T18:35:00.000Z",
        "voteCount": 2,
        "content": "This is mentioned in the URL you have shared above that JOB are replicated \n\nIn Multi-AZ deployments, SQL Server Agent jobs are replicated from the primary host to the secondary host when the job replication feature is turned on. For more information, see Turning on SQL Server Agent job replication."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/amazon/view/66483-exam-aws-certified-database-specialty-topic-1-question-167/",
    "body": "A database specialist needs to configure an Amazon RDS for MySQL DB instance to close non-interactive connections that are inactive after 900 seconds.<br>What should the database specialist do to accomplish this task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom DB parameter group and set the wait_timeout parameter value to 900. Associate the DB instance with the custom parameter group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the MySQL database and run the SET SESSION wait_timeout=900 command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the my.cnf file and set the wait_timeout parameter value to 900. Restart the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the default DB parameter group and set the wait_timeout parameter value to 900."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T19:52:00.000Z",
        "voteCount": 3,
        "content": "A. Create a custom DB parameter group and set the wait_timeout parameter value to 900. Associate the DB instance with the custom parameter group.\n\nwait_timeout: The number of seconds the server waits for activity on a non-interactive TCP/IP or UNIX File connection before closing it."
      },
      {
        "date": "2022-02-03T10:48:00.000Z",
        "voteCount": 4,
        "content": "A is the right answer, done that many times...."
      },
      {
        "date": "2021-12-28T03:21:00.000Z",
        "voteCount": 3,
        "content": "Managed RDS needs parameter grou.\nwait_timeout: The number of seconds the server waits for activity on a non-interactive TCP/IP or UNIX File connection before closing it."
      },
      {
        "date": "2021-12-25T06:26:00.000Z",
        "voteCount": 1,
        "content": "A is answer"
      },
      {
        "date": "2021-12-11T14:26:00.000Z",
        "voteCount": 3,
        "content": "For me, answer is A\nhttps://aws.amazon.com/fr/blogs/database/best-practices-for-configuring-parameters-for-amazon-rds-for-mysql-part-3-parameters-related-to-security-operational-manageability-and-connectivity-timeout/"
      },
      {
        "date": "2021-12-15T11:50:00.000Z",
        "voteCount": 1,
        "content": "Yes, the correct answer is A\nFrom part one of that blog series:\n\"You can set parameters globally using a parameter group. Alternatively, you can set them for a particular session using the SET command.\"\nhttps://aws.amazon.com/blogs/database/best-practices-for-configuring-parameters-for-amazon-rds-for-mysql-part-1-parameters-related-to-performance/\nYou can't edit files directly on the RDS servers, must use SET command or Parameter Group"
      },
      {
        "date": "2021-12-02T03:24:00.000Z",
        "voteCount": 2,
        "content": "C. the answer is wrong, but the link is right!"
      },
      {
        "date": "2021-12-24T11:25:00.000Z",
        "voteCount": 2,
        "content": "This is an Amazon managed service, you cannot edit my.cnf file yourself.  The correct answer is A"
      },
      {
        "date": "2021-11-27T10:52:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B"
      },
      {
        "date": "2021-11-21T10:58:00.000Z",
        "voteCount": 1,
        "content": "Actually, I think all the answers could be wrong because you are not supposed to edit the .cnf file in RDS, but you could set the parameter globally with SET GLOBAL wait_timeout = xxxx; there should be an option for that"
      },
      {
        "date": "2021-11-21T10:52:00.000Z",
        "voteCount": 1,
        "content": "It should be C!"
      },
      {
        "date": "2021-11-21T10:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is wrong, we should open the my.cnf file in /etc/mysql and then restart the server with  service mysql restart"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/amazon/view/68542-exam-aws-certified-database-specialty-topic-1-question-168/",
    "body": "A company is running its production databases in a 3 TB Amazon Aurora MySQL DB cluster. The DB cluster is deployed to the us-east-1 Region. For disaster recovery (DR) purposes, the company's database specialist needs to make the DB cluster rapidly available in another AWS Region to cover the production load with an RTO of less than 2 hours.<br>What is the MOST operationally efficient solution to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement an AWS Lambda function to take a snapshot of the production DB cluster every 2 hours, and copy that snapshot to an Amazon S3 bucket in the DR Region. Restore the snapshot to an appropriately sized DB cluster in the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a cross-Region read replica in the DR Region with the same instance type as the current primary instance. If the read replica in the DR Region needs to be used for production, promote the read replica to become a standalone DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a smaller DB cluster in the DR Region. Configure an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) enabled to replicate data from the current production DB cluster to the DB cluster in the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora global database that spans two Regions. Use AWS Database Migration Service (AWS DMS) to migrate the existing database to the new global database."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-27T00:37:00.000Z",
        "voteCount": 2,
        "content": "B. Add a cross-Region read replica in the DR Region\n\nthis is MOST operationally efficient solution"
      },
      {
        "date": "2023-05-10T04:09:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-01-07T11:19:00.000Z",
        "voteCount": 2,
        "content": "why not the option A ? replica consumes instance and data cost while snapshots costs less and restore of 3 TB can happen in less than 2 hours to achieve an RTO of 2 hours"
      },
      {
        "date": "2023-07-21T22:53:00.000Z",
        "voteCount": 1,
        "content": "Because copy a 3TB snapshot to another region using lamdba, it will cost far beyond 15 mins."
      },
      {
        "date": "2023-09-27T00:40:00.000Z",
        "voteCount": 1,
        "content": "cost-saving is not a requirement here, but operationally efficient is required in this case."
      },
      {
        "date": "2022-05-01T05:24:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\n\nRTO is 2 hours. With 3 TB database, cross-region replica \n\nGlobal Database was good but DMS ?"
      },
      {
        "date": "2022-01-12T09:44:00.000Z",
        "voteCount": 3,
        "content": "Would have chosen Global Database, but DMS is not needed there. So D is wrong."
      },
      {
        "date": "2021-12-28T04:19:00.000Z",
        "voteCount": 2,
        "content": "RTO is 2 hours. With 3 TB database, cross-region replica is a better option"
      },
      {
        "date": "2021-12-24T12:47:00.000Z",
        "voteCount": 3,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/amazon/view/65739-exam-aws-certified-database-specialty-topic-1-question-169/",
    "body": "A company has an on-premises SQL Server database. The users access the database using Active Directory authentication. The company successfully migrated its database to Amazon RDS for SQL Server. However, the company is concerned about user authentication in the AWS Cloud environment.<br>Which solution should a database specialist provide for the user to authenticate?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Active Directory Federation Services (AD FS) on premises and configure it with an on-premises Active Directory. Set up delegation between the on- premises AD FS and AWS Security Token Service (AWS STS) to map user identities to a role using theAmazonRDSDirectoryServiceAccess managed IAM policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a forest trust between the on-premises Active Directory and AWS Directory Service for Microsoft Active Directory. Use AWS SSO to configure an Active Directory user delegated to access the databases in RDS for SQL Server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Active Directory Connector to redirect directory requests to the company's on-premises Active Directory without caching any information in the cloud. Use the RDS master user credentials to connect to the DB instance and configure SQL Server logins and users from the Active Directory users and groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a forest trust between the on-premises Active Directory and AWS Directory Service for Microsoft Active Directory. Ensure RDS for SQL Server is using mixed mode authentication. Use the RDS master user credentials to connect to the DB instance and configure SQL Server logins and users from the Active Directory users and groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-15T05:46:00.000Z",
        "voteCount": 6,
        "content": "D =&gt; You need to use sql authentication with master user credential for configuring  SQL Server logins and users from the Active Directory users and groups, so for me mixed mode  authentication is a MUST, I go with D.\n\nfrom: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html\n...\n6. Use the Amazon RDS master user credentials to connect to the SQL Server DB instance as you do any other DB instance. Because the DB instance is joined to the AWS Managed Microsoft AD domain, you can provision SQL Server logins and users from the Active Directory users and groups in their domain. (These are known as SQL Server \"Windows\" logins.) Database permissions are managed through standard SQL Server permissions granted and revoked to these Windows logins.\n..."
      },
      {
        "date": "2021-11-12T04:29:00.000Z",
        "voteCount": 5,
        "content": "Answer is not D. Mixed mode is for both AD and Sql users. Question doesnt require that. Answer is B"
      },
      {
        "date": "2021-11-12T18:42:00.000Z",
        "voteCount": 1,
        "content": "did you take the exam  recently . how many question came from the dump"
      },
      {
        "date": "2021-11-21T22:31:00.000Z",
        "voteCount": 2,
        "content": "I think you are right, I vote for B"
      },
      {
        "date": "2021-12-09T05:24:00.000Z",
        "voteCount": 4,
        "content": "\"Amazon RDS uses mixed mode for Windows Authentication\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html\nThe correct answer is D"
      },
      {
        "date": "2023-08-31T13:33:00.000Z",
        "voteCount": 1,
        "content": "B. Establish a forest trust between the on-premises Active Directory and AWS Directory Service for Microsoft Active Directory. Use AWS SSO to configure an Active Directory user delegated to access the databases in RDS for SQL Server. \n\nhttps://aws.amazon.com/what-is/sso/#:~:text=Single%20sign%2Don%20(SSO),with%20one%2Dtime%20user%20authentication."
      },
      {
        "date": "2022-12-24T16:40:00.000Z",
        "voteCount": 1,
        "content": "B is my answer.  I don't understand why others select D.  I will continue reading reasons why"
      },
      {
        "date": "2022-10-14T14:03:00.000Z",
        "voteCount": 5,
        "content": "Answer is D. you have to login to the instance to map the aws directory with sql server logins"
      },
      {
        "date": "2022-10-14T14:05:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html\nUse the Amazon RDS master user credentials to connect to the SQL Server DB instance as you do any other DB instance. Because the DB instance is joined to the AWS Managed Microsoft AD domain, you can provision SQL Server logins and users from the Active Directory users and groups in their domain. (These are known as SQL Server \"Windows\" logins.) Database permissions are managed through standard SQL Server permissions granted and revoked to these Windows logins."
      },
      {
        "date": "2023-04-05T08:04:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS uses mixed mode for Windows Authentication. This approach means that the master user (the name and password used to create your SQL Server DB instance) uses SQL Authentication. Because the master user account is a privileged credential, you should restrict access to this account.\n\nTo get Windows Authentication using an on-premises or self-hosted Microsoft Active Directory, create a forest trust. The trust can be one-way or two-way. For more information on setting up forest trusts using AWS Directory Service, see When to create a trust relationship in the AWS Directory Service Administration Guide."
      },
      {
        "date": "2022-06-27T17:41:00.000Z",
        "voteCount": 2,
        "content": "The link reference clearly shows after the forest trust you can either connect with sso or access link. The option D has multiple problem 1. connect using master creds [not required &amp; unwanted] 2. Mixed mode will allow both the AD and the regular connection which the client didn't want in the first place."
      },
      {
        "date": "2022-06-21T10:04:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-04-29T09:38:00.000Z",
        "voteCount": 1,
        "content": "D uses master user access"
      },
      {
        "date": "2022-04-08T16:04:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2022-03-04T12:57:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-02-23T17:27:00.000Z",
        "voteCount": 3,
        "content": "Per - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html\nAmazon RDS uses mixed mode for Windows Authentication. This approach means that the master user (the name and password used to create your SQL Server DB instance) uses SQL Authentication. Because the master user account is a privileged credential, you should restrict access to this account.\n\nTo get Windows Authentication using an on-premises or self-hosted Microsoft Active Directory, create a forest trust."
      },
      {
        "date": "2021-12-09T07:46:00.000Z",
        "voteCount": 2,
        "content": "D for me\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html"
      },
      {
        "date": "2021-12-02T10:20:00.000Z",
        "voteCount": 2,
        "content": "Nope, Right answer is D:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html\nRead it and understand."
      },
      {
        "date": "2021-11-09T15:56:00.000Z",
        "voteCount": 3,
        "content": "Answer is D. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/amazon/view/65890-exam-aws-certified-database-specialty-topic-1-question-170/",
    "body": "A company uses an Amazon Redshift cluster to run its analytical workloads. Corporate policy requires that the company's data be encrypted at rest with customer managed keys. The company's disaster recovery plan requires that backups of the cluster be copied into another AWS Region on a regular basis.<br>How should a database specialist automate the process of backing up the cluster data in compliance with these policies?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the AWS Key Management Service (AWS KMS) customer managed key from the source Region to the destination Region. Set up an AWS Glue job in the source Region to copy the latest snapshot of the Amazon Redshift cluster from the source Region to the destination Region. Use a time-based schedule in AWS Glue to run the job on a daily basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Key Management Service (AWS KMS) customer managed key in the destination Region. Create a snapshot copy grant in the destination Region specifying the new key. In the source Region, configure cross-Region snapshots for the Amazon Redshift cluster specifying the destination Region, the snapshot copy grant, and retention periods for the snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the AWS Key Management Service (AWS KMS) customer-managed key from the source Region to the destination Region. Create Amazon S3 buckets in each Region using the keys from their respective Regions. Use Amazon EventBridge (Amazon CloudWatch Events) to schedule an AWS Lambda function in the source Region to copy the latest snapshot to the S3 bucket in that Region. Configure S3 Cross-Region Replication to copy the snapshots to the destination Region, specifying the source and destination KMS key IDs in the replication configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the same customer-supplied key materials to create a CMK with the same private key in the destination Region. Configure cross-Region snapshots in the source Region targeting the destination Region. Specify the corresponding CMK in the destination Region to encrypt the snapshot."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-15T08:18:00.000Z",
        "voteCount": 11,
        "content": "Answer is B: https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot"
      },
      {
        "date": "2023-03-25T14:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is B - https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      },
      {
        "date": "2022-04-29T19:01:00.000Z",
        "voteCount": 2,
        "content": "new KMS CMK in the destination Region\n-&gt; snapshot copy grant in the destination Region specifying the new key\n-&gt;In the source Region, configure cross-Region snapshots for the Amazon Redshift cluster specifying \n- the destination Region, \n- the snapshot copy grant, \n- and retention periods for the snapshot."
      },
      {
        "date": "2022-04-13T07:14:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2022-01-20T05:48:00.000Z",
        "voteCount": 1,
        "content": "Option B."
      },
      {
        "date": "2021-11-12T05:55:00.000Z",
        "voteCount": 3,
        "content": "Option B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/amazon/view/65814-exam-aws-certified-database-specialty-topic-1-question-171/",
    "body": "A database specialist is launching a test graph database using Amazon Neptune for the first time. The database specialist needs to insert millions of rows of test observations from a .csv file that is stored in Amazon S3. The database specialist has been using a series of API calls to upload the data to the Neptune DB instance.<br>Which combination of steps would allow the database specialist to upload the data faster? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure Amazon Cognito returns the proper AWS STS tokens to authenticate the Neptune DB instance to the S3 bucket hosting the CSV file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the vertices and edges are specified in different .csv files with proper header column formatting.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to move data from Amazon S3 to the Neptune Loader.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCurl the S3 URI while inside the Neptune DB instance and then run the addVertex or addEdge commands.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure an IAM role for the Neptune DB instance is configured with the appropriate permissions to allow access to the file in the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 VPC endpoint and issue an HTTP POST to the database's loader endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BEF",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-18T07:52:00.000Z",
        "voteCount": 1,
        "content": "BEF https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html"
      },
      {
        "date": "2022-09-26T17:45:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/neptune/latest/userguide/dms-neptune.html\nAWS Database Migration Service (AWS DMS) can load data into Neptune from supported source databases quickly and securely."
      },
      {
        "date": "2022-04-29T10:27:00.000Z",
        "voteCount": 1,
        "content": "B. specification in separate csv\nE. access from Neptune to S3\nF. S3 vpc endpoint to run Loader command"
      },
      {
        "date": "2022-03-12T07:02:00.000Z",
        "voteCount": 2,
        "content": "Amazon Neptune provides a Loader command for loading data from external files directly into a Neptune DB instance. You can use this command instead of executing a large number of INSERT statements, addV and addE steps, or other API calls.\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
      },
      {
        "date": "2022-03-04T19:54:00.000Z",
        "voteCount": 4,
        "content": "The question is saying the professional has already uploaded the data. What was needed to complete the upload?\n\nQue: \"The database professional uploaded the data to the Neptune DB instance through a series of API calls. Which sequence of actions enables the database professional to upload the data most quickly?\"\nAns: Setup S3 VPC endpoint. Setup the IAM roles with GET and LIST permissions to S3. Make sure the  contents of CSV are accurate. These are (B) (E) (F)!!"
      },
      {
        "date": "2022-03-01T23:59:00.000Z",
        "voteCount": 1,
        "content": "Correct order of steps"
      },
      {
        "date": "2022-02-24T11:26:00.000Z",
        "voteCount": 1,
        "content": "Correct order of steps"
      },
      {
        "date": "2022-02-23T16:37:00.000Z",
        "voteCount": 1,
        "content": "B,E,F: Correct"
      },
      {
        "date": "2022-01-14T00:38:00.000Z",
        "voteCount": 2,
        "content": "BEF,\n\nDisagree with C:\ncurl is issued from CLI to load the data from S3 to Neptune, not \"log in\" to to the database and run those commands:\n\ncurl -X POST https://your-neptune-endpoint:port/loader \\ \n     -H 'Content-Type: application/json' \\ \n     -d ' \n     { \n       \"source\" : \"s3://bucket-name/object-key-name\", \n       \"format\" : \"opencypher\",\n       \"userProvidedEdgeIds\": \"TRUE\", \n       \"iamRoleArn\" : \"arn:aws:iam::account-id:role/role-name\", \n       \"region\" : \"region\", \n       \"failOnError\" : \"FALSE\", \n       \"parallelism\" : \"MEDIUM\", \n     }'"
      },
      {
        "date": "2022-01-14T00:39:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I meant disagree with D"
      },
      {
        "date": "2021-12-11T21:35:00.000Z",
        "voteCount": 3,
        "content": "I think BEF \nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-optimize.html"
      },
      {
        "date": "2021-12-09T08:07:00.000Z",
        "voteCount": 1,
        "content": "DEF for me\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"
      },
      {
        "date": "2021-12-08T10:23:00.000Z",
        "voteCount": 4,
        "content": "BEF is the correct answer. Look out for the \"got you\"\nAmazon Neptune provides a Loader command for loading data from external files directly into a Neptune DB instance. You can use this command instead of executing a large number of INSERT statements, addV and addE steps, or other API calls.\nThis makes D incorrect"
      },
      {
        "date": "2021-12-09T06:31:00.000Z",
        "voteCount": 1,
        "content": "Agree with BEF\n\"To load Apache TinkerPop Gremlin data using the CSV format, you must specify the vertices and the edges in separate files.\"\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format-gremlin.html"
      },
      {
        "date": "2021-11-30T02:52:00.000Z",
        "voteCount": 2,
        "content": "CEF is the correct one!"
      },
      {
        "date": "2021-11-25T05:41:00.000Z",
        "voteCount": 1,
        "content": "Bcd  Loader does not need separate a addvertex addedge"
      },
      {
        "date": "2021-11-10T14:33:00.000Z",
        "voteCount": 4,
        "content": "DEF\nFrom https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html\n1. Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n2. Create an IAM role with Read and List access to the bucket.\n3. Create an Amazon S3 VPC endpoint.\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n5. The Neptune DB instance assumes the IAM role to load the data from the bucket."
      },
      {
        "date": "2021-11-21T22:47:00.000Z",
        "voteCount": 2,
        "content": "that makes sense but BCF also seem necessary for quick upload, I imagine the full question is missing"
      },
      {
        "date": "2021-11-24T13:03:00.000Z",
        "voteCount": 2,
        "content": "I'm not a Neptune expert but option C, moving data to the Neptune \"Loader\" doesn't make sense. Neptune Loader is just an API which helps to load data to a NeptuneDB, right?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/amazon/view/65815-exam-aws-certified-database-specialty-topic-1-question-172/",
    "body": "A company is using Amazon DynamoDB global tables for an online gaming application. The game has players around the world. As the game has become more popular, the volume of requests to DynamoDB has increased significantly. Recently, players have reported that the game state is inconsistent between players in different countries. A database specialist observes that the ReplicationLatency metric for some of the replica tables is too high.<br>Which approach will alleviate the problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure all replica tables to use DynamoDB auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a DynamoDB Accelerator (DAX) cluster on each of the replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the primary table to use DynamoDB auto scaling and the replica tables to use manually provisioned capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the table-level write throughput limit service quota to a higher value."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-23T21:41:00.000Z",
        "voteCount": 1,
        "content": "Make sure that you have enough provisioned capacity to perform replicated writes to all global table Regions. To verify, use DynamoDB auto scaling or on-demand capacity mode.\nhttps://repost.aws/knowledge-center/replication-delay-global-table-dynamodb"
      },
      {
        "date": "2023-04-13T03:06:00.000Z",
        "voteCount": 1,
        "content": "B\nbecause network latency caused the inconsistent."
      },
      {
        "date": "2022-04-29T11:34:00.000Z",
        "voteCount": 1,
        "content": "It is important that the replica tables and secondary indexes in your global table have identical write capacity settings to ensure proper replication of data."
      },
      {
        "date": "2022-02-24T13:58:00.000Z",
        "voteCount": 1,
        "content": "autoscaling with fix it"
      },
      {
        "date": "2021-12-09T08:14:00.000Z",
        "voteCount": 1,
        "content": "A for me\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_reqs_bestpractices.html"
      },
      {
        "date": "2021-11-10T15:20:00.000Z",
        "voteCount": 2,
        "content": "Option A. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_reqs_bestpractices.html"
      },
      {
        "date": "2021-11-10T15:20:00.000Z",
        "voteCount": 1,
        "content": "Option A. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_reqs_bestpractices.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/amazon/view/66628-exam-aws-certified-database-specialty-topic-1-question-173/",
    "body": "A company runs a MySQL database for its ecommerce application on a single Amazon RDS DB instance. Application purchases are automatically saved to the database, which causes intensive writes. Company employees frequently generate purchase reports. The company needs to improve database performance and reduce downtime due to patching for upgrades.<br>Which approach will meet these requirements with the LEAST amount of operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a Multi-AZ deployment of the RDS for MySQL DB instance, and enable Memcached in the MySQL option group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a Multi-AZ deployment of the RDS for MySQL DB instance, and set up replication to a MySQL DB instance running on Amazon EC2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a Multi-AZ deployment of the RDS for MySQL DB instance, and add a read replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a read replica and promote it to an Amazon Aurora MySQL DB cluster master. Then enable Amazon Aurora Serverless."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-29T13:41:00.000Z",
        "voteCount": 2,
        "content": "Enable a Multi-AZ deployment of the RDS for MySQL DB instance, and add a read replica.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
      },
      {
        "date": "2022-03-11T05:38:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. \nPer - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html\n\nUsing a read replica to reduce downtime when upgrading a MySQL database. If your MySQL DB instance is currently in use with a production application, you can use the following procedure to upgrade the database version for your DB instance. This procedure can reduce the amount of downtime for your application.\n\nBy using a read replica, you can perform most of the maintenance steps ahead of time and minimize the necessary changes during the actual outage. With this technique, you can test and prepare the new DB instance without making any changes to your existing DB instance."
      },
      {
        "date": "2022-03-04T22:15:00.000Z",
        "voteCount": 1,
        "content": "C is the correct  answer. The question mentions LEAST operational overhead and the employees frequently create purchase reports (Read  heavy). Moving queries to RR will open up the  Primary to take on additional loads wrt writes."
      },
      {
        "date": "2021-11-24T18:05:00.000Z",
        "voteCount": 2,
        "content": "Answer : C"
      },
      {
        "date": "2021-11-24T04:33:00.000Z",
        "voteCount": 1,
        "content": "The best option here is C. But I would choose Aurora Multi-Master if it was an option"
      },
      {
        "date": "2022-02-25T00:46:00.000Z",
        "voteCount": 4,
        "content": "it has a high volume writes, since when can you write on a read replica?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/amazon/view/66252-exam-aws-certified-database-specialty-topic-1-question-174/",
    "body": "An ecommerce company is migrating its core application database to Amazon Aurora MySQL. The company is currently performing online transaction processing<br>(OLTP) stress testing with concurrent database sessions. During the first round of tests, a database specialist noticed slow performance for some specific write operations.<br>Reviewing Amazon CloudWatch metrics for the Aurora DB cluster showed 90% CPU utilization.<br>Which steps should the database specialist take to MOST effectively identify the root cause of high CPU utilization and slow performance? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Enhanced Monitoring at less than 30 seconds of granularity to review the operating system metrics before the next round of tests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the VolumeBytesUsed metric in CloudWatch to see if there is a spike in write I/O.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview Amazon RDS Performance Insights to identify the top SQL statements and wait events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview Amazon RDS API calls in AWS CloudTrail to identify long-running queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Advance Auditing to log QUERY events in Amazon CloudWatch before the next round of tests."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-17T12:08:00.000Z",
        "voteCount": 15,
        "content": "A,C i smy pick.\nB: VolumeBytesUsed metric is for total used storage size, not for CPU"
      },
      {
        "date": "2022-10-29T10:15:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/\n\nA, C is the answer"
      },
      {
        "date": "2022-10-15T06:31:00.000Z",
        "voteCount": 1,
        "content": "E is for auditing including query events. For performance I go with A and C"
      },
      {
        "date": "2022-09-22T02:03:00.000Z",
        "voteCount": 2,
        "content": "Guys.. why are you all Posting RDS Docs Links? This Question is about Aurora which is why I go with A and E"
      },
      {
        "date": "2023-04-18T08:23:00.000Z",
        "voteCount": 1,
        "content": "Performance Insights is currently available for Amazon Aurora PostgreSQL compatible Edition, MySQL compatible Edition, PostgreSQL, MySQL, Oracle, SQL Server, and MariaDB."
      },
      {
        "date": "2023-05-27T10:52:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PerfInsights.Overview.html\n\nyou can use Perf Insight in *Aurora* to identify the most heavy-hitting queries. Nobody uses Advance Auditing to solve performance problems..."
      },
      {
        "date": "2022-09-22T02:02:00.000Z",
        "voteCount": 1,
        "content": "Guys.. why are you all Posting RDS Docs Links? This Question is about Aurora which is why I go with A and E"
      },
      {
        "date": "2022-08-13T02:12:00.000Z",
        "voteCount": 1,
        "content": "A,C are related to performance. B is related to storage. D is related to security (API calls tracking) and E (Advanced Auditing) does not exist."
      },
      {
        "date": "2022-10-04T20:02:00.000Z",
        "voteCount": 1,
        "content": "Using Advanced Auditing with an Amazon Aurora MySQL DB cluster\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html"
      },
      {
        "date": "2022-04-29T07:46:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/\n\nA. Enhanced Monitoring checks OS\nxB.  VolumeBytesUsed is the amount of storage used \nC.  Performance Insights"
      },
      {
        "date": "2022-02-23T16:42:00.000Z",
        "voteCount": 2,
        "content": "Per - https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-slow-query/\n\nTo analyze the workload contributing to resource consumption, use Performance Insights. Performance Insights will provide a graphic analysis of all your queries and any waits that are contributing to increased resource consumption.\n\nYou can also use Enhanced Monitoring to retrieve the list of operating systems involved in your workload and the underlying system metrics. By default, the monitoring interval for Enhanced Monitoring is 60 seconds. It's a best practice to set this to 1-5 second intervals for more granular data points."
      },
      {
        "date": "2022-02-23T15:13:00.000Z",
        "voteCount": 1,
        "content": "A,C : PI and Enhanced Monitoring"
      },
      {
        "date": "2021-12-11T12:00:00.000Z",
        "voteCount": 1,
        "content": "A,C is the solution"
      },
      {
        "date": "2021-12-09T07:29:00.000Z",
        "voteCount": 1,
        "content": "A,C for me\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/"
      },
      {
        "date": "2021-12-09T05:01:00.000Z",
        "voteCount": 1,
        "content": "A and C is correct.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-slow-query/"
      },
      {
        "date": "2021-11-29T13:13:00.000Z",
        "voteCount": 1,
        "content": "C,D is my choice \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/logging-using-cloudtrail.html\n:\"enableCloudwatchLogsExports\": [\n            \"audit\",\n            \"error\",\n            \"general\",\n            \"slowquery\""
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/amazon/view/65861-exam-aws-certified-database-specialty-topic-1-question-175/",
    "body": "An online advertising company is implementing an application that displays advertisements to its users. The application uses an Amazon DynamoDB table as a data store. The application also uses a DynamoDB Accelerator (DAX) cluster to cache its reads. Most of the reads are from the GetItem query and the<br>BatchGetItem query. Consistency of reads is not a requirement for this application.<br>Upon deployment, the application cache is not performing as expected. Specific strongly consistent queries that run against the DAX cluster are taking many milliseconds to respond instead of microseconds.<br>How can the company improve the cache behavior to increase application performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the DAX cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure DAX to be an item cache with no query cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse eventually consistent reads instead of strongly consistent reads.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DAX cluster with a higher TTL for the item cache."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-23T20:58:00.000Z",
        "voteCount": 1,
        "content": "As strong consistency is not a requirement"
      },
      {
        "date": "2022-04-29T18:12:00.000Z",
        "voteCount": 2,
        "content": "extremely consistent =&gt; bypass DAX"
      },
      {
        "date": "2022-02-24T15:27:00.000Z",
        "voteCount": 2,
        "content": "Eventually consistent reads are good"
      },
      {
        "date": "2022-01-20T05:21:00.000Z",
        "voteCount": 4,
        "content": "I vote for C"
      },
      {
        "date": "2021-11-27T13:16:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-11-26T08:44:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2021-11-11T16:51:00.000Z",
        "voteCount": 1,
        "content": "Option B. If you specify zero as the&nbsp;query cache&nbsp;TTL setting, the query response will not be cached."
      },
      {
        "date": "2021-11-14T11:30:00.000Z",
        "voteCount": 2,
        "content": "I was wrong. The correct answer is C. Consistent queries are sent directly to dynamodb so the response time is longer."
      },
      {
        "date": "2021-11-14T01:20:00.000Z",
        "voteCount": 1,
        "content": "I am not sure about that one \"Strongly consistent reads may have higher latency than eventually consistent reads\", hence eventually consistent reads have lower latency, we are trying to lower the latency, C may be right"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/amazon/view/67082-exam-aws-certified-database-specialty-topic-1-question-176/",
    "body": "A company is running its critical production workload on a 500 GB Amazon Aurora MySQL DB cluster. A database engineer must move the workload to a new<br>Amazon Aurora Serverless MySQL DB cluster without data loss.<br>Which solution will accomplish the move with the LEAST downtime and the LEAST application impact?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing DB cluster and update the Aurora configuration to \u05d2\u20acServerless.\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the existing DB cluster and restore it to a new Aurora Serverless DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora Serverless replica from the existing DB cluster and promote it to primary when the replica lag is minimal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate the data between the existing DB cluster and a new Aurora Serverless DB cluster by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-15T08:34:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nYou can now mix and match Serverless and normal Auroras in the same cluster."
      },
      {
        "date": "2023-07-25T04:40:00.000Z",
        "voteCount": 1,
        "content": "The Answer is B for Serverless V2."
      },
      {
        "date": "2023-07-08T15:21:00.000Z",
        "voteCount": 3,
        "content": "By the way, in Serverless V2 it is possible to fail over: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.upgrade.html#aurora-serverless-v2.switch-from-provisioned\nThis question is a bit old."
      },
      {
        "date": "2023-05-19T13:04:00.000Z",
        "voteCount": 2,
        "content": "Creating serverless readreplica and promoting whenever lag is minimal is the best option I can go with - C\nDMS is the longest route"
      },
      {
        "date": "2023-02-08T07:01:00.000Z",
        "voteCount": 1,
        "content": "You can\u00b4t create an Aurora Serverless replica from the existing Aurora DB cluster. With no downtime, the answer is definitely D"
      },
      {
        "date": "2022-12-26T18:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-migrate-provisioned-serverless/"
      },
      {
        "date": "2022-12-24T04:21:00.000Z",
        "voteCount": 2,
        "content": "I am really confused here. Pls refer to https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html  -  Which clearly says that \"Aurora Replicas connect to the same storage volume as the primary DB instance, but support read operations only\". Which means there is no lag at all given Read replicas connect the same storage. Hence i will go for \"C\" which is the simplest from operations perspective"
      },
      {
        "date": "2023-05-27T11:25:00.000Z",
        "voteCount": 1,
        "content": "You really need to study again, very carefully. Replica is a basic concept in RDS and Aurora. Replicas and primary are the type of nodes in the *SAME* cluster, be it a RDS cluster or Aurora cluster. So when you are creating an \"Aurora Serverless Replica\", there has to be an Aurora *Serverless* Primary in the first place. That doesn't exist in the question - there is only an \"Aurora MySQL DB cluster\"\nIn other words, \"You can\u00b4t create an Aurora Serverless replica from the existing Aurora DB cluster\""
      },
      {
        "date": "2024-03-06T02:59:00.000Z",
        "voteCount": 1,
        "content": "English is way harder than databases apparently"
      },
      {
        "date": "2022-10-11T15:22:00.000Z",
        "voteCount": 1,
        "content": "C vs D : looks take same amount of to do finial data sync &amp; C is much simple than D\nC: Just auto sync by Aurora Replica\nD: Sync data by CDC"
      },
      {
        "date": "2022-05-29T07:36:00.000Z",
        "voteCount": 1,
        "content": "Both C and D provides near Zero down time but the option C says \"...when the replica lag is minimal\" for me this is unacceptable for a \"business's mission-critical production workload\", B is out, impossible without downtime. I will go with D"
      },
      {
        "date": "2022-04-30T20:35:00.000Z",
        "voteCount": 4,
        "content": "D. Replicate the data between the existing DB cluster and a new Aurora Serverless DB cluster by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) enabled.\n\nfor DMS: Aurora Serverless can not be source but can be target"
      },
      {
        "date": "2022-04-13T01:17:00.000Z",
        "voteCount": 2,
        "content": "because of the limited downtime requirement I'll go with DMS and D"
      },
      {
        "date": "2022-03-06T09:01:00.000Z",
        "voteCount": 3,
        "content": "B looks tempting but it will result in downtime. D wont have any downtime"
      },
      {
        "date": "2022-01-26T13:39:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2022-01-26T13:43:00.000Z",
        "voteCount": 1,
        "content": "You can't use AWS Database Migration Service and Change Data Capture (CDC) with Aurora Serverless DB clusters. Only provisioned Aurora DB clusters support CDC with AWS DMS as a source.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations"
      },
      {
        "date": "2022-01-26T13:45:00.000Z",
        "voteCount": 4,
        "content": "Sorry, D is correct. The doc state it does not support as Source, but the Aurora Serverless in this question is target."
      },
      {
        "date": "2023-09-30T11:14:00.000Z",
        "voteCount": 1,
        "content": "That applies only to Serverless V1.\nYou can use AWS Database Migration Service (DMS) and Change Data Capture (CDC) with Aurora Serverless v2 DB clusters though. \nImplementing Change Data Capute (CDC) with Aurora Serverless v2. https://dev.to/omarrosadio/implementing-change-data-capute-cdc-with-aurora-serverless-v2-4i1l"
      },
      {
        "date": "2021-12-28T04:12:00.000Z",
        "voteCount": 2,
        "content": "The requirement to Minimum Downtime and from Provisioned to Serverless. The best choice is DMS."
      },
      {
        "date": "2021-12-24T12:41:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2021-12-07T12:03:00.000Z",
        "voteCount": 2,
        "content": "B. https://aws.amazon.com/rds/aurora/faqs/\nAmazon Aurora is drop-in compatible with existing MySQL open-source databases and adds support for new releases regularly. This means you can easily migrate MySQL databases to and from Aurora using standard import/export tools or snapshots. It also means that most of the code, applications, drivers, and tools you already use with MySQL databases today can be used with Aurora with little or no change."
      },
      {
        "date": "2022-03-06T09:00:00.000Z",
        "voteCount": 1,
        "content": "yes but snapshot restore has downtime"
      },
      {
        "date": "2022-03-06T09:08:00.000Z",
        "voteCount": 2,
        "content": "Since  you would need to stop the writes on the source  for this to work &gt; Take snapshot and restore. then repoint the app - depending on the size of the database this could be substantial. With DMS its zero and the question is asking about LEAST DOWNTIME  and  LEAST APPLICATION IMPACT"
      },
      {
        "date": "2021-12-02T04:10:00.000Z",
        "voteCount": 4,
        "content": "D.\nhttps://medium.com/@souri29/how-to-migrate-from-amazon-rds-aurora-or-mysql-to-amazon-aurora-serverless-55f9a4a74078"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/amazon/view/67569-exam-aws-certified-database-specialty-topic-1-question-177/",
    "body": "A company is building a web application on AWS. The application requires the database to support read and write operations in multiple AWS Regions simultaneously. The database also needs to propagate data changes between Regions as the changes occur. The application must be highly available and must provide latency of single-digit milliseconds.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB global tables\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB streams with AWS Lambda to replicate the data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn Amazon ElastiCache for Redis cluster with cluster mode enabled and multiple shards",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn Amazon Aurora global database"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-12T01:11:00.000Z",
        "voteCount": 5,
        "content": "A: DynamoDB Global tables\n\nAurora Global Databases provides a writer and a reader endpoints in the primary region but only a reader endpoints in other region. Although strongly consistent, it does not fulfill the requirements that \"there are plenty of read / write activities\" in all regions."
      },
      {
        "date": "2023-09-12T15:58:00.000Z",
        "voteCount": 1,
        "content": "A: DynamoDB Global tables\n\nbecause of below\nA:  DynamoDB providing single-digit millisecond latency \nD:  Aurora Global Database with typical cross-Region replication latencies below 1 second."
      },
      {
        "date": "2022-04-30T17:33:00.000Z",
        "voteCount": 1,
        "content": "A. Amazon DynamoDB global tables"
      },
      {
        "date": "2022-04-13T05:06:00.000Z",
        "voteCount": 3,
        "content": "we need to write in several regions -&gt; Aurora Global is single master, so DynamoDB Global is an answer"
      },
      {
        "date": "2021-12-15T08:48:00.000Z",
        "voteCount": 3,
        "content": "The questions states that the database needs to \"support concurrent read and write activities in several AWS Regions.\"  Aurora Global databases only allow writes to the single master in a single region.  DynamoDB Global tables allow read and write to all instances in all regions. \nAnswer is A"
      },
      {
        "date": "2021-12-23T21:06:00.000Z",
        "voteCount": 1,
        "content": "I agree,  the answer is A"
      },
      {
        "date": "2021-12-10T07:44:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2021-12-10T07:46:00.000Z",
        "voteCount": 2,
        "content": "The question states that - Additionally, the database must communicate data changes across Regions as they occur. \nDynamoDB is eventual consistency\nAurora is immediate consistency"
      },
      {
        "date": "2022-05-22T11:50:00.000Z",
        "voteCount": 1,
        "content": "Did you read this line? \"the database supports concurrent read and write activities in several AWS Regions\". Does Aurora Global Database support this feature?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/amazon/view/65858-exam-aws-certified-database-specialty-topic-1-question-178/",
    "body": "A company is using Amazon Neptune as the graph database for one of its products. The company's data science team accidentally created large amounts of temporary information during an ETL process. The Neptune DB cluster automatically increased the storage space to accommodate the new data, but the data science team deleted the unused information.<br>What should a database specialist do to avoid unnecessary charges for the unused cluster volume space?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the cluster volume. Restore the snapshot in another cluster with a smaller volume size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to turn on automatic resizing of the cluster volume.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the cluster data into a new Neptune DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Neptune read replica to the cluster. Promote this replica as a new primary DB instance. Reset the storage space of the cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-11T15:43:00.000Z",
        "voteCount": 11,
        "content": "Option C. The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. \nCreating and restoring a snapshot does not reduce the amount of storage allocated for your DB cluster, because a snapshot retains the original image of the cluster's underlying storage."
      },
      {
        "date": "2021-11-13T13:01:00.000Z",
        "voteCount": 3,
        "content": "you are actually right, taking from AWS documentation \"The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. See Neptune's data export service and utility for an easy way to export data from a DB cluster, and Neptune's bulk loader for an easy way to import data back into Neptune.\""
      },
      {
        "date": "2022-02-27T08:34:00.000Z",
        "voteCount": 1,
        "content": "Neptune storage allocation\nEven though a Neptune cluster volume can grow to 64 TiB, you are only charged for the space actually allocated. The total space allocated is determined by the storage high water mark, which is the maximum amount allocated to the cluster volume at any time during its existence.\n\nThis means that even if user data is removed from a cluster volume, such as by a drop query like g.V().drop(), the total allocated space remains the same. Neptune does automatically optimize the unused allocated space for reuse in the future."
      },
      {
        "date": "2022-04-29T13:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-storage.html#feature-overview-storage-best-practices\n\nThe only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster."
      },
      {
        "date": "2022-03-13T08:29:00.000Z",
        "voteCount": 3,
        "content": "C\n\n\"The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. \"\n\n\"Creating and restoring a snapshot does not reduce the amount of storage allocated for your DB cluster, because a snapshot retains the original image of the cluster's underlying storage.\"\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-storage.html"
      },
      {
        "date": "2022-03-04T22:29:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. Its very easy to get tricked into \"Restoring from a snapshot\" but remember - restoring from a snapshot WONT change the storage capacity (in console RDS Snapshot &gt; Restore &gt;  under Allocate Capacity its greyed out)."
      },
      {
        "date": "2021-12-20T15:55:00.000Z",
        "voteCount": 4,
        "content": "Option C"
      },
      {
        "date": "2021-12-16T20:13:00.000Z",
        "voteCount": 2,
        "content": "Ans is C\nThe only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster."
      },
      {
        "date": "2021-11-24T04:36:00.000Z",
        "voteCount": 2,
        "content": "Option C. As for every provisioned storage in AWS, you cant reduce the capacity choosen, you need to create a new one and transfer the data."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/amazon/view/66480-exam-aws-certified-database-specialty-topic-1-question-179/",
    "body": "A database specialist is responsible for designing a highly available solution for online transaction processing (OLTP) using Amazon RDS for MySQL production databases. Disaster recovery requirements include a cross-Region deployment along with an RPO of 5 minutes and RTO of 30 minutes.<br>What should the database specialist do to align to the high availability and disaster recovery requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Multi-AZ deployment in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse read replica deployments in all Availability Zones of the secondary Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Multi-AZ and read replica deployments within a Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Multi-AZ and deploy a read replica in a secondary Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T19:36:00.000Z",
        "voteCount": 3,
        "content": "D. Use Multi-AZ and deploy a read replica in a secondary Region.\n\nHigh availability = Multi AZ\nDisaster recovery = Read replica in a secondary region (or standby instance)"
      },
      {
        "date": "2022-04-13T01:32:00.000Z",
        "voteCount": 3,
        "content": "HA -&gt; Multi AZ, cross region DR -&gt; read replica in a secondary region"
      },
      {
        "date": "2022-02-08T06:37:00.000Z",
        "voteCount": 1,
        "content": "I guess there must be a 6 hours limit somehow"
      },
      {
        "date": "2021-12-25T15:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-12-24T11:13:00.000Z",
        "voteCount": 4,
        "content": "High availability = Multi AZ\nDisaster recovery = Read replica in a secondary region"
      },
      {
        "date": "2021-11-29T13:58:00.000Z",
        "voteCount": 3,
        "content": "Answer: A       Multi-AZ is for High Availability, Read Replicas are for Scalability, which the \n                         question does not mention. A is the only choice that does not include a \n                         needing a Read Replica."
      },
      {
        "date": "2021-12-15T11:34:00.000Z",
        "voteCount": 3,
        "content": "The question asks for High Availability AND Disaster Recovery.  So you need both Multi-AZ for the High Availabilty and a Read Replica for the DR.\nAnswer is D"
      },
      {
        "date": "2021-11-21T10:43:00.000Z",
        "voteCount": 3,
        "content": "Shouldn't it be D?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/amazon/view/67242-exam-aws-certified-database-specialty-topic-1-question-180/",
    "body": "A media company wants to use zero-downtime patching (ZDP) for its Amazon Aurora MySQL database. Multiple processing applications are using SSL certificates to connect to database endpoints and the read replicas.<br>Which factor will have the LEAST impact on the success of ZDP?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBinary logging is enabled, or binary log replication is in progress.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCurrent SSL connections are open to the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporary tables or table locks are in use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe value of the lower_case_table_names server parameter was set to 0 when the tables were created.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-03-06T10:22:00.000Z",
        "voteCount": 10,
        "content": "ZeroDowntime patching wont work if any of the following is true:\n- Binary Logging is Enabled\n- Long Running queries/transactions are in progress\n- Temporary Tables are in use\n- Table Locks are in use\n- SSL connections are Open\n\nBy Elimination The answer is (D) - which actually makes sense since it makes no difference to patching."
      },
      {
        "date": "2022-04-30T15:13:00.000Z",
        "voteCount": 1,
        "content": "Binary Logging is now gracefully handled"
      },
      {
        "date": "2023-09-30T11:36:00.000Z",
        "voteCount": 1,
        "content": "Even if it's gracefully handled now - option D has the LEAST impact on ZDP.\nIt's not even mentioned in the list of exclusions / constraints above."
      },
      {
        "date": "2021-12-06T12:43:00.000Z",
        "voteCount": 8,
        "content": "I think its A\n\nIn Aurora MySQL 2.10 and higher, Aurora can perform a zero-downtime patch when binary log replication is enabled. Aurora MySQL automatically drops the connection to the binlog target during a ZDP operation. Aurora MySQL automatically reconnects to the binlog target and resumes replication after the restart finishes.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Patching.html"
      },
      {
        "date": "2022-05-19T03:37:00.000Z",
        "voteCount": 4,
        "content": "The question is asking MINIMUM influence. So it's not A. The answer is D."
      },
      {
        "date": "2023-04-10T18:34:00.000Z",
        "voteCount": 1,
        "content": "A\nZDP might not complete successfully under the following conditions:\n\nLong-running queries or transactions are in progress. If Aurora can perform ZDP in this case, any open transactions are canceled.\n\nOpen Secure Socket Layer (SSL) connections exist.\n\nTemporary tables or table locks are in use, for example while data definition language (DDL) statements run. If Aurora can perform ZDP in this case, any open transactions are canceled.\n\nPending parameter changes exist."
      },
      {
        "date": "2023-09-26T23:34:00.000Z",
        "voteCount": 1,
        "content": "A. Binary logging is enabled, or binary log replication is in progress. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Patching.html\n\n\"In Aurora MySQL version 2.10 and higher and version 3, Aurora can perform a zero-downtime patch whether or not binary log replication is enabled. If binary log replication is enabled, Aurora MySQL automatically drops the connection to the binlog target during a ZDP operation. Aurora MySQL automatically reconnects to the binlog target and resumes replication after the restart finishes.\""
      },
      {
        "date": "2023-01-07T13:47:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. ZDP might not complete successfully under the following conditions:\n\n\nLong-running queries or transactions are in progress. If Aurora can perform ZDP in this case, any open transactions are canceled.\nOpen Secure Socket Layer (SSL) connections exist.\nTemporary tables or table locks are in use, for example while data definition language (DDL) statements run. If Aurora can perform ZDP in this case, any open transactions are canceled.\nPending parameter changes exist.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Patching.html"
      },
      {
        "date": "2023-01-07T13:48:00.000Z",
        "voteCount": 1,
        "content": "lower_case_table_names parameter change need reboot"
      },
      {
        "date": "2022-05-22T06:37:00.000Z",
        "voteCount": 1,
        "content": "if question likes :\nWhich element will have tnothing on ZDP's success.  so we can easliy asnswer ."
      },
      {
        "date": "2022-05-22T06:33:00.000Z",
        "voteCount": 1,
        "content": "MINIMUM influence  os key to answer question"
      },
      {
        "date": "2022-04-30T15:12:00.000Z",
        "voteCount": 3,
        "content": "A. Binary logging is enabled, or binary log replication is in progress. (used to block before MySQL 2.10, no longer affects ZDP )\nx B. Current SSL connections are open to the database. (affects ZDP)\nC. Temporary tables or table locks are in use.(affects ZDP)\nD. The value of the lower_case_table_names server parameter was set to 0 when the tables were created. (unrelated to ZDP)"
      },
      {
        "date": "2021-12-23T20:51:00.000Z",
        "voteCount": 2,
        "content": "I think D has no influence on ZDP's success and B and C has major influence, so, A should have the minimum influence.. Thoughts?"
      },
      {
        "date": "2022-02-13T09:28:00.000Z",
        "voteCount": 5,
        "content": "lower_case_table_names  need to reboot instance"
      },
      {
        "date": "2023-09-30T11:41:00.000Z",
        "voteCount": 1,
        "content": "Good point. But - if it was set to 0 after tables were created, the database would've been rebooted for it to come into effect. Otherwise, nothing would change during the ZDP patch window."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/amazon/view/66035-exam-aws-certified-database-specialty-topic-1-question-181/",
    "body": "A financial services company has an application deployed on AWS that uses an Amazon Aurora PostgreSQL DB cluster. A recent audit showed that no log files contained database administrator activity. A database specialist needs to recommend a solution to provide database access and activity logs. The solution should use the least amount of effort and have a minimal impact on performance.<br>Which solution should the database specialist recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora Database Activity Streams on the database in synchronous mode. Connect the Amazon Kinesis data stream to Kinesis Data Firehose. Set the Kinesis Data Firehose destination to an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudTrail trail in the Region where the database runs. Associate the database activity logs with the trail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Aurora Database Activity Streams on the database in asynchronous mode. Connect the Amazon Kinesis data stream to Kinesis Data Firehose. Set the Firehose destination to an Amazon S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow connections to the DB cluster through a bastion host only. Restrict database access to the bastion host and application servers. Push the bastion host logs to Amazon CloudWatch Logs using the CloudWatch Logs agent."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-25T14:21:00.000Z",
        "voteCount": 1,
        "content": "C. Enable Aurora Database Activity Streams on the database in asynchronous mode.\n\nAsynchronous mode favors database performance so to alleviate performance impact."
      },
      {
        "date": "2022-06-06T23:39:00.000Z",
        "voteCount": 1,
        "content": "If the question states that we are able to have negligible effect on performance, should we use sync mode instead as it prioritize accuracy over performance ? \n\nAsynchronous mode favors database performance over the accuracy of the activity stream.\n\nThe synchronous mode favors the accuracy of the activity stream over database performance.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html#DBActivityStreams.Overview.sync-mode\n\nI believe the answer should be sync mode."
      },
      {
        "date": "2022-04-29T17:40:00.000Z",
        "voteCount": 2,
        "content": "- asynchronous mode because our solution should have negligible effect on performance"
      },
      {
        "date": "2022-02-24T15:17:00.000Z",
        "voteCount": 2,
        "content": "Async for performance"
      },
      {
        "date": "2021-11-27T12:55:00.000Z",
        "voteCount": 4,
        "content": "Answer : C .. Here are the details : https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html"
      },
      {
        "date": "2021-11-14T10:51:00.000Z",
        "voteCount": 2,
        "content": "Option C asynchronous mode because our solution should have negligible effect on performance"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/amazon/view/66371-exam-aws-certified-database-specialty-topic-1-question-182/",
    "body": "A company uses a single-node Amazon RDS for MySQL DB instance for its production database. The DB instance runs in an AWS Region in the United States.<br>A week before a big sales event, a new maintenance update is available for the DB instance. The maintenance update is marked as required. The company wants to minimize downtime for the DB instance and asks a database specialist to make the DB instance highly available until the sales event ends.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefer the maintenance update until the sales event is over.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica with the latest update. Initiate a failover before the sales event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica with the latest update. Transfer all read-only traffic to the read replica during the sales event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the DB instance into a Multi-AZ deployment. Apply the maintenance update.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T12:57:00.000Z",
        "voteCount": 2,
        "content": "Multi-AZ doesn't stop the outage.  B is better however the solution doesn't explain all of the necessary steps e.g. create read replica and making it primary by failing over."
      },
      {
        "date": "2022-11-04T05:55:00.000Z",
        "voteCount": 2,
        "content": "Only one answer provides high availability here."
      },
      {
        "date": "2022-10-15T07:19:00.000Z",
        "voteCount": 3,
        "content": "Key word in the question is  \"to make the DB instance highly available until the sales event ends.\" hence answer is D"
      },
      {
        "date": "2023-04-11T03:54:00.000Z",
        "voteCount": 1,
        "content": "Agree with D.\nMulti-AZ can serve failover while the primary is doing the patch, the standby will be promoted as the new primary."
      },
      {
        "date": "2022-09-14T01:04:00.000Z",
        "voteCount": 2,
        "content": "D is correct because that is the only option that makes the DB highly available. \n\nThe requirement is \"The company wants to minimize downtime for the DB instance and asks a database specialist to make the DB instance highly available until the sales event ends\" \n\nThe read replica needs manual effort to be promoted to primary. Hence B is wrong."
      },
      {
        "date": "2022-07-07T20:04:00.000Z",
        "voteCount": 4,
        "content": "B is the correct answer \n\nHow to minimize downtime using read replicas\nTypically in a self-managed or on-premises environment, a DBA minimizes downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn\u2019t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica, promoting the replica, and then routing traffic to the promoted replica.\n\nOne other caveat about upgrade downtime is how Multi-AZ fits into the picture. One common fallacy is that Multi-AZ configurations prevents downtime during an upgrade. We do recommend that you use Multi-AZ for high availability, because it can prevent extended downtime due to hardware failure or a network outage. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn\u2019t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime."
      },
      {
        "date": "2022-07-16T10:24:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/"
      },
      {
        "date": "2022-07-16T10:25:00.000Z",
        "voteCount": 2,
        "content": "B is correct \nhttps://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/"
      },
      {
        "date": "2022-06-26T10:45:00.000Z",
        "voteCount": 2,
        "content": "Reading through all of the comments and the link for /rds-required-maintenance page, it's obvious that multi az is the solution to \"MINIMIZE\" the down time."
      },
      {
        "date": "2022-05-16T06:09:00.000Z",
        "voteCount": 1,
        "content": "A should be answer. As multi-az mode will update to both db engines."
      },
      {
        "date": "2022-04-29T17:51:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\n\nmulti AZ outage will be the time for failover (60 sec)"
      },
      {
        "date": "2022-04-11T04:35:00.000Z",
        "voteCount": 1,
        "content": "I'll go with B\nUsing a read replica to reduce downtime when upgrading a MySQL database\nsection - Using a read replica to reduce downtime when upgrading a MySQL database"
      },
      {
        "date": "2022-03-04T23:13:00.000Z",
        "voteCount": 2,
        "content": "It has to be D since this wording in the question is key \"the maintenance update has been designated as necessary. \""
      },
      {
        "date": "2022-02-27T09:18:00.000Z",
        "voteCount": 1,
        "content": "question is missing more info\nA is good as you keep an environment that you know is working \nwith D , you can't guaranty problem with the patch and need time to test it"
      },
      {
        "date": "2022-02-16T13:21:00.000Z",
        "voteCount": 1,
        "content": "i go with A,\nthe update not not urgent so they can wait"
      },
      {
        "date": "2022-03-04T23:14:00.000Z",
        "voteCount": 1,
        "content": "the update IS mandatory. Its right there in the question: \"the maintenance update has been designated as necessary.\"  - That necessitates converting the DB to Multi-AZ and since deferment is not possible due to the maint updates mandatory nature"
      },
      {
        "date": "2022-01-28T02:53:00.000Z",
        "voteCount": 1,
        "content": "D. \n\nWith B, the database endpoint needs to be updated in the applications. That requires testing to ensure that the connectivity works."
      },
      {
        "date": "2021-12-27T03:52:00.000Z",
        "voteCount": 1,
        "content": "I prefer B\n1. This is \"database\" required maintenance.\n2. Major DB engine you need both primary and read replica downtime\n3. Miner DB engine upgrade you can upgrade read replica first.\nhttps://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/"
      },
      {
        "date": "2021-12-16T21:32:00.000Z",
        "voteCount": 2,
        "content": "Ans is  D"
      },
      {
        "date": "2021-11-27T13:04:00.000Z",
        "voteCount": 3,
        "content": "This explains : https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/"
      },
      {
        "date": "2021-11-26T08:28:00.000Z",
        "voteCount": 4,
        "content": "I think A.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-downtime-impact/\n \"the DB engine version upgrade happens to both the primary and standby hosts at the same time. Therefore, a DB engine version upgrade doesn't benefit from a Multi-AZ deployment\""
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/amazon/view/65834-exam-aws-certified-database-specialty-topic-1-question-183/",
    "body": "A company is migrating a database in an Amazon RDS for SQL Server DB instance from one AWS Region to another. The company wants to minimize database downtime during the migration.<br>Which strategy should the company choose for this cross-Region migration?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the source database using native backup to an Amazon S3 bucket in the same Region. Then restore the backup in the target Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the source database using native backup to an Amazon S3 bucket in the same Region. Use Amazon S3 Cross-Region Replication to copy the backup to an S3 bucket in the target Region. Then restore the backup in the target Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Database Migration Service (AWS DMS) to replicate data between the source and the target databases. Once the replication is in sync, terminate the DMS task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an RDS for SQL Server cross-Region read replica in the target Region. Once the replication is in sync, promote the read replica to master.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-13T15:27:00.000Z",
        "voteCount": 13,
        "content": "Minimum downtime &gt; AWS DMS : Option C"
      },
      {
        "date": "2022-12-02T08:29:00.000Z",
        "voteCount": 10,
        "content": "Amazon RDS for SQL Server now support Cross Region Read Replica that can be promoted to standalone instance.\nSo Answer D is the best fit ."
      },
      {
        "date": "2023-09-09T00:26:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCC"
      },
      {
        "date": "2023-06-24T11:56:00.000Z",
        "voteCount": 1,
        "content": "C is supported now"
      },
      {
        "date": "2022-12-27T09:19:00.000Z",
        "voteCount": 2,
        "content": "Now Read Replica for RDS SQL Server is supported. Now what? Go with C or D? BTW, does AWS update their exams with new features?"
      },
      {
        "date": "2022-11-21T13:35:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS for SQL Server now support Cross Region Read Replica that can be promoted to standalone instance. \nSo Answer D is the best fit ."
      },
      {
        "date": "2022-09-05T07:49:00.000Z",
        "voteCount": 1,
        "content": "Cross-Region disaster recovery of Amazon RDS for SQL Server - is possible   https://aws.amazon.com/blogs/database/cross-region-disaster-recovery-of-amazon-rds-for-sql-server/.   Why not D?"
      },
      {
        "date": "2022-09-23T13:18:00.000Z",
        "voteCount": 3,
        "content": "From link, \"SQL Server MS Replication is one such feature that, as of this writing, isn\u2019t yet available in Amazon RDS for SQL Server. However, you can to use AWS Database Migration Service (AWS DMS) to do continuous replication\""
      },
      {
        "date": "2023-03-21T13:12:00.000Z",
        "voteCount": 1,
        "content": "Agree with you - C\nAfter reviewing the link."
      },
      {
        "date": "2022-06-21T19:05:00.000Z",
        "voteCount": 1,
        "content": "C will provide minimum down time"
      },
      {
        "date": "2022-04-29T12:03:00.000Z",
        "voteCount": 2,
        "content": "A. slow\nB. slow \nC. source-&gt; AWS DMS -&gt; target region database -&gt; sync -&gt; terminate DMS\nD. cross-Region read replica not supported"
      },
      {
        "date": "2023-05-22T04:43:00.000Z",
        "voteCount": 2,
        "content": "As of Nov 16, 2022 cross-Region read replica IS supported :)\n\nHopefully the exams have been updated to reflect this."
      },
      {
        "date": "2022-03-13T01:02:00.000Z",
        "voteCount": 3,
        "content": "Option C.\n\nThe following aren't supported on Amazon RDS for SQL Server:\n- Creating a read replica in a different AWS Region (a cross-Region read replica)\n- Backup retention of read replicas\n- Point-in-time recovery from read replicas\n- Manual snapshots of read replicas\n- Multi-AZ read replicas\n- Creating read replicas of read replicas\n- Synchronization of user logins to read replicas\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2023-01-07T14:20:00.000Z",
        "voteCount": 1,
        "content": "- Creating a read replica in a different AWS Region (a cross-Region read replica) \nis supported. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2022-02-24T14:06:00.000Z",
        "voteCount": 1,
        "content": "A and B will be slow\nD is not possible yet"
      },
      {
        "date": "2022-02-16T02:39:00.000Z",
        "voteCount": 1,
        "content": "C\nMinimum downtime &gt; AWS DMS"
      },
      {
        "date": "2021-12-11T22:59:00.000Z",
        "voteCount": 1,
        "content": "A - for me, the question is about transferring database from one region to another. Native backup restore is supported for RDS SQL Server using S3."
      },
      {
        "date": "2022-03-04T21:15:00.000Z",
        "voteCount": 2,
        "content": "you missed  the \"minimum downtime\""
      },
      {
        "date": "2021-12-09T08:25:00.000Z",
        "voteCount": 4,
        "content": "C for me \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html\nWith Amazon RDS, you can create a MariaDB, MySQL, Oracle, or PostgreSQL read replica in a different AWS Region from the source DB instance. Creating a cross-Region read replica isn't supported for SQL Server on Amazon RDS."
      },
      {
        "date": "2021-12-04T03:26:00.000Z",
        "voteCount": 2,
        "content": "Option B seems correct from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Procedural.Importing.html, yet there is a requirement that throughout the transfer that the downtime needs to be minimal, therefore, AWS DMS would be the choice to fulfill the requirement. For detail, one can refer to this blog post, https://aws.amazon.com/blogs/database/replicate-and-transform-data-in-amazon-aurora-postgresql-across-multiple-regions-using-aws-dms/, and notice that AWS DMS supports abundant sources and targets."
      },
      {
        "date": "2022-07-21T20:20:00.000Z",
        "voteCount": 1,
        "content": "maybe you mean C"
      },
      {
        "date": "2021-11-11T07:49:00.000Z",
        "voteCount": 4,
        "content": "should be C"
      },
      {
        "date": "2021-11-11T06:42:00.000Z",
        "voteCount": 6,
        "content": "A,B - No minimal downtime\nD- Cross Region replica not supported for RDS SQL Server\nAns : C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/amazon/view/65888-exam-aws-certified-database-specialty-topic-1-question-184/",
    "body": "A financial company is hosting its web application on AWS. The application's database is hosted on Amazon RDS for MySQL with automated backups enabled.<br>The application has caused a logical corruption of the database, which is causing the application to become unresponsive. The specific time of the corruption has been identified, and it was within the backup retention period.<br>How should a database specialist recover the database to the most recent point before corruption?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the point-in-time restore capability to restore the DB instance to the specified time. No changes to the application connection string are required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the point-in-time restore capability to restore the DB instance to the specified time. Change the application connection string to the new, restored DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore using the latest automated backup. Change the application connection string to the new, restored DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore using the appropriate automated backup. No changes to the application connection string are required."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-14T11:00:00.000Z",
        "voteCount": 17,
        "content": "It has to be B \"Please note: When you perform a restore operation to a point in time or from a DB Snapshot, a new DB Instance is created with a new endpoint (the old DB Instance can be deleted if so desired). This is done to enable you to create multiple DB Instances from a specific DB Snapshot or point in time.\""
      },
      {
        "date": "2023-08-30T22:33:00.000Z",
        "voteCount": 1,
        "content": "B. Use the point-in-time restore capability to restore the DB instance to the specified time. Change the application connection string to the new, restored DB instance."
      },
      {
        "date": "2023-04-08T05:47:00.000Z",
        "voteCount": 1,
        "content": "B is right\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html"
      },
      {
        "date": "2023-01-08T04:46:00.000Z",
        "voteCount": 1,
        "content": "B - New Instance Creates, need to change connection string from application.\nA - Not correct. Not inplace PITR, this feature support in Aurora Mysql"
      },
      {
        "date": "2022-04-29T18:36:00.000Z",
        "voteCount": 4,
        "content": "restore operation to a point in time or from a DB Snapshot, a new DB Instance is created"
      },
      {
        "date": "2022-03-07T04:25:00.000Z",
        "voteCount": 1,
        "content": "Got this question in my exam. (i cleared it). B is correct"
      },
      {
        "date": "2022-02-24T15:36:00.000Z",
        "voteCount": 2,
        "content": "Point app to restored instance"
      },
      {
        "date": "2022-01-13T05:31:00.000Z",
        "voteCount": 1,
        "content": "I vote C"
      },
      {
        "date": "2021-12-25T15:54:00.000Z",
        "voteCount": 4,
        "content": "PIT restore requires a new instance to be created. Hence, the answer is B"
      },
      {
        "date": "2021-11-30T07:01:00.000Z",
        "voteCount": 1,
        "content": "B is the correct one"
      },
      {
        "date": "2021-11-12T05:21:00.000Z",
        "voteCount": 2,
        "content": "Option B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/amazon/view/66062-exam-aws-certified-database-specialty-topic-1-question-185/",
    "body": "A database specialist is designing an application to answer one-time queries. The application will query complex customer data and provide reports to end users.<br>These reports can include many fields. The database specialist wants to give users the ability to query the database by using any of the provided fields.<br>The database's traffic volume will be high but variable during peak times. However, the database will not have much traffic at other times during the day.<br>Which solution will meet these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB with provisioned capacity mode and auto scaling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB with on-demand capacity mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora with auto scaling enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora in a serverless mode\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-01T00:52:00.000Z",
        "voteCount": 11,
        "content": "Hard question. As \"database specialist want to enable users to query the database using any of the fields offered\" I think DynamoDB is out of the question. \nSo it leave C and D. As max load is not known in the peak hours, (variables), then then D is the only option. In C, with autoscaling, you would need to define min and max ACUs for the autoscaling."
      },
      {
        "date": "2022-12-25T17:12:00.000Z",
        "voteCount": 1,
        "content": "DynamoDB out of question?  I thought it would qualigy because GSIs are predetermined and flexible.  I'm confused...."
      },
      {
        "date": "2022-03-05T11:34:00.000Z",
        "voteCount": 5,
        "content": "Guys - I see this constant confusion about when to use On Demand vs Provisioned w/ Autoscaling. There's a  HUGE HUGE difference in cost when it comes to On Demand especially for large applications (typically AWS asks about  large applications). It can be  SEVEN times higher. Autoscaling can adjust to almost negligible capacity during off peak hours  to large capacity during peak . Usually the winner here is autoscaling with Provisioned!!\n\nWhile on-demand delivers the best fit for scalability, the cost is approximately seven times higher than provisioned capacity. In addition, provisioned capacity offers the option to purchase reserved capacity, which can save between 40% and 80% compared to non-reserved provisioned capacity. This means on-demand could cost between 15 times and 20 times more than reserved provisioned capacity for some configurations. For small applications, the flexibility of on-demand may be worth the extra cost, but for large applications, it can mean spending hundreds or thousands of dollars more per month"
      },
      {
        "date": "2023-08-30T22:47:00.000Z",
        "voteCount": 1,
        "content": "D. Amazon Aurora in a serverless mode"
      },
      {
        "date": "2022-05-30T13:21:00.000Z",
        "voteCount": 1,
        "content": "Large amounts of client data would require large amounts of RCUs.\nQuery the database using any of the fields offered ins't a good match for key/value storage."
      },
      {
        "date": "2022-04-30T07:22:00.000Z",
        "voteCount": 2,
        "content": "RDS for query on any field\nC. auto scaling can be option if we know more about load\nD. Aurora Serverless for current info"
      },
      {
        "date": "2021-12-02T05:16:00.000Z",
        "voteCount": 4,
        "content": "Option D"
      },
      {
        "date": "2021-12-01T00:02:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items"
      },
      {
        "date": "2021-11-30T23:59:00.000Z",
        "voteCount": 2,
        "content": "I voted for D as well."
      },
      {
        "date": "2021-11-15T02:45:00.000Z",
        "voteCount": 2,
        "content": "Option D."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/amazon/view/79511-exam-aws-certified-database-specialty-topic-1-question-186/",
    "body": "A financial services company runs an on-premises MySQL database for a critical application. The company is dissatisfied with its current database disaster recovery (DR) solution. The application experiences a significant amount of downtime whenever the database fails over to its DR facility. The application also experiences slower response times when reports are processed on the same database. To minimize the downtime in DR situations, the company has decided to migrate the database to AWS. The company requires a solution that is highly available and the most cost-effective.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS for MySQL Multi-AZ DB instance and configure a read replica in a different Availability Zone. Configure the application to reference the replica instance endpoint and report queries to reference the primary DB instance endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS for MySQL Multi-AZ DB instance and configure a read replica in a different Availability Zone. Configure the application to reference the primary DB instance endpoint and report queries to reference the replica instance endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora DB cluster and configure an Aurora Replica in a different Availability Zone. Configure the application to reference the cluster endpoint and report queries to reference the reader endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora DB cluster and configure an Aurora Replica in a different Availability Zone. Configure the application to reference the primary DB instance endpoint and report queries to reference the replica instance endpoint."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-22T05:04:00.000Z",
        "voteCount": 4,
        "content": "What bothers me with A, B and D is that they refer to  \"replica instance endpoint\" either as the application DB endpoint, or as a report query endpoint, instead of a \"reader endpoint\". \n\nUsing a \"replica instance endpoint\" defeats the failover mechanism, so that will never make sense."
      },
      {
        "date": "2023-05-14T03:09:00.000Z",
        "voteCount": 2,
        "content": "C is correct. Aurora is 20% more expensive than RDS per instance. However, if you want RDS to have high availability and even read replicas, you need 3 instances and Aurora only needs 2."
      },
      {
        "date": "2023-04-23T19:42:00.000Z",
        "voteCount": 1,
        "content": "I would go for B as the MOST cost effective is the keyword. It also satisfies the condition of Highly Available"
      },
      {
        "date": "2023-04-08T06:27:00.000Z",
        "voteCount": 1,
        "content": "I think B for cost-effective solution"
      },
      {
        "date": "2022-12-25T17:30:00.000Z",
        "voteCount": 1,
        "content": "Aurora is 20% more expensive."
      },
      {
        "date": "2022-12-19T22:12:00.000Z",
        "voteCount": 1,
        "content": "Keyword: significant amount of downtime, \n\nFailover\n\nIn RDS, Failover to read replica is done manually, which could lead to data loss. You can use Multi-AZ (Standby instance) feature for automatic failover, and to prevent downtime and data loss.\n\nIn Aurora, Failover to read replica is done automatically to prevent data loss. Failover time is faster on Aurora."
      },
      {
        "date": "2022-12-17T22:43:00.000Z",
        "voteCount": 2,
        "content": "It's B. looking for cost-effective approach.\nAurora instances will cost you ~20% more than RDS MySQL. If you create Aurora read replicas then the cost of your Aurora cluster will double. Aurora is only available on certain RDS instance sizes."
      },
      {
        "date": "2022-12-04T14:12:00.000Z",
        "voteCount": 1,
        "content": "The company requires a solution that is highly available and the most cost-effective.\nKeyword is \"highly available\"\nMy answer is B"
      },
      {
        "date": "2022-09-05T08:06:00.000Z",
        "voteCount": 2,
        "content": "reader endpoint - is the keyword"
      },
      {
        "date": "2022-09-03T04:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-14T01:11:00.000Z",
        "voteCount": 1,
        "content": "Changing to C as B will need 3 instances whereas is aurora can do the same with 2 instances."
      },
      {
        "date": "2022-09-02T11:29:00.000Z",
        "voteCount": 1,
        "content": "Seems that it's C. https://aws.amazon.com/about-aws/whats-new/2016/09/reader-end-point-for-amazon-aurora/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/amazon/view/79512-exam-aws-certified-database-specialty-topic-1-question-187/",
    "body": "A company with 500,000 employees needs to supply its employee list to an application used by human resources. Every 30 minutes, the data is exported using the LDAP service to load into a new Amazon DynamoDB table. The data model has a base table with Employee ID for the partition key and a global secondary index with Organization ID as the partition key.<br>While importing the data, a database specialist receives ProvisionedThroughputExceededException errors. After increasing the provisioned write capacity units<br>(WCUs) to 50,000, the specialist receives the same errors. Amazon CloudWatch metrics show a consumption of 1,500 WCUs.<br>What should the database specialist do to address the issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the data model to avoid hot partitions in the global secondary index.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable auto scaling for the table to automatically increase write capacity during bulk imports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the table to use on-demand capacity instead of provisioned capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of retries on the bulk loading application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T11:33:00.000Z",
        "voteCount": 6,
        "content": "Answer A - hot partition. https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/"
      },
      {
        "date": "2023-05-22T05:34:00.000Z",
        "voteCount": 1,
        "content": "Given the CloudWatch metric, the error cannot be due to write capacity, so any answer consisting in adjusting write capacity has to be discarded."
      },
      {
        "date": "2023-01-18T13:23:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/31468379/how-to-solve-throughput-error-for-dynamodb\nhttps://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/ProvisionedThroughputExceededException.html"
      },
      {
        "date": "2022-09-25T11:11:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-09-14T01:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/amazon/view/79663-exam-aws-certified-database-specialty-topic-1-question-188/",
    "body": "A company has an application that uses an Amazon DynamoDB table as its data store. During normal business days, the throughput requirements from the application are uniform and consist of 5 standard write calls per second to the DynamoDB table. Each write call has 2 KB of data.<br>For 1 hour each day, the company runs an additional automated job on the DynamoDB table that makes 20 write requests per second. No other application writes to the DynamoDB table. The DynamoDB table does not have to meet any additional capacity requirements.<br>How should a database specialist configure the DynamoDB table's capacity to meet these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB provisioned capacity with 5 WCUs and auto scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB provisioned capacity with 5 WCUs and a write-through cache that DynamoDB Accelerator (DAX) provides.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB provisioned capacity with 10 WCUs and auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB provisioned capacity with 10 WCUs and no auto scaling."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T21:08:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nIt's C. 5x2KB = 10 WCU + auto scaling needed as well for peak time."
      },
      {
        "date": "2022-12-27T03:26:00.000Z",
        "voteCount": 1,
        "content": "C is the answer based on this doc.\n\nIf the batch occurs at scheduled times, you can schedule an increase to your auto- scaling capacity before it runs\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CostOptimization_TableCapacityMode.html"
      },
      {
        "date": "2022-09-03T04:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/amazon/view/79668-exam-aws-certified-database-specialty-topic-1-question-189/",
    "body": "A company wants to build a new invoicing service for its cloud-native application on AWS. The company has a small development team and wants to focus on service feature development and minimize operations and maintenance as much as possible. The company expects the service to handle billions of requests and millions of new records every day. The service feature requirements, including data access patterns are well-defined. The service has an availability target of<br>99.99% with a milliseconds latency requirement. The database for the service will be the system of record for invoicing data.<br>Which database solution meets these requirements at the LOWEST cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Neptune",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora PostgreSQL Serverless",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for PostgreSQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-27T00:58:00.000Z",
        "voteCount": 1,
        "content": "B. Amazon Aurora PostgreSQL Serverless\n\n-- SLA 99.99% for Aurora,   https://aws.amazon.com/rds/aurora/sla/  \n while RDS with only 99.95%,  https://aws.amazon.com/rds/sla/\n\n-- invoicing data, this has to be relational database with SQL query capability. DynamoDB being key-value NoSQL service does not fit for this.\n\n-- minimize operations and maintenance as much as possible, Serverless could be a good fit.\n\n-- cost, RDS is cheaper than Aurora, however RDS with SLA 99.95% without meeting the availability requirement.\n\nTherefore  B. Amazon Aurora PostgreSQL Serverless"
      },
      {
        "date": "2023-10-07T16:41:00.000Z",
        "voteCount": 1,
        "content": "switching to D. Amazon DynamoDB. \n\nfrom below user case DynamoDB as a data store for an online shop which includes invoicing data.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-online-shop.html"
      },
      {
        "date": "2023-08-05T01:01:00.000Z",
        "voteCount": 2,
        "content": "Because the SLA is 99.99"
      },
      {
        "date": "2023-10-01T08:46:00.000Z",
        "voteCount": 1,
        "content": "The SLA of Aurora is 99.99%\nhttps://aws.amazon.com/rds/aurora/sla/"
      },
      {
        "date": "2023-04-23T17:42:00.000Z",
        "voteCount": 3,
        "content": "As Invoicing will mostly use relational data and not NOSQL, so I wont go with Dynamodb. \nAurora serverless is less costly and provides millisecond latency, availability of 99.99%. Capacity to process billions of records."
      },
      {
        "date": "2023-03-25T17:52:00.000Z",
        "voteCount": 2,
        "content": "Clearly DynamoDB because of millisecond latency and 99.99 availability SLA. RDS maximum SLA is between 99 to 99.5 only whereas DynamoDB could go between 99.99 to 99.999\nhttps://aws.amazon.com/about-aws/whats-new/2018/06/amazon-dynamodb-announces-a-monthly-service-level-agreement/\n\nhttps://aws.amazon.com/rds/sla/"
      },
      {
        "date": "2023-02-13T06:53:00.000Z",
        "voteCount": 1,
        "content": "It's asking for lower cost, billions of requests and millions of new records every day has a huge cost in DynamoDB. I'll go for C, PostgreSQL on RDS"
      },
      {
        "date": "2022-09-25T11:13:00.000Z",
        "voteCount": 1,
        "content": "Known patterns, minimum maintenance, miliseconds latency"
      },
      {
        "date": "2022-09-11T01:44:00.000Z",
        "voteCount": 3,
        "content": "invoice system requires ACID ?"
      },
      {
        "date": "2022-09-03T04:23:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-09-02T21:37:00.000Z",
        "voteCount": 4,
        "content": "Known patterns, minimum maintenance, miliseconds latency - vote for DynamoDB."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/amazon/view/79669-exam-aws-certified-database-specialty-topic-1-question-190/",
    "body": "Application developers have reported that an application is running slower as more users are added. The application database is running on an Amazon Aurora<br>DB cluster with an Aurora Replica. The application is written to take advantage of read scaling through reader endpoints. A database specialist looks at the performance metrics of the database and determines that, as new users were added to the database, the primary instance CPU utilization steadily increased while the Aurora Replica CPU utilization remained steady.<br>How can the database specialist improve database performance while ensuring minimal downtime?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Aurora DB cluster to add more replicas until the overall load stabilizes. Then, reduce the number of replicas once the application meets service level objectives.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the primary instance to a larger instance size that offers more CPU capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify a replica to a larger instance size that has more CPU capacity. Then, promote the modified replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the Aurora DB cluster to one that has an instance size with more CPU capacity. Then, swap the names of the old and new DB clusters."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T21:41:00.000Z",
        "voteCount": 5,
        "content": "Minimal downtime - operations first on replica and then promotion - Answer C."
      },
      {
        "date": "2022-09-14T01:15:00.000Z",
        "voteCount": 1,
        "content": "Promotion of the  read replica will cause downtime, there will be no downtime with option D."
      },
      {
        "date": "2022-10-15T08:15:00.000Z",
        "voteCount": 2,
        "content": "D will result in data loss. C is correct"
      },
      {
        "date": "2022-09-30T12:06:00.000Z",
        "voteCount": 3,
        "content": "True, but as soon as you \"restore\" something, that is old data.  How can you flip to old data (i.e. your backup) while the system is still up and running?  Aren't you potentially losing transactions?"
      },
      {
        "date": "2023-10-01T10:33:00.000Z",
        "voteCount": 1,
        "content": "A+D appear to be distractors,\nSo there's an agreement that the DB instance class must be vertically up-scaled.\nThe difference between B+C is that B applies it directlty to the Primary Instance and C to a Reader (replica) instance first.\nI believe that C is the wa to do it.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.InstanceScaling"
      },
      {
        "date": "2023-05-22T05:56:00.000Z",
        "voteCount": 1,
        "content": "The thing here is that it is not stated whether we are looking at a Multi-AZ deployment. If that were to be the case, I understand that upgrading the type of the primary instance would cause very little downtime - just the time it would take to failover to the (upgraded) standby instance."
      },
      {
        "date": "2023-04-23T17:29:00.000Z",
        "voteCount": 1,
        "content": "You can also modify DB instances in a DB cluster to accomplish tasks such as changing its DB instance class \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Modifying.html#Aurora.Modifying.Instance\nHence, C can be apt for minimum downtime."
      },
      {
        "date": "2023-03-19T16:10:00.000Z",
        "voteCount": 1,
        "content": "Answer from ChatGPT4 \n\n\nBased on the given scenario, the primary instance's CPU utilization is increasing, while the Aurora Replica CPU utilization remains steady. This indicates that the write workload has increased, whereas read queries are already well-distributed using the reader endpoint. To improve the database performance with minimal downtime, you can implement the following steps:\n1. Scale the primary instance vertically by increasing its instance size:\n   - Identify the appropriate Amazon Aurora DB instance size that can handle the increased workload and CPU utilization.\n   - Modify the primary instance within the Aurora DB cluster to the new instance size. The failover process will cause a short downtime during the upgrade.\n\n\nThe short downtime will smaller than the replica"
      },
      {
        "date": "2023-03-25T21:37:00.000Z",
        "voteCount": 2,
        "content": "Wrong answer \n\nIt will cause downtime:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Modifying.html#:~:text=specified%20DB%20instance-,An%20outage%20occurs%20during%20this%20change.,-DB%20instance%20identifier"
      },
      {
        "date": "2023-04-11T02:11:00.000Z",
        "voteCount": 1,
        "content": "Please be very careful using ChatGPT for AWS, it is very helpful, but it is not always correct.\nit could not have deep comprehension.\nThe correct answer is C"
      },
      {
        "date": "2023-12-10T10:22:00.000Z",
        "voteCount": 1,
        "content": "Remember, garbage in garbage out. ChatGPT is the future but we are not there yet.  When required to associate facts and reasoning by itself, it behaves like a high schooler.  Keep talking never say I don't know.   C is the better answer."
      },
      {
        "date": "2022-12-25T19:14:00.000Z",
        "voteCount": 3,
        "content": "The question reports that the CPU issue is on the primary instance and not on the RR."
      },
      {
        "date": "2022-11-24T08:16:00.000Z",
        "voteCount": 2,
        "content": "C is wrong , You can't \"Modify\" to a large instance,  You have to drop exist one then re-create a newer replica . so D is better than C"
      },
      {
        "date": "2023-04-17T15:25:00.000Z",
        "voteCount": 1,
        "content": "It is true. For gaining high cpu, you have to recreate instance , no way to \u201cmodify\u201d"
      },
      {
        "date": "2022-10-15T08:13:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect due to the possible higher RTO and RPO(higher Downtime). C is correct"
      },
      {
        "date": "2022-09-30T12:10:00.000Z",
        "voteCount": 1,
        "content": "C.  There is minimal downtime with C, but NO downtime with D.  The only issue is that D potentially up to date with the latest transactions (i.e. you used a backup)."
      },
      {
        "date": "2022-09-30T12:11:00.000Z",
        "voteCount": 1,
        "content": "The only issue is that D potentially NOT up to date with the latest transactions (i.e. you used a backup)."
      },
      {
        "date": "2022-09-03T04:24:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/amazon/view/79691-exam-aws-certified-database-specialty-topic-1-question-191/",
    "body": "A company's development team needs to have production data restored in a staging AWS account. The production database is running on an Amazon RDS for<br>PostgreSQL Multi-AZ DB instance, which has AWS KMS encryption enabled using the default KMS key. A database specialist planned to share the most recent automated snapshot with the staging account, but discovered that the option to share snapshots is disabled in the AWS Management Console.<br>What should the database specialist do to resolve this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable automated backups in the DB instance. Share both the automated snapshot and the default KMS key with the staging account. Restore the snapshot in the staging account and enable automated backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the automated snapshot specifying a custom KMS encryption key. Share both the copied snapshot and the custom KMS encryption key with the staging account. Restore the snapshot to the staging account within the same Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance to use a custom KMS encryption key. Share both the automated snapshot and the custom KMS encryption key with the staging account. Restore the snapshot in the staging account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the automated snapshot while keeping the default KMS key. Share both the snapshot and the default KMS key with the staging account. Restore the snapshot in the staging account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T00:31:00.000Z",
        "voteCount": 8,
        "content": "Agree with B - https://aws.amazon.com/premiumsupport/knowledge-center/rds-snapshots-share-account/"
      },
      {
        "date": "2022-09-25T11:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-03T04:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/amazon/view/79692-exam-aws-certified-database-specialty-topic-1-question-192/",
    "body": "A software-as-a-service (SaaS) company is using an Amazon Aurora Serverless DB cluster for its production MySQL database. The DB cluster has general logs and slow query logs enabled. A database engineer must use the most operationally efficient solution with minimal resource utilization to retain the logs and facilitate interactive search and analysis.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function to ship database logs to an Amazon S3 bucket. Use Amazon Athena and Amazon QuickSight to search and analyze the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the logs from the DB cluster and store them in Amazon S3 by using manual scripts. Use Amazon Athena and Amazon QuickSight to search and analyze the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function to ship database logs to an Amazon S3 bucket. Use Amazon Elasticsearch Service (Amazon ES) and Kibana to search and analyze the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch Logs Insights to search and analyze the logs when the logs are automatically uploaded by the DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T06:50:00.000Z",
        "voteCount": 5,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/aurora-serverless-logs-enable-view/"
      },
      {
        "date": "2023-10-06T12:57:00.000Z",
        "voteCount": 2,
        "content": "A, B , C are trap as you don't need manual scripts to publish the logs to S3 in RDS or Aurora, it can be done by settings and options"
      },
      {
        "date": "2022-09-03T04:29:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-03T00:34:00.000Z",
        "voteCount": 4,
        "content": "Minimal resource utilization - I'd go with D - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/amazon/view/79693-exam-aws-certified-database-specialty-topic-1-question-193/",
    "body": "A retail company uses Amazon Redshift Spectrum to run complex analytical queries on objects that are stored in an Amazon S3 bucket. The objects are joined with multiple dimension tables that are stored in an Amazon Redshift database. The company uses the database to create monthly and quarterly aggregated reports. Users who attempt to run queries are reporting the following error message: error: Spectrum Scan Error: Access throttled<br>Which solution will resolve this error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck file sizes of fact tables in Amazon S3, and look for large files. Break up large files into smaller files of equal size between 100 MB and 1 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of queries that users can run in parallel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck file sizes of fact tables in Amazon S3, and look for small files. Merge the small files into larger files of at least 64 MB in size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview and optimize queries that submit a large aggregation step to Redshift Spectrum."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T00:37:00.000Z",
        "voteCount": 8,
        "content": "C for sure according to the link - https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html"
      },
      {
        "date": "2024-02-02T09:15:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html"
      },
      {
        "date": "2024-01-02T06:28:00.000Z",
        "voteCount": 1,
        "content": "If your Redshift Spectrum requests frequently get throttled by Amazon S3, reduce the number of Amazon S3 GET/HEAD requests that Redshift Spectrum makes to Amazon S3. To do this, try merging small files into larger files. We recommend using file sizes of 64 MB or larger.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html"
      },
      {
        "date": "2023-02-25T01:20:00.000Z",
        "voteCount": 1,
        "content": "Option D suggests reviewing and optimizing the queries to submit a large aggregation step to Redshift Spectrum. This is the most appropriate solution to resolve the error, as it directly addresses the root cause of the issue. By optimizing the queries, it is possible to reduce the amount of data scanned by Redshift Spectrum, which will reduce the number of scan requests and prevent throttling."
      },
      {
        "date": "2022-10-28T11:05:00.000Z",
        "voteCount": 1,
        "content": "C.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html#spectrum-troubleshooting-access-throttled"
      },
      {
        "date": "2022-09-30T12:17:00.000Z",
        "voteCount": 4,
        "content": "C.  Both S3 and AWS KMS can throttle..the fix is always the same...larger/less files.\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html"
      },
      {
        "date": "2022-09-26T06:53:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html#spectrum-troubleshooting-access-throttled"
      },
      {
        "date": "2022-09-06T07:28:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-09-03T04:30:00.000Z",
        "voteCount": 1,
        "content": "A is correct \nhttps://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-troubleshooting.html"
      },
      {
        "date": "2022-09-14T01:22:00.000Z",
        "voteCount": 3,
        "content": "Sorry, the documentation points to C."
      },
      {
        "date": "2023-10-01T10:56:00.000Z",
        "voteCount": 1,
        "content": "Yes, \"To do this, try merging small files into larger files. We recommend using file sizes of 64 MB or larger.\" --&gt; C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/amazon/view/79712-exam-aws-certified-database-specialty-topic-1-question-194/",
    "body": "A company's applications store data in Amazon Aurora MySQL DB clusters. The company has separate AWS accounts for its production, test, and development environments. To test new functionality in the test environment, the company's development team requires a copy of the production database four times a day.<br>Which solution meets this requirement with the MOST operational efficiency?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a manual snapshot in the production account. Share the snapshot with the test account. Restore the database from the snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a manual snapshot in the production account. Export the snapshot to Amazon S3. Copy the snapshot to an S3 bucket in the test account. Restore the database from the snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the Aurora DB cluster with the test account. Create a snapshot of the production database in the test account. Restore the database from the snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the Aurora DB cluster with the test account. Create a clone of the production database in the test account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-30T14:17:00.000Z",
        "voteCount": 2,
        "content": "D. Share the Aurora DB cluster with the test account. Create a clone of the production database in the test account."
      },
      {
        "date": "2023-02-25T01:23:00.000Z",
        "voteCount": 1,
        "content": "This solution provides the most operational efficiency because it eliminates the need to take and share manual snapshots or export them to S3. By sharing the Aurora DB cluster with the test account, the development team can create a clone of the production database in their environment whenever they need it. This can be done quickly and easily, without any interruption to the production database. Additionally, the clone can be used for testing new functionality without affecting the production environment."
      },
      {
        "date": "2022-09-26T07:09:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Managing.Clone.Cross-Account"
      },
      {
        "date": "2022-09-03T04:31:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-03T01:23:00.000Z",
        "voteCount": 2,
        "content": "It's D. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Managing.Clone.Cross-Account"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/amazon/view/79715-exam-aws-certified-database-specialty-topic-1-question-195/",
    "body": "An application reads and writes data to an Amazon RDS for MySQL DB instance. A new reporting dashboard needs read-only access to the database. When the application and reports are both under heavy load, the database experiences performance degradation. A database specialist needs to improve the database performance.<br>What should the database specialist do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the DB instance. Configure the reports to connect to the replication instance endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the DB instance. Configure the application and reports to connect to the cluster endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Multi-AZ deployment. Configure the reports to connect to the standby replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Multi-AZ deployment. Configure the application and reports to connect to the cluster endpoint."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T01:28:00.000Z",
        "voteCount": 8,
        "content": "Take off only reports from the primary to read replica. It's A. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
      },
      {
        "date": "2023-02-25T01:29:00.000Z",
        "voteCount": 1,
        "content": "Create a read replica of the DB instance. Configure the application and reports to connect to the cluster endpoint.\n\nBy creating a read replica of the DB instance and directing the reporting dashboard to connect to the replica, the application can continue to read and write data to the primary DB instance without impacting the performance of the reporting dashboard. The read replica can handle the additional read-only load from the reporting dashboard, improving overall performance. By configuring the application and reports to connect to the cluster endpoint, the load can be distributed across the primary DB instance and the read replica. This approach provides a scalable and efficient solution for improving database performance while maintaining read-only access to the reporting dashboard."
      },
      {
        "date": "2023-04-17T14:12:00.000Z",
        "voteCount": 1,
        "content": "The reason not to choose B is \u2014 both application and reports to connect to the cluster endpoint \u2014 nothing can distribute the workload to the new replica. \nThus, to choose A to distribute the workload"
      },
      {
        "date": "2022-10-19T23:08:00.000Z",
        "voteCount": 1,
        "content": "Why not C - With Multi-AZ DB cluster we should be able to achieve this??"
      },
      {
        "date": "2022-11-10T08:18:00.000Z",
        "voteCount": 1,
        "content": "It's a Standby Replica, not a Read replica, So it should be A"
      },
      {
        "date": "2023-10-01T11:15:00.000Z",
        "voteCount": 1,
        "content": "Multi-AZ serves for HA, not for performance improvement. The latter is asked here --&gt; A."
      },
      {
        "date": "2023-09-19T11:47:00.000Z",
        "voteCount": 1,
        "content": "standby replica configured for high availability in a Multi-AZ deployment. Replication with the standby replica is synchronous. Unlike a read replica, a standby replica can't serve read traffic."
      },
      {
        "date": "2022-09-26T07:11:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-09-03T04:32:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/amazon/view/79720-exam-aws-certified-database-specialty-topic-1-question-196/",
    "body": "A company is loading sensitive data into an Amazon Aurora MySQL database. To meet compliance requirements, the company needs to enable audit logging on the Aurora MySQL DB cluster to audit database activity. This logging will include events such as connections, disconnections, queries, and tables queried. The company also needs to publish the DB logs to Amazon CloudWatch to perform real-time data analysis.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the default option group parameters to enable Advanced Auditing. Restart the database for the changes to take effect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom DB cluster parameter group. Modify the parameters for Advanced Auditing. Modify the cluster to associate the new custom DB parameter group with the Aurora MySQL DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the database. Create a new DB instance, and enable custom auditing and logging to CloudWatch. Deactivate the DB instance that has no logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail for the DB instance. Create a filter that provides only connections, disconnections, queries, and tables queried."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T02:05:00.000Z",
        "voteCount": 9,
        "content": "Vote for B. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html"
      },
      {
        "date": "2023-10-01T11:22:00.000Z",
        "voteCount": 2,
        "content": "There's nothing like an 'option' group. There's a default parameter group, and that cannot directly be modified. However, you can create a new custom parameter group and change the settings in there."
      },
      {
        "date": "2022-09-26T07:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html"
      },
      {
        "date": "2022-09-03T04:34:00.000Z",
        "voteCount": 3,
        "content": "B is correct, custom parameter group"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/amazon/view/79856-exam-aws-certified-database-specialty-topic-1-question-197/",
    "body": "A company has an on-premises production Microsoft SQL Server with 250 GB of data in one database. A database specialist needs to migrate this on-premises<br>SQL Server to Amazon RDS for SQL Server. The nightly native SQL Server backup file is approximately 120 GB in size. The application can be down for an extended period of time to complete the migration. Connectivity between the on-premises environment and AWS can be initiated from on-premises only.<br>How can the database be migrated from on-premises to Amazon RDS with the LEAST amount of effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the SQL Server database using a native SQL Server backup. Upload the backup files to Amazon S3. Download the backup files on an Amazon EC2 instance and restore them from the EC2 instance into the new production RDS instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the SQL Server database using a native SQL Server backup. Upload the backup files to Amazon S3. Restore the backup files from the S3 bucket into the new production RDS instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision and configure AWS DMS. Set up replication between the on-premises SQL Server environment to replicate the database to the new production RDS instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the SQL Server database using AWS Backup. Once the backup is complete, restore the completed backup to an Amazon EC2 instance and move it to the new production RDS instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-14T01:26:00.000Z",
        "voteCount": 6,
        "content": "B is correct since DMS requires connectivity from AWS to on-premise, which is not allowed."
      },
      {
        "date": "2024-01-15T08:06:00.000Z",
        "voteCount": 1,
        "content": "B is the least effort."
      },
      {
        "date": "2023-05-26T13:05:00.000Z",
        "voteCount": 1,
        "content": "b is the most convenient"
      },
      {
        "date": "2022-09-03T10:55:00.000Z",
        "voteCount": 2,
        "content": "Connectivity only possibile from on-premise, so it's B over C - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Procedural.Importing.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/amazon/view/79857-exam-aws-certified-database-specialty-topic-1-question-198/",
    "body": "A database specialist needs to delete user data and sensor data 1 year after it was loaded in an Amazon DynamoDB table. TTL is enabled on one of the attributes. The database specialist monitors TTL rates on the Amazon CloudWatch metrics for the table and observes that items are not being deleted as expected.<br>What is the MOST likely reason that the items are not being deleted?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe TTL attribute's value is set as a Number data type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe TTL attribute's value is set as a Binary data type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe TTL attribute's value is a timestamp in the Unix epoch time format in seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe TTL attribute's value is set with an expiration of 1 year."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T05:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-before-you-start.html\nThe TTL attribute\u2019s value must be a top-level Number data type.\nThe TTL attribute\u2019s value must be a timestamp in Unix epoch time format in seconds. \nThe TTL attribute value must be a datetimestamp with an expiration of no more than five years in the past.\n\nTherefore only B is not a valid condition."
      },
      {
        "date": "2022-09-26T07:28:00.000Z",
        "voteCount": 2,
        "content": "only B is wrong. A, C and D are correct conditions for TTL to work."
      },
      {
        "date": "2022-09-06T15:03:00.000Z",
        "voteCount": 1,
        "content": "A, C AND D are correct values for the deletion to work. So, only possibility for the deletion not to work is B. So, the correct answer is B"
      },
      {
        "date": "2022-09-03T10:57:00.000Z",
        "voteCount": 2,
        "content": "Seems that ther is wrong attribute type i.e. Binarny. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html#time-to-live-ttl-before-you-start-formatting"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/amazon/view/79865-exam-aws-certified-database-specialty-topic-1-question-199/",
    "body": "A company has deployed an application that uses an Amazon RDS for MySQL DB cluster. The DB cluster uses three read replicas. The primary DB instance is an<br>8XL-sized instance, and the read replicas are each XL-sized instances.<br>Users report that database queries are returning stale data. The replication lag indicates that the replicas are 5 minutes behind the primary DB instance. Status queries on the replicas show that the SQL_THREAD is 10 binlogs behind the IO_THREAD and that the IO_THREAD is 1 binlog behind the primary.<br>Which changes will reduce the lag? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy two additional read replicas matching the existing replica DB instance size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the primary DB instance to an Amazon Aurora MySQL DB cluster and add three Aurora Replicas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the read replicas to the same Availability Zone as the primary DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance size of the primary DB instance within the same instance class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance size of the read replicas to the same size and class as the primary DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T11:13:00.000Z",
        "voteCount": 9,
        "content": "Read replicas should be of the same size and class as primary DB. https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-high-replica-lag/\n\nAurora seems to be better in handling the lags - https://www.quora.com/What-is-the-difference-between-a-RDS-read-replica-and-an-Amazon-Aurora-Read-replica"
      },
      {
        "date": "2023-04-11T19:37:00.000Z",
        "voteCount": 2,
        "content": "CE.\nC - the reason to choose C is to reduce the network latency, the binlog lap is caused by network latency, replica server CPU too low .... etc.\nNot to choose B -- because Multi-AZ is for high availability, not for reducing the binlog lap."
      },
      {
        "date": "2023-05-01T05:09:00.000Z",
        "voteCount": 1,
        "content": "The answers are B and E. In the problem, the SQL Thread is the problem rather than the IO Thread. Therefore, application rather than transmission is a problem, so C is not the answer. And in Aurora, you can read directly from the replica."
      },
      {
        "date": "2023-02-14T12:42:00.000Z",
        "voteCount": 2,
        "content": "BE the answer my friend!"
      },
      {
        "date": "2022-09-26T07:35:00.000Z",
        "voteCount": 2,
        "content": "BE seems to be answer."
      },
      {
        "date": "2022-09-26T07:35:00.000Z",
        "voteCount": 2,
        "content": "BE seems to be answer."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/amazon/view/79871-exam-aws-certified-database-specialty-topic-1-question-200/",
    "body": "A company is using Amazon Aurora MySQL as the database for its retail application on AWS. The company receives a notification of a pending database upgrade and wants to ensure upgrades do not occur before or during the most critical time of year. Company leadership is concerned that an Amazon RDS maintenance window will cause an outage during data ingestion.<br>Which step can be taken to ensure that the application is not interrupted?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable weekly maintenance on the DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone the DB cluster and migrate it to a new copy of the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose to defer the upgrade and then find an appropriate down time for patching.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Aurora Replica and promote it to primary at the time of patching."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-21T14:49:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/performing-major-version-upgrades-for-amazon-aurora-mysql-with-minimum-downtime/"
      },
      {
        "date": "2023-08-19T16:22:00.000Z",
        "voteCount": 3,
        "content": "C. Choose to defer the upgrade and then find an appropriate down time for patching.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#USER_UpgradeDBInstance.Maintenance.Viewing"
      },
      {
        "date": "2022-12-25T19:47:00.000Z",
        "voteCount": 1,
        "content": "We are able to defer to a later date"
      },
      {
        "date": "2022-12-18T04:06:00.000Z",
        "voteCount": 1,
        "content": "The ans is C. Defer.\nMain ask is:\n\nWhich step can be taken to ensure that the application is not interrupted?\n\nCloning have downtime. \nHowever, for the Aurora MySQL major version upgrade procedure with minimum database downtime, you can use database clone and in-place upgrade process (using Aurora MySQL 1.22.3 or higher version). The following diagram illustrates this updated architecture."
      },
      {
        "date": "2022-12-07T22:41:00.000Z",
        "voteCount": 1,
        "content": "Based on doc, B is correct."
      },
      {
        "date": "2022-11-13T06:43:00.000Z",
        "voteCount": 1,
        "content": "Change to C, clone will cause outage"
      },
      {
        "date": "2022-11-13T06:42:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2022-10-30T15:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is B baksed on mbar94 link"
      },
      {
        "date": "2022-09-26T09:50:00.000Z",
        "voteCount": 3,
        "content": "Defer upgrade if solution. Clone will be overkill for this use case."
      },
      {
        "date": "2022-09-22T11:52:00.000Z",
        "voteCount": 1,
        "content": "I also think it's c"
      },
      {
        "date": "2022-09-11T19:29:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C . Clone is for test not a solution for outage."
      },
      {
        "date": "2022-09-03T11:31:00.000Z",
        "voteCount": 4,
        "content": "Agree with B - https://aws.amazon.com/blogs/database/performing-major-version-upgrades-for-amazon-aurora-mysql-with-minimum-downtime/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/amazon/view/79881-exam-aws-certified-database-specialty-topic-1-question-201/",
    "body": "An ecommerce company uses Amazon DynamoDB as the backend for its payments system. A new regulation requires the company to log all data access requests for financial audits. For this purpose, the company plans to use AWS logging and save logs to Amazon S3<br>How can a database specialist activate logging on the database?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail to monitor DynamoDB control-plane operations. Create a DynamoDB stream to monitor data-plane operations. Pass the stream to Amazon Kinesis Data Streams. Use that stream as a source for Amazon Kinesis Data Firehose to store the data in an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail to monitor DynamoDB data-plane operations. Create a DynamoDB stream to monitor control-plane operations. Pass the stream to Amazon Kinesis Data Streams. Use that stream as a source for Amazon Kinesis Data Firehose to store the data in an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two trails in AWS CloudTrail. Use Trail1 to monitor DynamoDB control-plane operations. Use Trail2 to monitor DynamoDB data-plane operations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail to monitor DynamoDB data-plane and control-plane operations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T12:05:00.000Z",
        "voteCount": 10,
        "content": "It's D - https://aws.amazon.com/about-aws/whats-new/2021/04/you-now-can-use-aws-cloudtrail-to-log-amazon-dynamodb-streams-da/.\n\nNo need of separator trails as in C."
      },
      {
        "date": "2023-10-01T11:47:00.000Z",
        "voteCount": 1,
        "content": "If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for DynamoDB. \nThat can include events from the data and the control plane.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html"
      },
      {
        "date": "2023-01-08T05:45:00.000Z",
        "voteCount": 1,
        "content": "what's the difference between C and D? if you have a trail setup then still you can see the logs but only recent history but that is not the requirements. so setup a trail to push the data to CloudWatch. \nAs per parle101 link"
      },
      {
        "date": "2022-12-27T08:58:00.000Z",
        "voteCount": 2,
        "content": "Ans should be D as per this\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html"
      },
      {
        "date": "2022-12-27T08:53:00.000Z",
        "voteCount": 1,
        "content": "The logs should be saved to S3. C and D do not meet that requirement"
      },
      {
        "date": "2023-04-14T03:08:00.000Z",
        "voteCount": 1,
        "content": "D\nCloudTrail -- default to collect logs in S3."
      },
      {
        "date": "2023-04-05T09:46:00.000Z",
        "voteCount": 1,
        "content": "If you think this you should be studying for this cert... Or even any Associate cert for that matter..."
      },
      {
        "date": "2022-10-20T19:15:00.000Z",
        "voteCount": 1,
        "content": "Separating two trails (With option C) is also right answer???"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/amazon/view/79883-exam-aws-certified-database-specialty-topic-1-question-202/",
    "body": "A vehicle insurance company needs to choose a highly available database to track vehicle owners and their insurance details. The persisted data should be immutable in the database, including the complete and sequenced history of changes over time with all the owners and insurance transfer details for a vehicle.<br>The data should be easily verifiable for the data lineage of an insurance claim.<br>Which approach meets these requirements with MINIMAL effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a blockchain to store the insurance details. Validate the data using a hash function to verify the data lineage of an insurance claim.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table to store the insurance details. Validate the data using AWS DMS validation by moving the data to Amazon S3 to verify the data lineage of an insurance claim.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon QLDB ledger to store the insurance details. Validate the data by choosing the ledger name in the digest request to verify the data lineage of an insurance claim.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora database to store the insurance details. Validate the data using AWS DMS validation by moving the data to Amazon S3 to verify the data lineage of an insurance claim."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T12:12:00.000Z",
        "voteCount": 5,
        "content": "Immutable history of changes - agree with C."
      },
      {
        "date": "2023-09-12T21:02:00.000Z",
        "voteCount": 1,
        "content": "C. Create an Amazon QLDB ledger \n\nhttps://aws.amazon.com/qldb/faqs/"
      },
      {
        "date": "2023-05-10T08:56:00.000Z",
        "voteCount": 1,
        "content": "Agreed C, QLDB is best for this req"
      },
      {
        "date": "2022-12-23T09:46:00.000Z",
        "voteCount": 3,
        "content": "C : Amazon Quantum Ledger Database (QLDB) - Maintain an immutable, cryptographically verifiable log of data changes"
      },
      {
        "date": "2022-12-18T19:39:00.000Z",
        "voteCount": 1,
        "content": "Ans is C"
      },
      {
        "date": "2022-09-26T10:06:00.000Z",
        "voteCount": 4,
        "content": "immutable"
      },
      {
        "date": "2022-09-10T04:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/amazon/view/79885-exam-aws-certified-database-specialty-topic-1-question-203/",
    "body": "A company stores session history for its users in an Amazon DynamoDB table. The company has a large user base and generates large amounts of session data.<br>Teams analyze the session data for 1 week, and then the data is no longer needed. A database specialist needs to design an automated solution to purge session data that is more than 1 week old.<br>Which strategy meets these requirements with the MOST operational efficiency?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions state machine with a DynamoDB DeleteItem operation that uses the ConditionExpression parameter to delete items older than a week. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled rule that runs the Step Functions state machine on a weekly basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to delete items older than a week from the DynamoDB table. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled rule that triggers the Lambda function on a weekly basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon DynamoDB Streams on the table. Use a stream to invoke an AWS Lambda function to delete items older than a week from the DynamoDB table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable TTL on the DynamoDB table and set a Number data type as the TTL attribute. DynamoDB will automatically delete items that have a TTL that is less than the current time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T12:16:00.000Z",
        "voteCount": 7,
        "content": "Textbook question for TTL - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
      },
      {
        "date": "2023-09-12T21:09:00.000Z",
        "voteCount": 1,
        "content": "D. Enable TTL on the DynamoDB table"
      },
      {
        "date": "2022-12-18T19:42:00.000Z",
        "voteCount": 1,
        "content": "D. TTL less effort"
      },
      {
        "date": "2022-11-22T12:28:00.000Z",
        "voteCount": 1,
        "content": "This is wrong : delete items that have a TTL that is less than the current time. Most Voted\nthey need to delete 1 week old , not old than current time"
      },
      {
        "date": "2023-05-23T01:46:00.000Z",
        "voteCount": 1,
        "content": "TTL is a misnomer in the case of DynamoDB. It is actually \"the time after which an item is no longer needed\""
      },
      {
        "date": "2023-10-01T11:53:00.000Z",
        "voteCount": 2,
        "content": "Yes. The value would be set to the time in exactly one week. Then it expires automatically."
      },
      {
        "date": "2022-09-10T04:11:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/amazon/view/79939-exam-aws-certified-database-specialty-topic-1-question-204/",
    "body": "A company conducted a security audit of its AWS infrastructure. The audit identified that data was not encrypted in transit between application servers and a<br>MySQL database that is hosted in Amazon RDS.<br>After the audit, the company updated the application to use an encrypted connection. To prevent this problem from occurring again, the company's database team needs to configure the database to require in-transit encryption for all connections.<br>Which solution will meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the parameter group in use by the DB instance, and set the require_secure_transport parameter to ON.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the database, and use ALTER USER to enable the REQUIRE SSL option on the database user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the security group in use by the DB instance, and remove port 80 to prevent unencrypted connections from being established.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the DB instance, and enable the Require Transport Layer Security option."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-12T21:50:00.000Z",
        "voteCount": 3,
        "content": "A. Update the parameter group in use by the DB instance, and set the require_secure_transport parameter to ON\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/mysql-ssl-connections.html\n\n\n\"Use the require_secure_transport parameter to require that all user connections to your MySQL DB instance use SSL/TLS. By default, the require_secure_transport parameter is set to OFF. You can set the require_secure_transport parameter to ON to require SSL/TLS for connections to your DB instance.\""
      },
      {
        "date": "2022-09-26T10:08:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-rds-mysql-supports-ssl-tls-connections/"
      },
      {
        "date": "2022-09-03T22:17:00.000Z",
        "voteCount": 4,
        "content": "It's A. https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-rds-mysql-supports-ssl-tls-connections/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/amazon/view/79941-exam-aws-certified-database-specialty-topic-1-question-205/",
    "body": "A database specialist is designing an enterprise application for a large company. The application uses Amazon DynamoDB with DynamoDB Accelerator (DAX).<br>The database specialist observes that most of the queries are not found in the DAX cache and that they still require DynamoDB table reads.<br>What should the database specialist review first to improve the utility of DAX?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DynamoDB ConsumedReadCapacityUnits metric",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe trust relationship to perform the DynamoDB API calls",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DAX cluster's TTL setting\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe validity of customer-specified AWS Key Management Service (AWS KMS) keys for DAX encryption at rest"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T10:13:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.cluster-management.html#DAX.cluster-management.custom-settings.ttl"
      },
      {
        "date": "2023-04-10T11:18:00.000Z",
        "voteCount": 3,
        "content": "C is right\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.cluster-management.html"
      },
      {
        "date": "2023-04-14T05:30:00.000Z",
        "voteCount": 1,
        "content": "Agree, TTL setting default time is 5 minutes, to adjust TTL time to a longer time."
      },
      {
        "date": "2023-01-08T07:57:00.000Z",
        "voteCount": 2,
        "content": "A is the wrong answer.Reasons\n- it is not setting to change, it is function that returns The number of read capacity units consumed over the specified time period, so you can track how much of your provisioned throughput is used.\n- it is talking about dynamoDB metric, nothing to do with DAX"
      },
      {
        "date": "2022-09-11T19:51:00.000Z",
        "voteCount": 2,
        "content": "I think C.\n\nWhen you create a DAX cluster, the following default settings are used:\n\nAutomatic cache eviction enabled with Time to Live (TTL) of 5 minutes\n\nNo preference for Availability Zones\n\nNo preference for maintenance windows\n\nNotifications disabled"
      },
      {
        "date": "2022-09-10T04:15:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-09-03T22:21:00.000Z",
        "voteCount": 2,
        "content": "Agree with A - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.sizing-guide.html"
      },
      {
        "date": "2022-12-18T19:51:00.000Z",
        "voteCount": 1,
        "content": "Agree on this we should check first the cache missed stats,\n\nIf you also already have a DAX cluster, remember that the DynamoDB ConsumedReadCapacityUnits metric only accounts for cache misses. So, to get an idea of the read capacity units per second handled by your DAX cluster, divide the number by your cache miss rate (that is, 1 - cache hit rate)."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/amazon/view/80194-exam-aws-certified-database-specialty-topic-1-question-206/",
    "body": "A company plans to use AWS Database Migration Service (AWS DMS) to migrate its database from one Amazon EC2 instance to another EC2 instance as a full load task. The company wants the database to be inactive during the migration. The company will use a dms.t3.medium instance to perform the migration and will use the default settings for the migration.<br>Which solution will MOST improve the performance of the data migration?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of tables that are loaded in parallel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop all indexes on the source tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the processing mode from the batch optimized apply option to transactional mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Multi-AZ on the target database while the full load task is in progress."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-15T01:10:00.000Z",
        "voteCount": 1,
        "content": "A\nDropping indexes on the source will not help; this is intended for the target."
      },
      {
        "date": "2024-01-02T10:54:00.000Z",
        "voteCount": 3,
        "content": "By default, AWS DMS loads eight tables at a time. You might see some performance improvement by increasing this slightly when using a very large replication server, such as a dms.c4.xlarge or larger instance. However, at some point, increasing this parallelism reduces performance. If your replication server is relatively small, such as a dms.t2.medium, we recommend that you reduce the number of tables loaded in parallel.\n\nFor a full load task, we recommend that you drop primary key indexes, secondary indexes, referential integrity constraints, and data manipulation language (DML) triggers. Or you can delay their creation until after the full load tasks are complete. You don't need indexes during a full load task, and indexes incur maintenance overhead if they are present. Because the full load task loads groups of tables at a time, referential integrity constraints are violated. Similarly, insert, update, and delete triggers can cause errors, for example if a row insert is triggered for a previously bulk loaded table. Other types of triggers also affect performance due to added processing."
      },
      {
        "date": "2023-12-22T05:17:00.000Z",
        "voteCount": 1,
        "content": "A is not recommended for smaller instances like t2/t3.  C is not correct because they will run in default settings which is transactional mode.   D is distractor as it is not relevant for performance."
      },
      {
        "date": "2023-10-02T11:52:00.000Z",
        "voteCount": 3,
        "content": "x A. Actually, for a medium replication instance it's recommended to decrease the number of tables that are loaded in parallel... It's not evident that this helps. It probably would for very large instances.\nB. Dropping all indexes on the source tables - YES that clearly helps, because \"Indexes, triggers, and referential integrity constraints can affect your migration performance and cause your migration to fail.\" \nx C. Change the processing mode from the batch optimized apply option to transactional mode. - NO, the opposite would be good for performance (batch mode).\nx D. Enable Multi-AZ on the target database while the full load task is in progress. - Nice try - a distractor. Multi-AZ increases reliability, but that's not a direct boost to performance really.\n\nHence, B should be the right answer.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html"
      },
      {
        "date": "2023-09-11T05:40:00.000Z",
        "voteCount": 1,
        "content": "A. Increase the number of tables that are loaded in parallel.\n\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance"
      },
      {
        "date": "2023-12-20T00:43:00.000Z",
        "voteCount": 1,
        "content": "By default, AWS DMS loads eight tables at a time. You might see some performance improvement by increasing this slightly when using a very large replication server, such as a dms.c4.xlarge or larger instance. However, at some point, increasing this parallelism reduces performance. If your replication server is relatively small, such as a dms.t2.medium, we recommend that you reduce the number of tables loaded in parallel."
      },
      {
        "date": "2023-05-23T02:20:00.000Z",
        "voteCount": 3,
        "content": "A. Increase the number of tables that are loaded in parallel. - that MIGHT help\nB. Drop all indexes on the source tables.- dropping indexes on the destination tables would help, but not on the source tables\nC. Change the processing mode from the batch optimized apply option to transactional mode. - actually it is quite the opposite - the purpose of transactional mode is to preserve data consistency at all time, which is not required here\nD. Enable Multi-AZ on the target database while the full load task is in progress.  - that would slow down the migration since it would consume CPU and IO for other purposes than data migration\n\nSo A it is..."
      },
      {
        "date": "2023-01-08T08:15:00.000Z",
        "voteCount": 1,
        "content": "xA. Increase the number of tables that are loaded in parallel. - weak instance\nxB. Drop all indexes on the source tables. - should be on destination\nC. Change the processing mode from the batch optimized apply option to transactional mode. - database is already off-line and full load mode so temporarily lapse is very much NOT a concern as eventually it gets caught at the end.\nxD. Enable Multi-AZ on the target database while the full load task is in progress. - it slow down the process"
      },
      {
        "date": "2023-01-08T08:23:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html"
      },
      {
        "date": "2023-01-08T08:27:00.000Z",
        "voteCount": 1,
        "content": "-AWS DMS loads eight tables at a time - recommend instance dms.c4.xlarge or larger, on dms.t2.medium reduce parellism \n-Optimizing change processing - batches rather transaction - if you are ok with temporary lapse and disable referential at the destination\n-multiple tasks for a single migration can improve performance- not related tables in a group in a task\n-turn off backups and Multi-AZ on the target, also if any logs\n-full load task, drop primary key indexes, secondary indexes, referential integrity constraints. \n-On full load+CDC - pause the replication task before the CDC phase to build indexes and create referential integrity constraints before you restart the task\n-parallel full load for bigger tables based on partitions"
      },
      {
        "date": "2022-12-27T20:59:00.000Z",
        "voteCount": 1,
        "content": "So i dont get it , \n\nA -- may be helpful \nB - DMS doesnt take indexes into account during migration , unless they are manually created on target\nC - transaction mode is default setting - no need to switch it from batch \nD- Not helpful"
      },
      {
        "date": "2022-12-07T19:13:00.000Z",
        "voteCount": 2,
        "content": "To indicate the maximum number of tables to load in parallel, set the MaxFullLoadSubTasks option. The default is 8; the maximum value is 49.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.FullLoad.html"
      },
      {
        "date": "2022-12-01T04:53:00.000Z",
        "voteCount": 1,
        "content": "There is no much confusion on the query.. Most of the action items are for the target DB as per the URL..  Answer should be C.."
      },
      {
        "date": "2022-11-22T16:00:00.000Z",
        "voteCount": 2,
        "content": "A :  Correct\nXB Drop index on target DB \nXC Using batch  \nXD. Not help"
      },
      {
        "date": "2022-10-29T00:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-10-20T22:03:00.000Z",
        "voteCount": 2,
        "content": "Optimizing change processing\nBy default, AWS DMS processes changes in a transactional mode, which preserves transactional integrity. If you can afford temporary lapses in transactional integrity, you can use the batch optimized apply option instead. This option efficiently groups transactions and applies them in batches for efficiency purposes. Using the batch optimized apply option almost always violates referential integrity constraints. So we recommend that you turn these constraints off during the migration process and turn them on again as part of the cutover process."
      },
      {
        "date": "2022-09-26T10:25:00.000Z",
        "voteCount": 3,
        "content": "C is incorrect as it's mentioned in questions that DMS is running with default setting and default is transactional mode. D will have no effect on performance.  A will help but dms.t3.medium is not strong enough to handle excess parallelism. It may in fact degrade performance. With all these I feel B seems to be answer. Even with B I guess best practice will be to drop indexes on Destination side and not on source. But It's mentioned database will be inactive I believe dropping indexes on source and recreating later can help. \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance"
      },
      {
        "date": "2022-11-03T11:58:00.000Z",
        "voteCount": 1,
        "content": "They mention it's running with default so that you understand what's being used currently. Changing to batch optimized from transactional will improve performance as described."
      },
      {
        "date": "2022-11-04T10:24:00.000Z",
        "voteCount": 1,
        "content": "Had this backwards. B is correct."
      },
      {
        "date": "2022-09-24T06:22:00.000Z",
        "voteCount": 1,
        "content": "For a full load task, we recommend that you drop primary key indexes, secondary indexes, referential integrity constraints, and data manipulation language (DML) triggers. Or you can delay their creation until after the full load tasks are complete. You don't need indexes during a full load task, and indexes incur maintenance overhead if they are present. Because the full load task loads groups of tables at a time, referential integrity constraints are violated. Similarly, insert, update, and delete triggers can cause errors, for example if a row insert is triggered for a previously bulk loaded table. Other types of triggers also affect performance due to added processing.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html"
      },
      {
        "date": "2022-10-20T22:01:00.000Z",
        "voteCount": 2,
        "content": "Whole phrase mentioned above, in the doc refers to the target, not the source."
      },
      {
        "date": "2022-10-20T22:02:00.000Z",
        "voteCount": 1,
        "content": "I will go with C - Same Doc explains it - Optimizing change processing\nBy default, AWS DMS processes changes in a transactional mode, which preserves transactional integrity. If you can afford temporary lapses in transactional integrity, you can use the batch optimized apply option instead. This option efficiently groups transactions and applies them in batches for efficiency purposes. Using the batch optimized apply option almost always violates referential integrity constraints. So we recommend that you turn these constraints off during the migration process and turn them on again as part of the cutover process."
      },
      {
        "date": "2022-09-19T17:44:00.000Z",
        "voteCount": 2,
        "content": "B, t3 medium still too weak to handle large parallel workload."
      },
      {
        "date": "2022-09-18T04:08:00.000Z",
        "voteCount": 2,
        "content": "I go with A, its mentioned as Full load task and not CDC, so as per this link, it should be A\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/amazon/view/80196-exam-aws-certified-database-specialty-topic-1-question-207/",
    "body": "A finance company migrated its 3 \u05c0\u00a2\u05c0\u2019 on-premises PostgreSQL database to an Amazon Aurora PostgreSQL DB cluster. During a review after the migration, a database specialist discovers that the database is not encrypted at rest. The database must be encrypted at rest as soon as possible to meet security requirements. The database specialist must enable encryption for the DB cluster with minimal downtime.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the unencrypted DB cluster using the AWS Management Console. Enable encryption and choose to apply the change immediately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the unencrypted DB cluster and restore it to a new DB cluster with encryption enabled. Update any database connection strings to reference the new DB cluster endpoint, and then delete the unencrypted DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted Aurora Replica of the unencrypted DB cluster. Promote the Aurora Replica as the new master.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DB cluster with encryption enabled and use the pg_dump and pg_restore utilities to load data to the new DB cluster. Update any database connection strings to reference the new DB cluster endpoint, and then delete the unencrypted DB cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-02T10:57:00.000Z",
        "voteCount": 2,
        "content": "it's B"
      },
      {
        "date": "2023-03-25T23:10:00.000Z",
        "voteCount": 4,
        "content": "I'll go with B \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html#:~:text=You%20can%27t%20convert%20an%20unencrypted%20DB%20cluster%20to%20an%20encrypted%20one.%20However%2C%20you%20can%20restore%20an%20unencrypted%20snapshot%20to%20an%20encrypted%20Aurora%20DB%20cluster.%20To%20do%20this%2C%20specify%20a%20KMS%20key%20when%20you%20restore%20from%20the%20unencrypted%20snapshot."
      },
      {
        "date": "2023-02-17T03:21:00.000Z",
        "voteCount": 1,
        "content": "It's B \nhttps://catalog.us-east-1.prod.workshops.aws/workshops/aad9ff1e-b607-45bc-893f-121ea5224f24/en-US/rds/aurora/2-encryptexistingcluster"
      },
      {
        "date": "2022-12-27T12:57:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/best-practices-for-migrating-postgresql-databases-to-amazon-rds-and-amazon-aurora/"
      },
      {
        "date": "2022-12-27T08:20:00.000Z",
        "voteCount": 2,
        "content": "Point number 4 in limitations - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html#Overview.Encryption.Limitations"
      },
      {
        "date": "2022-09-26T10:33:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html"
      },
      {
        "date": "2022-09-10T04:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-04T22:13:00.000Z",
        "voteCount": 3,
        "content": "It's B. A and C for sure incorrect - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/amazon/view/80197-exam-aws-certified-database-specialty-topic-1-question-208/",
    "body": "A company has a 4 \u05c0\u00a2\u05c0\u2019 on-premises Oracle Real Application Clusters (RAC) database. The company wants to migrate the database to AWS and reduce licensing costs. The company's application team wants to store JSON payloads that expire after 28 hours. The company has development capacity if code changes are required.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB and leverage the Time to Live (TTL) feature to automatically expire the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS for Oracle with Multi-AZ. Create an AWS Lambda function to purge the expired data. Schedule the Lambda function to run daily using Amazon EventBridge.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DocumentDB with a read replica in a different Availability Zone. Use DocumentDB change streams to expire the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora PostgreSQL with Multi-AZ and leverage the Time to Live (TTL) feature to automatically expire the data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T10:40:00.000Z",
        "voteCount": 4,
        "content": "Expiration + Json + Capacity to have code change = Option A"
      },
      {
        "date": "2022-09-10T04:23:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-09-04T22:14:00.000Z",
        "voteCount": 3,
        "content": "JSON + expiration = Dynamo DB. It's A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/amazon/view/80198-exam-aws-certified-database-specialty-topic-1-question-209/",
    "body": "A database specialist is working on an Amazon RDS for PostgreSQL DB instance that is experiencing application performance issues due to the addition of new workloads. The database has 5 \u05c0\u00a2\u05c0\u2019 of storage space with Provisioned IOPS. Amazon CloudWatch metrics show that the average disk queue depth is greater than<br>200 and that the disk I/O response time is significantly higher than usual.<br>What should the database specialist do to improve the performance of the application immediately?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Provisioned IOPS rate on the storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the available storage space.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse General Purpose SSD (gp2) storage with burst credits.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica to offload Read IOPS from the DB instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-23T19:54:00.000Z",
        "voteCount": 5,
        "content": "What is \u05c0\u00a2\u05c0\u2019 in the question guys?"
      },
      {
        "date": "2023-08-13T06:53:00.000Z",
        "voteCount": 2,
        "content": "TB. Google the answer text should reveal the original question somewhere."
      },
      {
        "date": "2024-01-15T01:15:00.000Z",
        "voteCount": 2,
        "content": "A \nAlthough it may still take some time to \"rebalance\" the storage"
      },
      {
        "date": "2022-12-28T06:10:00.000Z",
        "voteCount": 3,
        "content": "Increasing the provisioned IO capacity will immediately alleviate the latency."
      },
      {
        "date": "2022-12-27T09:30:00.000Z",
        "voteCount": 2,
        "content": "Ans - A\nGetting the best performance from Amazon RDS Provisioned IOPS SSD storage:\nIf your workload is I/O constrained, using Provisioned IOPS SSD storage can increase the number of I/O requests that the system can process concurrently. Increased concurrency allows for decreased latency because I/O requests spend less time in a queue. Decreased latency allows for faster database commits, which improves response time and allows for higher database throughput.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html"
      },
      {
        "date": "2022-11-04T10:26:00.000Z",
        "voteCount": 2,
        "content": "Key word is \"immediate.\" Replica will take time to create."
      },
      {
        "date": "2022-10-15T10:30:00.000Z",
        "voteCount": 1,
        "content": "answer A due to \"immediate improvement\""
      },
      {
        "date": "2022-09-26T10:51:00.000Z",
        "voteCount": 2,
        "content": "I think both A and D are correct but as last line of question says we want immediate improvement then A should be answer as read replica creation will take some time."
      },
      {
        "date": "2022-09-10T04:31:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-09-04T22:26:00.000Z",
        "voteCount": 2,
        "content": "It seems that D - https://aws.amazon.com/blogs/database/best-storage-practices-for-running-production-workloads-on-hosted-databases-with-amazon-rds-or-amazon-ec2/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/amazon/view/80200-exam-aws-certified-database-specialty-topic-1-question-210/",
    "body": "A software company uses an Amazon RDS for MySQL Multi-AZ DB instance as a data store for its critical applications. During an application upgrade process, a database specialist runs a custom SQL script that accidentally removes some of the default permissions of the master user.<br>What is the MOST operationally efficient way to restore the default permissions of the master user?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance and set a new master user password.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Secrets Manager to modify the master user password and restart the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new master user for the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the IAM user that owns the DB instance, and add missing permissions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T20:02:00.000Z",
        "voteCount": 2,
        "content": "A. \nIf you accidentally delete the permissions for the master user, you can restore them by modifying the DB instance and setting a new master user password."
      },
      {
        "date": "2022-12-03T10:44:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.MasterAccounts.html"
      },
      {
        "date": "2022-09-10T04:32:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-09-04T22:28:00.000Z",
        "voteCount": 2,
        "content": "A is correct."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/amazon/view/80201-exam-aws-certified-database-specialty-topic-1-question-211/",
    "body": "A company is launching a new Amazon RDS for MySQL Multi-AZ DB instance to be used as a data store for a custom-built application. After a series of tests with point-in-time recovery disabled, the company decides that it must have point-in-time recovery reenabled before using the DB instance to store production data.<br>What should a database specialist do so that point-in-time recovery can be successful?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable binary logging in the DB parameter group used by the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance and enable audit logs to be pushed to Amazon CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance and configure a backup retention period\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a scheduled job to create manual DB instance snapshots."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T12:27:00.000Z",
        "voteCount": 5,
        "content": "https://aws.amazon.com/blogs/database/setting-up-a-binlog-server-for-amazon-rds-mysql-and-mariadb-using-mariadb-maxscale/\n\"After you run the command, it\u2019s okay to enable backup retention on the RDS instance by using the AWS CLI or the console. Enabling backup retention also enables binary logging.\""
      },
      {
        "date": "2022-10-29T18:26:00.000Z",
        "voteCount": 3,
        "content": "You can restore a DB instance to a specific point in time (PITR), creating a new DB instance. To support PITR, your DB instances must have backup retention set to a nonzero value.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-backup-sqlserver.html\n\nAnswer : C"
      },
      {
        "date": "2022-09-26T11:01:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is c.  Setting non-zero value in backup retention turns on automatic backups and also turns on bin logging which is required for PIT. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.MySQL.BinaryFormat.html"
      },
      {
        "date": "2022-09-10T04:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-04-26T12:16:00.000Z",
        "voteCount": 1,
        "content": "Binary logging is already configured because it's a Multi-AZ"
      },
      {
        "date": "2022-09-04T22:32:00.000Z",
        "voteCount": 2,
        "content": "Backup retention period must be set - it's C. https://aws.amazon.com/blogs/storage/point-in-time-recovery-and-continuous-backup-for-amazon-rds-with-aws-backup/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/amazon/view/80202-exam-aws-certified-database-specialty-topic-1-question-212/",
    "body": "A company has a database fleet that includes an Amazon RDS for MySQL DB instance. During an audit, the company discovered that the data that is stored on the DB instance is unencrypted.<br>A database specialist must enable encryption for the DB instance. The database specialist also must encrypt all connections to the DB instance.<br>Which combination of actions should the database specialist take to meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the RDS console, choose \u05d2\u20acEnable encryption\u05d2\u20ac to encrypt the DB instance by using an AWS Key Management Service (AWS KMS) key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the read replica of the unencrypted DB instance by using an AWS Key Management Service (AWS KMS) key. Fail over the read replica to the primary DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the unencrypted DB instance. Encrypt the snapshot by using an AWS Key Management Service (AWS KMS) key. Restore the DB instance from the encrypted snapshot. Delete the original DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequire SSL connections for applicable database user accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSL/TLS from the application to encrypt a connection to the DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable SSH encryption on the DB instance."
    ],
    "answer": "CDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "CEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-02T11:46:00.000Z",
        "voteCount": 1,
        "content": "CDE .."
      },
      {
        "date": "2023-09-03T21:43:00.000Z",
        "voteCount": 1,
        "content": "CDE ......"
      },
      {
        "date": "2022-12-28T06:16:00.000Z",
        "voteCount": 1,
        "content": "F is incorrect because you cannot SSH into the DB instance."
      },
      {
        "date": "2022-12-21T09:10:00.000Z",
        "voteCount": 1,
        "content": "CDE is correct"
      },
      {
        "date": "2022-10-29T00:24:00.000Z",
        "voteCount": 1,
        "content": "ACE\nTo encrypt a new DB instance, choose Enable encryption on the Amazon RDS console\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Enabling"
      },
      {
        "date": "2022-10-29T00:22:00.000Z",
        "voteCount": 1,
        "content": "ACD\nTo encrypt a new DB instance, choose Enable encryption on the Amazon RDS console\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Enabling"
      },
      {
        "date": "2022-10-21T15:31:00.000Z",
        "voteCount": 1,
        "content": "CEF - We need to force SSL from DB too"
      },
      {
        "date": "2022-12-27T11:38:00.000Z",
        "voteCount": 1,
        "content": "F means \"SSH\" not SSL, hence, incorrect. CDE is the answer"
      },
      {
        "date": "2022-09-26T11:05:00.000Z",
        "voteCount": 3,
        "content": "CDE seems to be correct. Both A and B are not possible."
      },
      {
        "date": "2022-09-04T22:38:00.000Z",
        "voteCount": 3,
        "content": "I'd go for CDE - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/amazon/view/80203-exam-aws-certified-database-specialty-topic-1-question-213/",
    "body": "A company has an ecommerce website that runs on AWS. The website uses an Amazon RDS for MySQL database. A database specialist wants to enforce the use of temporary credentials to access the database.<br>Which solution will meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MySQL native database authentication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Secrets Manager to rotate the credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Identity and Access Management (IAM) database authentication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Parameter Store for authentication."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T06:21:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAM.html"
      },
      {
        "date": "2022-09-26T11:06:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-09-10T04:36:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-09-04T22:39:00.000Z",
        "voteCount": 3,
        "content": "Agree with C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/amazon/view/80205-exam-aws-certified-database-specialty-topic-1-question-214/",
    "body": "A manufacturing company has an. inventory system that stores information in an Amazon Aurora MySQL DB cluster. The database tables are partitioned. The database size has grown to 3 TB. Users run one-time queries by using a SQL client. Queries that use an equijoin to join large tables are taking a long time to run.<br>Which action will improve query performance with the LEAST operational effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to a new Amazon Redshift data warehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable hash joins on the database by setting the variable optimizer_switch to hash_join=on.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the DB cluster. Create a new DB instance by using the snapshot, and enable parallel query mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Aurora read replica."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-10T04:38:00.000Z",
        "voteCount": 5,
        "content": "B is correct\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.BestPractices.html"
      },
      {
        "date": "2023-09-02T16:31:00.000Z",
        "voteCount": 1,
        "content": "B. Enable hash joins on the database by setting the variable optimizer_switch to hash_join=on\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.BestPractices.html#:~:text=the%20MySQL%20documentation.-,Optimizing%20large%20Aurora%20MySQL%20join%20queries%20with%20hash%20joins,-When%20you%20need"
      },
      {
        "date": "2023-05-02T04:38:00.000Z",
        "voteCount": 3,
        "content": "Equal joins between large tables require hash joins. Read replicas do not reduce query times."
      },
      {
        "date": "2023-03-27T10:56:00.000Z",
        "voteCount": 1,
        "content": "Option B is not the correct solution because enabling hash joins on the database by setting the variable optimizer_switch to hash_join=on can impact the performance of other queries and may not necessarily improve the performance of the equijoin queries."
      },
      {
        "date": "2023-02-26T14:46:00.000Z",
        "voteCount": 1,
        "content": "Based on the scenario, the LEAST operational effort would be to add an Aurora read replica (option D). This is because adding a read replica is a simple and quick process that does not require any changes to the existing cluster or schema. It allows offloading read traffic to the replica, which can help to reduce the load on the primary cluster and improve query performance."
      },
      {
        "date": "2023-02-26T14:48:00.000Z",
        "voteCount": 1,
        "content": "Option C, taking a snapshot of the DB cluster and creating a new DB instance with parallel query mode enabled, can also improve query performance. However, this option requires more operational effort as it involves taking a snapshot, creating a new instance, and modifying the instance settings to enable parallel query mode. Additionally, this option may result in increased storage costs due to the creation of a new instance.\nEnabling hash join on the database by setting the variable optimizer_switch to hash_join=on may improve performance for some specific queries, but it's not a recommended solution for improving performance in general. It is a query-specific tuning parameter that requires careful analysis and testing, as it may not always provide the best performance improvement or may even degrade performance in some cases."
      },
      {
        "date": "2023-02-26T14:48:00.000Z",
        "voteCount": 1,
        "content": "In contrast, increasing the number of Aurora replicas will distribute the read load across multiple nodes and improve read performance for all queries, not just specific ones. Additionally, creating a read replica is a simpler and more straightforward operation compared to tuning specific queries by modifying database parameters."
      },
      {
        "date": "2023-02-26T14:49:00.000Z",
        "voteCount": 1,
        "content": "So, my answer is D"
      },
      {
        "date": "2022-12-18T20:13:00.000Z",
        "voteCount": 3,
        "content": "B is correct.\nWhen you need to join a large amount of data by using an equijoin, a hash join can improve query performance."
      },
      {
        "date": "2022-09-04T22:41:00.000Z",
        "voteCount": 1,
        "content": "Agree with B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/amazon/view/80206-exam-aws-certified-database-specialty-topic-1-question-215/",
    "body": "A company is running a business-critical application on premises by using Microsoft SQL Server. A database specialist is planning to migrate the instance with several databases to the AWS Cloud. The database specialist will use SQL Server Standard edition hosted on Amazon EC2 Windows instances. The solution must provide high availability and must avoid a single point of failure in the SQL Server deployment architecture.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon RDS for SQL Server Multi-AZ DB instances. Use Amazon S3 as a shared storage option to host the databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Always On Failover Cluster Instances as a single SQL Server instance. Use Multi-AZ Amazon FSx for Windows File Server as a shared storage option to host the databases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Always On availability groups to group one or more user databases that fail over together across multiple SQL Server instances. Use Multi-AZ Amazon FSx for Windows File Server as a shared storage option to host the databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Load Balancer to distribute database traffic across multiple EC2 instances in multiple Availability Zones. Use Amazon S3 as a shared storage option to host the databases."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-27T08:07:00.000Z",
        "voteCount": 9,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/ec2-fci.html\nAn FCI is generally preferable over an Always on availability group when:\nYou\u2019re using SQL Server Standard edition instead of Enterprise edition."
      },
      {
        "date": "2022-12-07T12:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. Key word \"SQL Server Standard edition\""
      },
      {
        "date": "2022-11-22T11:18:00.000Z",
        "voteCount": 1,
        "content": "B is right. C is incorrect because it is mentioned to use FSx for hosting databases."
      },
      {
        "date": "2022-10-21T16:18:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-10-18T08:16:00.000Z",
        "voteCount": 3,
        "content": "C - https://aws.amazon.com/about-aws/whats-new/2021/02/amazon-rds-for-sql-server-now-supports-always-on-availability-groups-for-standard-edition/"
      },
      {
        "date": "2022-10-15T11:05:00.000Z",
        "voteCount": 1,
        "content": "C requires dedicated storage for each instance B is correct"
      },
      {
        "date": "2022-10-09T05:56:00.000Z",
        "voteCount": 1,
        "content": "An FCI is generally preferable over an Always on availability group when you\u2019re using SQL Server Standard edition instead of Enterprise edition."
      },
      {
        "date": "2022-09-26T11:20:00.000Z",
        "voteCount": 1,
        "content": "RDS is not in picture. FsX for shared storage on windows ec2 instances. C is correct."
      },
      {
        "date": "2022-09-05T16:13:00.000Z",
        "voteCount": 4,
        "content": "B: SQL Server Standard edition: FCI.\nFor C you need SQL Server Enterprise Edition."
      },
      {
        "date": "2022-10-21T16:17:00.000Z",
        "voteCount": 3,
        "content": "Agree. SQL AAG needs Enterprise Edition"
      },
      {
        "date": "2023-10-02T12:41:00.000Z",
        "voteCount": 1,
        "content": "Or not? \nhttps://aws.amazon.com/about-aws/whats-new/2021/02/amazon-rds-for-sql-server-now-supports-always-on-availability-groups-for-standard-edition/"
      },
      {
        "date": "2022-09-04T22:44:00.000Z",
        "voteCount": 3,
        "content": "Agree with C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/amazon/view/80210-exam-aws-certified-database-specialty-topic-1-question-216/",
    "body": "A company is planning to use Amazon RDS for SQL Server for one of its critical applications. The company's security team requires that the users of the RDS for<br>SQL Server DB instance are authenticated with on-premises Microsoft Active Directory credentials.<br>Which combination of steps should a database specialist take to meet this requirement? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtend the on-premises Active Directory to AWS by using AD Connector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user that uses the AmazonRDSDirectoryServiceAccess managed IAM policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a directory by using AWS Directory Service for Microsoft Active Directory.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Active Directory domain controller on Amazon EC2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that uses the AmazonRDSDirectoryServiceAccess managed IAM policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a one-way forest trust from the AWS Directory Service for Microsoft Active Directory directory to the on-premises Active Directory.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CEF",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-15T00:51:00.000Z",
        "voteCount": 3,
        "content": "CEF\nWe need IAM Role, not IAM User"
      },
      {
        "date": "2023-04-11T02:43:00.000Z",
        "voteCount": 1,
        "content": "cef is correct"
      },
      {
        "date": "2022-09-10T04:41:00.000Z",
        "voteCount": 1,
        "content": "CEF is the answer"
      },
      {
        "date": "2023-04-05T08:32:00.000Z",
        "voteCount": 2,
        "content": "E -- \"If you use the AWS CLI or Amazon RDS API to create your SQL Server DB instance, create an AWS Identity and Access Management (IAM) role. This role uses the managed IAM policy AmazonRDSDirectoryServiceAccess and allows Amazon RDS to make calls to your directory. If you use the console to create your SQL Server DB instance, AWS creates the IAM role for you.\""
      },
      {
        "date": "2022-09-04T22:53:00.000Z",
        "voteCount": 3,
        "content": "CEF is correct."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/amazon/view/80207-exam-aws-certified-database-specialty-topic-1-question-217/",
    "body": "A company is developing an application that performs intensive in-memory operations on advanced data structures such as sorted sets. The application requires sub-millisecond latency for reads and writes. The application occasionally must run a group of commands as an ACID-compliant operation. A database specialist is setting up the database for this application. The database specialist needs the ability to create a new database cluster from the latest backup of the production cluster.<br>Which type of cluster should the database specialist create to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon ElastiCache for Memcached",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Neptune",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon ElastiCache for Redis\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB Accelerator (DAX)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-08T15:01:00.000Z",
        "voteCount": 5,
        "content": "C is right  as advanced data structures supported in Redis not in memcache. https://aws.amazon.com/elasticache/redis-vs-memcached/"
      },
      {
        "date": "2024-01-15T00:53:00.000Z",
        "voteCount": 1,
        "content": "C\n\"advanced data structures such as sorted sets\" rules out DynamoDB and MemcacheD"
      },
      {
        "date": "2023-10-02T12:58:00.000Z",
        "voteCount": 1,
        "content": "D.\nDAX supports in-memory operations, it's use case is caching.\nDynamoDB supports transactional consistency (ACID).\nYou can restore a DynamoDB database from a backup.\nWould be the best choice for me."
      },
      {
        "date": "2023-09-12T22:30:00.000Z",
        "voteCount": 1,
        "content": "C. Amazon ElastiCache for Redis\n\nhttps://aws.amazon.com/elasticache/redis-vs-memcached/\n\nadvanced data structures supported in Redis not in memcache."
      },
      {
        "date": "2023-09-03T22:05:00.000Z",
        "voteCount": 1,
        "content": "sortset . in memory operator &gt; C (redis)"
      },
      {
        "date": "2022-12-28T07:37:00.000Z",
        "voteCount": 2,
        "content": "Elasticache for Redis is appropriate for complex data structures, ACID compliant and can be restored from a backup unlike Memcache."
      },
      {
        "date": "2022-09-26T11:42:00.000Z",
        "voteCount": 2,
        "content": "Sorted sets seems to be key. I think C is answer."
      },
      {
        "date": "2022-09-05T16:15:00.000Z",
        "voteCount": 4,
        "content": "D is the right answer.\nC is wrong since Redis transactions are not fully ACID compliant.\nMemcached is a simple key-value store.\nNeptune is a graph database."
      },
      {
        "date": "2022-09-22T00:05:00.000Z",
        "voteCount": 3,
        "content": "I dont think DAX is ACID compliant"
      },
      {
        "date": "2022-09-04T22:47:00.000Z",
        "voteCount": 2,
        "content": "Sorted sets - https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.html#elasticache-for-redis-use-cases-gaming"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/amazon/view/80220-exam-aws-certified-database-specialty-topic-1-question-218/",
    "body": "A company uses Amazon Aurora MySQL as the primary database engine for many of its applications. A database specialist must create a dashboard to provide the company with information about user connections to databases. According to compliance requirements, the company must retain all connection logs for at least 7 years.<br>Which solution will meet these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable advanced auditing on the Aurora cluster to log CONNECT events. Export audit logs from Amazon CloudWatch to Amazon S3 by using an AWS Lambda function that is invoked by an Amazon EventBridge (Amazon CloudWatch Events) scheduled event. Build a dashboard by using Amazon QuickSight.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCapture connection attempts to the Aurora cluster with AWS Cloud Trail by using the DescribeEvents API operation. Create a CloudTrail trail to export connection logs to Amazon S3. Build a dashboard by using Amazon QuickSight.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a database activity stream for the Aurora cluster. Push the activity records to an Amazon Kinesis data stream. Build a dynamic dashboard by using AWS Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the DatabaseConnections metric for the Aurora DB instances to Amazon CloudWatch. Build a dashboard by using CloudWatch dashboards."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T00:44:00.000Z",
        "voteCount": 7,
        "content": "It's A, any thoughts? https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html"
      },
      {
        "date": "2023-10-02T13:06:00.000Z",
        "voteCount": 2,
        "content": "A. Reasonable solution: Advanced auditing -&gt; CloudWatch -&gt; CloudWatch Events -&gt; S3 -&gt; QuickSight. \nx B.  Sounds like a lot of manual work. AWS offers more automation than that.\nx C. A dynamic dashboard by using AWS Lambda? Funny idea. And where is the data stored for 7 years then?\nx D. Doesn't even mention data storage for 7 years."
      },
      {
        "date": "2023-09-07T14:45:00.000Z",
        "voteCount": 2,
        "content": "A. Enable advanced auditing on the Aurora cluster to log CONNECT events.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html"
      },
      {
        "date": "2023-06-25T07:56:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2023-07-02T10:46:00.000Z",
        "voteCount": 4,
        "content": "As DatabaseConnections metric just shows the number of connections, not the audit of who connected."
      },
      {
        "date": "2023-01-08T10:08:00.000Z",
        "voteCount": 1,
        "content": "A is right may be. B is wrong cuz DescribeEvents API operation - does not return login details"
      },
      {
        "date": "2022-12-02T11:27:00.000Z",
        "voteCount": 1,
        "content": "A is absolutely Correct"
      },
      {
        "date": "2022-09-14T02:47:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/amazon/view/80221-exam-aws-certified-database-specialty-topic-1-question-219/",
    "body": "A company requires near-real-time notifications when changes are made to Amazon RDS DB security groups.<br>Which solution will meet this requirement with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an RDS event notification subscription for DB security group events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that monitors DB security group changes. Create an Amazon Simple Notification Service (Amazon SNS) topic for notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail. Configure notifications for the detection of changes to DB security groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon CloudWatch alarm for RDS metrics about changes to DB security groups."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T00:45:00.000Z",
        "voteCount": 11,
        "content": "A, RDS Event notification https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html#USER_Events.Messages.security-group"
      },
      {
        "date": "2023-05-23T05:23:00.000Z",
        "voteCount": 1,
        "content": "If you read the document, you will see:\n\n\"DB security groups are resources for EC2-Classic. EC2-Classic was retired on August 15, 2022. If you haven't migrated from EC2-Classic to a VPC, we recommend that you migrate as soon as possible. \""
      },
      {
        "date": "2023-10-07T07:50:00.000Z",
        "voteCount": 1,
        "content": "LEAST operational overhead - Event notification for SG, https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html#USER_Events.Messages.security-group"
      },
      {
        "date": "2023-10-02T13:14:00.000Z",
        "voteCount": 1,
        "content": "They keyword is \"near real-time\".\nIt's fulfilled by CloudWatch.\nRDS event notifications might take up to five minutes to be delivered. That doesn't satisfy the requirement."
      },
      {
        "date": "2023-10-03T06:11:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html#Delivery%20of%20RDS%20event%20notifications"
      },
      {
        "date": "2023-09-07T15:06:00.000Z",
        "voteCount": 1,
        "content": "A. Configure an RDS event notification subscription\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html"
      },
      {
        "date": "2023-06-11T18:15:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is:\n\nC. Turn on AWS CloudTrail. Configure notifications for the detection of changes to DB security groups.\n\nExplanation:\n\nAWS CloudTrail captures all API calls for Amazon RDS as events, including calls from the Amazon RDS console and from code calls to the Amazon RDS APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for RDS. You can then use Amazon EventBridge (formerly known as CloudWatch Events) to detect and react to changes on your AWS resources like the modifications in your RDS Security Group.\n\nOptions A, B, and D are incorrect:\n\nA. Configuring an RDS event notification subscription will not cover DB security group changes, it's more targeted towards DB instance state changes, failover etc."
      },
      {
        "date": "2023-06-11T18:23:00.000Z",
        "voteCount": 2,
        "content": "Sorry.. I've just checked.\nSource type of resource this subscription will consume events from we can select Security group event configuration change.\nSo I will select A"
      },
      {
        "date": "2022-09-14T02:48:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/amazon/view/80225-exam-aws-certified-database-specialty-topic-1-question-220/",
    "body": "A development team asks a database specialist to create a copy of a production Amazon RDS for MySQL DB instance every morning. The development team will use the copied DB instance as a testing environment for development. The original DB instance and the copy will be hosted in different VPCs of the same AWS account. The development team wants the copy to be available by 6 AM each day and wants to use the same endpoint address each day.<br>Which combination of steps should the database specialist take to meet these requirements MOST cost-effectively? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the production database each day before the 6 AM deadline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS for MySQL DB instance from the snapshot. Select the desired DB instance size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate a defined Amazon Route 53 CNAME record to point to the copied DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Database Migration Service (AWS DMS) migration task to copy the snapshot to the copied DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CopySnapshot action on the production DB instance to create a snapshot before 6 AM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate a defined Amazon Route 53 alias record to point to the copied DB instance."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "ABF",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T11:52:00.000Z",
        "voteCount": 5,
        "content": "ABC is correct. E is I guess for copying EBS volume snapshots and not for RDS.  DMS is not required and it's not alias record."
      },
      {
        "date": "2024-01-15T01:02:00.000Z",
        "voteCount": 1,
        "content": "ABF\nSee https://aaron-janes.medium.com/are-you-using-route53-with-cname-or-alias-records-to-route-dns-requests-in-your-organization-56166ded5cd4"
      },
      {
        "date": "2023-09-07T16:10:00.000Z",
        "voteCount": 2,
        "content": "ABC is correct"
      },
      {
        "date": "2023-02-21T07:46:00.000Z",
        "voteCount": 3,
        "content": "CNAME records can route traffic to Elastic Beanstalk environments and RDSs, Alias records routes traffic to EC2, ELB, S3 and so on"
      },
      {
        "date": "2022-12-28T10:51:00.000Z",
        "voteCount": 1,
        "content": "Can someone explain why a CNAME record isn't chosen over ALIAS record? From my understanding Alias records do not support RDS DNS endpoints.   Any thoughts?"
      },
      {
        "date": "2022-12-25T02:05:00.000Z",
        "voteCount": 2,
        "content": "\"C\" is not right. Pls refer to https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html"
      },
      {
        "date": "2022-12-25T02:19:00.000Z",
        "voteCount": 1,
        "content": "Agree, ABC  -  https://jayendrapatil.com/aws-route-53-alias-vs-cname/"
      },
      {
        "date": "2022-12-02T11:24:00.000Z",
        "voteCount": 2,
        "content": "ABC is correct"
      },
      {
        "date": "2022-09-10T04:45:00.000Z",
        "voteCount": 2,
        "content": "ABC is correct"
      },
      {
        "date": "2022-09-05T00:49:00.000Z",
        "voteCount": 1,
        "content": "BCE, any thoughts?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/amazon/view/80224-exam-aws-certified-database-specialty-topic-1-question-221/",
    "body": "A software company is conducting a security audit of its three-node Amazon Aurora MySQL DB cluster.<br>Which finding is a security concern that needs to be addressed?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe AWS account root user does not have the minimum privileges required for client applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncryption in transit is not configured for all Aurora native backup processes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach Aurora DB cluster node is not in a separate private VPC with restricted access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM credentials used by the application are not rotated regularly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-09T16:02:00.000Z",
        "voteCount": 2,
        "content": "D. should rotate IAM credentials regularly.\n\nB is incorrect, Aurora has no native backups, it only has automated (continuous) backups and snapshots."
      },
      {
        "date": "2023-04-28T18:12:00.000Z",
        "voteCount": 2,
        "content": "A  is a TRAP.   root for AWS Account, which you won't touch it."
      },
      {
        "date": "2023-04-11T05:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html"
      },
      {
        "date": "2022-12-28T10:55:00.000Z",
        "voteCount": 1,
        "content": "I chose D however I believe A is somewhat confusing because root shouldn't be used by the application and it is privilege shouldn't be minimized for application use.  Very confusing"
      },
      {
        "date": "2022-09-27T08:17:00.000Z",
        "voteCount": 4,
        "content": "Go with D, not A.  A is saying root does NOT have the required minimum permissions for the application.  Which is fine....if you're not using root for the application, then who cares.  Nowhere does it say, \"root is being used for application access\"."
      },
      {
        "date": "2022-09-26T11:55:00.000Z",
        "voteCount": 2,
        "content": "Rotate your IAM credentials regularly."
      },
      {
        "date": "2022-09-20T08:28:00.000Z",
        "voteCount": 2,
        "content": "D, Rotate your IAM credentials regularly."
      },
      {
        "date": "2022-09-14T02:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-07T16:15:00.000Z",
        "voteCount": 4,
        "content": "A is the not the answer. Root user should NOT be used for the applications."
      },
      {
        "date": "2022-09-05T00:47:00.000Z",
        "voteCount": 3,
        "content": "It's A - minimum priviledge rule should be always maintained."
      },
      {
        "date": "2023-09-05T15:06:00.000Z",
        "voteCount": 1,
        "content": "But not having the minimun privilege is not a security concern so the ans is D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/amazon/view/80227-exam-aws-certified-database-specialty-topic-1-question-222/",
    "body": "A company has an AWS CloudFormation stack that defines an Amazon RDS DB instance. The company accidentally deletes the stack and loses recent data from the DB instance. A database specialist must change the CloudFormation template for the RDS resource to reduce the chance of accidental data loss from the DB instance in the future.<br>Which combination of actions should the database specialist take to meet this requirement? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the DeletionProtection property to True.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the MultiAZ property to True.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the TerminationProtection property to True.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the DeleteAutomatedBackups property to False.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the DeletionPolicy attribute to No.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the DeletionPolicy attribute to Retain.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "ABF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T00:51:00.000Z",
        "voteCount": 6,
        "content": "It's duplicated question - see Question no. 2\nAnswer ADF."
      },
      {
        "date": "2023-09-09T17:50:00.000Z",
        "voteCount": 2,
        "content": "Answer ADF\n\nA - https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-rds-now-provides-database-deletion-protection/\n\nD - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\n\nF - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"
      },
      {
        "date": "2022-12-28T11:04:00.000Z",
        "voteCount": 2,
        "content": "Can someone explain why making the DB instance a multi AZ not a good option?  This way if primary is deleted the standby becomes a standalone. Disabling \"Delete automated backups\" only ensures that the database can be restored.  Any thoughts?"
      },
      {
        "date": "2023-03-27T05:14:00.000Z",
        "voteCount": 1,
        "content": "same question here"
      },
      {
        "date": "2023-04-19T08:43:00.000Z",
        "voteCount": 3,
        "content": "Because Multi-AZ will not protect CloudFormation from deleting the instance."
      },
      {
        "date": "2022-10-22T20:51:00.000Z",
        "voteCount": 1,
        "content": "Agree, This is a Duplicate"
      },
      {
        "date": "2022-09-26T11:55:00.000Z",
        "voteCount": 2,
        "content": "Answer ADF."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/amazon/view/80229-exam-aws-certified-database-specialty-topic-1-question-223/",
    "body": "A company has branch offices in the United States and Singapore. The company has a three-tier web application that uses a shared database. The database runs on an Amazon RDS for MySQL DB instance that is hosted in the us-west-2 Region. The application has a distributed front end that is deployed in us-west-2 and in the ap-southeast-1 Region. The company uses this front end as a dashboard that provides statistics to sales managers in each branch office.<br>The dashboard loads more slowly in the Singapore branch office than in the United States branch office. The company needs a solution so that the dashboard loads consistently for users in each location.<br>Which solution will meet these requirements in the MOST operationally efficient way?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the DB instance in us-west-2. Create a new DB instance in ap-southeast-2 from the snapshot. Reconfigure the ap-southeast-1 front-end dashboard to access the new DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS read replica in ap-southeast-1 from the primary DB instance in us-west-2. Reconfigure the ap-southeast-1 front-end dashboard to access the read replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DB instance in ap-southeast-1. Use AWS Database Migration Service (AWS DMS) and change data capture (CDC) to update the new DB instance in ap-southeast-1. Reconfigure the ap-southeast-1 front-end dashboard to access the new DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS read replica in us-west-2, where the primary DB instance resides. Create a read replica in ap-southeast-1 from the read replica in us-west-2. Reconfigure the ap-southeast-1 front-end dashboard to access the read replica in ap-southeast-1."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T11:58:00.000Z",
        "voteCount": 5,
        "content": "B is more logical. C will work but it has operational overhead."
      },
      {
        "date": "2023-09-04T20:08:00.000Z",
        "voteCount": 3,
        "content": "B. Create an RDS read replica in ap-southeast-1 from the primary DB instance"
      },
      {
        "date": "2022-09-05T00:52:00.000Z",
        "voteCount": 2,
        "content": "It's B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/amazon/view/80230-exam-aws-certified-database-specialty-topic-1-question-224/",
    "body": "A company is using an Amazon ElastiCache for Redis cluster to host its online shopping website. Shoppers receive the following error when the website's application queries the cluster:<br><img src=\"/assets/media/exam-media/04237/0014900001.png\" class=\"in-exam-image\"><br>Which solutions will resolve this memory issues with the LEAST amount of effort? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the TTL value for keys on the node.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose a larger node type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTest different values in the parameter group for the maxmemory-policy parameter to find the ideal value to use.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor the EngineCPUUtilization Amazon CloudWatch metric. Create an AWS Lambda function to delete keys on nodes when a threshold is reached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the TTL value for keys on the node."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T00:58:00.000Z",
        "voteCount": 5,
        "content": "ABC - https://aws.amazon.com/premiumsupport/knowledge-center/oom-command-not-allowed-redis/"
      },
      {
        "date": "2023-09-05T22:00:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/oom-command-not-allowed-redis"
      },
      {
        "date": "2023-04-11T06:38:00.000Z",
        "voteCount": 2,
        "content": "ABC are correct\nhttps://repost.aws/knowledge-center/oom-command-not-allowed-redis"
      },
      {
        "date": "2022-12-18T22:17:00.000Z",
        "voteCount": 2,
        "content": "To resolve this error and to prevent clients from receiving OOM command not allowed error messages, do some combination of the following:\n\nSet a TTL value for keys on your node.\nUpdate the parameter group to use a different maxmemory-policy parameter.\nDelete some existing keys manually to free up memory.\nChoose a larger node type."
      },
      {
        "date": "2022-09-26T12:01:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/oom-command-not-allowed-redis/"
      },
      {
        "date": "2022-09-26T12:01:00.000Z",
        "voteCount": 2,
        "content": "ABC is answer. https://aws.amazon.com/premiumsupport/knowledge-center/oom-command-not-allowed-redis/"
      },
      {
        "date": "2022-09-16T19:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is BCE"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/amazon/view/80232-exam-aws-certified-database-specialty-topic-1-question-225/",
    "body": "A company uses Microsoft SQL Server on Amazon RDS in a Multi-AZ deployment as the database engine for its application. The company was recently acquired by another company. A database specialist must rename the database to follow a new naming standard.<br>Which combination of steps should the database specialist take to rename the database? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off automatic snapshots for the DB instance. Rename the database with the rdsadmin.dbo.rds_modify_db_name stored procedure. Turn on the automatic snapshots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off Multi-AZ for the DB instance. Rename the database with the rdsadmin.dbo.rds_modify_db_name stored procedure. Turn on Multi-AZ Mirroring.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete all existing snapshots for the DB instance. Use the rdsadmin.dbo.rds_modify_db_name stored procedure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the application with the new database connection string.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the DNS record for the DB instance."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-05T22:09:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.RenamingDB.html"
      },
      {
        "date": "2023-04-11T06:42:00.000Z",
        "voteCount": 3,
        "content": "BD is correct\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.RenamingDB.html"
      },
      {
        "date": "2023-03-28T00:09:00.000Z",
        "voteCount": 1,
        "content": "Option B is incorrect because turning off Multi-AZ for the DB instance is not necessary to rename the database."
      },
      {
        "date": "2022-09-26T12:04:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.RenamingDB.html"
      },
      {
        "date": "2022-09-14T02:55:00.000Z",
        "voteCount": 2,
        "content": "BD is correct\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.RenamingDB.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.RenamingDB.html"
      },
      {
        "date": "2022-09-14T02:55:00.000Z",
        "voteCount": 3,
        "content": "BD is correct\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.RenamingDB.html"
      },
      {
        "date": "2022-09-05T01:00:00.000Z",
        "voteCount": 3,
        "content": "Agree, it's B and D."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/amazon/view/80233-exam-aws-certified-database-specialty-topic-1-question-226/",
    "body": "A company hosts an on-premises Microsoft SQL Server Enterprise edition database with Transparent Data Encryption (TDE) enabled. The database is 20 TB in size and includes sparse tables. The company needs to migrate the database to Amazon RDS for SQL Server during a maintenance window that is scheduled for an upcoming weekend. Data-at-rest encryption must be enabled for the target DB instance.<br>Which combination of steps should the company take to migrate the database to AWS in the MOST operationally efficient manner? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate from the on-premises source database to the RDS for SQL Server target database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable TDE. Create a database backup without encryption. Copy the backup to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the backup to the RDS for SQL Server DB instance. Enable TDE for the RDS for SQL Server DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Snowball Edge device. Copy the database backup to the device. Send the device to AWS. Restore the database from Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the data with client-side encryption before transferring the data to Amazon RDS."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T12:16:00.000Z",
        "voteCount": 7,
        "content": "A - DMS is out of question as it doesn't support sparse tables. \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SQLServer.html\n\nD - D will not work as migration is scheduled for an upcoming weekend and snowball takes time for shipping. \n\nE - E will come into picture if using DMS and we can't use DMS. \n\nB and C is correct. Though as per latest updates disabling TDE is no longer required. \n\nhttps://aws.amazon.com/blogs/database/migrate-tde-enabled-sql-server-databases-to-amazon-rds-for-sql-server/"
      },
      {
        "date": "2022-12-08T08:15:00.000Z",
        "voteCount": 4,
        "content": "B and C are correct. But with the new release, we don't have to disable TDE. \nhttps://aws.amazon.com/blogs/database/migrate-tde-enabled-sql-server-databases-to-amazon-rds-for-sql-server/"
      },
      {
        "date": "2022-09-21T18:19:00.000Z",
        "voteCount": 1,
        "content": "DMS doesn't support sparse tables.   \nhttps://aws.amazon.com/blogs/database/migrate-tde-enabled-sql-server-databases-to-amazon-rds-for-sql-server/"
      },
      {
        "date": "2022-09-20T10:55:00.000Z",
        "voteCount": 3,
        "content": "B &amp; C, TDE needs certificate created and installed."
      },
      {
        "date": "2022-09-05T16:17:00.000Z",
        "voteCount": 1,
        "content": "A is wrong because Limitations on using SQL Server as a source for AWS DMS: The SQL Server endpoint doesn't support the use of sparse tables.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SQLServer.html"
      },
      {
        "date": "2022-09-05T01:08:00.000Z",
        "voteCount": 1,
        "content": "A and E, any thoughts?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/amazon/view/80240-exam-aws-certified-database-specialty-topic-1-question-227/",
    "body": "A database specialist wants to ensure that an Amazon Aurora DB cluster is always automatically upgraded to the most recent minor version available. Noticing that there is a new minor version available, the database specialist has issues an AWS CLI command to enable automatic minor version updates. The command runs successfully, but checking the Aurora DB cluster indicates that no update to the Aurora version has been made.<br>What might account for this? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe new minor version has not yet been designated as preferred and requires a manual upgrade.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfiguring automatic upgrades using the AWS CLI is not supported. This must be enabled expressly using the AWS Management Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApplying minor version upgrades requires sufficient free space.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe AWS CLI command did not include an apply-immediately parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAurora has detected a breaking change in the new minor version and has automatically rejected the upgrade."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-04T10:38:00.000Z",
        "voteCount": 5,
        "content": "Key is \"automatically upgraded.\"\n\n\"When Amazon RDS designates a minor engine version as the preferred minor engine version, each database that meets both of the following conditions is upgraded to the minor engine version automatically\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Upgrading.html"
      },
      {
        "date": "2023-04-11T05:07:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS designates a minor engine version as the preferred minor engine version when the following conditions are met:\n\nThe database is running a minor version of the DB engine that is lower than the preferred minor engine version.\n\nThe database has auto minor version upgrade enabled."
      },
      {
        "date": "2023-12-16T06:45:00.000Z",
        "voteCount": 1,
        "content": "When Amazon RDS designates a minor engine version as the preferred minor engine version,\neach database that meets both of the following conditions is upgraded to the minor engine\nversion automatically.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Upgrad\ning.html\nCall the modify-db-instance Amazon CLI command. Specify the name of your DB instance for the\n--db-instance-identifier option and true for the --auto-minor-version-upgrade option. Optionally,\nspecify the --apply-immediately option to immediately enable this setting for your DB instance.\nRun a separate modify-db-instance command for each DB instance in the cluster.\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Patching.html#AuroraMySQL.Updates.AMVU"
      },
      {
        "date": "2023-05-23T07:03:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Upgrading.html"
      },
      {
        "date": "2023-03-04T09:23:00.000Z",
        "voteCount": 1,
        "content": "I will go with C&amp;D too.\nhttps://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-to-major-and-minor-versions-of-postgresql/"
      },
      {
        "date": "2023-01-08T12:01:00.000Z",
        "voteCount": 1,
        "content": "C cannot be right as STORAGE_FULL is related to Major version upgrade as it consumes its upgrade binaries consumes additional space. but its not the case with Minor I guess."
      },
      {
        "date": "2023-01-08T12:12:00.000Z",
        "voteCount": 1,
        "content": "Also refer this, A is the right option as auto minor upgrade happens only when absolutely necessary and you can view that using the CLI if its ready or not. in that you only can manually upgrade.. https://repost.aws/questions/QUT0JuX6IhSAyXaSdQK5SW3A/rds-postgre-sql-minor-version-upgrade-wasnt-performed"
      },
      {
        "date": "2022-10-18T16:37:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Upgrading.html"
      },
      {
        "date": "2023-01-08T12:14:00.000Z",
        "voteCount": 1,
        "content": "right it clarifies A is right .. When Amazon RDS designates a minor engine version as the preferred minor engine version, each database that meets both of the following conditions is upgraded to the minor engine version automatically:"
      },
      {
        "date": "2023-01-08T12:15:00.000Z",
        "voteCount": 1,
        "content": "The database is running a minor version of the DB engine that is lower than the preferred minor engine version."
      },
      {
        "date": "2022-10-08T22:29:00.000Z",
        "voteCount": 1,
        "content": "Cx : The command runs successfully\nCall the modify-db-instance Amazon CLI command. Specify the name of your DB instance for the --db-instance-identifier option and true for the --auto-minor-version-upgrade option. Optionally, specify the --apply-immediately option to immediately enable this setting for your DB instance. Run a separate modify-db-instance command for each DB instance in the cluster.\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.Patching.html#AuroraMySQL.Updates.AMVU"
      },
      {
        "date": "2022-10-08T22:23:00.000Z",
        "voteCount": 1,
        "content": "Cx : The command runs successfully"
      },
      {
        "date": "2022-09-05T01:14:00.000Z",
        "voteCount": 1,
        "content": "Agree, CD"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/amazon/view/80241-exam-aws-certified-database-specialty-topic-1-question-228/",
    "body": "A security team is conducting an audit for a financial company. The security team discovers that the database credentials of an Amazon RDS for MySQL DB instance are hardcoded in the source code. The source code is stored in a shared location for automatic deployment and is exposed to all users who can access the location.<br>A database specialist must use encryption to ensure that the credentials are not visible in the source code.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Key Management Service (AWS KMS) key to encrypt the most recent database backup. Restore the backup as a new database to activate encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the source code to access the credentials in an AWS Systems Manager Parameter Store secure string parameter that is encrypted by AWS Key Management Service (AWS KMS). Access the code with calls to Systems Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in an AWS Systems Manager Parameter Store secure string parameter that is encrypted by AWS Key Management Service (AWS KMS). Access the credentials with calls to Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Key Management Service (AWS KMS) key to encrypt the DB instance at rest. Activate RDS encryption in transit by using SSL certificates."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-10T06:07:00.000Z",
        "voteCount": 4,
        "content": "We store parameters, not code"
      },
      {
        "date": "2023-03-11T23:19:00.000Z",
        "voteCount": 2,
        "content": "Option B: Storing the source code to access the credentials in an AWS Systems Manager Parameter Store secure string parameter that is encrypted by AWS KMS is the best solution. Systems Manager Parameter Store provides a centralized location to store and manage sensitive data, such as database credentials. By using a secure string parameter encrypted by AWS KMS, the credentials will not be visible in the source code. Calls to Systems Manager can then be used to retrieve the credentials when needed.\n\nOption C: Storing the credentials in an AWS Systems Manager Parameter Store secure string parameter that is encrypted by AWS KMS is similar to option B, but it does not address the issue of the credentials being hardcoded in the source code. The source code will still contain the code to access the credentials, which defeats the purpose of encryption."
      },
      {
        "date": "2023-04-08T03:16:00.000Z",
        "voteCount": 1,
        "content": "Not B.\nOption B is not a complete solution as it only stores the source code to access the credentials in the AWS Systems Manager Parameter Store. It does not encrypt the credentials themselves."
      },
      {
        "date": "2023-05-21T14:42:00.000Z",
        "voteCount": 3,
        "content": "You don't seem to have any realworld experience... Nobody stores source code in Parameter Store... Not just that, you completely miss the point of option C (the correct one) too"
      },
      {
        "date": "2022-09-26T12:23:00.000Z",
        "voteCount": 3,
        "content": "only creds in system manager secure parameter."
      },
      {
        "date": "2022-09-05T01:16:00.000Z",
        "voteCount": 4,
        "content": "It's C, only credentials in System Manager Store."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/amazon/view/80243-exam-aws-certified-database-specialty-topic-1-question-229/",
    "body": "A gaming company is evaluating Amazon ElastiCache as a solution to manage player leaderboards. Millions of players around the world will complete in annual tournaments. The company wants to implement an architecture that is highly available. The company also wants to ensure that maintenance activities have minimal impact on the availability of the gaming platform.<br>Which combination of steps should the company take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an ElastiCache for Redis cluster with read replicas and Multi-AZ enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an ElastiCache for Memcached global datastore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a single-node ElastiCache for Redis cluster with automatic backups enabled. In the event of a failure, create a new cluster and restore data from the most recent backup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default maintenance window to apply any required system changes and mandatory updates as soon as they are available.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose a preferred maintenance window at the time of lowest usage to apply any required changes and mandatory updates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T01:18:00.000Z",
        "voteCount": 7,
        "content": "It's AE - https://aws.amazon.com/blogs/database/configuring-amazon-elasticache-for-redis-for-higher-availability/"
      },
      {
        "date": "2024-01-02T16:19:00.000Z",
        "voteCount": 2,
        "content": "Memcached doesn't support High availability\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html"
      },
      {
        "date": "2023-05-10T11:44:00.000Z",
        "voteCount": 1,
        "content": "AE... Gaming leaderboard &gt; Redis"
      },
      {
        "date": "2022-09-26T12:25:00.000Z",
        "voteCount": 2,
        "content": "AE is correct"
      },
      {
        "date": "2022-12-16T20:29:00.000Z",
        "voteCount": 1,
        "content": "Why not BE? What prompted you to choose Redis instead of MemcacheD?"
      },
      {
        "date": "2023-04-26T13:14:00.000Z",
        "voteCount": 2,
        "content": "Memcached doesn't support HA"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/amazon/view/80245-exam-aws-certified-database-specialty-topic-1-question-230/",
    "body": "A company's database specialist implements an AWS Database Migration Service (AWS DMS) task for change data capture (CDC) to replicate data from an on- premises Oracle database to Amazon S3. When usage of the company's application increases, the database specialist notices multiple hours of latency with the<br>CDC.<br>Which solutions will reduce this latency? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the DMS task to run in full large binary object (LOB) mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the DMS task to run in limited large binary object (LOB) mode.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Multi-AZ replication instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad tables in parallel by creating multiple replication instances for sets of tables that participate in common transactions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate tables in parallel by creating multiple DMS tasks for sets of tables that do not participate in common transactions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-03T10:08:00.000Z",
        "voteCount": 2,
        "content": "B: Reducing LOB transfer helps, yes.\nE: \"If you have sets of tables that don't participate in common transactions, then divide your migration into multiple tasks. This can help increase performance.\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"
      },
      {
        "date": "2023-07-15T10:18:00.000Z",
        "voteCount": 1,
        "content": "DMS does to support writing to S3. Correct?"
      },
      {
        "date": "2023-07-15T10:19:00.000Z",
        "voteCount": 1,
        "content": "it does\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"
      },
      {
        "date": "2022-12-30T06:28:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dms-high-source-latency/"
      },
      {
        "date": "2022-09-26T12:30:00.000Z",
        "voteCount": 4,
        "content": "A is incorrect. C is not applicable. D is incorrect."
      },
      {
        "date": "2022-09-14T03:08:00.000Z",
        "voteCount": 2,
        "content": "BE is correct"
      },
      {
        "date": "2022-09-05T01:21:00.000Z",
        "voteCount": 3,
        "content": "BE would help, any thoughts?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/amazon/view/80836-exam-aws-certified-database-specialty-topic-1-question-231/",
    "body": "An ecommerce company is running AWS Database Migration Service (AWS DMS) to replicate an on-premises Microsoft SQL Server database to Amazon RDS for SQL Server. The company has set up an AWS Direct Connect connection from its on-premises data center to AWS. During the migration, the company's security team receives an alarm that is related to the migration. The security team mandates that the DMS replication instance must not be accessible from public<br>IP addresses.<br>What should a database specialist do to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a VPN connection to encrypt the traffic over the Direct Connect connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DMS replication instance by disabling the publicly accessible option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the DMS replication instance. Recreate the DMS replication instance with the publicly accessible option disabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new replication VPC subnet group with private subnets. Modify the DMS replication instance by selecting the newly created VPC subnet group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T10:55:00.000Z",
        "voteCount": 10,
        "content": "Option C is correct. Option D has a slight caveat. While, you could create a new DMS subnet group (within DMS console) with private subnets however, you can't attach this newly subnet group to an existing DMS instance. This isn't supported and I have tested that in AWS console. What this note (https://repost.aws/knowledge-center/dms-disable-public-access) is describing is slightly different than some of us are interpreting and choosing option D. As per this note, you could remove the public subnets and choose only the private subnets of an EXISTING SUBNET Group and I've tried this. I modified the existing subnet group of an existing DMS instance. I removed all public subnets and chose only 2 private subnets. This worked however, option D is not talking about modifying an existing subnet group. It is saying that we should create a new subnet group and associate that with existing DMS instance which I've already mentioned above is an unsupported option at this moment. Therefore option D is incorrect."
      },
      {
        "date": "2022-09-26T16:41:00.000Z",
        "voteCount": 9,
        "content": "D is most easy way to fix the issue.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dms-disable-public-access/\n\nTo disable public access to your replication instance, delete the replication instance and then recreate it. Before you can delete a replication instance, you must delete all the tasks that use the replication instance.\n\nInstead of recreating the replication instance, you can change the subnets that are in the subnet group that is associated with the replication instance to private subnets."
      },
      {
        "date": "2024-04-13T22:01:00.000Z",
        "voteCount": 1,
        "content": "I vote D"
      },
      {
        "date": "2024-01-04T07:02:00.000Z",
        "voteCount": 1,
        "content": "it's C \nhttps://repost.aws/knowledge-center/dms-disable-public-access"
      },
      {
        "date": "2023-05-23T08:09:00.000Z",
        "voteCount": 1,
        "content": "Changing the instance from Public to Private would require deleting all the taskj, delete the replication instance, recreate the instance, recreate the tasks...\n\nFrom the Knowledge Center:\n\"Instead of recreating the replication instance, you can change the subnets that are in the subnet group that is associated with the replication instance to private subnets.\""
      },
      {
        "date": "2023-03-04T09:38:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dms-disable-public-access/ - C is the answer"
      },
      {
        "date": "2023-02-14T08:19:00.000Z",
        "voteCount": 1,
        "content": "I would try D, and looking the comments shouldn't be any problem."
      },
      {
        "date": "2022-12-30T08:59:00.000Z",
        "voteCount": 1,
        "content": "The question is confusing because it says that the \"replication instance must not be accessible from public\" which means it should be in a private subnet group which also means that the instance must be dropped and re-created in the private subnet group but the option doesn't say to re-create the instance.  \nIP addresses."
      },
      {
        "date": "2022-12-28T09:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is D:  Instead of recreating the replication instance, you can change the subnets that are in the subnet group that is associated with the replication instance to private subnets. A private subnet is a subnet that isn't routed to an internet gateway. Instances in a private subnet can't communicate with a public IP address, even if they have a public IP address. For more information, see Setting up a network for a replication instance. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html"
      },
      {
        "date": "2022-12-28T05:35:00.000Z",
        "voteCount": 1,
        "content": "D as it is indeed possible to change subnet group to private according to https://aws.amazon.com/premiumsupport/knowledge-center/dms-disable-public-access\nActually, C and D are correct, but D could be easier"
      },
      {
        "date": "2022-11-22T12:50:00.000Z",
        "voteCount": 1,
        "content": "C. \nD is wrong . We cannot modify DMS instance to assign a new subnet group with private subnets ."
      },
      {
        "date": "2022-12-28T05:33:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dms-disable-public-access/ says:\n\"Instead of recreating the replication instance, you can change the subnets that are in the subnet group that is associated with the replication instance to private subnets. A private subnet is a subnet that isn't routed to an internet gateway. Instances in a private subnet can't communicate with a public IP address, even if they have a public IP address.\""
      },
      {
        "date": "2022-10-15T12:14:00.000Z",
        "voteCount": 2,
        "content": "C will ensure there will be no public access"
      },
      {
        "date": "2022-10-09T00:01:00.000Z",
        "voteCount": 2,
        "content": "Dx : Not create VPC subnet. Change the subnets that are in the subnet group that is associated with the replication instance to private subnets."
      },
      {
        "date": "2022-09-27T09:07:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dms-disable-public-access/"
      },
      {
        "date": "2022-09-27T08:54:00.000Z",
        "voteCount": 1,
        "content": "From link provided, obviously C or D would work.  I'd vote D, as it would be an easier fix.  Kind of a dumb question, as there are two legit answers."
      },
      {
        "date": "2022-09-14T03:12:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-09-09T06:35:00.000Z",
        "voteCount": 4,
        "content": "delete and create again with new publicly accessible option. https://aws.amazon.com/premiumsupport/knowledge-center/dms-disable-public-access/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/amazon/view/80837-exam-aws-certified-database-specialty-topic-1-question-232/",
    "body": "A company is using an Amazon Aurora MySQL database with Performance Insights enabled. A database specialist is checking Performance Insights and observes an alert message that starts with the following phrase: `Performance Insights is unable to collect SQL Digest statistics on new queries`\u00a6`<br>Which action will resolve this alert message?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTruncate the events_statements_summary_by_digest table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the AWS Key Management Service (AWS KMS) key that is used to enable Performance Insights.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the value for the performance_schema parameter in the parameter group to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable and reenable Performance Insights to be effective in the next maintenance window."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-08T12:39:00.000Z",
        "voteCount": 1,
        "content": "C is opposite. performance_schema parameter must be set to 0"
      },
      {
        "date": "2023-01-08T12:40:00.000Z",
        "voteCount": 1,
        "content": "For automatic management. so that it can automatically truncate the table"
      },
      {
        "date": "2023-01-08T12:40:00.000Z",
        "voteCount": 1,
        "content": "if not truncate table manually events_statements_summary_by_digest"
      },
      {
        "date": "2022-11-06T13:41:00.000Z",
        "voteCount": 1,
        "content": "In this situation, MariaDB and MySQL don't track SQL queries. To address this issue, Performance Insights automatically truncates the digest table when both of the following conditions are met:\n\n    The table is full.\n\n    Performance Insights manages the Performance Schema automatically.\n\n    For automatic management, the performance_schema parameter must be set to 0 and the Source must not be set to user. If Performance Insights isn't managing the Performance Schema automatically, see Turning on the Performance Schema for Performance Insights on Amazon RDS for MariaDB or MySQL."
      },
      {
        "date": "2022-09-27T09:09:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.AnalyzeDBLoad.AdditionalMetrics.MySQL.html"
      },
      {
        "date": "2022-09-10T06:30:00.000Z",
        "voteCount": 4,
        "content": "A is correct \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.AnalyzeDBLoad.AdditionalMetrics.MySQL.html"
      },
      {
        "date": "2022-09-07T01:56:00.000Z",
        "voteCount": 2,
        "content": "Agree, it's A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/amazon/view/80838-exam-aws-certified-database-specialty-topic-1-question-233/",
    "body": "A bike rental company operates an application to track its bikes. The application receives location and condition data from bike sensors. The application also receives rental transaction data from the associated mobile app.<br>The application uses Amazon DynamoDB as its database layer. The company has configured DynamoDB with provisioned capacity set to 20% above the expected peak load of the application. On an average day, DynamoDB used 22 billion read capacity units (RCUs) and 60 billion write capacity units (WCUs). The application is running well. Usage changes smoothly over the course of the day and is generally shaped like a bell curve. The timing and magnitude of peaks vary based on the weather and season, but the general shape is consistent.<br>Which solution will provide the MOST cost optimization of the DynamoDB database layer?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the DynamoDB tables to use on-demand capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Auto Scaling and configure time-based scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB capacity-based auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB Accelerator (DAX)."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-10T08:28:00.000Z",
        "voteCount": 1,
        "content": "C looks correct. Predictable workload"
      },
      {
        "date": "2022-10-31T20:12:00.000Z",
        "voteCount": 3,
        "content": "Answer C Capacity based or target scaling\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
      },
      {
        "date": "2022-09-14T03:13:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-09-07T01:57:00.000Z",
        "voteCount": 4,
        "content": "Vote for C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/amazon/view/80839-exam-aws-certified-database-specialty-topic-1-question-234/",
    "body": "A company has a quarterly customer survey. The survey uses an Amazon EC2 instance that is hosted in a public subnet to host a customer survey website. The company uses an Amazon RDS DB instance that is hosted in a private subnet in the same VPC to store the survey results.<br>The company takes a snapshot of the DB instance after a survey is complete, deletes the DB instance, and then restores the DB instance from the snapshot when the survey needs to be conducted again. A database specialist discovers that the customer survey website times out when it attempts to establish a connection to the restored DB instance.<br>What is the root cause of this problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe VPC peering connection has not been configured properly for the EC2 instance to communicate with the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe route table of the private subnet that hosts the DB instance does not have a NAT gateway configured for communication with the EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe public subnet that hosts the EC2 instance does not have an internet gateway configured for communication with the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe wrong security group was associated with the new DB instance when it was restored from the snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T01:59:00.000Z",
        "voteCount": 8,
        "content": "Agree with D."
      },
      {
        "date": "2023-12-13T20:30:00.000Z",
        "voteCount": 1,
        "content": "D. The wrong security group was associated with the new DB instance when it was restored from the snapshot."
      },
      {
        "date": "2023-10-18T05:52:00.000Z",
        "voteCount": 3,
        "content": "what a weird customer, why would anyone practice this?"
      },
      {
        "date": "2022-09-14T03:14:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/amazon/view/80849-exam-aws-certified-database-specialty-topic-1-question-235/",
    "body": "A company wants to improve its ecommerce website on AWS. A database specialist decides to add Amazon ElastiCache for Redis in the implementation stack to ease the workload off the database and shorten the website response times. The database specialist must also ensure the ecommerce website is highly available within the company's AWS Region.<br>How should the database specialist deploy ElastiCache to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an ElastiCache for Redis cluster using the AWS CLI with the -cluster-enabled switch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an ElastiCache for Redis cluster and select read replicas in different Availability Zones.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch two ElastiCache for Redis clusters in two different Availability Zones. Configure Redis streams to replicate the cache from the primary cluster to another.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an ElastiCache cluster in the primary Availability Zone and restore the cluster's snapshot to a different Availability Zone during disaster recovery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-09T19:12:00.000Z",
        "voteCount": 1,
        "content": "B. read replicas in multi-AZ. \n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html"
      },
      {
        "date": "2023-03-11T23:45:00.000Z",
        "voteCount": 3,
        "content": "B. Launch an ElastiCache for Redis cluster and select read replicas in different Availability Zones.\n\nBy launching an ElastiCache for Redis cluster with read replicas in different Availability Zones, the database specialist can ensure high availability of the ecommerce website within the company's AWS Region. ElastiCache automatically provisions and maintains the read replicas in different Availability Zones to enable faster read access and to provide high availability of the cache. This architecture helps to reduce latency and improve website response times, while also ensuring that the ecommerce website is highly available in case of a failure in one Availability Zone."
      },
      {
        "date": "2022-12-02T10:45:00.000Z",
        "voteCount": 2,
        "content": "B look great"
      },
      {
        "date": "2022-10-09T00:37:00.000Z",
        "voteCount": 1,
        "content": "You can enable Multi-AZ only on Redis (cluster mode disabled) clusters that have at least one available read replica. Clusters without read replicas do not provide high availability or fault tolerance."
      },
      {
        "date": "2022-10-04T07:30:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-09-07T02:59:00.000Z",
        "voteCount": 3,
        "content": "It's B - https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/amazon/view/80851-exam-aws-certified-database-specialty-topic-1-question-236/",
    "body": "An online gaming company is using an Amazon DynamoDB table in on-demand mode to store game scores. After an intensive advertisement campaign in South<br>America, the average number of concurrent users rapidly increases from 100,000 to 500,000 in less than 10 minutes every day around 5 PM.<br>The on-call software reliability engineer has observed that the application logs contain a high number of DynamoDB throttling exceptions caused by game score insertions around 5 PM. Customer service has also reported that several users are complaining about their scores not being registered.<br>How should the database administrator remediate this issue at the lowest cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable auto scaling and set the target usage rate to 90%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the table to provisioned mode and enable auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the table to provisioned mode and set the throughput to the peak value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a DynamoDB Accelerator cluster and use it to access the DynamoDB table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-23T08:44:00.000Z",
        "voteCount": 4,
        "content": "That is the most complete answer. A looks plausible, but you cannot enable auto-scaling in on-demand mode."
      },
      {
        "date": "2022-12-17T18:46:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/"
      },
      {
        "date": "2022-09-07T03:06:00.000Z",
        "voteCount": 4,
        "content": "B is fine, A is too little buffer with 90% target utilization rate."
      },
      {
        "date": "2022-12-17T18:52:00.000Z",
        "voteCount": 1,
        "content": "Q claims, lowest cost. Opt.A is lower than B. Hence unsure of B as a right Ans."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/amazon/view/80852-exam-aws-certified-database-specialty-topic-1-question-237/",
    "body": "An IT company wants to reduce its database operation costs in its development environment. The company's workflow creates an Amazon Aurora MySQL DB cluster for each development group. The DB clusters are used for only 8 hours a day. The DB clusters can be deleted at the end of a development cycle, which lasts 2 weeks.<br>Which solution will meet these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation templates. Deploy a stack with a DB cluster for each development group. Delete the stack at the end of each development cycle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Aurora cloning feature. Deploy a single development and test Aurora DB instance. Create clone instances for the development groups. Delete the clones at the end of each development cycle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora Replicas. From the primary writer instance, create read replicas for each development group. Promote each read replica to a standalone DB cluster Delete the standalone DB cluster at the end of each development cycle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora Serverless. Restore a current Aurora snapshot to an Aurora Serverless cluster for each development group. Select the option to pause the compute capacity on the cluster after a specified amount of time with no activity. Delete the Aurora Serverless cluster at the end of each development cycle.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T23:08:00.000Z",
        "voteCount": 6,
        "content": "Answer is D. Looking for Cost Effective option... Go with Aurora serverless with pause option.\nPause after inactivity \u2013 The amount of time with no database traffic to scale to zero processing capacity. When database traffic resumes, Aurora automatically resumes processing capacity and scales to handle the traffic."
      },
      {
        "date": "2022-11-08T01:15:00.000Z",
        "voteCount": 2,
        "content": "It will be B, read question 59 discussion for clarification."
      },
      {
        "date": "2022-11-08T01:15:00.000Z",
        "voteCount": 1,
        "content": "single dev &amp; test instanace\n-&gt; Aurora clone\n-&gt; delete clones at end of dev cycle\n\nAmazon Aurora now allows you to create clones between Aurora Serverless v1 and provisioned Aurora DB clusters to enable quick sharing of data."
      },
      {
        "date": "2022-09-14T03:17:00.000Z",
        "voteCount": 1,
        "content": "D is correct, the question has been repeated in the set"
      },
      {
        "date": "2022-09-07T03:07:00.000Z",
        "voteCount": 4,
        "content": "It's D - Aurora Serverless with option to pause."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/amazon/view/80854-exam-aws-certified-database-specialty-topic-1-question-238/",
    "body": "A gaming company uses Amazon Aurora Serverless for one of its internal applications. The company's developers use Amazon RDS Data API to work with the<br>Aurora Serverless DB cluster. After a recent security review, the company is mandating security enhancements. A database specialist must ensure that access to<br>RDS Data API is private and never passes through the public internet.<br>What should the database specialist do to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Aurora Serverless cluster by selecting a VPC with private subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Aurora Serverless cluster by unchecking the publicly accessible option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an interface VPC endpoint that uses AWS PrivateLink for RDS Data API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway VPC endpoint for RDS Data API."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T03:09:00.000Z",
        "voteCount": 10,
        "content": "C can be confirmed - https://aws.amazon.com/about-aws/whats-new/2020/02/amazon-rds-data-api-now-supports-aws-privatelink/"
      },
      {
        "date": "2023-10-11T13:12:00.000Z",
        "voteCount": 1,
        "content": "D is gateway VPC is for Dynamo DB"
      },
      {
        "date": "2023-10-07T09:15:00.000Z",
        "voteCount": 1,
        "content": "interface VPC end point\nAWS Private link\n\nis this all part of the Database syllabus? gosh I don't know how many things we need to learn"
      },
      {
        "date": "2023-03-11T23:54:00.000Z",
        "voteCount": 2,
        "content": "To meet the requirement of ensuring that access to RDS Data API is private and never passes through the public internet, the database specialist should create an interface VPC endpoint that uses AWS PrivateLink for RDS Data API. AWS PrivateLink provides secure access to services that are hosted on AWS, and it keeps all the network traffic within the AWS network. By creating an interface VPC endpoint for RDS Data API, the database specialist can ensure that all traffic between the application and the Aurora Serverless cluster is routed through this endpoint, and never goes over the public internet.\n\nTherefore, the correct answer is option C: Create an interface VPC endpoint that uses AWS PrivateLink for RDS Data API."
      },
      {
        "date": "2022-12-18T23:10:00.000Z",
        "voteCount": 1,
        "content": "C. \nYou can now use AWS PrivateLink to privately access Amazon RDS Data API for Aurora Serverless from your Amazon Virtual Private Cloud (Amazon VPC) without using public IPs, and without requiring the traffic to traverse across the Internet."
      },
      {
        "date": "2022-09-14T03:17:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/amazon/view/80855-exam-aws-certified-database-specialty-topic-1-question-239/",
    "body": "A startup company in the travel industry wants to create an application that includes a personal travel assistant to display information for nearby airports based on user location. The application will use Amazon DynamoDB and must be able to access and display attributes such as airline names, arrival times, and flight numbers. However, the application must not be able to access or display pilot names or passenger counts.<br>Which solution will meet these requirements MOST cost-effectively?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a proxy tier between the application and DynamoDB to regulate access to specific tables, items, and attributes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse IAM policies with a combination of IAM conditions and actions to implement fine-grained access control.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB resource policies to regulate access to specific tables, items, and attributes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to extract only allowed attributes from tables based on user profiles."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T03:11:00.000Z",
        "voteCount": 8,
        "content": "It's B - https://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/"
      },
      {
        "date": "2022-09-14T03:18:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/amazon/view/80857-exam-aws-certified-database-specialty-topic-1-question-240/",
    "body": "A large IT hardware manufacturing company wants to deploy a MySQL database solution in the AWS Cloud. The solution should quickly create copies of the company's production databases for test purposes. The solution must deploy the test databases in minutes, and the test data should match the latest production data as closely as possible. Developers must also be able to make changes in the test database and delete the instances afterward.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage Amazon RDS for MySQL with write-enabled replicas running on Amazon EC2. Create the test copies using a mysqidump backup from the RDS for MySQL DB instances and importing them into the new EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage Amazon Aurora MySQL. Use database cloning to create multiple test copies of the production DB clusters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage Amazon Aurora MySQL. Restore previous production DB instance snapshots into new test copies of Aurora MySQL DB clusters to allow them to make changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage Amazon RDS for MySQL. Use database cloning to create multiple developer copies of the production DB instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T20:10:00.000Z",
        "voteCount": 1,
        "content": "B. Amazon Aurora MySQL, database cloning"
      },
      {
        "date": "2023-09-07T20:09:00.000Z",
        "voteCount": 2,
        "content": "B. Amazon Aurora MySQL, database cloning \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\n\"Aurora cloning is especially useful for quickly setting up test environments using your production data, without risking data corruption. You can use clones for many types of applications, such as the following:\nExperiment with potential changes (schema changes and parameter group changes, for example) to assess all impacts.\nRun workload-intensive operations, such as exporting data or running analytical queries on the clone.\nCreate a copy of your production DB cluster for development, testing, or other purposes.\""
      },
      {
        "date": "2022-09-14T03:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-07T03:13:00.000Z",
        "voteCount": 4,
        "content": "Quick and easy deploy - Aurora with cloning feature. It's B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/amazon/view/80858-exam-aws-certified-database-specialty-topic-1-question-241/",
    "body": "A company's application development team wants to share an automated snapshot of its Amazon RDS database with another team. The database is encrypted with a custom AWS Key Management Service (AWS KMS) key under the \"WeShare\" AWS account. The application development team needs to share the DB snapshot under the \"WeReceive\" AWS account.<br>Which combination of actions must the application development team take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd access from the \"WeReceive\" account to the custom AWS KMS key policy of the sharing team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a copy of the DB snapshot, and set the encryption option to disable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the DB snapshot by setting the DB snapshot visibility option to public.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a copy of the DB snapshot, and set the encryption option to enable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the DB snapshot by using the default AWS KMS encryption key."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-24T00:52:00.000Z",
        "voteCount": 3,
        "content": "Cannot be C because you cannot share an automated snapshot with another account\n\nhttps://repost.aws/knowledge-center/rds-snapshots-share-account\nYou can't share automated Amazon RDS snapshots with other AWS accounts. To share an automated snapshot, copy the snapshot to make a manual version, and then share that copy."
      },
      {
        "date": "2023-02-11T10:08:00.000Z",
        "voteCount": 1,
        "content": "AD  without doubt"
      },
      {
        "date": "2022-12-01T08:30:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D is the best choice of answer."
      },
      {
        "date": "2022-10-31T21:53:00.000Z",
        "voteCount": 3,
        "content": "Answer AC.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-snapshots-share-account/\n\nOpen the Amazon RDS console.\nChoose Snapshots from the left navigation pane.\nChoose the DB snapshot that you want to copy.\nChoose Actions, and then choose Share Snapshot.\nChoose the DB snapshot visibility:\nPublic allows all AWS accounts to restore a DB instance from your manual DB snapshot.\nPrivate allows only AWS accounts that you specify to restore a DB instance from your manual DB snapshot.\nIn the AWS Account ID field, enter the ID of the AWS account that you want to permit to restore a DB instance from your manual DB snapshot. Then, choose Add."
      },
      {
        "date": "2022-10-31T21:56:00.000Z",
        "voteCount": 2,
        "content": "May be A &amp; D is correct because we need to copy automated snapshot to share with other account"
      },
      {
        "date": "2022-09-07T03:16:00.000Z",
        "voteCount": 3,
        "content": "It's A&amp;D. https://aws.amazon.com/premiumsupport/knowledge-center/rds-snapshots-share-account/"
      },
      {
        "date": "2022-12-18T02:57:00.000Z",
        "voteCount": 1,
        "content": "URL mentioned above, in my opinion points to Ans A&amp;E. Where do we have ref for D??"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/amazon/view/80861-exam-aws-certified-database-specialty-topic-1-question-242/",
    "body": "A company is using Amazon Redshift as its data warehouse solution. The Redshift cluster handles the following types of workloads:<br>\u2711 Real-time inserts through Amazon Kinesis Data Firehose<br>\u2711 Bulk inserts through COPY commands from Amazon S3<br>\u2711 Analytics through SQL queries<br>Recently, the cluster has started to experience performance issues.<br>Which combination of actions should a database specialist take to improve the cluster's performance? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Kinesis Data Firehose delivery stream to stream the data to Amazon S3 with a high buffer size and to load the data into Amazon Redshift by using the COPY command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream real-time data into Redshift temporary tables before loading the data into permanent tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor bulk inserts, split input files on Amazon S3 into multiple files to match the number of slices on Amazon Redshift. Then use the COPY command to load data into Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor bulk inserts, use the parallel parameter in the COPY command to enable multi-threading.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimize analytics SQL queries to use sort keys.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvoid using temporary tables in analytics SQL queries."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-08T02:04:00.000Z",
        "voteCount": 7,
        "content": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/\n\nTip #6: Improving the efficiency of temporary tables\n\nTip #9: Maintaining efficient data loads\nAmazon Redshift best practices suggest using the COPY command to perform data loads of file-based data. \n\nTip #3: Sort key recommendation\nSorting a table on an appropriate sort key can accelerate query performance, especially queries with range-restricted predicates, by requiring fewer table blocks to be read from disk.\n\nHence BCE"
      },
      {
        "date": "2024-01-14T03:09:00.000Z",
        "voteCount": 1,
        "content": "ACE\nNo question about C and E\nFor A vs B... Using B may result in data loss. Temporary tables are better fit for storing temporary results."
      },
      {
        "date": "2023-01-08T13:42:00.000Z",
        "voteCount": 1,
        "content": "there is no mention how temp table is used here to conclude it optimises or not.."
      },
      {
        "date": "2022-10-15T13:13:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/"
      },
      {
        "date": "2022-10-11T08:11:00.000Z",
        "voteCount": 4,
        "content": "Ax:  Kinesis Data Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.\nB: The proper use of temporary tables can significantly improve performance of some ETL operations."
      },
      {
        "date": "2022-09-27T11:54:00.000Z",
        "voteCount": 2,
        "content": "C for sure - from Tip #9 here.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/"
      },
      {
        "date": "2022-09-15T22:36:00.000Z",
        "voteCount": 2,
        "content": "A is wrong because \"Note that in circumstances where data delivery to the destination is falling behind data ingestion into the delivery stream, Kinesis Data Firehose raises the buffer size automatically to catch up and make sure that all data is delivered to the destination.\"\nReference: https://aws.amazon.com/kinesis/data-firehose/faqs/"
      },
      {
        "date": "2022-09-14T03:35:00.000Z",
        "voteCount": 2,
        "content": "ACE is correct"
      },
      {
        "date": "2022-09-07T03:19:00.000Z",
        "voteCount": 1,
        "content": "It's ACE."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/amazon/view/80863-exam-aws-certified-database-specialty-topic-1-question-243/",
    "body": "An information management services company is storing JSON documents on premises. The company is using a MongoDB 3.6 database but wants to migrate to<br>AWS. The solution must be compatible, scalable, and fully managed. The solution also must result in as little downtime as possible during the migration.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Database Migration Service (AWS DMS) replication instance, a source endpoint for MongoDB, and a target endpoint of Amazon DocumentDB (with MongoDB compatibility).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Database Migration Service (AWS DMS) replication instance, a source endpoint for MongoDB, and a target endpoint of a MongoDB image that is hosted on Amazon EC2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the mongodump and mongorestore tools to migrate the data from the source MongoDB deployment to Amazon DocumentDB (with MongoDB compatibility).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the mongodump and mongorestore tools to migrate the data from the source MongoDB deployment to a MongoDB image that is hosted on Amazon EC2."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T17:31:00.000Z",
        "voteCount": 2,
        "content": "A is the Answer\n\nAmazon DocumentDB (with MongoDB compatibility) ----&gt; fully managed\nAWS DMS             ----&gt; with minimal downtime during the migration"
      },
      {
        "date": "2022-12-01T08:25:00.000Z",
        "voteCount": 2,
        "content": "A is the Answer \n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html#docdb-migration-approaches"
      },
      {
        "date": "2022-09-27T12:01:00.000Z",
        "voteCount": 2,
        "content": "DMS is the slowest, but has the least downtime.  From link: https://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html#docdb-migration-approaches"
      },
      {
        "date": "2022-09-14T03:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct. DMS for no downtime"
      },
      {
        "date": "2022-09-07T03:24:00.000Z",
        "voteCount": 3,
        "content": "According to the link, little downtime is achieved with DMS approach, option A. https://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html#docdb-migration-approaches"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/amazon/view/80864-exam-aws-certified-database-specialty-topic-1-question-244/",
    "body": "A company stores critical data for a department in Amazon RDS for MySQL DB instances. The department was closed for 3 weeks and notified a database specialist that access to the RDS DB instances should not be granted to anyone during this time. To meet this requirement, the database specialist stopped all the<br>DB instances used by the department but did not select the option to create a snapshot. Before the 3 weeks expired, the database specialist discovered that users could connect to the database successfully.<br>What could be the reason for this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen stopping the DB instance, the option to create a snapshot should have been selected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen stopping the DB instance, the duration for stopping the DB instance should have been selected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStopped DB instances will automatically restart if the number of attempted connections exceeds the threshold set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStopped DB instances will automatically restart if the instance is not manually started after 7 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T14:02:00.000Z",
        "voteCount": 6,
        "content": "it cost me almost $2000 because of this, it was by SQL Server enterprise which automatically restart for weeks"
      },
      {
        "date": "2023-10-18T07:08:00.000Z",
        "voteCount": 1,
        "content": "sad story :("
      },
      {
        "date": "2024-03-06T03:36:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html#USER_StopInstance.Operation"
      },
      {
        "date": "2023-09-07T17:42:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\n\"By default, you can stop an Amazon RDS database instance for up to seven days at a time. After seven days, the instance restarts so that it doesn't miss any maintenance updates.\nTo stop your instance for more than seven days, you can use Step Functions to automate the workflow without missing a maintenance window.\"\n\nhttps://repost.aws/knowledge-center/rds-stop-seven-days"
      },
      {
        "date": "2022-12-01T08:22:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-stop-seven-days/"
      },
      {
        "date": "2022-09-14T03:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-07T03:26:00.000Z",
        "voteCount": 3,
        "content": "It's D - https://aws.amazon.com/premiumsupport/knowledge-center/rds-stop-seven-days/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/amazon/view/80875-exam-aws-certified-database-specialty-topic-1-question-245/",
    "body": "A company uses an on-premises Microsoft SQL Server database to host relational and JSON data and to run daily ETL and advanced analytics. The company wants to migrate the database to the AWS Cloud. Database specialist must choose one or more AWS services to run the company's workloads.<br>Which solution will meet these requirements in the MOST operationally efficient manner?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift for relational data. Use Amazon DynamoDB for JSON data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift for relational data and JSON data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS for relational data. Use Amazon Neptune for JSON data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift for relational data. Use Amazon S3 for JSON data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-24T20:25:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/super-overview.html. It's B."
      },
      {
        "date": "2023-09-07T18:57:00.000Z",
        "voteCount": 1,
        "content": "B. Use Amazon Redshift \n\n\"By using semistructured data support in Amazon Redshift, you can ingest and store semistructured data in your Amazon Redshift data warehouses. Using the SUPER data type and PartiQL language, Amazon Redshift expands data warehouse capability to integrate with both SQL and NoSQL data sources. This way, Amazon Redshift enables efficient analytics on relational and semistructured stored data such as JSON.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/super-overview.html"
      },
      {
        "date": "2023-03-26T01:54:00.000Z",
        "voteCount": 1,
        "content": "Use the SUPER data type if you need to insert or update small batches of JSON data with low latency\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/super-overview.html#:~:text=Use%20the%20SUPER%20data%20type%20if%20you%20need%20to%20insert%20or%20update%20small%20batches%20of%20JSON%20data%20with%20low%20latency"
      },
      {
        "date": "2023-03-04T13:15:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/json-functions.html  A is the answer . AWS itself is recommending not to use Redshift for JSON . It recommends using SUPER"
      },
      {
        "date": "2023-01-08T15:02:00.000Z",
        "voteCount": 1,
        "content": "requirement is complex analytics which is never supported by DynamoDB..."
      },
      {
        "date": "2022-12-21T16:25:00.000Z",
        "voteCount": 3,
        "content": "B.\nBy using semistructured data support in Amazon Redshift, you can ingest and store semistructured data in your Amazon Redshift data warehouses. Using the SUPER data type and PartiQL language, Amazon Redshift expands data warehouse capability to integrate with both SQL and NoSQL data sources. This way, Amazon Redshift enables efficient analytics on relational and semistructured stored data such as JSON."
      },
      {
        "date": "2022-10-15T13:25:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/super-overview.htm"
      },
      {
        "date": "2022-10-15T14:01:00.000Z",
        "voteCount": 1,
        "content": "\" to run daily ETL and advanced analytics\" is a keyword here"
      },
      {
        "date": "2022-10-04T18:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct I would say. I wouldn't use DynamoDB for advanced analytics, so that takes out A"
      },
      {
        "date": "2022-09-14T03:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-09-07T04:20:00.000Z",
        "voteCount": 2,
        "content": "ETL and advanced analytics - Redshift, JSON Data for sure DynamoDB. It's A."
      },
      {
        "date": "2023-10-04T12:02:00.000Z",
        "voteCount": 3,
        "content": "Don't agree. DynamoDB cannot perform Advanced analytics, as demanded. It would be capable of hosting JSON data, yes, but that's not enough.\nHence, B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/amazon/view/80876-exam-aws-certified-database-specialty-topic-1-question-246/",
    "body": "A company plans to migrate a MySQL-based application from an on-premises environment to AWS. The application performs database joins across several tables and uses indexes for faster query response times. The company needs the database to be highly available with automatic failover.<br>Which solution on AWS will meet these requirements with the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS DB instance with a read replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS Multi-AZ DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Amazon DynamoDB global tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy multiple Amazon RDS DB instances. Use Amazon Route 53 DNS with failover health checks configured."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T04:21:00.000Z",
        "voteCount": 5,
        "content": "High availability = Multi AZ"
      },
      {
        "date": "2023-09-07T19:59:00.000Z",
        "voteCount": 4,
        "content": "B. Deploy an Amazon RDS Multi-AZ \n\nhttps://aws.amazon.com/rds/features/multi-az/\n\n\"Support high availability for your application with automatic database failover that completes as quickly as 60 seconds with zero data loss and no manual intervention.\"\n\n\"In an Amazon RDS Multi-AZ deployment, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention.\""
      },
      {
        "date": "2023-04-12T08:16:00.000Z",
        "voteCount": 1,
        "content": "B: High Availability and Automatic Failover"
      },
      {
        "date": "2022-12-23T08:56:00.000Z",
        "voteCount": 1,
        "content": "B: its MySQL, multiple joins, failover"
      },
      {
        "date": "2022-09-21T19:30:00.000Z",
        "voteCount": 3,
        "content": "High availability = Multi AZ"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/amazon/view/80879-exam-aws-certified-database-specialty-topic-1-question-247/",
    "body": "A social media company is using Amazon DynamoDB to store user profile data and user activity data. Developers are reading and writing the data, causing the size of the tables to grow significantly. Developers have started to face performance bottlenecks with the tables.<br>Which solution should a database specialist recommend to read items the FASTEST without consuming all the provisioned throughput for the tables?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Scan API operation in parallel with many workers to read all the items. Use the Query API operation to read multiple items that have a specific partition key and sort key. Use the GetItem API operation to read a single item.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Scan API operation with a filter expression that allows multiple items to be read. Use the Query API operation to read multiple items that have a specific partition key and sort key. Use the GetItem API operation to read a single item.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Scan API operation with a filter expression that allows multiple items to be read. Use the Query API operation to read a single item that has a specific primary key. Use the BatchGetItem API operation to read multiple items.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Scan API operation in parallel with many workers to read all the items. Use the Query API operation to read a single item that has a specific primary key Use the BatchGetItem API operation to read multiple items."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-11T06:57:00.000Z",
        "voteCount": 1,
        "content": "\"read multiple items that have a specific partition key and sort key\"\n\nThe sentence iftself is wrong, or I'm completely blind. The query will only read multiple items if you have a pk+sk and you query by pk, but if you have pk+sk and query with both, only one result will be presented."
      },
      {
        "date": "2023-06-17T22:57:00.000Z",
        "voteCount": 2,
        "content": "A and D are out of the question since we cannot consume all the capacity.\nBetween B and C. Since C is using BatchGetItem so it can read at max 100 items at a time while B is using Query API that can read as many items as matched with the query. Moreover reading a single item with GetItem is way faster than Querying that item as suggested in C. \nSo B is my answer."
      },
      {
        "date": "2023-01-08T15:43:00.000Z",
        "voteCount": 1,
        "content": "Parellel scan does not meet the requirement of \"without consuming all the provisioned throughput for the tables?\" as it can consume fully quickly unless if we handle it carefully. so Option B is the right one."
      },
      {
        "date": "2022-11-16T18:56:00.000Z",
        "voteCount": 2,
        "content": "Option should be A. As filter will still bring all the associated records which doesnt help the use case instead use the parallelscan which scan operation can logically divide the table into multiple segments."
      },
      {
        "date": "2023-01-08T15:41:00.000Z",
        "voteCount": 2,
        "content": "it says should not consume all throughput but A does that.\n\nA parallel scan with a large number of workers can easily consume all of the provisioned throughput for the table or index being scanned. It is best to avoid such scans if the table or index is also incurring heavy read or write activity from other applications.\n\nTo control the amount of data returned per request, use the Limit parameter. This can help prevent situations where one worker consumes all of the provisioned throughput, at the expense of all other workers."
      },
      {
        "date": "2022-10-04T18:22:00.000Z",
        "voteCount": 3,
        "content": "Why not A\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\n&gt; The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition.\n\nTo address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel."
      },
      {
        "date": "2022-09-27T12:12:00.000Z",
        "voteCount": 2,
        "content": "B.  Comparing to C - which has BatchGetItem as the last step.  This will do up to 100 GetItem requests.  \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.ReadData.html\n\nDefinitely not A or D."
      },
      {
        "date": "2024-04-09T16:28:00.000Z",
        "voteCount": 1,
        "content": "You are right. And it may do so in parallel which loads on throughput. \"In order to minimize response latency, BatchGetItem may retrieve items in parallel\" https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html"
      },
      {
        "date": "2022-09-07T04:38:00.000Z",
        "voteCount": 1,
        "content": "Agree, it's B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/amazon/view/80880-exam-aws-certified-database-specialty-topic-1-question-248/",
    "body": "A pharmaceutical company's drug search API is using an Amazon Neptune DB cluster. A bulk uploader process automatically updates the information in the database a few times each week. A few weeks ago during a bulk upload, a database specialist noticed that the database started to respond frequently with a<br>ThrottlingException error. The problem also occurred with subsequent uploads.<br>The database specialist must create a solution to prevent ThrottlingException errors for the database. The solution must minimize the downtime of the cluster.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica that uses a larger instance size than the primary DB instance. Fail over the primary DB instance to the read replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a read replica to each Availability Zone. Use an instance for the read replica that is the same size as the primary DB instance. Keep the traffic between the API and the database within the Availability Zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica that uses a larger instance size than the primary DB instance. Offload the reads from the primary DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake the latest backup, and restore it in a DB cluster of a larger size. Point the application to the newly created DB cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-10T13:11:00.000Z",
        "voteCount": 1,
        "content": "A. Create a read replica that uses a larger instance size than the primary DB instance. Fail over the primary DB instance to the read replica.\n\nA. should resolve the error with bulk upload because of promoting the replica to full writer instance with larger instance size."
      },
      {
        "date": "2023-05-24T01:41:00.000Z",
        "voteCount": 3,
        "content": "The throttling occurs during data upload. Adding read replicas won't solve the problem."
      },
      {
        "date": "2023-01-08T16:31:00.000Z",
        "voteCount": 3,
        "content": "C is wrong. this is not about read, it is about write. so primary is getting throttled so move to bigger one..."
      },
      {
        "date": "2023-01-01T18:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct to me.  Why are failing over to another instance as described in A.  Does this solve this problem?"
      },
      {
        "date": "2023-05-22T07:43:00.000Z",
        "voteCount": 1,
        "content": "because it's to solve a problem with a \"bulk upload\", ie with a write not a read. Failing to a read replica means that the read replica is now the main instance"
      },
      {
        "date": "2022-12-21T16:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\nYou can achieve read scaling for your Neptune DB cluster by creating up to 15 Neptune replicas in the DB cluster. Each Neptune replica returns the same data from the cluster volume with minimal replica lag (often considerably less than 100 milliseconds after the primary instance has written an update). As your read traffic increases, you can create additional Neptune replicas and connect to them directly to distribute the read load for your DB cluster. Neptune replicas don't have to be of the same DB instance class as the primary instance."
      },
      {
        "date": "2023-10-05T11:54:00.000Z",
        "voteCount": 1,
        "content": "But the question is about writing (updating), not reading. Nothing to separate in traffic. Hence, I'd choose A which is the most straightforward way to increase the capacity of the database."
      },
      {
        "date": "2022-09-28T13:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer.  there is no down-time compare to other options\nhttps://docs.aws.amazon.com/neptune/latest/userguide/manage-console-add-replicas.html\n\nNeptune replicas connect to the same storage volume as the primary DB instance and support only read operations. Neptune replicas can offload read workloads from the primary DB instance."
      },
      {
        "date": "2022-09-27T12:27:00.000Z",
        "voteCount": 2,
        "content": "A - Readers do not need to be of the same size.  Create a larger sized reader, and failover.\n\nNot D.  How is backup and recover \"no downtime\".  Dude, you just lost data potentially using last nights backup.\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/neptune-gdb-disaster-recovery.html#neptune-gdb-managed-failover"
      },
      {
        "date": "2022-09-14T03:41:00.000Z",
        "voteCount": 1,
        "content": "D is correct for no downtime"
      },
      {
        "date": "2022-10-11T07:37:00.000Z",
        "voteCount": 2,
        "content": "D is not correct. Recovery using latest backup results in data loss."
      },
      {
        "date": "2022-09-07T04:44:00.000Z",
        "voteCount": 4,
        "content": "I'd go for A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/amazon/view/80882-exam-aws-certified-database-specialty-topic-1-question-249/",
    "body": "A global company is developing an application across multiple AWS Regions. The company needs a database solution with low latency in each Region and automatic disaster recovery. The database must be deployed in an active-active configuration with automatic data synchronization between Regions.<br>Which solution will meet these requirements with the LOWEST latency?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS with cross-Region read replicas",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB global tables\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora global database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Athena and Amazon S3 with S3 Cross Region Replication"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T04:48:00.000Z",
        "voteCount": 5,
        "content": "I would choose B."
      },
      {
        "date": "2023-09-10T13:30:00.000Z",
        "voteCount": 2,
        "content": "B. Amazon DynamoDB global tables\n\nhttps://aws.amazon.com/dynamodb/global-tables/\n\n\"Amazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provide you 99.999% availability, increased application resiliency, and improved business continuity. As global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions, you can achieve fast, local read and write performance.\""
      },
      {
        "date": "2023-05-24T01:49:00.000Z",
        "voteCount": 2,
        "content": "DynamoDB is the only database that provides truly automated fail-over (cross-region) and active-active (multi-active) access."
      },
      {
        "date": "2023-04-02T16:17:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT:\nIf the restored Amazon DocumentDB cluster endpoint is not accessible, the database specialist should check the cluster's security group settings to ensure that the appropriate inbound rules are configured to allow incoming connections.\n\nTherefore, none of the options presented in the question would solve the connectivity issue.\n\nA and B refer to the cluster's parameter group, which does not affect connectivity to the cluster endpoint. Parameter groups are used to configure database engine settings."
      },
      {
        "date": "2023-04-02T16:20:00.000Z",
        "voteCount": 1,
        "content": "This is for question  97"
      },
      {
        "date": "2023-04-02T16:18:00.000Z",
        "voteCount": 1,
        "content": "C suggests configuring the interface VPC endpoint and associating the new Amazon DocumentDB cluster. However, this option is not relevant to the issue at hand as it relates to network connectivity within the VPC.\n\nD suggests using AWS DataSync to run the syncInstances command, which is also not relevant to this issue. DataSync is a data transfer service, and running the syncInstances command does not solve connectivity issues to an Amazon DocumentDB cluster.\n\nTherefore, the database specialist should verify the security group rules for the Amazon DocumentDB cluster to ensure that the necessary inbound rules are configured to allow incoming connections."
      },
      {
        "date": "2022-10-31T22:01:00.000Z",
        "voteCount": 3,
        "content": "active-active is dynamo DB global table\nAnswer B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/amazon/view/81106-exam-aws-certified-database-specialty-topic-1-question-250/",
    "body": "A pharmaceutical company uses Amazon Quantum Ledger Database (Amazon QLDB) to store its clinical trial data records. The company has an application that runs as AWS Lambda functions. The application is hosted in the private subnet in a VPC.<br>The application does not have internet access and needs to read some of the clinical data records. The company is concerned that traffic between the QLDB ledger and the VPC could leave the AWS network. The company needs to secure access to the QLDB ledger and allow the VPC traffic to have read-only access.<br>Which security strategy should a database specialist implement to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the QLDB ledger into a private database subnet inside the VPC. Run the Lambda functions inside the same VPC in an application private subnet. Ensure that the VPC route table allows read-only flow from the application subnet to the database subnet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink VPC endpoint for the QLDB ledger. Attach a VPC policy to the VPC endpoint to allow read-only traffic for the Lambda functions that run inside the VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a security group to the QLDB ledger to allow access from the private subnets inside the VPC where the Lambda functions that access the QLDB ledger are running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPN connection to ensure pairing of the private subnet where the Lambda functions are running with the private subnet where the QLDB ledger is deployed."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T23:58:00.000Z",
        "voteCount": 5,
        "content": "I'd go with B - https://docs.aws.amazon.com/qldb/latest/developerguide/vpc-endpoints.html"
      },
      {
        "date": "2023-05-10T13:40:00.000Z",
        "voteCount": 2,
        "content": "B...VPC endpoint is the AWS recommended way to connect services within VPC. For the req of keeping traffic private (avoid public internet) AWS PrivateLink is the option."
      },
      {
        "date": "2022-12-02T09:56:00.000Z",
        "voteCount": 2,
        "content": "AWS PrivateLink VPC endpoint"
      },
      {
        "date": "2022-10-29T10:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is B as per mbar94 link"
      },
      {
        "date": "2022-10-16T06:26:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/qldb/latest/developerguide/vpc-endpoints.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/amazon/view/81107-exam-aws-certified-database-specialty-topic-1-question-251/",
    "body": "An ecommerce company uses a backend application that stores data in an Amazon DynamoDB table. The backend application runs in a private subnet in a VPC and must connect to this table.<br>The company must minimize any network latency that results from network connectivity issues, even during periods of heavy application usage. A database administrator also needs the ability to use a private connection to connect to the DynamoDB table from the application.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse network ACLs to ensure that any outgoing or incoming connections to any port except DynamoDB are deactivated. Encrypt API calls by using TLS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC endpoint for DynamoDB in the application's VPC. Use the VPC endpoint to access the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that has access to DynamoDB. Restrict outgoing access only to this Lambda function from the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a VPN to route all communication to DynamoDB through the company's own corporate network infrastructure."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-08T00:00:00.000Z",
        "voteCount": 7,
        "content": "It's B - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      },
      {
        "date": "2022-09-21T18:59:00.000Z",
        "voteCount": 2,
        "content": "Always VPC endpoint for services outside VPC - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/amazon/view/81109-exam-aws-certified-database-specialty-topic-1-question-252/",
    "body": "A company's database specialist is building an Amazon RDS for Microsoft SQL Server DB instance to store hundreds of records in CSV format. A customer service tool uploads the records to an Amazon S3 bucket.<br>An employee who previously worked at the company already created a custom stored procedure to map the necessary CSV fields to the database tables. The database specialist needs to implement a solution that reuses this previous work and minimizes operational overhead.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 event to invoke an AWS Lambda function. Configure the Lambda function to parse the .csv file and use a SQL client library to run INSERT statements to load the data into the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom .NET app that is hosted on Amazon EC2. Configure the .NET app to load the .csv file and call the custom stored procedure to insert the data into the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the .csv file from Amazon S3 to the RDS D drive by using an AWS msdb stored procedure. Call the custom stored procedure to insert the data from the RDS D drive into the tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 event to invoke AWS Step Functions to parse the .csv file and call the custom stored procedure to insert the data into the tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-24T20:51:00.000Z",
        "voteCount": 10,
        "content": "\"The database specialist needs to implement a solution that reuses this previous work\". Option D is to reuse previous work."
      },
      {
        "date": "2023-01-05T03:15:00.000Z",
        "voteCount": 4,
        "content": "C is using the previous work"
      },
      {
        "date": "2022-12-01T14:42:00.000Z",
        "voteCount": 5,
        "content": "exec msdb.dbo.rds_download_from_s3\n@s3_arn_of_file='arn:aws:s3:::&lt;bucket_name&gt;/&lt;file_name&gt;',\n@rds_file_path='D:\\S3\\&lt;custom_folder_name&gt;\\&lt;file_name&gt;',\n@overwrite_file=1;"
      },
      {
        "date": "2022-12-28T15:11:00.000Z",
        "voteCount": 1,
        "content": "Already done really!"
      },
      {
        "date": "2023-12-12T02:48:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. D is wrong. S3 event cannot call step function directly. If we need to use AWS STEP we can setup s3 event call lambda and lamdba call step."
      },
      {
        "date": "2023-05-24T02:03:00.000Z",
        "voteCount": 3,
        "content": "The msdb stored procedure in question is msdb.dbo.rds_download_from_s3.\n\nThe only piece missing is how to trigger the import...\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/User.SQLServer.Options.S3-integration.html#Appendix.SQLServer.Options.S3-integration.using"
      },
      {
        "date": "2023-03-26T02:10:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/User.SQLServer.Options.S3-integration.html#:~:text=RDS%20stored%20procedure-,msdb.dbo.rds_download_from_s3,-with%20the%20following"
      },
      {
        "date": "2023-02-26T12:01:00.000Z",
        "voteCount": 2,
        "content": "This solution is the best option as it reuses the custom stored procedure that was previously created and minimizes operational overhead. The data can be downloaded from Amazon S3 to the RDS instance and loaded into the database tables using the custom stored procedure. The use of an AWS msdb stored procedure can automate the downloading of the data to the RDS instance."
      },
      {
        "date": "2022-12-02T09:50:00.000Z",
        "voteCount": 2,
        "content": "Download S3 File"
      },
      {
        "date": "2022-11-22T11:00:00.000Z",
        "voteCount": 2,
        "content": "Alternative C allows the file to be downloaded to S3 and then a local copy with the stored procedure would work"
      },
      {
        "date": "2022-11-08T06:54:00.000Z",
        "voteCount": 3,
        "content": "Step 1: Download S3 Files\nAmazon RDS for SQL Server comes with several custom stored procedures and functions. These are located in the msdb database. The stored procedure to download files from S3 is \"rds_download_from_s3\". The syntax for this stored procedure is shown here:\n\nexec msdb.dbo.rds_download_from_s3 \n     @s3_arn_of_file='arn:aws:s3:::&lt;bucket_name&gt;/&lt;file_name&gt;', \n     @rds_file_path='D:\\S3\\&lt;custom_folder_name&gt;\\&lt;file_name&gt;', \n     @overwrite_file=1;"
      },
      {
        "date": "2022-11-08T06:52:00.000Z",
        "voteCount": 3,
        "content": "https://www.mssqltips.com/sqlservertip/6619/rds-sql-server-data-import-from-amazon-s3/\n\nAmazon Web Service (AWS) recently announced a new feature of its Relational Database Service (RDS) for SQL Server. This feature allows a native integration between Amazon RDS SQL Server and Amazon S3. With this integration, it's now possible to import files from an Amazon S3 bucket into a local folder of the RDS instance. Similarly, files from that folder can be exported to S3. The RDS local folder path is D:\\S3\\."
      },
      {
        "date": "2022-10-20T10:15:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer is C based on the following article:  https://www.mssqltips.com/sqlservertip/6619/rds-sql-server-data-import-from-amazon-s3/"
      },
      {
        "date": "2022-09-08T00:01:00.000Z",
        "voteCount": 2,
        "content": "Minimum operational overhead would be for A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/amazon/view/81110-exam-aws-certified-database-specialty-topic-1-question-253/",
    "body": "A company hosts a 2 TB Oracle database in its on-premises data center. A database specialist is migrating the database from on premises to an Amazon Aurora<br>PostgreSQL database on AWS.<br>The database specialist identifies a problem that relates to compatibility Oracle stores metadata in its data dictionary in uppercase, but PostgreSQL stores the metadata in lowercase. The database specialist must resolve this problem to complete the migration.<br>What is the MOST operationally efficient solution that meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOverride the default uppercase format of Oracle schema by encasing object names in quotation marks during creation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) mapping rules with rule-action as convert-lowercase.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool conversion agent to convert the metadata from uppercase to lowercase.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue job that is attached to an AWS Database Migration Service (AWS DMS) replication task to convert the metadata from uppercase to lowercase."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-08T00:03:00.000Z",
        "voteCount": 6,
        "content": "Agree with B - https://aws.amazon.com/premiumsupport/knowledge-center/dms-mapping-oracle-postgresql/"
      },
      {
        "date": "2023-04-21T16:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-postgresql-migration-playbook/chap-oracle-aurora-pg.tables.case.html\n In most cases, you\u2019ll want to use AWS DMS transformations to change schema, table, and column names to lower case."
      },
      {
        "date": "2023-01-02T17:50:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Transformations.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/amazon/view/81113-exam-aws-certified-database-specialty-topic-1-question-254/",
    "body": "A financial company is running an Amazon Redshift cluster for one of its data warehouse solutions. The company needs to generate connection logs, user logs, and user activity logs. The company also must make these logs available for future analysis.<br>Which combination of steps should a database specialist take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the database configuration of the cluster by enabling audit logging. Direct the logging to a specified log group in Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the database configuration of the cluster by enabling audit logging. Direct the logging to a specified Amazon S3 bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the cluster by enabling continuous delivery of AWS CloudTrail logs to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new parameter group with the enable_user_activity_logging parameter set to true. Configure the cluster to use the new parameter group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the system table to enable logging for each user."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-16T06:46:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html"
      },
      {
        "date": "2023-02-26T11:46:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A and B"
      },
      {
        "date": "2023-10-07T09:52:00.000Z",
        "voteCount": 3,
        "content": "Please don't vote for answers if you cannot give a reason. That is useless for other users and contorts the votes. Thank you.\nNot against you but generally addressed to all who log in unsubstantiated answers."
      },
      {
        "date": "2022-11-04T05:34:00.000Z",
        "voteCount": 4,
        "content": "AWS CloudWatch Logs are stored indefinitely and CloudWatch Log Insights is used to analyze the logs and query upon them.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\n\"Log retention \u2013 By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group, keeping the indefinite retention, or choosing a retention period between 10 years and one day.\""
      },
      {
        "date": "2022-10-29T09:51:00.000Z",
        "voteCount": 2,
        "content": "Anser: BD \nas S3 we can store logs for future analysis"
      },
      {
        "date": "2022-09-27T13:01:00.000Z",
        "voteCount": 1,
        "content": "A and B\nFrom here: https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\n\n\"Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift exports logs to Amazon CloudWatch, or creates and uploads logs to Amazon S3, that capture data from the time audit logging is enabled to the present time. \""
      },
      {
        "date": "2022-11-08T11:14:00.000Z",
        "voteCount": 2,
        "content": "Changing to A and D.  For \"user activity logs\" we need D."
      },
      {
        "date": "2023-04-08T12:16:00.000Z",
        "voteCount": 4,
        "content": "AD.\n For the user activity log, you must also enable the enable_user_activity_logging database parameter. If you enable only the audit logging feature, but not the associated parameter, the database audit logs log information for only the connection log and user log, but not for the user activity log. The enable_user_activity_logging parameter is not enabled (false) by default. You can set it to true to enable the user activity log. For more information, see Amazon Redshift parameter groups."
      },
      {
        "date": "2023-08-31T09:59:00.000Z",
        "voteCount": 1,
        "content": "Why not B D"
      },
      {
        "date": "2023-10-07T09:58:00.000Z",
        "voteCount": 1,
        "content": "Because the native application in AWS to keep and analyse logs is CloudWatch Logs. Storing them on S3 would incur additional manual effort and integration."
      },
      {
        "date": "2022-09-20T19:04:00.000Z",
        "voteCount": 4,
        "content": "B &amp; D, send to S3 for future use."
      },
      {
        "date": "2022-09-08T00:09:00.000Z",
        "voteCount": 4,
        "content": "I'd go for AD."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/amazon/view/81114-exam-aws-certified-database-specialty-topic-1-question-255/",
    "body": "A bank is using an Amazon RDS for MySQL DB instance in a proof of concept. A database specialist is evaluating automated database snapshots and cross-<br><br>Region snapshot copies as -<br>part of this proof of concept. After validating three automated snapshots successfully, the database specialist realizes that the fourth snapshot was not created.<br>Which of the following are possible reasons why the snapshot was not created? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA copy of the automated snapshot for this DB instance is in progress within the same AWS Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA copy of a manual snapshot for this DB instance is in progress for only certain databases within the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe RDS maintenance window is not specified.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DB instance is in the STORAGE_FULL state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRDS event notifications have not been enabled."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-11T10:27:00.000Z",
        "voteCount": 2,
        "content": "A+D  are the ones"
      },
      {
        "date": "2022-11-08T07:06:00.000Z",
        "voteCount": 3,
        "content": "Refer  question 118"
      },
      {
        "date": "2022-09-27T13:04:00.000Z",
        "voteCount": 4,
        "content": "From here:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\n\nAutomated backups follow these rules:\n\nYour DB instance must be in the AVAILABLE state for automated backups to occur. Automated backups don't occur while your DB instance is in a state other than AVAILABLE, for example STORAGE_FULL.\n\nAutomated backups don't occur while a DB snapshot copy is running in the same AWS Region for the same DB instance."
      },
      {
        "date": "2022-09-27T12:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html"
      },
      {
        "date": "2022-09-08T00:10:00.000Z",
        "voteCount": 1,
        "content": "Agree with AD."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/amazon/view/81143-exam-aws-certified-database-specialty-topic-1-question-256/",
    "body": "A company recently migrated its line-of-business (LOB) application to AWS. The application uses an Amazon RDS for SQL Server DB instance as its database engine.<br>The company must set up cross-Region disaster recovery for the application. The company needs a solution with the lowest possible RPO and RTO.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cross-Region read replica of the DB instance. Promote the read replica at the time of failover.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up SQL replication from the DB instance to an Amazon EC2 instance in the disaster recovery Region. Promote the EC2 instance as the primary server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS KMS) for ongoing replication of the DB instance in the disaster recovery Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake manual snapshots of the DB instance in the primary Region. Copy the snapshots to the disaster recovery Region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-09T10:28:00.000Z",
        "voteCount": 7,
        "content": "Only two options in sql server cross region replication. copy snapshots and DMS\nhttps://aws.amazon.com/blogs/database/cross-region-disaster-recovery-of-amazon-rds-for-sql-server/"
      },
      {
        "date": "2022-12-18T19:17:00.000Z",
        "voteCount": 6,
        "content": "https://aws.amazon.com/blogs/database/use-cross-region-read-replicas-with-amazon-relational-database-service-for-sql-server/\n\nNov 2022 - Release date hence \"A\" is a right choice as of now"
      },
      {
        "date": "2023-07-02T22:31:00.000Z",
        "voteCount": 3,
        "content": "As of November 2022, Amazon RDS for SQL Server supports cross-Region read replicas, refer to Use cross-Region read replicas with Amazon Relational Database Service for SQL Server to learn more."
      },
      {
        "date": "2023-05-24T04:11:00.000Z",
        "voteCount": 5,
        "content": "AWS does not seem overly keen on using DMS for heterogeneous replications, and seems to prefer native tools or replication."
      },
      {
        "date": "2023-03-26T02:20:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html#SQLServer.ReadReplicas.limitations.options:~:text=When%20you%20promote%20a%20SQL%20Server%20cross%2DRegion%20read%20replica%2C"
      },
      {
        "date": "2022-12-19T19:22:00.000Z",
        "voteCount": 4,
        "content": "I'll go with A.\n\nAmazon Relational Database Service (Amazon RDS) for SQL Server now supports Cross Region Read Replica. Cross Region Read Replica enables managed disaster recovery capability for mission critical databases by allowing a read replica in another region be \"promoted\" as a new standalone production database."
      },
      {
        "date": "2022-12-03T08:27:00.000Z",
        "voteCount": 3,
        "content": "I will go with (A). https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-sql-server-cross-region-read-replica/"
      },
      {
        "date": "2022-12-01T14:53:00.000Z",
        "voteCount": 3,
        "content": "RDS for SQL Server now supports cross Region Replica.  (released Nov 2022)\nhttps://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-sql-server-cross-region-read-replica/"
      },
      {
        "date": "2022-10-17T22:37:00.000Z",
        "voteCount": 1,
        "content": "C, as cross-region read replica is not supported for RDS SQL Server"
      },
      {
        "date": "2022-09-27T13:18:00.000Z",
        "voteCount": 1,
        "content": "C, as per link"
      },
      {
        "date": "2022-09-21T18:54:00.000Z",
        "voteCount": 2,
        "content": "Definitely C is correct - https://aws.amazon.com/blogs/database/cross-region-disaster-recovery-of-amazon-rds-for-sql-server/"
      },
      {
        "date": "2023-04-19T08:04:00.000Z",
        "voteCount": 1,
        "content": "Agree - Continuous replication\nTo meet very aggressive RPO and RTO requirements, your DR strategy needs to consider continuous replication capability from your source RDS SQL Server to the target RDS SQL Server in your DR Region."
      },
      {
        "date": "2022-09-08T02:11:00.000Z",
        "voteCount": 2,
        "content": "It's A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/amazon/view/81449-exam-aws-certified-database-specialty-topic-1-question-257/",
    "body": "A company runs hundreds of Microsoft SQL Server databases on Windows servers in its on-premises data center. A database specialist needs to migrate these databases to Linux on AWS.<br>Which combination of steps should the database specialist take to meet this requirement? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall AWS Systems Manager Agent on the on-premises servers. Use Systems Manager Run Command to install the Windows to Linux replatforming assistant for Microsoft SQL Server Databases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Run Command to install and configure the AWS Schema Conversion Tool on the on-premises servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the Amazon EC2 console, launch EC2 instances and select a Linux AMI that includes SQL Server. Install and configure AWS Systems Manager Agent on the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the AWS Management Console, set up Amazon RDS for SQL Server DB instances with Linux as the operating system. Install AWS Systems Manager Agent on the DB instances by using an options group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Windows to Linux replatforming assistant tool. Enter configuration details of the source and destination databases. Start migration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the AWS Management Console, set up AWS Database Migration Service (AWS DMS) by entering details of the source SQL Server database and the destination SQL Server database on AWS. Start migration."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-11T07:14:00.000Z",
        "voteCount": 5,
        "content": "Migration using backup and restore.  https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/replatform-sql-server.html\n\nBx: Not STS, Dx: Not RDS, Fx: Not DMS"
      },
      {
        "date": "2023-10-07T23:37:00.000Z",
        "voteCount": 1,
        "content": "Okay the Key is \"hundreds of database\", wondering why can't we simply use DMS but that's hectic, as we can simply backup and restore but the answer E slightly mislead saying \"Start Migration\" instead of backup and restore, great learning"
      },
      {
        "date": "2023-10-07T10:16:00.000Z",
        "voteCount": 1,
        "content": "Why not B, D or F?\nB. You don't want to convert the schema (using AWS Schema Conversion Tool) because you're just replatforming SQL Server on-prem to SQL Server on AWS.\nD. AN RDS for SQL Server instance doesn't have an OS that you can modify. It'S just RDS (Platform as a service).\nF. The task is to re-platform from Windows to Linux, not simply to migrate the data. For this, (E) is better suited."
      },
      {
        "date": "2023-08-29T13:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/replatform-sql-server.html\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-microsoft-sql-server-database-to-microsoft-sql-server-on-amazon-ec2-running-linux.html"
      },
      {
        "date": "2023-02-14T08:55:00.000Z",
        "voteCount": 1,
        "content": "After reading the doc we need to use windows to linux tool"
      },
      {
        "date": "2022-10-29T15:32:00.000Z",
        "voteCount": 2,
        "content": "https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Leverage_automation_to_re-platform_SQL_Server_to_Linux_WIN322-R1.pdf\n\nAnswer ACE"
      },
      {
        "date": "2022-09-09T11:54:00.000Z",
        "voteCount": 3,
        "content": "ACE as many databases to migrate"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/amazon/view/81144-exam-aws-certified-database-specialty-topic-1-question-258/",
    "body": "A company is running a blogging platform. A security audit determines that the Amazon RDS DB instance that is used by the platform is not configured to encrypt the data at rest. The company must encrypt the DB instance within 30 days.<br>What should a database specialist do to meet this requirement with the LEAST amount of downtime?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the DB instance, and enable encryption. When the read replica is available, promote the read replica and update the endpoint that is used by the application. Delete the unencrypted DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the DB instance. Make an encrypted copy of the snapshot. Restore the encrypted snapshot. When the new DB instance is available, update the endpoint that is used by the application. Delete the unencrypted DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new encrypted DB instance. Perform an initial data load, and set up logical replication between the two DB instances When the new DB instance is in sync with the source DB instance, update the endpoint that is used by the application. Delete the unencrypted DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the DB instance to an Amazon Aurora DB cluster, and enable encryption. When the DB cluster is available, update the endpoint that is used by the application to the cluster endpoint. Delete the unencrypted DB instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-11T06:41:00.000Z",
        "voteCount": 5,
        "content": "When the new, encrypted copy of the DB instance becomes available, you can point your applications to the new database. However, if your project doesn\u2019t allow for significant downtime for this activity, you need an alternate approach that helps minimize the downtime. This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime."
      },
      {
        "date": "2023-08-29T14:21:00.000Z",
        "voteCount": 1,
        "content": "C. Create a new encrypted DB instance. Perform an initial data load, and set up logical replication between the two DB instances When the new DB instance is in sync with the source DB instance, update the endpoint that is used by the application. Delete the unencrypted DB instance.\n\n\"However, if your project doesn\u2019t allow for significant downtime for this activity, you need an alternate approach that helps minimize the downtime. This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime. \"\n\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"
      },
      {
        "date": "2023-05-24T04:30:00.000Z",
        "voteCount": 2,
        "content": "B would work too, but there would be (significantly) more downtime."
      },
      {
        "date": "2023-03-28T10:36:00.000Z",
        "voteCount": 2,
        "content": "It says : the LEAST amount of downtime. Creating a replication is much more effort than copying the snapshot. So it is B."
      },
      {
        "date": "2023-07-30T03:46:00.000Z",
        "voteCount": 3,
        "content": "It's not about effort but downtime. B has more downtime than C. Thus C is the correct answer"
      },
      {
        "date": "2022-10-29T09:38:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\nAnswer : B"
      },
      {
        "date": "2023-10-07T10:25:00.000Z",
        "voteCount": 1,
        "content": "Given the above link, it would be B.\n\n\"However, if your project doesn\u2019t allow for significant downtime for this activity, you need an alternate approach that helps minimize the downtime. This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime.\"\n\nThat leads to C."
      },
      {
        "date": "2022-10-16T07:06:00.000Z",
        "voteCount": 2,
        "content": "minimum downtime. \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"
      },
      {
        "date": "2022-09-27T13:12:00.000Z",
        "voteCount": 3,
        "content": "Solution expects minimal downtime. \n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"
      },
      {
        "date": "2022-09-14T04:00:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-09T10:40:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Enabling"
      },
      {
        "date": "2022-09-08T07:27:00.000Z",
        "voteCount": 3,
        "content": "My choice is B"
      },
      {
        "date": "2022-09-08T02:13:00.000Z",
        "voteCount": 1,
        "content": "Agree with C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/amazon/view/89714-exam-aws-certified-database-specialty-topic-1-question-259/",
    "body": "A database specialist is planning to migrate a MySQL database to Amazon Aurora. The database specialist wants to configure the primary DB cluster in the us-west-2 Region and the secondary DB cluster in the eu-west-1 Region. In the event of a disaster recovery scenario, the database must be available in eu-west-1 with an RPO of a few seconds. Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora MySQL with the primary DB cluster in us-west-2 and a cross-Region Aurora Replica in eu-west-1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora MySQL with the primary DB cluster in us-west-2 and binlog-based external replication to eu-west-1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Aurora MySQL global database with the primary DB cluster in us-west-2 and the secondary DB cluster in eu-west-1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora MySQL with the primary DB cluster in us-west-2. Use AWS Database Migration Service (AWS DMS) change data capture (GDC) replication to the secondary DB cluster in eu-west-1"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-01T15:14:00.000Z",
        "voteCount": 8,
        "content": "Recovery point objective (RPO) \u2013 The amount of data that can be lost (measured in time). For an Aurora global database, RPO is typically measured in seconds. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html"
      },
      {
        "date": "2024-01-14T02:57:00.000Z",
        "voteCount": 1,
        "content": "I believe both A and C meet all the requirements.\nGenerally, C is the preferred option."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/amazon/view/89715-exam-aws-certified-database-specialty-topic-1-question-260/",
    "body": "An ecommerce company is planning to launch a custom customer relationship management (CRM) application on AWS. The development team selected Microsoft SQL Server as the database engine for this deployment. The CRM application will require operating system access because the application will manipulate files and packages on the server hosting the database. A senior database engineer must help the application team select a suitable deployment model for SQL Server. The deployment model should be optimized for the workload requirements.<br><br>Which deployment option should the database engineer choose that involves the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun SQL Server on Amazon EC2 and grant elevated privileges for both the database instance and the host operating system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for SQL Server and grant elevated privileges for both the database instance and the host operating system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun SQL Server on Amazon EC2 and grant elevated privileges for the database instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn Amazon RDS Custom for SQL Server and grant elevated privileges for both the database instance and the host operating system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-29T20:05:00.000Z",
        "voteCount": 3,
        "content": "D. An Amazon RDS Custom for SQL Server and grant elevated privileges for both the database instance and the host operating system.\n\n\"Amazon RDS Custom is a managed database service for legacy, custom, and packaged applications that require access to the underlying operating system and database environment. Amazon RDS Custom automates setup, operation, and scaling of databases in the cloud while granting customers access to the database and underlying operating system to configure settings, install patches, and enable native features to meet the dependent application's requirements.\"\n\nhttps://aws.amazon.com/rds/custom/faqs/"
      },
      {
        "date": "2023-07-17T08:26:00.000Z",
        "voteCount": 2,
        "content": "D. Amazon RDS Custom"
      },
      {
        "date": "2023-05-24T04:37:00.000Z",
        "voteCount": 4,
        "content": "Assuming that the CRM application must run on the same host as the database server to \"manipulate files and packages\", the DB server will have to run on  an EC2 instance. So the only question is what level of privileges the application will need to have on that EC2 instance"
      },
      {
        "date": "2022-12-19T19:47:00.000Z",
        "voteCount": 2,
        "content": "D. Amazon RDS Custom"
      },
      {
        "date": "2022-12-02T09:14:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS Custom is supported for the Oracle and Microsoft SQL Server database engines"
      },
      {
        "date": "2022-12-01T15:17:00.000Z",
        "voteCount": 2,
        "content": "Amazon RDS Custom automates database administration tasks and operations. RDS Custom makes it possible for you as a database administrator to access and customize your database environment and operating system. With RDS Custom, you can customize to meet the requirements of legacy, custom, and packaged applications."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/amazon/view/89457-exam-aws-certified-database-specialty-topic-1-question-261/",
    "body": "A company uses Amazon DynamoDB to store its customer data. The DynamoDB table is designed with the user ID as the partition key value and multiple other non-key attributes. An external application needs to access data for specific user IDs. The external application must have access only to items with specific partition key values.<br><br>What should the database specialist do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the dynamodb:ReturnValues condition key in the external application's IAM policy to grant access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a projection expression to select specific users from the DynamoDB table for the external application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ExecuteStatementAPI operation to select specific users from the DynamoDB table for the external application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the dynamodb:LeadingKeys condition key in the external application's IAM policy to grant access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T15:09:00.000Z",
        "voteCount": 9,
        "content": "D\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html\ndynamodb:LeadingKeys condition key to limit user actions only on the items whose UserID partition key value matches the Login"
      },
      {
        "date": "2022-12-12T12:47:00.000Z",
        "voteCount": 2,
        "content": "Answer: D"
      },
      {
        "date": "2022-12-10T21:18:00.000Z",
        "voteCount": 4,
        "content": "ynamodb:LeadingKeys condition key to limit"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/amazon/view/90154-exam-aws-certified-database-specialty-topic-1-question-262/",
    "body": "A city\u2019s weather forecast team is using Amazon DynamoDB in the data tier for an application. The application has several components. The analysis component of the application requires repeated reads against a large dataset. The application has started to temporarily consume all the read capacity in the DynamoDB table and is negatively affecting other applications that need to access the same data.<br><br>Which solution will resolve this issue with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB Accelerator (DAX)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudFront in front of DynamoDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a DynamoDB table with a local secondary index (LSI)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ElastiCache in front of DynamoDB"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T21:45:00.000Z",
        "voteCount": 3,
        "content": "always dax for optimizing dynamodb reads"
      },
      {
        "date": "2023-05-10T17:43:00.000Z",
        "voteCount": 1,
        "content": "A, DAX is correct"
      },
      {
        "date": "2022-12-09T06:41:00.000Z",
        "voteCount": 3,
        "content": "DAX saves from overprovisioning of read capacity"
      },
      {
        "date": "2022-12-05T16:47:00.000Z",
        "voteCount": 1,
        "content": "A correct answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/amazon/view/89459-exam-aws-certified-database-specialty-topic-1-question-263/",
    "body": "A company is creating a serverless application that uses multiple AWS services and stores data on an Amazon RDS DB instance. The database credentials must be stored securely. An AWS Lambda function must be able to access the credentials. The company also must rotate the database password monthly by using an automated solution.<br><br>What should a database specialist do to meet those requirements in the MOST secure manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials by using AWS Systems Manager Parameter Store. Enable automatic rotation of the password. Use the AWS Cloud Development Kit (AWS CDK) in the Lambda function to retrieve the credentials from Parameter Store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the database credentials by using AWS Key Management Service (AWS KMS). Store the credentials in Amazon S3. Use an S3 Lifecycle policy to rotate the password. Retrieve the credentials by using Python code in Lambda",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials by using AWS Secrets Manager. Enable automatic rotation of the password. Configure the Lambda function to use the Secrets Manager API to retrieve the credentials\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database credentials in an Amazon DynamoDB table. Assign an IAM role to the Lambda function to grant the Lambda function read-only access to the DynamoDB table. Rotate the password by using another Lambda function that runs monthly"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T17:46:00.000Z",
        "voteCount": 1,
        "content": "C, Sec mgr for secure storage of creds and automated rotation"
      },
      {
        "date": "2022-12-09T06:43:00.000Z",
        "voteCount": 4,
        "content": "Secret manager is the service which provides credential storing and rotating."
      },
      {
        "date": "2023-10-07T10:41:00.000Z",
        "voteCount": 1,
        "content": "The key is 'rotate'. AWS Systems Manager Parameter Store (A) doesn't support rotating secrets."
      },
      {
        "date": "2022-12-02T23:29:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. Secret Manager"
      },
      {
        "date": "2022-12-02T09:03:00.000Z",
        "voteCount": 3,
        "content": "AWS Secrets Manager"
      },
      {
        "date": "2022-12-01T18:32:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. Secret Manager is for Credentials storing"
      },
      {
        "date": "2022-12-01T06:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is C for AWS Secrets Manager for Credentials"
      },
      {
        "date": "2022-11-30T15:10:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/amazon/view/89461-exam-aws-certified-database-specialty-topic-1-question-264/",
    "body": "A company wants to migrate its on-premises Oracle database to Amazon RDS for PostgreSQL by using AWS Database Migration Service (AWS DMS). A database specialist needs to evaluate the migration task settings and data type conversion in an AWS DMS task.<br><br>What should the database specialist do to identify the optimal migration method?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) database migration assessment report",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) multiserver assessor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS DMS pre-migration assessment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS DMS data validation tool"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T23:01:00.000Z",
        "voteCount": 1,
        "content": "C. AWS DMS pre-migration assessment\n\n\n\"With the current release of this feature, the task assessment report includes information about unsupported and partially supported data types in AWS DMS. The assessment process scans all the source data types in the tables that are to be migrated. It then compares this list with the supported data types for this type of database and generates a report. This report indicates the data types that are not supported or that are partially supported for the present database migration.\"\n\nhttps://aws.amazon.com/blogs/database/migration-validation-part-1-introducing-migration-assessment-in-aws-database-migration-service/#:~:text=With%20the%20current,present%20database%20migration."
      },
      {
        "date": "2023-05-24T05:57:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.AssessmentReport.html"
      },
      {
        "date": "2023-04-28T22:02:00.000Z",
        "voteCount": 3,
        "content": "Key word: evaluate the migration task settings and data type conversion \n\nPer :  Enabling and working with premigration assessments for a task https://docs.aws.amazon.com/zh_cn/dms/latest/userguide/CHAP_Tasks.AssessmentReport.html\nQuote:\nEach individual assessment evaluates a specific element of a supported relational source or target database depending on considerations such as the migration type, supported objects, index configuration, and other task settings, such as table mappings that identify the schemas and tables to migrate.\n\nSupported by: https://aws.amazon.com/blogs/database/migration-validation-part-1-introducing-migration-assessment-in-aws-database-migration-service/\nQuote: \nAWS DMS does a great job of helping you move your data between multiple supported sources and targets. However, migrations can be difficult, especially when you\u2019re moving from one database engine to another (known as a heterogeneous migration)."
      },
      {
        "date": "2023-03-28T18:25:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer. Option C makes more sense in case of homogeneous migration but this is a case of heterogenous migration"
      },
      {
        "date": "2023-02-14T13:44:00.000Z",
        "voteCount": 4,
        "content": "I will go with C\nhttps://aws.amazon.com/blogs/database/migration-validation-part-1-introducing-migration-assessment-in-aws-database-migration-service/"
      },
      {
        "date": "2023-02-13T05:51:00.000Z",
        "voteCount": 2,
        "content": "A - Because the DMS don't valide diferents database types."
      },
      {
        "date": "2023-02-14T13:44:00.000Z",
        "voteCount": 2,
        "content": "Not totally agree \n\nhttps://aws.amazon.com/blogs/database/migration-validation-part-1-introducing-migration-assessment-in-aws-database-migration-service/"
      },
      {
        "date": "2023-04-02T13:52:00.000Z",
        "voteCount": 1,
        "content": "To identify the optimal migration method for migrating an on-premises Oracle database to Amazon RDS for PostgreSQL by using AWS DMS, the database specialist should use the AWS Schema Conversion Tool (AWS SCT) database migration assessment report.\nAn AWS DMS pre-migration assessment can be used to check the source database for compatibility with AWS DMS, but it does not provide a recommended migration approach."
      },
      {
        "date": "2023-01-05T11:33:00.000Z",
        "voteCount": 2,
        "content": "Should be C"
      },
      {
        "date": "2023-01-02T19:24:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP-Reports.html"
      },
      {
        "date": "2022-12-28T16:20:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2022-11-30T15:13:00.000Z",
        "voteCount": 3,
        "content": "Answer C\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.AssessmentReport.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/amazon/view/89462-exam-aws-certified-database-specialty-topic-1-question-265/",
    "body": "An ecommerce company runs an application on Amazon RDS for SQL Server 2017 Enterprise edition. Due to the increase in read volume, the company\u2019s application team is planning to offload the read transactions by adding a read replica to the RDS for SQL Server DB instance.<br><br>What architectural conditions should a database specialist set? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the automatic backups are turned on for the RDS DB instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the backup retention value is set to 0 for the RDSDB instance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the RDS DB instance is set to Multi-AZ\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the RDS DB instance is set to Single-AZ",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the RDS DB instance is in a stopped state to turn on the read replica"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T15:18:00.000Z",
        "voteCount": 5,
        "content": "AC\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2023-09-06T12:23:00.000Z",
        "voteCount": 1,
        "content": "A. Ensure that the automatic backups are turned on for the RDS DB instance\nC. Ensure the RDS DB instance is set to Multi-AZ\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html"
      },
      {
        "date": "2023-02-14T02:34:00.000Z",
        "voteCount": 1,
        "content": "AC it is"
      },
      {
        "date": "2022-12-23T07:16:00.000Z",
        "voteCount": 2,
        "content": "AC\nyou must enable automatic backups on the source DB instance. To do so, you set the backup retention period to a value other than 0. The source DB instance must be a Multi-AZ deployment with Always On Availability Groups (AGs). Setting this type of deployment also enforces that automatic backups are enabled."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/amazon/view/89463-exam-aws-certified-database-specialty-topic-1-question-266/",
    "body": "A company uses AWS Lambda functions in a private subnet in a VPC to run application logic. The Lambda functions must not have access to the public internet. Additionally, all data communication must remain within the private network. As part of a new requirement, the application logic needs access to an Amazon DynamoDB table.<br><br>What is the MOST secure way to meet this new requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision the DynamoDB table inside the same VPC that contains the Lambda functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway VPC endpoint for DynamoDB to provide access to the table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a network ACL to only allow access to the DynamoDB table from the VPC",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a security group to only allow access to the DynamoDB table from the VPC"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T15:20:00.000Z",
        "voteCount": 8,
        "content": "B\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      },
      {
        "date": "2022-12-18T10:25:00.000Z",
        "voteCount": 5,
        "content": "B.\nA gateway VPC endpoint"
      },
      {
        "date": "2023-09-06T12:34:00.000Z",
        "voteCount": 4,
        "content": "B. Create a gateway VPC endpoint for DynamoDB to provide access to the table\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html\n\n\n\"You can access Amazon DynamoDB from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to DynamoDB.\""
      },
      {
        "date": "2022-12-14T03:08:00.000Z",
        "voteCount": 3,
        "content": "B- VPC endpoint for DynamoDB"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/amazon/view/89464-exam-aws-certified-database-specialty-topic-1-question-267/",
    "body": "A startup company is developing electric vehicles. These vehicles are expected to send real-time data to the AWS Cloud for data analysis. This data will include trip metrics, trip duration, and engine temperature. The database team decides to store the data for 15 days using Amazon DynamoDB.<br><br>How can the database team achieve this with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon DynamoDB Accelerator (DAX) on the DynamoDB table. Use Amazon EventBridge (Amazon CloudWatch Events) to poll the DynamoDB table and drop items after 15 days",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on DynamoDB Streams for the DynamoDB table to push the data from DynamoDB to another storage location. Use AWS Lambda to poll and terminate items older than 15 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the TTL feature for the DynamoDB table. Use the TTL attribute as a timestamp and set the expiration of items to 15 days\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to poll the list of DynamoDB tables every 15 days. Drop the existing table and create a new table"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-06T12:42:00.000Z",
        "voteCount": 3,
        "content": "C. Turn on the TTL feature for the DynamoDB table.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n\n\"Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table and only consumes throughput when the deletion is replicated to additional Regions. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload\u2019s needs.\""
      },
      {
        "date": "2023-09-03T23:48:00.000Z",
        "voteCount": 1,
        "content": "\uff23\uff23\uff23\uff23\uff23\uff23\uff23"
      },
      {
        "date": "2023-05-10T18:04:00.000Z",
        "voteCount": 1,
        "content": "C...Expiration after 15 days, use TTL feature"
      },
      {
        "date": "2022-12-10T21:27:00.000Z",
        "voteCount": 1,
        "content": "Definitely C"
      },
      {
        "date": "2022-12-02T04:50:00.000Z",
        "voteCount": 3,
        "content": "C is the answer, Less overhead"
      },
      {
        "date": "2022-12-02T04:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2022-11-30T15:21:00.000Z",
        "voteCount": 2,
        "content": "Answer  C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/amazon/view/90144-exam-aws-certified-database-specialty-topic-1-question-268/",
    "body": "A company is using an Amazon RDS Multi-AZ DB instance in its development environment. The DB instance uses General Purpose SSD storage. The DB instance provides data to an application that has I/O constraints and high online transaction processing (OLTP) workloads. The users report that the application is slow.<br><br>A database specialist finds a high degree of latency in the database writes. The database specialist must decrease the database latency by designing a solution that minimizes operational overhead.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEliminate the Multi-AZ deployment. Run the DB instance in only one Availability Zone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the DB instance. Use the default storage type. Reload the data from an automatic snapshot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the storage to Provisioned IOPS SSD on the DB instance that is running\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the DB instance. Use Provisioned IOPS SSD storage. Reload the data from an automatic snapshot"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T02:27:00.000Z",
        "voteCount": 2,
        "content": "C seems to be the answer.\nA can also help, especially for Dev. However, we do not have enough information to rule out the Multi-AZ. Also, the question does not mention price so C will do"
      },
      {
        "date": "2023-09-06T13:08:00.000Z",
        "voteCount": 3,
        "content": "C. Switch the storage to Provisioned IOPS SSD \n\nYou can change the settings of a DB instance to accomplish tasks such as adding additional storage or changing the DB instance class.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html"
      },
      {
        "date": "2024-04-08T14:55:00.000Z",
        "voteCount": 1,
        "content": "I agree with you. But be careful about what storage type your RDS instance migrates from or to. Any conversion between magnetic and SSD causes downtime according to the above website. It isn't applied to this case, though, just for your information."
      },
      {
        "date": "2022-12-13T23:07:00.000Z",
        "voteCount": 3,
        "content": "ans is C,\nchanging from GP into privisoned IOPS does NOT make downtime and could apply immediately"
      },
      {
        "date": "2022-12-13T23:07:00.000Z",
        "voteCount": 2,
        "content": "ans is C,\nchanging from GP into privisoned IOPS does NOT make downtime and could apply immediately"
      },
      {
        "date": "2022-12-05T17:09:00.000Z",
        "voteCount": 1,
        "content": "D right answer"
      },
      {
        "date": "2022-12-07T20:03:00.000Z",
        "voteCount": 1,
        "content": "Storage type can be modified.\nStorage type\n\nThe storage type that you want to use.\nIf you choose General Purpose SSD (gp3), you can provision additional Provisioned IOPS and Storage throughput under Advanced settings.\nIf you choose Provisioned IOPS SSD (io1), enter the Provisioned IOPS value.\nAfter Amazon RDS begins to modify your DB instance to change the storage size or type, you can't submit another request to change the storage size, performance, or type for six hours.\n\nFor more information, see Amazon RDS storage types\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html"
      },
      {
        "date": "2022-12-14T10:38:00.000Z",
        "voteCount": 5,
        "content": "Answer C: I've tested and the storage and instance classes can be modified on the fly without downtime"
      },
      {
        "date": "2022-12-05T14:40:00.000Z",
        "voteCount": 1,
        "content": "C\nYou can modify storage class. There will be performance issue while storage is moved to new class."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/amazon/view/90812-exam-aws-certified-database-specialty-topic-1-question-269/",
    "body": "A company wants to migrate its on-premises Oracle database to a managed open-source database engine in Amazon RDS by using AWS Database Migration Service (AWS DMS). A database specialist needs to identify the target engine in Amazon RDS based on the conversion percentage of database code objects such as stored procedures, functions, views, and database storage objects. The company will select the engine that has the least manual conversion effort.<br><br>What should the database specialist do to identify the target engine?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) database migration assessment report",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) multiserver assessor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS DMS pre-migration assessment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS DMS data validation tool"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-20T17:12:00.000Z",
        "voteCount": 5,
        "content": "The need is to identify appropriate target db. To determine the best target direction for your overall environment, create a multiserver assessment report.\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html"
      },
      {
        "date": "2023-10-08T00:22:00.000Z",
        "voteCount": 1,
        "content": "why do you need multi server when you want to analyse a single database and the single server SCT assessment also provide recommendation for target DB"
      },
      {
        "date": "2023-09-06T14:04:00.000Z",
        "voteCount": 2,
        "content": "B. Use AWS SCT multiserver  assessment\n\n\"To determine the best target direction for your overall environment, create a multiserver assessment report.\nA multiserver assessment report evaluates multiple servers based on input that you provide for each schema definition that you want to assess. Your schema definition contains database server connection parameters and the full name of each schema. After assessing each schema, AWS SCT produces a summary, aggregated assessment report for database migration across your multiple servers. This report shows the estimated complexity for each possible migration target.\"\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html"
      },
      {
        "date": "2023-03-28T11:02:00.000Z",
        "voteCount": 1,
        "content": "Definitely A. Where does it say  multiserver assessor which is used to assess multiple databases in a single instance or multiple instances."
      },
      {
        "date": "2023-05-26T06:46:00.000Z",
        "voteCount": 4,
        "content": "The \"multi\" part is the target... For example are you moving to RDS Postgres or Aurora Postgres. You need to read carefully before misleading people with comments that show you clearly do not understand anything"
      },
      {
        "date": "2023-03-24T05:45:00.000Z",
        "voteCount": 1,
        "content": "I think it might be option B, the question talks about just one single Oracle database server, no multiple servers"
      },
      {
        "date": "2023-01-29T07:17:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html"
      },
      {
        "date": "2022-12-29T13:31:00.000Z",
        "voteCount": 1,
        "content": "A.\nThis is a single Oracle Database according to the question, so, no need to go multiserver. It will provide effort on converting such objects in the database migration assessemnt report:\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Summary.html"
      },
      {
        "date": "2022-12-19T20:35:00.000Z",
        "voteCount": 2,
        "content": "B.\nA multiserver assessment report evaluates multiple servers based on input that you provide for each schema definition that you want to assess. Your schema definition contains database server connection parameters and the full name of each schema. After assessing each schema, AWS SCT produces a summary, aggregated assessment report for database migration across your multiple servers. This report shows the estimated complexity for each possible migration target."
      },
      {
        "date": "2022-12-09T07:25:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html"
      },
      {
        "date": "2022-12-09T07:25:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/amazon/view/89467-exam-aws-certified-database-specialty-topic-1-question-270/",
    "body": "A retail company runs its production database on Amazon RDS for SOL Server. The company wants more flexibility in backing up and restoring the database. A database specialist needs to create a native backup and restore strategy. The solution must take native SQL Server backups and store them in a highly scalable manner.<br><br>Which combination of stops should the database specialist take to meet those requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon S3 destination bucket. Establish a trust relationship with an IAM role that includes permissions for Amazon RDS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon FSx for Windows File Server destination file system. Establish a trust relationship with an IAM role that includes permissions for Amazon RDS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an option group. Add the SQLSERVER_BACKUP_RESTORE option to the option group\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing default option group. Add the SQLSERVER_BACKUP_RESTORE option to the option group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the database by using the native BACKUP DATABASE TSQL command. Restore the database by using the RESTORE DATABASE TSQL command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the database by using the rds_backup_database stored procedure. Restore the database by using the rds_restore_database stored procedure.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-14T07:37:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.BackupRestore.html"
      },
      {
        "date": "2023-09-06T14:49:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/native-backup-rds-sql-server\nideally for C. to add below for a more complete process.\nassociate the newly created option group with the DB instance."
      },
      {
        "date": "2023-07-13T22:54:00.000Z",
        "voteCount": 1,
        "content": "\"highly scalable manner\" - S3 max limit is 5TB. What about database that's 55TB+?  Amazon FSx for Windows File Server limit is 64TB. \nrds_backup_database : @number_of_files \u2013 The number of files into which the backup will be divided (chunked). The maximum number is 10.  10 x5TB  = 50TB of backup can be stored in S3"
      },
      {
        "date": "2023-07-17T10:12:00.000Z",
        "voteCount": 3,
        "content": "how would you initiate a backup to FSx from AWS RDS SQL Server?\nit does S3 only\n\nhttps://sqlbak.com/blog/how-to-backup-and-restore-amazon-rds-sql-server/#:~:text=Backup%201%20Once%20the%20SqlBak%20app%20is%20installed%2C,then%20click%20on%20the%20%E2%80%9CRun%20now%E2%80%9D%20button.%20"
      },
      {
        "date": "2022-12-13T23:11:00.000Z",
        "voteCount": 3,
        "content": "default option group cannot be modified"
      },
      {
        "date": "2022-11-30T15:48:00.000Z",
        "voteCount": 3,
        "content": "ACF\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.BackupRestore.html\nhttps://aws.amazon.com/premiumsupport/knowledge-center/native-backup-rds-sql-server/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/amazon/view/91120-exam-aws-certified-database-specialty-topic-1-question-271/",
    "body": "A company has a variety of Amazon Aurora DB clusters. Each of these DB clusters has various configurations that meet specific sets of requirements. Depending on the team and the use case, these configurations can be organized into broader categories. A database specialist wants to implement a solution to make the storage and modification of the configuration parameters more systematic.<br><br>Which AWS service or feature should the database specialist use to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Systems Manager Parameter Store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDB parameter group\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Config with the Amazon RDS managed rules",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Secrets Manager"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-06T16:06:00.000Z",
        "voteCount": 3,
        "content": "B. DB parameter group \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/parameter-groups-overview.html"
      },
      {
        "date": "2023-07-23T10:50:00.000Z",
        "voteCount": 2,
        "content": "I vote for B"
      },
      {
        "date": "2023-04-03T04:16:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/parameter-groups-overview.html\n---\n\"Default and custom parameter groups\nIf you create a DB instance without specifying a DB parameter group, the DB instance uses a default DB parameter group. Likewise, if you create a Multi-AZ DB cluster without specifying a DB cluster parameter group, the DB cluster uses a default DB cluster parameter group. Each default parameter group contains database engine defaults and Amazon RDS system defaults based on the engine, compute class, and allocated storage of the instance.\""
      },
      {
        "date": "2023-04-03T04:20:00.000Z",
        "voteCount": 4,
        "content": "Sorry,\nB is correct. Not A. ( I input A by mistake)"
      },
      {
        "date": "2022-12-29T02:24:00.000Z",
        "voteCount": 3,
        "content": "B as RDS does not support parameter store"
      },
      {
        "date": "2022-12-23T07:38:00.000Z",
        "voteCount": 3,
        "content": "B\nA DB cluster parameter group acts as a container for engine configuration values that are applied to every DB instance in an Aurora DB cluster. For example, the Aurora shared storage model requires that every DB instance in an Aurora cluster use the same setting for parameters such as innodb_file_per_table. Thus, parameters that affect the physical storage layout are part of the cluster parameter group."
      },
      {
        "date": "2022-12-21T17:21:00.000Z",
        "voteCount": 3,
        "content": "B. check Hira101's comment"
      },
      {
        "date": "2022-12-14T03:21:00.000Z",
        "voteCount": 3,
        "content": "B - As RDS can not access the Parameter store:\nYou can also reference parameters in a number of other AWS services, including the following:\n\nAmazon Elastic Compute Cloud (Amazon EC2)\n\nAmazon Elastic Container Service (Amazon ECS)\n\nAWS Secrets Manager\n\nAWS Lambda\n\nAWS CloudFormation\n\nAWS CodeBuild\n\nAWS CodePipeline\n\nAWS CodeDeploy"
      },
      {
        "date": "2023-09-06T16:02:00.000Z",
        "voteCount": 1,
        "content": "Agreed,  RDS can not use Parameter Store.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html#parameter-store-features"
      },
      {
        "date": "2022-12-12T19:59:00.000Z",
        "voteCount": 2,
        "content": "Answer : B"
      },
      {
        "date": "2023-04-03T04:03:00.000Z",
        "voteCount": 1,
        "content": "While AWS Systems Manager Parameter Store, AWS Config with the Amazon RDS managed rules, and AWS Secrets Manager are all useful AWS services, they do not directly address the requirement to manage configuration parameters for Amazon Aurora DB clusters."
      },
      {
        "date": "2022-12-12T00:00:00.000Z",
        "voteCount": 2,
        "content": "Parameter Store, a capability of AWS Systems Manager, provides secure, hierarchical storage for configuration data management and secrets management. \n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/amazon/view/89480-exam-aws-certified-database-specialty-topic-1-question-272/",
    "body": "Accompany is using Amazon Redshift for its data warehouse. During review of the last few AWS monthly bills, a database specialist notices charges for Amazon Redshift backup storage. The database specialist needs to decrease the cost of these charges in the future and must create a solution that provides notification if the charges exceed a threshold.<br><br>Which combination of actions will moot those requirements with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate all manual snapshots to the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an automated snapshot schedule to take a snapshot once each day",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch billing alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic if the threshold is exceeded\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a serverless AWS Glue job to run every 4 hours to describe cluster snapshots and send an email message if the threshold is exceeded",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete manual snapshots that are not required anymore\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-23T10:41:00.000Z",
        "voteCount": 2,
        "content": "I vote for CE"
      },
      {
        "date": "2023-04-03T04:41:00.000Z",
        "voteCount": 3,
        "content": "AC\nI compared A and E:\nIn many cases, as a DBA, we can not delete the manual backup without the permission of APP owners.\nA. Migrating all manual snapshots to the S3 Standard-IA storage class is a cost-saving measure, as the S3 Standard-IA storage class is less expensive than other storage classes for infrequently accessed data. This will help reduce the cost of Amazon Redshift backup storage charges.\nE. Deleting manual snapshots that are no longer required can help save on storage costs, but it may not be necessary if the storage is already migrated to S3 Standard-IA."
      },
      {
        "date": "2023-04-28T22:26:00.000Z",
        "voteCount": 1,
        "content": "Your concerns make a point, while Option E has eliminate the possibilities of further requirement.\nSo ,it would be no harm to select E."
      },
      {
        "date": "2023-01-09T04:38:00.000Z",
        "voteCount": 2,
        "content": "I guess it is A as it can guarantee low cost storage in future as well, how do know which manual snapshots are needed and which are not , if you know you can so the same with A as well"
      },
      {
        "date": "2022-12-26T23:14:00.000Z",
        "voteCount": 3,
        "content": "going with E just because moving all snapshots is not an ideal solutions"
      },
      {
        "date": "2022-12-14T10:06:00.000Z",
        "voteCount": 2,
        "content": "Moving unnecessary manual snapshots to S3-IA simply reduces cost but deleting unnecessary snapshots helps more.  Also moving all snapshots may not be ideal for business' needs."
      },
      {
        "date": "2022-12-13T23:50:00.000Z",
        "voteCount": 2,
        "content": "manual snapshot will stay forever if its not deleted manually"
      },
      {
        "date": "2022-11-30T18:22:00.000Z",
        "voteCount": 4,
        "content": "Answer EC \nAmazon Redshift charges for manual snapshots you take using the console, application programming interface (API), or command-line interface (CLI). Redshift Automated snapshots, which are created using Amazon Redshift's snapshot scheduling feature, are offered at no charge and can be retained for a maximum of 35 days"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/amazon/view/89703-exam-aws-certified-database-specialty-topic-1-question-273/",
    "body": "An online bookstore recently migrated its database from on-premises Oracle to Amazon Aurora PostgreSQL 13. The bookstore uses scheduled jobs to run customized SQL scripts to administer the Oracle database, running hours-long maintenance tasks, such as partition maintenance and statistics gathering. The bookstore's application team has reached out to a database specialist seeking an ideal replacement for scheduling jobs with Aurora PostgreSQL.<br><br>What should the database specialist implement to meet these requirements with MINIMAL operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EC2 instance to run on a schedule to initiate database maintenance jobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Batch with AWS Step Functions to schedule long-running database maintenance tasks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridae (Amazon CloudWatch Events) rule with AWS Lambda that runs on a schedule to initiate database maintenance jobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the pg_cron extension in the Aurora PostgreSOL database and schedule the database maintenance tasks by using the cron.schedule function\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-01T13:42:00.000Z",
        "voteCount": 6,
        "content": "Answer D \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/PostgreSQL_pg_cron.html"
      },
      {
        "date": "2023-04-03T04:54:00.000Z",
        "voteCount": 2,
        "content": "D\npg_cron is a simple cron-based job scheduler for PostgreSQL (10 or higher) that runs inside the database as an extension. It uses the same syntax as regular cron, but it allows you to schedule PostgreSQL commands directly from the database. You can also use '[1-59] seconds' to schedule a job based on an interval.\nhttps://github.com/citusdata/pg_cron"
      },
      {
        "date": "2024-03-23T21:53:00.000Z",
        "voteCount": 1,
        "content": "they need new production other than posgre"
      },
      {
        "date": "2024-01-14T01:21:00.000Z",
        "voteCount": 2,
        "content": "B\nLambda can run for only 15 minutes. Batch and StepFunctions are better fit for long-running tasks.\nPG_CRON will also work, but only within the scope of the database. Batch and StepFunctions are more flexible as this will allow orchestration of multiple steps, in and outside the DB"
      },
      {
        "date": "2023-10-08T10:46:00.000Z",
        "voteCount": 3,
        "content": "Why wouldn't you use AWS Batch with Step Functions for long-running tasks (B)?\nIt's not too straightforward but a valid option?\nWhy do you have to revert to the pg_cron package (D) when there're AWS options?\n\nhttps://hands-on.cloud/aws-step-functions-long-running-tasks/#:~:text=Long-Running%20Jobs%20In%20AWS%20Cloud,-There're%20multiple&amp;text=AWS%20Batch%20is%20used%20for,can%20do%20a%20bit%20more."
      },
      {
        "date": "2022-12-28T05:07:00.000Z",
        "voteCount": 4,
        "content": "Ans: D\nI check for the closest answers and then research on my own. That way I get a chance to figure which answer is right and why."
      },
      {
        "date": "2022-12-14T09:59:00.000Z",
        "voteCount": 1,
        "content": "I'm confused.  What's the correct answer? Sorry I'm new to this site.  My answer was C but it says \"Suggested Answer\" smh"
      },
      {
        "date": "2022-12-19T20:48:00.000Z",
        "voteCount": 2,
        "content": "Most of the time community answers are the correct ones."
      },
      {
        "date": "2022-12-13T23:51:00.000Z",
        "voteCount": 4,
        "content": "Answer D"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/amazon/view/90979-exam-aws-certified-database-specialty-topic-1-question-274/",
    "body": "A company is preparing to release a new application. During a load test on the application before launch, the company noticed that its Amazon RDS for MySQL database responded more slowly than expected. As a result, the application did not meet performance goals. A database specialist must determine which SQL statements are consuming the most load.<br><br>Which set of steps should the database specialist take to obtain this information?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to RDS Performance Insights. Select the database that is associated with the application. Update the counter metrics to show top_sql. Update the time range to when the load test occurred. Review the top SQL statements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to RDS Performance Insights. Select the database that is associated with the application. Update the time range to when the load test occurred. Change the slice to SQL. Review the top SQL statements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to Amazon CloudWatch. Select the metrics for the appropriate DB instance. Review the top SQL statements metric for the time range when the load test occurred. Create a CloudWatch dashboard to watch during future load tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to Amazon CloudWatch. Find the log group for the application's database. Review the top-sql-statements log file for the time range when the load test occurred."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T21:57:00.000Z",
        "voteCount": 5,
        "content": "B is correct . \"A\" does mention performance insights but the steps are not correct."
      },
      {
        "date": "2023-11-12T16:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.Components.AvgActiveSessions.TopLoadItemsTable.TopSQL.html"
      },
      {
        "date": "2023-07-23T10:36:00.000Z",
        "voteCount": 2,
        "content": "I vote for B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/amazon/view/89481-exam-aws-certified-database-specialty-topic-1-question-275/",
    "body": "A company is using an Amazon Aurora PostgreSQL DB cluster for the backend of its mobile application. The application is running continuously and a database specialist is satisfied with high availability and fast failover, but is concerned about performance degradation after failover.<br><br>How can the database specialist minimize the performance degradation after failover?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable cluster cache management for the Aurora DB cluster and set the promotion priority for the writer DB instance and replica to tier-0\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable cluster cache management tor the Aurora DB cluster and set the promotion priority for the writer DB instance and replica to tier-1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Query Plan Management for the Aurora DB cluster and perform a manual plan capture",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Query Plan Management for the Aurora DB cluster and force the query optimizer to use the desired plan"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T18:29:00.000Z",
        "voteCount": 5,
        "content": "A\nFailover priority of writer and reader needs to be Tier-0\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2024-01-14T01:31:00.000Z",
        "voteCount": 1,
        "content": "A\nTier-o is needed as per https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.cluster-cache-mgmt.html"
      },
      {
        "date": "2022-12-23T07:54:00.000Z",
        "voteCount": 1,
        "content": "A\nSet the value of the apg_ccm_enabled cluster parameter to 1 and both writer and reader tier 0 for failover priority."
      },
      {
        "date": "2022-12-21T12:30:00.000Z",
        "voteCount": 1,
        "content": "I don\u2019t know guys &amp; gals, distributing optimizer plans globally seem to me to be a better way to get consistent query performance from the failover. \n\nhttps://aws.amazon.com/about-aws/whats-new/2018/12/amazon-aurora-postgresql-compatibility-adds-query-plan-management/"
      },
      {
        "date": "2022-12-13T23:54:00.000Z",
        "voteCount": 1,
        "content": "A\nFailover priority of writer and reader needs to be Tier-0"
      },
      {
        "date": "2022-12-10T21:59:00.000Z",
        "voteCount": 2,
        "content": "refer the link given by Sab"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/amazon/view/90980-exam-aws-certified-database-specialty-topic-1-question-276/",
    "body": "A company wants to move its on-premises Oracle database to an Amazon Aurora PostgreSQL DB cluster. The source database includes 500 GB of data. 900 stored procedures and functions, and application source code with embedded SQL statements. The company understands there are some database code objects and custom features that may not be automatically converted and may need some manual intervention. Management would like to complete this migration as fast as possible with minimal downtime.<br><br>Which tools and approach should be used to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to perform data migration and to automatically create all schemas with Aurora PostgreSQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DMS to perform data migration and use the AWS Schema Conversion Tool (AWS SCT) to automatically generate the converted code\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) to automatically convert all types of Oracle schemas to PostgreSQL and migrate the data to Aurora",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the dump and pg_dump utilities for both data migration and schema conversion"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T23:24:00.000Z",
        "voteCount": 3,
        "content": "B. AWS DMS for data migration and AWS SCT for generating the converted code\n\n\"AWS SCT will automatically assess and convert the source database schema and a majority of the database code objects, including views, stored procedures, and functions, to a format compatible with the target database. Any objects that cannot be automatically converted are clearly marked as action items with prescriptive instructions on how to convert, so that they can be manually converted to complete the migration.\nAWS SCT can also scan your application source codes for embedded SQL statements and convert them as part of a database-schema-conversion project. During this process, AWS SCT performs cloud-native code optimization by converting legacy Oracle and SQL Server functions to their equivalent AWS service, helping to modernize the applications at the same time of database migration. Once schema conversion is complete, it can help migrate data from a range of data warehouses to Amazon Redshift using built-in data migration agents.\"\n\nhttps://aws.amazon.com/dms/schema-conversion-tool/"
      },
      {
        "date": "2023-04-03T06:24:00.000Z",
        "voteCount": 1,
        "content": "B\nUsing AWS DMS for data migration is a suitable approach as it can migrate data from Oracle to Aurora PostgreSQL with minimal downtime. However, it does not address the issue of converting the Oracle code to PostgreSQL.\n\nUsing AWS SCT can help with the conversion of stored procedures and functions, as well as application source code with embedded SQL statements. AWS SCT also allows for customization and manual intervention where needed. It can generate the converted code and provide recommendations for any manual changes required."
      },
      {
        "date": "2023-03-11T11:21:00.000Z",
        "voteCount": 1,
        "content": "B, but this is not SCS exam question"
      },
      {
        "date": "2023-02-11T13:17:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer SCT doesn't migrate data"
      },
      {
        "date": "2022-12-10T22:01:00.000Z",
        "voteCount": 2,
        "content": "B is correct . SCT can not be used for migration , It helps to  generate  code for conversion of non compatible components / items ."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/amazon/view/89482-exam-aws-certified-database-specialty-topic-1-question-277/",
    "body": "A company recently launched a mobile app that has grown in popularity during the last week. The company started development in the cloud and did not initially follow security best practices during development of the mobile app. The mobile app gives customers the ability to use the platform anonymously. Platform architects use Amazon ElastiCache for Redis in a VPC to manage session affinity (sticky sessions) and cookies for customers.<br><br>The company's security team now mandates encryption in transit and encryption at rest for all traffic. A database specialist is using the AWS CLI to comply with this mandate.<br><br>Which combination of steps should the database specialist take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual backup of the existing Redis replication group by using the create-snapshot command. Restore from the backup by using the create-replication-group command\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the --transit-encryption-enabled parameter on the new Redis replication group\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the --at-rest-encryption-enabled parameter on the existing Redis replication group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the --transit-encryption-enabled parameter on the existing Redis replication group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the --at-rest-encryption-enabled parameter on the new Redis replication group\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual backup of the existing Redis replication group by using the CreateBackupSelection command. Restore from the backup by using the StartRestoreJob command"
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T18:35:00.000Z",
        "voteCount": 8,
        "content": "ABE\nTLS and Encryption at rest cannot be configured on existing cluster.\nSnapshot CLI command - create-snapshot\nRestore snapshot to a new cluster :- create-cache-cluster or create-replication-group"
      },
      {
        "date": "2022-12-14T00:07:00.000Z",
        "voteCount": 5,
        "content": "ABE\nTLS and Encryption at rest cannot be configured on existing cluster."
      },
      {
        "date": "2023-09-14T07:46:00.000Z",
        "voteCount": 1,
        "content": "TLS and encryption at rest cannot be configured on existing cluster"
      },
      {
        "date": "2023-03-05T15:41:00.000Z",
        "voteCount": 1,
        "content": "Could be DAB too . https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/amazon/view/89484-exam-aws-certified-database-specialty-topic-1-question-278/",
    "body": "A company is using Amazon DocumentDB (with MongoDB compatibility) to manage its complex documents. Users report that an Amazon DocumentDB cluster takes a long time to return query results. A database specialist must investigate and resolve this issue.<br><br>Which of the following can the database specialist use to investigate the query plan and analyze the query performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS X-Ray deep linking",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon CloudWatch Logs Insights",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMongoDB explain() method\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS CloudTrail with a custom filter"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T17:19:00.000Z",
        "voteCount": 2,
        "content": "C. MongoDB explain() method"
      },
      {
        "date": "2023-05-24T07:08:00.000Z",
        "voteCount": 3,
        "content": "explain() in order to retrieve the query plan, since that was the question..."
      },
      {
        "date": "2022-12-01T13:46:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/documentdb/latest/developerguide/user_diagnostics.html#user_diagnostics-query_plan"
      },
      {
        "date": "2022-11-30T18:39:00.000Z",
        "voteCount": 4,
        "content": "C\nIf a query runs slow, it could be because the query execution requires a full scan of the collection to choose the relevant documents. Sometimes creating appropriate indexes enables the query to run faster. To detect this scenario and decide the fields on which to create the indexes, use the explain command.\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/user_diagnostics.html#user_diagnostics-query_plan"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/amazon/view/89425-exam-aws-certified-database-specialty-topic-1-question-279/",
    "body": "A company's database specialist is migrating a production Amazon RDS for MySQL database to Amazon Aurora MySQL. The source database is configured for Multi-AZ. The company's production team wants to validate the target database before switching the associated application over to use the new database endpoint. The database specialist plans to use AWS Database Migration Service (AWS DMS) for the migration.<br><br>Which steps should the database specialist perform to meet the production team's requirement? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable automatic backups on the source database\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable automatic backups on the source database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable binary logging. Set the binlog format parameter to ROW on the source database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable binary logging. Set the binlog_format parameter to MIXED on the source database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the source primary database as the source endpoint for the DMS task. Configure the task as full load plus change data capture(CDC) to complete the migration\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the source secondary database as the source endpoint for the DMS task. Configure the task as full load plus change data capture (CDC) to complete the migration"
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T10:11:00.000Z",
        "voteCount": 6,
        "content": "I think the answer is ACE.  However, C should be binlog_format (not binlog format(. The information below is from https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.Homogeneous.\nWhen using an AWS-managed MySQL-compatible database as a source for AWS DMS, make sure that you have the following prerequisites for CDC:\n\nTo enable binary logs for RDS for MySQL and for RDS for MariaDB, enable automatic backups at the instance level.\nSet the binlog_format parameter to \"ROW\"."
      },
      {
        "date": "2024-01-14T01:41:00.000Z",
        "voteCount": 1,
        "content": "ACE\nThe binlog_format has to be ROW as per https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html"
      },
      {
        "date": "2023-12-04T00:50:00.000Z",
        "voteCount": 1,
        "content": "why not D? MIXED is recommended unless you have a need for a specific binlog format."
      },
      {
        "date": "2023-09-14T07:48:00.000Z",
        "voteCount": 1,
        "content": "ACE Is Correct"
      },
      {
        "date": "2022-12-02T05:58:00.000Z",
        "voteCount": 4,
        "content": "ACE should be right."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 280,
    "url": "https://www.examtopics.com/discussions/amazon/view/89704-exam-aws-certified-database-specialty-topic-1-question-280/",
    "body": "A media company hosts a highly available news website on AWS but needs to improve its page load time, especially during very popular news releases. Once a news page is published, it is very unlikely to change unless an error is identified. The company has decided to use Amazon ElastiCache.<br><br>What is the recommended strategy for this use case?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse ElastiCache for Memcached with write-through and long time to live (TTL)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse ElastiCache for Redis with lazy loading and short time to live (TTL)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse ElastiCache for Memcached with lazy loading and short time to live (TTL)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse ElastiCache for Redis with write-through and long time to live (TTL)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T15:53:00.000Z",
        "voteCount": 1,
        "content": "B is answer. Write through means low availability while providing fresh data, so A and D are out. Memcached is incorrect in terms of HA, because it doesn't have replication feature and would lose data when node failure occur. There's an exception, though. If Memcached is severless, C would be answer. It supports replication https://aws.amazon.com/blogs/aws/amazon-elasticache-serverless-for-redis-and-memcached-now-generally-available/"
      },
      {
        "date": "2023-08-15T11:12:00.000Z",
        "voteCount": 1,
        "content": "C. Noted that:\n1. The news website needs HA, not the cache system.\n2. Write-though can't rescue node failure or scaling out, whereas lazy-loading stale data can be overcome by adding TTL.\n3. Simpler, easy to scale out goes to Memcached; complex and fancy features goes to Redis.\nSo A and D dropped because no write-though. Pick C and drop B due to simpler, scale-out architecture.\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
      },
      {
        "date": "2023-05-24T07:48:00.000Z",
        "voteCount": 1,
        "content": "It does not matter if you lose a Memcached node - any data lost will be reloaded from the source. You do not need replication of data offered by Redis. \"Short TTL\" is a relative term. A new story is newsworthy 24 hrs? Is that short enough to qualify as short? How long is a long TTL?"
      },
      {
        "date": "2023-05-24T07:37:00.000Z",
        "voteCount": 2,
        "content": "This is not an easy one... One has to choose between\n* Redis or Memcached\n* write-through and long TTL, or short TTL with lazy loading\n\nI'll choose lazy loading because there is no way of saying whether an article will be popular\nI'll choose Memcached because there is no need for complex data structures and Memcached scales better vertically."
      },
      {
        "date": "2023-03-24T17:28:00.000Z",
        "voteCount": 2,
        "content": "A long time to live (TTL) is recommended in this scenario because news pages are unlikely to change frequently. Setting a long TTL will allow the cached data to be reused for a longer period, reducing the number of requests to the database and improving page load times.\n\nTherefore, the correct answer is D. Use ElastiCache for Redis with write-through and long time to live (TTL)."
      },
      {
        "date": "2023-03-12T07:36:00.000Z",
        "voteCount": 4,
        "content": "D. Use ElastiCache for Redis with write-through and long time to live (TTL).\n\nThe use case requires improving the page load time of a highly available news website, especially during popular news releases, where there is a high level of traffic. ElastiCache can help in achieving this by caching frequently accessed data in-memory, reducing the number of requests to the database and improving the overall performance of the website.\n\nRedis is recommended for this use case because it provides better performance for read-intensive workloads and has more advanced features for caching, such as write-through caching. Write-through caching ensures that any changes made to the data in the cache are also written back to the database, ensuring data consistency."
      },
      {
        "date": "2023-02-13T10:13:00.000Z",
        "voteCount": 2,
        "content": "long time to live as the articles are unlikely to change. Redis over Memcached, as it supports a wider range of data structures."
      },
      {
        "date": "2023-01-03T15:48:00.000Z",
        "voteCount": 2,
        "content": "Why not Memcache though?  Can someone please explain?"
      },
      {
        "date": "2023-04-03T08:31:00.000Z",
        "voteCount": 1,
        "content": "I agree with A.\nWe need to look at the difference between Memcache and Redis: Memcache for simple value return, but redis for complex data model."
      },
      {
        "date": "2023-04-19T07:01:00.000Z",
        "voteCount": 1,
        "content": "Because the requirement is for highly available site - Memcached can not be made highly available out of the box, Redis can."
      },
      {
        "date": "2022-12-28T06:26:00.000Z",
        "voteCount": 1,
        "content": "D is the answer - \nSince the news is not likely to change it will be there once it is written and with long TTL it will be there for longer time."
      },
      {
        "date": "2022-12-06T07:39:00.000Z",
        "voteCount": 1,
        "content": "D right answer"
      },
      {
        "date": "2022-12-07T20:20:00.000Z",
        "voteCount": 1,
        "content": "With Write through, if a node goes down it won't cache again until there is a write operation. So if the pages are not modified, caching won't happen. Also, with Write-Through, all the writes will be cached and it can reason for memory being used be less frequently accessed pages. High TTL can further increase that problem."
      },
      {
        "date": "2022-12-01T13:48:00.000Z",
        "voteCount": 3,
        "content": "Elasticache for redis since application needs to be highly available. Memcache dont have HA. Lazy Writing since there is no much modifications expected on page."
      },
      {
        "date": "2022-12-19T23:55:00.000Z",
        "voteCount": 2,
        "content": "How about the short TTL?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 281,
    "url": "https://www.examtopics.com/discussions/amazon/view/90820-exam-aws-certified-database-specialty-topic-1-question-281/",
    "body": "A company migrated an on-premises Oracle database to Amazon RDS for Oracle. A database specialist needs to monitor the latency of the database.<br><br>Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish RDS Performance insights metrics to Amazon CloudWatch. Add AWS CloudTrail filters to monitor database performance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Oracle Statspack. Enable the performance statistics feature to collect, store, and display performance data to monitor database performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable RDS Performance Insights to visualize the database load. Enable Enhanced Monitoring to view how different threads use the CPU\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DB parameter group that includes the AllocatedStorage, DBInstanceClassMemory, and DBInstanceVCPU variables. Enable RDS Performance Insights"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T00:37:00.000Z",
        "voteCount": 1,
        "content": "C\nAlthough I'd go with regular CloudWatch for monitoring latency. However, out of the provided answers, C is the only one that makes sense (barely)"
      },
      {
        "date": "2023-08-31T12:38:00.000Z",
        "voteCount": 3,
        "content": "C. Enable RDS Performance Insights to visualize the database load. Enable Enhanced Monitoring to view how different threads use the CPU\n\nhttps://aws.amazon.com/rds/performance-insights/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html"
      },
      {
        "date": "2023-07-19T08:09:00.000Z",
        "voteCount": 1,
        "content": "I vote for C."
      },
      {
        "date": "2022-12-22T07:27:00.000Z",
        "voteCount": 2,
        "content": "Can somebody please answer and explain the answers to the question no 271,272,277,282,283,284,285.\nThere are no discussions on these questions it seems."
      },
      {
        "date": "2022-12-09T08:24:00.000Z",
        "voteCount": 4,
        "content": "Enhance monitoring will give granular details and performance insights will give you database performance metrics"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 282,
    "url": "https://www.examtopics.com/discussions/amazon/view/89489-exam-aws-certified-database-specialty-topic-1-question-282/",
    "body": "A database administrator is working on transferring data from an on-premises Oracle instance to an Amazon RDS for Oracle DB instance through an AWS Database Migration Service (AWS DMS) task with ongoing replication only. The database administrator noticed that the migration task failed after running successfully for some time. The logs indicate that there was generic error. The database administrator wants to know which data definition language (DDL) statement caused this issue.<br><br>What should the database administrator do to identify' this issue in the MOST operationally efficient manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport AWS DMS logs to Amazon CloudWatch and identify the DDL statement from the AWS Management Console",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on logging for the AWS DMS task by setting the TARGET_LOAD action with the level of severity set to LOGGER_SEVERITY_DETAILED_DEBUG",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on DDL activity tracing in the RDS for Oracle DB instance parameter group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on logging for the AWS DMS task by setting the TARGET_APPLY action with the level of severity' set to LOGGER_SEVERITY_DETAILED_DEBUG\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T19:03:00.000Z",
        "voteCount": 5,
        "content": "Answer D\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.Logging.html"
      },
      {
        "date": "2024-01-14T00:40:00.000Z",
        "voteCount": 1,
        "content": "D\nTARGET_APPLY is for CDC. The question mentions the DMS task is CDC only.\nTARGE_LOAD is for the initial load"
      },
      {
        "date": "2023-05-28T17:00:00.000Z",
        "voteCount": 4,
        "content": "TARGET_APPLY \u2013 Data and data definition language (DDL) statements are applied to the target database. (correct)\nTARGET_LOAD \u2013 Data is loaded into the target database."
      },
      {
        "date": "2022-12-19T23:58:00.000Z",
        "voteCount": 4,
        "content": "D. \nTARGET_APPLY \u2013 Data and data definition language (DDL) statements are applied to the target database.\nLOGGER_SEVERITY_DETAILED_DEBUG \u2013 All information is written to the log."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 283,
    "url": "https://www.examtopics.com/discussions/amazon/view/104510-exam-aws-certified-database-specialty-topic-1-question-283/",
    "body": "A company is migrating its 200 GB on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The original database columns include NOT NULL and foreign key constraints. A database administrator needs to complete the migration while following best practices for database migrations.<br><br>Which option meets these requirements to migrate the database to AWS?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to migrate the database to an Aurora PostgreSQL DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to connect to the source database and load the data into the target Aurora PostgreSQL DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the PostgreSQL tools pg_dump and pg_restore to migrate to the Aurora PostgreSQL DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora PostgreSQL read replica and promote the read replica to become primary once it is synchronized."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-28T23:24:00.000Z",
        "voteCount": 5,
        "content": "about the concerns about heterogeneous for SCT:\nPer: \nhttps://aws.amazon.com/dms/schema-conversion-tool/\nSCT would be used during the migration between:\nSrc : PostgreSQL   Des : Aurora MySQL, Aurora PostgreSQL, MySQL, PostgreSQL\n\nA is the best practice\nC  downtime,transfer, reload,verify   ...  too much workloads"
      },
      {
        "date": "2023-10-08T09:32:00.000Z",
        "voteCount": 1,
        "content": "AWS encourages to use AWS services and it might expect you also to answer inline with that so will go with A as there is no proper difference between A and C"
      },
      {
        "date": "2023-10-08T02:55:00.000Z",
        "voteCount": 2,
        "content": "We can have downtime in this scenario. In the first link is mentioned that if you have more than 100 GB aws the pg_dump and pg_restore function MAY not be suitable for your usecase, but if time is not the problem here it could be suitable. The second link points out the current bp and there is nothing mentioned about DMS. I'm realy not sure about this one but I would go with c here.\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-full-load-pd_dump.html\n\nBest Practice:\nhttps://aws.amazon.com/blogs/database/best-practices-for-migrating-postgresql-databases-to-amazon-rds-and-amazon-aurora/"
      },
      {
        "date": "2023-09-14T07:56:00.000Z",
        "voteCount": 1,
        "content": "Only option here is A\nC. cannot be used - pg_dump and pg_restore is only for DBs 100 GB or less\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-full-load.html"
      },
      {
        "date": "2023-09-05T05:51:00.000Z",
        "voteCount": 1,
        "content": "A. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS)"
      },
      {
        "date": "2023-07-30T14:47:00.000Z",
        "voteCount": 1,
        "content": "I will pick A. Per linke, https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-full-load-publisher.html, 100G is the threshhold. If it is less than 100G, C works fine. But in this question, the database size is 200G. So I will go with A."
      },
      {
        "date": "2023-05-28T14:26:00.000Z",
        "voteCount": 1,
        "content": "NOT A- This is a homogeneous migration . SCT is required only for Heterogeneous migration. \nNOT D- Can't crate RR using On-prem PG as source\nNOT B - Irrelevant \n'C' is the answer - Easy, Error free and fast migration method"
      },
      {
        "date": "2023-05-25T00:02:00.000Z",
        "voteCount": 1,
        "content": "A. It is quite a bit of work to set up. There is no need for schema conversion since it is a homogeneous migration.\nB. What?\nC. If you need to create secondary database objects, then pg_dump and pg_restore is the most appropriate option. However, this option incurs a performance tradeoff compared to other options.\nD. Creating Aurora read-replicas from self-managed databases is not supported.\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-summary.html"
      },
      {
        "date": "2023-05-26T08:05:00.000Z",
        "voteCount": 1,
        "content": "\"There is no need for schema conversion since it is a homogeneous migration\" &gt;&gt;&gt; this alone means that you have never used DMS."
      },
      {
        "date": "2023-05-04T11:50:00.000Z",
        "voteCount": 1,
        "content": "Ans: C\n- Nothing said about downtime\n- 200Gb is not so big size"
      },
      {
        "date": "2023-05-26T08:05:00.000Z",
        "voteCount": 1,
        "content": "\"following best practices for database migrations\" &gt;&gt;&gt; that's what needed to be said about downtime. \n\nAnd where exactly did you get the notion that 200GB is not \"big\"."
      },
      {
        "date": "2023-04-17T18:20:00.000Z",
        "voteCount": 1,
        "content": "Dont need SCT as the migration is not heterogenous.(PostgreSQL to PostgreSQL)"
      },
      {
        "date": "2023-05-26T08:06:00.000Z",
        "voteCount": 1,
        "content": "SCT isn't just for conversion... It's also for schema copy, which DMS needs. SCT + DMS is almost always a guaranteed combo."
      },
      {
        "date": "2023-04-03T18:20:00.000Z",
        "voteCount": 3,
        "content": "A.\nCustomers looking to migrate self-managed PostgreSQL databases to Amazon RDS for PostgreSQL or Aurora PostgreSQL, can use one of the three main approaches.\n\nUse a native or third-party database migration method such as pg_dump and pg_restore for full load only migrations.\n\nUse a managed service such as AWS Database Migration Service (AWS DMS) for full load and ongoing replication.\n\nUse a native tool for full load and a managed AWS DMS service for ongoing replication. We call this strategy the hybrid approach.\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql.html"
      },
      {
        "date": "2023-04-18T19:01:00.000Z",
        "voteCount": 2,
        "content": "A \u2014 no downtime once use the replication for ongoing data change\nC \u2014 downtime"
      },
      {
        "date": "2023-06-18T00:46:00.000Z",
        "voteCount": 1,
        "content": "I also selected A initially but I think C is the correct answer.\nBecause here the issue is secondary database objects, those would not be migrated properlyu using A."
      },
      {
        "date": "2023-06-18T00:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-summary.html#:~:text=pg_dump%20and%20pg_restore%20is%20the%20most%20appropriate%20option"
      },
      {
        "date": "2023-03-30T15:11:00.000Z",
        "voteCount": 2,
        "content": "Option C is the best option here. Option A could have been correct if Schema conversion was not mentioned. Option B is not the most efficient way and option D is not applicable as Aurora doesn't support on-prem database source for creating read replica."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 284,
    "url": "https://www.examtopics.com/discussions/amazon/view/103811-exam-aws-certified-database-specialty-topic-1-question-284/",
    "body": "A company is working on migrating a large Oracle database schema with 3,500 stored procedures to Amazon Aurora PostgreSQL. An application developer is using the AWS Schema Conversion Tool (AWS SCT) to convert code from Oracle to Aurora PostgreSQL. However, the code conversion is taking a longer time with performance issues. The application team has reached out to a database specialist to improve the performance of the AWS SCT conversion.<br><br>What should the database specialist do to resolve the performance issues?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS SCT, turn on the balance speed with memory consumption performance option with the optimal memory settings on local desktop.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision the target Aurora PostgreSQL database with a higher instance class. In AWS SCT. turn on the balance speed with memory consumption performance option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS SCT, turn on the fast conversion with large memory consumption performance option and set the JavaOptions section to the maximum memory available.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a client Amazon EC2 machine with more CPU and memory resources in the same AWS Region as the Aurora PostgreSQL database."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-25T00:43:00.000Z",
        "voteCount": 1,
        "content": "A. Probably won't help\nB. Increasing the size of the Aurora instance will not improve the performance of AWS SCT.\nC. \"Use one of these methods to control the memory usage and performance of the AWS SCT tool.\"\nD. \"It's a best practice to install the AWS SCT on a separate machine that's in the same network as your source. This allows for better performance in the code conversion and data migration phases\""
      },
      {
        "date": "2023-04-28T23:32:00.000Z",
        "voteCount": 2,
        "content": "Key factor would be the performance of the SCT running env.\n\nhttps://repost.aws/knowledge-center/dms-optimize-aws-sct-performance\n\nFast conversion, but large memory consumption - This optimizes the speed of the conversion. But, it might need more memory for the object reference cache.\n\n+ Java \nIn the JavaOptions section, set the minimum and maximum memory available to the AWS SCT. This example sets a minimum of 4GB and a maximum of 40GB:"
      },
      {
        "date": "2023-03-30T15:23:00.000Z",
        "voteCount": 1,
        "content": "Definitely option C. check this link to validate - https://repost.aws/knowledge-center/dms-optimize-aws-sct-performance.\nThe goal is to improve performance and not memory consumption so, option A must be ruled out as balanced setting will compromise performance and option A doesn't even mention Java memory setting which is another key mechanism to improve SCT performance. Remaining options are distractors so, option C is the best."
      },
      {
        "date": "2023-03-27T12:55:00.000Z",
        "voteCount": 1,
        "content": "The answer is C\nRefer to the link below, the compute power of the target database does not matter. \n\"The performance of the AWS SCT depends on the memory that's available on the local machine that it's installed on. If you increase the memory that's available to the AWS SCT, then you also speed up the performance of your conversion. But this means that the AWS SCT also consumes more memory resources on your local machine.\"\n\nFurthermore fast conversion tells SCT to use optimise speed at the expense of memory"
      },
      {
        "date": "2023-03-24T17:39:00.000Z",
        "voteCount": 1,
        "content": "To resolve the performance issues of the AWS SCT conversion, the database specialist should provision the target Aurora PostgreSQL database with a higher instance class and turn on the balance speed with memory consumption performance option in AWS SCT.\n\nOption B is the correct answer because increasing the instance class of the target Aurora PostgreSQL database will improve its processing power and memory, which will allow it to handle the AWS SCT conversion more efficiently. Additionally, turning on the balance speed with memory consumption performance option in AWS SCT will optimize the conversion process for performance and memory usage."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 285,
    "url": "https://www.examtopics.com/discussions/amazon/view/103962-exam-aws-certified-database-specialty-topic-1-question-285/",
    "body": "A company has a 12-node Amazon Aurora MySQL DB cluster. The company wants to use three specific Aurora Replicas to handle the workload from one of its read-only applications.<br><br>Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CNAMEs to set up DNS aliases for the three Aurora Replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Aurora custom endpoint for the three Aurora Replicas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the cluster reader endpoint. Configure the failover priority of the three Aurora Replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the specific instance endpoints for each of the three Aurora Replicas."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-01T19:21:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B\nCustom endpoint\nA custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Custom"
      },
      {
        "date": "2023-03-30T15:25:00.000Z",
        "voteCount": 1,
        "content": "B (custom endpoint) is the correct option. Refer to this url for more details - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Custom"
      },
      {
        "date": "2023-03-26T07:35:00.000Z",
        "voteCount": 4,
        "content": "B custom endpoint https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Custom"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 286,
    "url": "https://www.examtopics.com/discussions/amazon/view/103963-exam-aws-certified-database-specialty-topic-1-question-286/",
    "body": "A company uses an Amazon Aurora MySQL DB cluster with the most recent version of the MySQL database engine. The company wants all data that is transferred between clients and the DB cluster to be encrypted.<br><br>What should a database specialist do to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on data encryption when modifying the DB cluster by using the AWS Management Console or by using the AWS CLI to call the modify-db-cluster command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the key pair for the DB instance. Reference that file from the --key-name option when connecting with a MySQL client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on data encryption by using AWS Key Management Service (AWS KMS). Use the AWS KMS key to encrypt the connections between a MySQL client and the Aurora DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the require_secure_transport parameter in the DB cluster parameter group. Download the root certificate for the DB instance. Reference that file from the --ssl-ca option when connecting with a MySQL client.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-08T03:13:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2023-09-05T16:38:00.000Z",
        "voteCount": 2,
        "content": "D. Turn on the require_secure_transport parameter in the DB cluster parameter group"
      },
      {
        "date": "2023-09-04T00:38:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDD"
      },
      {
        "date": "2023-03-26T07:42:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Security.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 287,
    "url": "https://www.examtopics.com/discussions/amazon/view/103964-exam-aws-certified-database-specialty-topic-1-question-287/",
    "body": "A database specialist needs to move an Amazon RDS DB instance from one AWS account to another AWS account.<br><br>Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate the DB instance from the source AWS account to the destination AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a DB snapshot of the DB instance. Share the snapshot with the destination AWS account. Create a new DB instance by restoring the snapshot in the destination AWS account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Multi-AZ deployment for the DB instance. Create a read replica for the DB instance in the source AWS account. Use the read replica to replicate the data into the DB instance in the destination AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to back up the DB instance in the source AWS account. Use AWS Resource Access Manager (AWS RAM) to restore the backup in the destination AWS account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-04T00:38:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBB"
      },
      {
        "date": "2023-05-11T05:20:00.000Z",
        "voteCount": 1,
        "content": "B is right. No migration (source to destination) involved, just move and best way is using snapshot sharing."
      },
      {
        "date": "2023-03-26T07:46:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-db-instance-to-another-vpc-or-account.html"
      },
      {
        "date": "2023-04-18T19:16:00.000Z",
        "voteCount": 1,
        "content": "Vote for B"
      },
      {
        "date": "2023-05-18T23:31:00.000Z",
        "voteCount": 1,
        "content": "From the same reference link, A is right as well. \nA --&gt; DMS: less downtime\nB-&gt; manual steps, more downtime"
      },
      {
        "date": "2023-07-24T14:10:00.000Z",
        "voteCount": 1,
        "content": "C even less downtime\nbut it the downtime requirements are not clear"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 288,
    "url": "https://www.examtopics.com/discussions/amazon/view/104401-exam-aws-certified-database-specialty-topic-1-question-288/",
    "body": "A company uses Amazon DynamoDB as a data store for multi-tenant data. Approximately 70% of the reads by the company's application are strongly consistent. The current key schema for the DynamoDB table is as follows:<br><br>Partition key: OrgID -<br><br>Sort key: TenantID#Version -<br><br>Due to a change in design and access patterns, the company needs to support strongly consistent lookups based on the new schema below:<br><br>Partition key: OrgID#TenantID -<br><br>Sort key: Version -<br><br>How can the database specialist implement this change?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global secondary index (GSI) on the existing table with the specified partition and sort key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a local secondary index (LSI) on the existing table with the specified partition and sort key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new table with the specified partition and sort key. Create an AWS Glue ETL job to perform the transformation and write the transformed data to the new table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new table with the specified partition and sort key. Use AWS Database Migration Service (AWS DMS) to migrate the data to the new table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-25T00:59:00.000Z",
        "voteCount": 6,
        "content": "xA. Create a global secondary index (GSI) on the existing table with the specified partition and sort key. - GSI does not support strongly consistent searches\nxB. Create a local secondary index (LSI) on the existing table with the specified partition and sort key. - the secondary index only contains the sort key, it does not help for the partition key, and it cannot be added after the table has been created.\nC. Create a new table with the specified partition and sort key. Create an AWS Glue ETL job to perform the transformation and write the transformed data to the new table. - by elimination, this has to be the correct answer\nxD. Create a new table with the specified partition and sort key. Use AWS Database Migration Service (AWS DMS) to migrate the data to the new table. - DMS only supports DynamoDB as a target"
      },
      {
        "date": "2023-07-10T07:52:00.000Z",
        "voteCount": 1,
        "content": "C\nyou can not update the sort key after the table is provisioned. However, you can create a new table and put the existing data in the newly created table, and delete the old table."
      },
      {
        "date": "2023-04-19T06:47:00.000Z",
        "voteCount": 1,
        "content": "GSI uses the same data and is eventually consistent - we need to combine two columns to create new primary key, plus it needs to be strongly consistent. Hence new table and glue, but a bit of a trick question none the less, since it asks for LEAST effort."
      },
      {
        "date": "2023-04-03T17:55:00.000Z",
        "voteCount": 1,
        "content": "A. Create a global secondary index (GSI) on the existing table with the specified partition and sort key.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.scenario\n\"By creating a GSI with the new partition and sort key, the existing table can continue to be used, and there is no need to create a new table or migrate the data. This approach can support strongly consistent lookups for the new access pattern while maintaining the existing access pattern. A GSI is ideal for handling queries with different access patterns without changing the primary key of the table.\""
      },
      {
        "date": "2023-04-03T18:03:00.000Z",
        "voteCount": 1,
        "content": "A maybe not the right answer --\n\"Data synchronization between tables and Global Secondary Indexes\nDynamoDB automatically synchronizes each global secondary index with its base table. When an application writes or deletes items in a table, any global secondary indexes on that table are updated asynchronously, using an eventually consistent model. Applications never write directly to an index."
      },
      {
        "date": "2023-03-29T10:17:00.000Z",
        "voteCount": 3,
        "content": "A. KO - GSI is not strongly consistent.\nB. KO - LSI cannot be created once the table exists.\nC. OK - New table  + glue\nD. KO - Dynamo is not a compatible source for DMS: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.html"
      },
      {
        "date": "2023-04-03T20:06:00.000Z",
        "voteCount": 1,
        "content": "GLUE is complex. I prefer D"
      },
      {
        "date": "2023-04-18T19:30:00.000Z",
        "voteCount": 1,
        "content": "You are right. Dynamo is not the source for DMS."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 289,
    "url": "https://www.examtopics.com/discussions/amazon/view/103965-exam-aws-certified-database-specialty-topic-1-question-289/",
    "body": "A company is using Amazon Aurora with Aurora Replicas. A database specialist needs to split up two read-only applications so that each application connects to a different set of DB instances. The database specialist wants to implement load balancing and high availability for the read-only applications.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a different instance endpoint for each application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the reader endpoint for both applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the reader endpoint for one application and an instance endpoint for the other application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse different custom endpoints for each application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-19T13:24:00.000Z",
        "voteCount": 1,
        "content": "A :  instance endpoint\n An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its own unique instance endpoint\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html"
      },
      {
        "date": "2023-09-11T06:06:00.000Z",
        "voteCount": 2,
        "content": "D. Use different custom endpoints for each application. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.Types:~:text=The%20custom%20endpoints,the%20same%20endpoint."
      },
      {
        "date": "2023-07-19T04:05:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2023-05-18T23:46:00.000Z",
        "voteCount": 1,
        "content": "B\nAs per the doc, If the cluster contains one or more Aurora Replicas, the reader endpoint load-balances each connection request among the Aurora Replicas. I\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.Types"
      },
      {
        "date": "2023-03-26T07:53:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.Types"
      },
      {
        "date": "2023-05-26T08:36:00.000Z",
        "voteCount": 3,
        "content": "has to be D as \"each application connects to a different set of DB instances\""
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 290,
    "url": "https://www.examtopics.com/discussions/amazon/view/103967-exam-aws-certified-database-specialty-topic-1-question-290/",
    "body": "A company uses an Amazon DynamoDB table to store data for an application. The application requires full access to the table. Some employees receive direct access to the table, but a security policy restricts their access to only certain fields. The company wants to begin using a DynamoDB Accelerator (DAX) cluster on top of the DynamoDB table.<br><br>How can the company ensure that the security policy is maintained after the implementation of the DAX cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM policies for the employees. Implement user-level separation that allows the employees to access the DAX cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM policies for the IAM service role of the DAX cluster. Implement user-level separation to allow access to DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM policies for the employees. Allow the employees to access the DAX cluster without allowing the employees to access the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM policies for the employees. Allow the employees to access the DynamoDB table without allowing the employees to access the DAX cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T00:54:00.000Z",
        "voteCount": 3,
        "content": "D\n\"The application requires full access to the table. Some employees receive direct access to the table, but a security policy restricts their access to only certain fields\"\nTherefore, the app will use DAX, and the employees can access the table directly."
      },
      {
        "date": "2023-09-14T07:19:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.access-control.html"
      },
      {
        "date": "2023-08-31T12:12:00.000Z",
        "voteCount": 2,
        "content": "D. Modify the IAM policies for the employees. Allow the employees to access the DynamoDB table without allowing the employees to access the DAX cluster. \n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.access-control.html\n\n\"If you are currently using IAM roles and policies to restrict access to DynamoDB tables data, then the use of DAX can subvert those policies. For example, a user could have access to a DynamoDB table via DAX but not have explicit access to the same table accessing DynamoDB directly\""
      },
      {
        "date": "2023-08-10T07:11:00.000Z",
        "voteCount": 1,
        "content": "What is the final correct answer D or C ? I am split between the two - 50-50 at the moment."
      },
      {
        "date": "2023-07-18T18:04:00.000Z",
        "voteCount": 1,
        "content": "It's D."
      },
      {
        "date": "2023-05-28T18:21:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.access-control.html\nAccoding to the Docs, D would be correct, since we won't be able to restrict users from accessing some parts of DAX while allowing the application to access all parts."
      },
      {
        "date": "2023-05-25T05:31:00.000Z",
        "voteCount": 4,
        "content": "xA. Already in place...\nxB. User-level separation not supported by DAX\nxC. Users of the DAX cluster \"inherit\" the DynamoDB policy of DAX\nD. The employees need to continue to access the Table directly from DynamoDB, while the applications use DAX.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.access-control.html"
      },
      {
        "date": "2023-05-26T10:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Answer C is wrong because DAX doesn't allow attribute-level access control.\n\nFor example, you can create a user role that only allows the user to perform read-only actions on a particular DynamoDB table. (For more information, see Identity and Access Management for Amazon DynamoDB.) By comparison, the DAX security model focuses on cluster security, and the ability of the cluster to perform DynamoDB API actions on your behalf."
      },
      {
        "date": "2023-03-26T08:06:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.access-control.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 291,
    "url": "https://www.examtopics.com/discussions/amazon/view/103968-exam-aws-certified-database-specialty-topic-1-question-291/",
    "body": "A company uses an Amazon Redshift cluster to support its business intelligence (BI) team. The cluster has a maintenance window that overlaps with some business report jobs that run long-running queries on the cluster. During a recent maintenance window, the cluster went offline and restarted for an update. The BI team wants to know which queries were terminated during the maintenance window.<br><br>What should a database specialist do to obtain this information?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLook for the terminated queries in the SVL_QLOG view.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLook for the terminated queries in the SVL_QUERY_REPORT view.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a scalar SQL user-defined function to find the terminated queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a federated query to find the terminated queries."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-26T08:10:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/r_SVL_QLOG.html"
      },
      {
        "date": "2023-04-04T05:14:00.000Z",
        "voteCount": 2,
        "content": "table column - abported:\n\"\nIf a query was stopped by the system or canceled by the user, this column contains 1. If the query ran to completion, this column contains 0. Queries that are canceled for workload management purposes and subsequently restarted also have a value of 1 in this column.\""
      },
      {
        "date": "2024-01-14T00:16:00.000Z",
        "voteCount": 1,
        "content": "SVL_QLOG view"
      },
      {
        "date": "2023-08-31T12:19:00.000Z",
        "voteCount": 2,
        "content": "A. Look for the terminated queries in the SVL_QLOG view."
      },
      {
        "date": "2023-03-31T07:52:00.000Z",
        "voteCount": 2,
        "content": "A is the correct option. We can query SVL_QLOG table to check for aborted queries\n\nselect query, elapsed, trim(label) querylabel from svl_qlog where aborted=1;"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 292,
    "url": "https://www.examtopics.com/discussions/amazon/view/103971-exam-aws-certified-database-specialty-topic-1-question-292/",
    "body": "A database specialist observes several idle connections in an Amazon RDS for MySQL DB instance. The DB instance is using RDS Proxy. An application is configured to connect to the proxy endpoint.<br><br>What should the database specialist do to control the idle connections in the database?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the MaxConnectionsPercent parameter through the RDS Proxy console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CALL mysql.rds_kill(thread-id) for the IDLE threads that are returned from the SHOW FULL PROCESSLIST command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the MaxIdleConnectionsPercent parameter for the RDS proxy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the max_connections configuration setting for the DB instance. Modify the ConnectionBorrowTimeout parameter for the RDS proxy."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-26T08:19:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-managing.html#rds-proxy-connection-pooling-tuning.maxidleconnectionspercent"
      },
      {
        "date": "2023-04-04T05:27:00.000Z",
        "voteCount": 2,
        "content": "MaxIdleConnectionsPercent\n\"You can control the number of idle database connections that RDS Proxy can keep in the connection pool. RDS Proxy considers a database connection in it's pool to be idle when there's been no activity on the connection for five minutes.\n\nYou specify the limit as a percentage of the maximum connections available for your database. The default value is 50 percent of MaxConnectionsPercent, and the upper limit is the value of MaxConnectionsPercent. With a high value, the proxy leaves a high percentage of idle database connections open. With a low value, the proxy closes a high percentage of idle database connections. If your workloads are unpredictable, consider setting a high value for MaxIdleConnectionsPercent. Doing so means that RDS Proxy can accommodate surges in activity without opening a lot of new database connections.\""
      },
      {
        "date": "2023-09-14T07:22:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-managing.html#rds-proxy-connection-pooling-tuning.maxidleconnectionspercent"
      },
      {
        "date": "2023-07-18T17:57:00.000Z",
        "voteCount": 1,
        "content": "It's C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 293,
    "url": "https://www.examtopics.com/discussions/amazon/view/103972-exam-aws-certified-database-specialty-topic-1-question-293/",
    "body": "An online retailer uses Amazon DynamoDB for its product catalog and order data. Some popular items have led to frequently accessed keys in the data, and the company is using DynamoDB Accelerator (DAX) as the caching solution to cater to the frequently accessed keys. As the number of popular products is growing, the company realizes that more items need to be cached. The company observes a high cache miss rate and needs a solution to address this issue.<br><br>What should a database specialist do to accommodate the changing requirements for DAX?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of nodes in the existing DAX cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DAX cluster with more nodes. Change the DAX endpoint in the application to point to the new cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DAX cluster using a larger node type. Change the DAX endpoint in the application to point to the new cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the node type in the existing DAX cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-26T08:27:00.000Z",
        "voteCount": 7,
        "content": "Larger nodes can enable the cluster to store more data in memory, reducing cache misses and improving overall application performance of the application. \n\nYou can't modify the node types on a running DAX cluster. Instead, you must create a new cluster with the desired node type. \n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.cluster-management.html#DAX.cluster-management.scaling"
      },
      {
        "date": "2023-04-04T05:51:00.000Z",
        "voteCount": 1,
        "content": "Scaling a DAX cluster\nThere are two options available for scaling a DAX cluster. The first option is horizontal scaling, where you add read replicas to the cluster. The second option is vertical scaling, where you select different node types."
      },
      {
        "date": "2023-05-26T08:30:00.000Z",
        "voteCount": 3,
        "content": "why do you consistently spread misinfo on the answers... The nodes contain the same set of data... If you scale them horizontally you simply increase throughput. Larger node type is required when you have cache misses"
      },
      {
        "date": "2023-09-14T07:25:00.000Z",
        "voteCount": 2,
        "content": "Create a new DAX cluster using a larger node type - Vertical scaling\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.cluster-management.html#DAX.cluster-management.scaling"
      },
      {
        "date": "2023-09-05T06:23:00.000Z",
        "voteCount": 1,
        "content": "C. Create a new DAX cluster using a larger node type."
      },
      {
        "date": "2023-07-18T17:52:00.000Z",
        "voteCount": 1,
        "content": "It's C."
      },
      {
        "date": "2023-05-11T05:48:00.000Z",
        "voteCount": 1,
        "content": "My vote is C"
      },
      {
        "date": "2023-04-13T05:15:00.000Z",
        "voteCount": 1,
        "content": "Either A or B, or C is a valid solution."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 294,
    "url": "https://www.examtopics.com/discussions/amazon/view/103973-exam-aws-certified-database-specialty-topic-1-question-294/",
    "body": "A financial services company is using AWS Database Migration Service (AWS DMS) to migrate its databases from on-premises to AWS. A database administrator is working on replicating a database to AWS from on-premises using full load and change data capture (CDC). During the CDC replication, the database administrator observed that the target latency was high and slowly increasing.<br><br>What could be the root causes for this high target latency? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere was ongoing maintenance on the replication instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe source endpoint was changed by modifying the task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoopback changes had affected the source and target instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere was no primary key or index in the target database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere were resource bottlenecks in the replication instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-31T08:21:00.000Z",
        "voteCount": 5,
        "content": "Clearly D &amp; E are the correct options as per this URL for High TARGET Latency\n\nhttps://repost.aws/knowledge-center/dms-high-target-latency"
      },
      {
        "date": "2023-04-04T07:08:00.000Z",
        "voteCount": 2,
        "content": "according to the link -- I vote DE\n If the CDCLatencySource isn't high, but the CDCLatencyTarget is high, the latency could be caused by the following:\n\nThere are no primary keys or indexes in the target\nThere are resource bottlenecks in the target\nThere are resource bottlenecks in the replication instance\nThere is a network issue between the replication instance and the target"
      },
      {
        "date": "2023-09-14T07:27:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/dms-high-target-latency"
      },
      {
        "date": "2023-09-05T13:00:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/dms-high-target-latency"
      },
      {
        "date": "2023-07-19T04:08:00.000Z",
        "voteCount": 2,
        "content": "It's DE"
      },
      {
        "date": "2023-03-26T08:34:00.000Z",
        "voteCount": 1,
        "content": "A not sure\nE sure\n\nhttps://repost.aws/knowledge-center/dms-high-source-latency"
      },
      {
        "date": "2023-09-05T12:59:00.000Z",
        "voteCount": 2,
        "content": "high target latency link as below\n\nhttps://repost.aws/knowledge-center/dms-high-target-latency"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 295,
    "url": "https://www.examtopics.com/discussions/amazon/view/103975-exam-aws-certified-database-specialty-topic-1-question-295/",
    "body": "A database specialist needs to set up an Amazon DynamoDB table. The table must exist in multiple AWS Regions and must provide point-in-time recovery of data.<br><br>Which combination of steps should the database specialist take to meet these requirements with the LEAST operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB Streams for a global table. Set the view type to new and old images.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB Streams for all replica tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a replica table for each Region. Ensure that table names are not already in use in each replica Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a replica table for each Region with a random suffix added to each table name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable point-in-time recovery for the global table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable point-in-time recovery for all replica tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T00:24:00.000Z",
        "voteCount": 2,
        "content": "ACF will meet the requirement"
      },
      {
        "date": "2023-07-19T04:09:00.000Z",
        "voteCount": 2,
        "content": "It's ACF"
      },
      {
        "date": "2023-03-26T08:44:00.000Z",
        "voteCount": 3,
        "content": "A https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables.tutorial.html\nC https://aws.amazon.com/dynamodb/global-tables/\nF https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/pointintimerecovery_beforeyoubegin.html"
      },
      {
        "date": "2023-04-13T03:23:00.000Z",
        "voteCount": 1,
        "content": "Creating a global table (AWS CLI)\nFollow these steps to create a global table Music using the AWS CLI. The following example creates a global table, with replica tables in the United States and in Europe.\n\nCreate a new table (Music) in US East (Ohio), with DynamoDB Streams enabled (NEW_AND_OLD_IMAGES)."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 296,
    "url": "https://www.examtopics.com/discussions/amazon/view/103976-exam-aws-certified-database-specialty-topic-1-question-296/",
    "body": "A company uses an Amazon Aurora MySQL DB cluster. A database specialist has configured the DB cluster to use the automated backup feature with a 10-day retention period. The company wants the database specialist to reduce the cost of the database backup storage as much as possible without causing downtime. It is more important for the company to optimize costs than it is to retain a large set of database backups.<br><br>Which set of actions should the database specialist take on the DB cluster to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the automated backup feature by changing the backup retention period to 0 days. Perform manual snapshots daily. Delete old snapshots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the backup retention period to 1 day. Remove old manual snapshots if they are no longer required.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the backup retention period at 10 days. Remove old manual snapshots if they are no longer required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the automated backup feature by changing the backup retention period to 0 days. Create a backup plan in AWS Backup to perform daily backups."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-14T07:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html"
      },
      {
        "date": "2023-07-18T14:59:00.000Z",
        "voteCount": 1,
        "content": "It's B."
      },
      {
        "date": "2023-05-28T21:32:00.000Z",
        "voteCount": 2,
        "content": "B looks most correct."
      },
      {
        "date": "2023-03-26T08:49:00.000Z",
        "voteCount": 4,
        "content": "You can't disable automated backups on Aurora. The backup retention period for Aurora is managed by the DB cluster.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 297,
    "url": "https://www.examtopics.com/discussions/amazon/view/103977-exam-aws-certified-database-specialty-topic-1-question-297/",
    "body": "A company is running a mobile app that has a backend database in Amazon DynamoDB. The app experiences sudden increases and decreases in activity throughout the day. The company\u2019s operations team notices that DynamoDB read and write requests are being throttled at different times, resulting in a negative customer experience.<br><br>Which solution will solve the throttling issue without requiring changes to the app?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a DynamoDB table in a secondary AWS Region. Populate the additional table by using DynamoDB Streams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon ElastiCache cluster in front of the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse on-demand capacity mode for the DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DynamoDB Accelerator (DAX)."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-05T17:19:00.000Z",
        "voteCount": 3,
        "content": "C. on-demand capacity mode \n\n\"With on-demand capacity mode, DynamoDB charges you for the data reads and writes your application performs on your tables. You do not need to specify how much read and write throughput you expect your application to perform because DynamoDB instantly accommodates your workloads as they ramp up or down.\"\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html#:~:text=With%20on%2Ddemand,up%20or%20down."
      },
      {
        "date": "2023-08-13T01:32:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-07-18T14:57:00.000Z",
        "voteCount": 1,
        "content": "It's C."
      },
      {
        "date": "2023-05-26T00:17:00.000Z",
        "voteCount": 2,
        "content": "A. That will not resolve the write throttling\nB. ElastiCache does not make sense with DynamoDB\nC. Why not provisioned w/autoscaling? Anyway, this is the answer that makes more sense\nD. Using DynamoDB Accelerator does require a change to the app"
      },
      {
        "date": "2023-04-17T16:48:00.000Z",
        "voteCount": 4,
        "content": "C Because the issue is with not only Read but writes also. DAX will only improve read performance"
      },
      {
        "date": "2023-04-04T10:36:00.000Z",
        "voteCount": 1,
        "content": "D\nDynamoDB Accelerator (DAX) is an in-memory cache that can be used to improve the performance of DynamoDB by caching frequently accessed data. It can help reduce the number of read requests to the backend DynamoDB table, thus reducing the likelihood of throttling.\nOption C, using on-demand capacity mode for the DynamoDB table, is a valid solution for avoiding throttling, but it may not be the most cost-effective option for applications with consistent or high workloads."
      },
      {
        "date": "2023-05-26T08:25:00.000Z",
        "voteCount": 1,
        "content": "The app experiences sudden increases and decreases in activity throughout the day. The company\u2019s operations team notices that DynamoDB read and write requests are being throttled at different times\n\n&gt;&gt; the pattern is not consistent. Also DAX, being a cache, helps nothing with writes"
      },
      {
        "date": "2023-03-26T08:52:00.000Z",
        "voteCount": 1,
        "content": "On demand."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 298,
    "url": "https://www.examtopics.com/discussions/amazon/view/103979-exam-aws-certified-database-specialty-topic-1-question-298/",
    "body": "A global company needs to migrate from an on-premises Microsoft SQL Server database to a highly available database solution on AWS. The company wants to modernize its application and keep operational costs low. The current database includes secondary indexes and stored procedures that need to be included in the migration. The company has limited availability of database specialists to support the migration and wants to automate the process.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate all database objects from the on-premises SQL Server database to a Multi-AZ deployment of Amazon Aurora MySQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) and the AWS Schema Conversion Tool (AWS SCT) to migrate all database objects from the on-premises SQL Server database to a Multi-AZ deployment of Amazon Aurora MySQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRehost the on-premises SQL Server as a SQL Server Always On availability group. Host members of the availability group on Amazon EC2 instances. Use AWS Database Migration Service (AWS DMS) to migrate all database objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRehost the on-premises SQL Server as a SQL Server Always On availability group. Host members of the availability group on Amazon EC2 instances in a single subnet that extends across multiple Availability Zones. Use SQL Server tools to migrate the data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-18T14:56:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2023-05-28T21:35:00.000Z",
        "voteCount": 2,
        "content": "B is right."
      },
      {
        "date": "2023-05-26T00:21:00.000Z",
        "voteCount": 2,
        "content": "A. Aurora certainly hits the spot wrt modernisation... but what about secondary objects?\nB. This takes care of secondary objects\nC. Rehosting is not \"modernisation\"\nD. Rehosting is not \"modernisation\""
      },
      {
        "date": "2023-05-11T06:19:00.000Z",
        "voteCount": 1,
        "content": "B is right, heterogeneous migration &gt; SCT + DMS"
      },
      {
        "date": "2023-03-31T10:49:00.000Z",
        "voteCount": 1,
        "content": "Option B. Since it is a heterogeneous migration we need both DMS and SCT to perform this migration and option B has both components.   \n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-a-microsoft-sql-server-database-to-aurora-mysql-by-using-aws-dms-and-aws-sct.html"
      },
      {
        "date": "2023-03-26T09:01:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-a-microsoft-sql-server-database-to-aurora-mysql-by-using-aws-dms-and-aws-sct.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 299,
    "url": "https://www.examtopics.com/discussions/amazon/view/103980-exam-aws-certified-database-specialty-topic-1-question-299/",
    "body": "A company is using an Amazon Aurora PostgreSQL DB cluster for a project. A database specialist must ensure that the database is encrypted at rest. The database size is 500 GB.<br><br>What is the FASTEST way to secure the data through encryption at rest in the DB cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a manual snapshot of the unencrypted DB cluster. Create an encrypted copy of that snapshot in the same AWS Region as the unencrypted snapshot. Restore a DB cluster from the encrypted snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Key Management Service (AWS KMS) key in the same AWS Region and create a new encrypted Aurora cluster using this key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a manual snapshot of the unencrypted DB cluster. Restore the unencrypted snapshot to a new encrypted Aurora PostgreSQL DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new encrypted Aurora PostgreSQL DB cluster. Use AWS Database Migration Service (AWS DMS) to migrate the data from the unencrypted DB cluster to the encrypted DB cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T00:32:00.000Z",
        "voteCount": 2,
        "content": "C\nAurora allows you to create encrypted DB out of unencrypted snapshot\n(this is not true for regular RDS, where A would be the right answer)"
      },
      {
        "date": "2023-08-16T15:01:00.000Z",
        "voteCount": 1,
        "content": "C. After try-n-error in the AWS RDS Snapshot console, got this (make sure the Engine column must be Aurora PostgreSQL):\nunencrypted snapshot -&gt; encrypted snapshot\nunencrypted snapshot -&gt; restore to encrypted instance\nencrypted snapshot -&gt; restore to encrypted instance\nencrypted snapshot -&gt; change key -&gt; encrypted snapshot\nA: No. I can't find any way in console that cp and encrypt a snapshot but marked as unencrypted.\nBTW, I can't unencrypted an encrypted snapshot in RDS Snapshot console. I even tried clear out the key ARN field then copy, but it will go back to whatever key it originally has after copy.\nNo comment / test on RDS non-Aurora snapshot cycle since the question scope doesn't cover."
      },
      {
        "date": "2023-07-23T08:18:00.000Z",
        "voteCount": 2,
        "content": "C\nhttps://repost.aws/knowledge-center/encrypt-rds-snapshots\nImportant: If you use Amazon Aurora, you can restore an unencrypted Aurora DB cluster snapshot to an encrypted Aurora DB cluster. However, you must specify an AWS Key Management Service (AWS KMS) encryption key when you restore from the unencrypted DB cluster snapshot."
      },
      {
        "date": "2023-07-18T14:53:00.000Z",
        "voteCount": 1,
        "content": "It's C."
      },
      {
        "date": "2023-05-28T21:47:00.000Z",
        "voteCount": 1,
        "content": "You cannot restore an unecrypted DB cluster from an encrypted snapshot."
      },
      {
        "date": "2023-04-29T00:43:00.000Z",
        "voteCount": 1,
        "content": "concur with  Mintwater."
      },
      {
        "date": "2023-04-29T00:49:00.000Z",
        "voteCount": 1,
        "content": "After review, I changed my mind to support C\nFor Aurora Cluster :\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html\nYou can't convert an unencrypted DB cluster to an encrypted one. However, you can restore an unencrypted snapshot to an encrypted Aurora DB cluster. To do this, specify a KMS key when you restore from the unencrypted snapshot."
      },
      {
        "date": "2023-04-16T05:50:00.000Z",
        "voteCount": 3,
        "content": "A is the best response\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations"
      },
      {
        "date": "2023-05-09T04:50:00.000Z",
        "voteCount": 1,
        "content": "It's RDS and this problem is Aurora. C is correct for Aurora."
      },
      {
        "date": "2023-04-08T20:44:00.000Z",
        "voteCount": 1,
        "content": "A - \nA. Take a manual snapshot of the unencrypted DB cluster. Create an encrypted copy of that snapshot in the same AWS Region as the unencrypted snapshot. Restore a DB cluster from the encrypted snapshot.\nLimitations of Amazon RDS encrypted DB instances\nThe following limitations exist for Amazon RDS encrypted DB instances:\n\nYou can only encrypt an Amazon RDS DB instance when you create it, not after the DB instance is created.\n\nHowever, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. For more information, see Copying a DB snapshot.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations"
      },
      {
        "date": "2023-05-09T04:50:00.000Z",
        "voteCount": 2,
        "content": "It's RDS and this problem is Aurora. C is correct for Aurora."
      },
      {
        "date": "2023-04-08T20:48:00.000Z",
        "voteCount": 1,
        "content": "to create a manual snapshot (unencrypted) =&gt; create another encrypted copy of that unencrypted snapshot ==&gt; restore the encrypted snapshot to the new encrypted instance"
      },
      {
        "date": "2023-03-26T09:05:00.000Z",
        "voteCount": 2,
        "content": "You can't convert an unencrypted DB cluster to an encrypted one. However, you can restore an unencrypted snapshot to an encrypted Aurora DB cluster. To do this, specify a KMS key when you restore from the unencrypted snapshot.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html"
      },
      {
        "date": "2023-04-04T08:17:00.000Z",
        "voteCount": 1,
        "content": "agree C"
      },
      {
        "date": "2023-04-04T08:28:00.000Z",
        "voteCount": 1,
        "content": "Not B:\nYou can't create an encrypted Aurora Replica from an unencrypted Aurora DB cluster. You can't create an unencrypted Aurora Replica from an encrypted Aurora DB cluster."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 300,
    "url": "https://www.examtopics.com/discussions/amazon/view/103981-exam-aws-certified-database-specialty-topic-1-question-300/",
    "body": "A database specialist is designing the database for a software-as-a-service (SaaS) version of an employee information application. In the current architecture, the change history of employee records is stored in a single table in an Amazon RDS for Oracle database. Triggers on the employee table populate the history table with historical records.<br><br>This architecture has two major challenges. First, there is no way to guarantee that the records have not been changed in the history table. Second, queries on the history table are slow because of the large size of the table and the need to run the queries against a large subset of data in the table.<br><br>The database specialist must design a solution that prevents modification of the historical records. The solution also must maximize the speed of the queries.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the current solution to an Amazon DynamoDB table. Use DynamoDB Streams to keep track of changes. Use DynamoDB Accelerator (DAX) to improve query performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite employee record history to Amazon Quantum Ledger Database (Amazon QLDB) for historical records and to an Amazon OpenSearch Service (Amazon Elasticsearch Service) domain for queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora PostgreSQL to store employee record history in a single table. Use Aurora Auto Scaling to provision more capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a solution that uses an Amazon Redshift cluster for historical records. Query the Redshift cluster directly as needed."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-08T00:16:00.000Z",
        "voteCount": 4,
        "content": "B. Amazon QLDB and Amazon OpenSearch Service\n\nhttps://aws.amazon.com/qldb/faqs/\n\nhttps://aws.amazon.com/opensearch-service/"
      },
      {
        "date": "2023-07-18T14:50:00.000Z",
        "voteCount": 2,
        "content": "It's B."
      },
      {
        "date": "2023-05-28T21:57:00.000Z",
        "voteCount": 3,
        "content": "\"prevents modification of the historical records\" automatically means QLDB, so B"
      },
      {
        "date": "2023-05-11T06:36:00.000Z",
        "voteCount": 1,
        "content": "B is right. Key is need for immutability of DB. Note the hint - one of major challenges - First, there is no way to guarantee that the records have not been changed in the history table"
      },
      {
        "date": "2023-03-26T09:07:00.000Z",
        "voteCount": 2,
        "content": "Maybe b"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 301,
    "url": "https://www.examtopics.com/discussions/amazon/view/104407-exam-aws-certified-database-specialty-topic-1-question-301/",
    "body": "A company is using Amazon Redshift. A database specialist needs to allow an existing Redshift cluster to access data from other Redshift clusters. Amazon RDS for PostgreSQL databases, and AWS Glue Data Catalog tables.<br><br>Which combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the required tables from the other Redshift clusters. Restore the snapshot into the existing Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate external tables in the existing Redshift database to connect to the AWS Glue Data Catalog tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnload the RDS tables and the tables from the other Redshift clusters into Amazon S3. Run COPY commands to load the tables into the existing Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse federated queries to access data in Amazon RDS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse data sharing to access data from the other Redshift clusters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue jobs to transfer the AWS Glue Data Catalog tables into Amazon S3. Create external tables in the existing Redshift database to access this data."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-08T12:55:00.000Z",
        "voteCount": 4,
        "content": "BDE are minimal efforts while the rest are copying the data and how about the future data? you can't be copying them all the time"
      },
      {
        "date": "2023-09-04T01:11:00.000Z",
        "voteCount": 1,
        "content": "MAYBE BDE"
      },
      {
        "date": "2023-07-18T11:11:00.000Z",
        "voteCount": 1,
        "content": "It's BDE"
      },
      {
        "date": "2023-03-31T12:11:00.000Z",
        "voteCount": 4,
        "content": "Option B (external tables) to connect Glue Data catalog tables from Redshift \nhttps://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html\n\nOption D (federated queries) to query Amazon RDS for PostgreSQL databases from Redshift\nhttps://docs.aws.amazon.com/redshift/latest/dg/federated-overview.html\n\nOption E (data sharing) to access data in other redshift clusters\nhttps://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html"
      },
      {
        "date": "2023-03-29T10:50:00.000Z",
        "voteCount": 1,
        "content": "Maybe BDE"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 302,
    "url": "https://www.examtopics.com/discussions/amazon/view/103989-exam-aws-certified-database-specialty-topic-1-question-302/",
    "body": "A company is planning to migrate a 40 TB Oracle database to an Amazon Aurora PostgreSQL DB cluster by using a single AWS Database Migration Service (AWS DMS) task within a single replication instance. During early testing, AWS DMS is not scaling to the company's needs. Full load and change data capture (CDC) are taking days to complete.<br><br>The source database server and the target DB cluster have enough network bandwidth and CPU bandwidth for the additional workload. The replication instance has enough resources to support the replication. A database specialist needs to improve database performance, reduce data migration time, and create multiple DMS tasks.<br><br>Which combination of changes will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the value of the ParallelLoadThreads parameter in the DMS task settings for the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a smaller set of tables with each DMS task. Set the MaxFullLoadSubTasks parameter to a higher value.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a smaller set of tables with each DMS task. Set the MaxFullLoadSubTasks parameter to a lower value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse parallel load with different data boundaries for larger tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the DMS tasks on a larger instance class. Increase local storage on the instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-08T05:34:00.000Z",
        "voteCount": 3,
        "content": "It's B&amp;D:\nA - Not applicable for Oracle see:https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.TargetMetadata.html#:~:text=ParallelLoadThreads%20%E2%80%93%20Specifies%20the%20number%20of%20%20threads,Kafka%2C%20or%20Amazon%20Elasticsearch%20Service%20target%20is%2032.\n\nB - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.FullLoad.html\n\nD - https://aws.amazon.com/blogs/database/speed-up-database-migration-by-using-aws-dms-with-parallel-load-and-filter-options/"
      },
      {
        "date": "2023-07-19T04:30:00.000Z",
        "voteCount": 1,
        "content": "It's BD"
      },
      {
        "date": "2023-05-30T03:13:00.000Z",
        "voteCount": 1,
        "content": "How about A and B? What is the data boundary meaning of option D?"
      },
      {
        "date": "2023-05-11T06:47:00.000Z",
        "voteCount": 1,
        "content": "BD my vote. Please note Question is asking 2 answer choices..."
      },
      {
        "date": "2023-04-29T01:15:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/database/speed-up-database-migration-by-using-aws-dms-with-parallel-load-and-filter-options/\n B + D"
      },
      {
        "date": "2023-03-31T12:26:00.000Z",
        "voteCount": 2,
        "content": "B and D are correct options"
      },
      {
        "date": "2023-03-26T09:52:00.000Z",
        "voteCount": 2,
        "content": "B and d\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.FullLoad.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 303,
    "url": "https://www.examtopics.com/discussions/amazon/view/103986-exam-aws-certified-database-specialty-topic-1-question-303/",
    "body": "A financial services company is running a MySQL database on premises. The database holds details about all customer interactions and the financial advice that the company provided. The write traffic to the database is well known and consistent. However, the read traffic is subject to significant and sudden increases for end-of-month reporting. The database is becoming overloaded during these periods of heavy read activity.<br><br>The company decides to move the database to AWS. A database specialist needs to propose a solution in the AWS Cloud that will scale to meet the variable read traffic requirements without affecting the performance of write traffic. Scaling events must not require any downtime.<br><br>What is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a MySQL primary node on Amazon EC2 in one Availability Zone. Deploy a MySQL read replica on Amazon EC2 in a different Availability Zone. Configure a scheduled scaling event to increase the CPU capacity and RAM capacity within the MySQL read replica the day before each known traffic surge. Configure a scheduled scaling event to reduce the CPU capacity and RAM capacity within the MySQL read replica the day after each known traffic surge.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon Aurora MySQL DB cluster. Select a Cross-AZ configuration with an Aurora Replica. Create an Aurora Auto Scaling policy to adjust the number of Aurora Replicas based on CPU utilization. Direct all read-only reporting traffic to the reader endpoint for the DB cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS for MySQL Multi-AZ database as a write database. Deploy a second RDS for MySQL Multi-AZ database that is configured as an auto scaling read-only database. Use AWS Database Migration Service (AWS DMS) to continuously replicate data from the write database to the read-only database. Direct all read-only reporting traffic to the reader endpoint for the read-only database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon DynamoDB database. Create a DynamoDB auto scaling policy to adjust the read capacity of the database based on target utilization. Direct all read traffic and write traffic to the DynamoDB database."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T23:54:00.000Z",
        "voteCount": 1,
        "content": "Only B makes sense."
      },
      {
        "date": "2023-09-07T12:07:00.000Z",
        "voteCount": 3,
        "content": "B. Deploy an Amazon Aurora MySQL DB cluster. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html"
      },
      {
        "date": "2023-09-07T12:08:00.000Z",
        "voteCount": 1,
        "content": "\" Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.\""
      },
      {
        "date": "2023-07-18T10:58:00.000Z",
        "voteCount": 1,
        "content": "It's B."
      },
      {
        "date": "2023-05-11T06:50:00.000Z",
        "voteCount": 1,
        "content": "B is my vote"
      },
      {
        "date": "2023-03-31T12:29:00.000Z",
        "voteCount": 1,
        "content": "B is the most operationally efficient way to scale read operations"
      },
      {
        "date": "2023-03-26T09:24:00.000Z",
        "voteCount": 1,
        "content": "Aurora ok"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 304,
    "url": "https://www.examtopics.com/discussions/amazon/view/103984-exam-aws-certified-database-specialty-topic-1-question-304/",
    "body": "A company has a Microsoft SQL Server 2017 Enterprise edition on Amazon RDS database with the Multi-AZ option turned on. Automatic backups are turned on and the retention period is set to 7 days. The company needs to add a read replica to the RDS DB instance.<br><br>How should a database specialist achieve this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off the Multi-AZ feature, add the read replica, and turn Multi-AZ back on again.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the backup retention period to 0, add the read replica, and set the backup retention period to 7 days again.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore a snapshot to a new RDS DB instance and add the DB instance as a replica to the original database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the new read replica without making any other changes to the RDS database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T23:57:00.000Z",
        "voteCount": 1,
        "content": "D\nNo need to disable multi-az before creating a replica"
      },
      {
        "date": "2023-07-18T10:55:00.000Z",
        "voteCount": 2,
        "content": "It's D."
      },
      {
        "date": "2023-05-11T06:53:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2023-03-26T09:20:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/database/using-in-region-read-replicas-in-amazon-rds-for-sql-server/"
      },
      {
        "date": "2023-04-04T14:50:00.000Z",
        "voteCount": 1,
        "content": "Thank you rdiaz, this is an exact link:\nWhen you create a read replica, Amazon RDS takes a snapshot of the primary DB instance and creates a new read-only instance from the snapshot. Creating or deleting the read replica doesn\u2019t require any downtime on the primary DB instance. You can create up to fifteen read replicas.\nNote: the same region, EE"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 305,
    "url": "https://www.examtopics.com/discussions/amazon/view/103654-exam-aws-certified-database-specialty-topic-1-question-305/",
    "body": "A company is using a 1 TB Amazon RDS for PostgreSQL DB instance to store user data. During a security review, a security engineer sees that the DB instance is not encrypted at rest.<br><br>How should a database specialist correct this issue with the LEAST amount of downtime and no data loss?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance by using the RDS management console, and enable encryption. Apply the changes immediately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual DB instance snapshot and then create an encrypted copy of that snapshot. Use this snapshot to create a new encrypted DB instance. Modify the application to connect to the new DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new encrypted DB instance and use AWS Database Migration Service (AWS DMS) to migrate the existing database to the encrypted DB instance. Once the instances are in sync, modify the application to connect to the new DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an encrypted read replica. Once the read replica is in sync, promote it to primary. Modify the application to connect to the new primary instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T23:59:00.000Z",
        "voteCount": 1,
        "content": "C gives the LEAST downtime\nB is easier and less risky, but requires hours of downtime"
      },
      {
        "date": "2023-09-07T12:20:00.000Z",
        "voteCount": 2,
        "content": "C. use AWS Database Migration Service (AWS DMS)\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\n\" This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime. \""
      },
      {
        "date": "2023-07-18T10:53:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2023-05-29T09:27:00.000Z",
        "voteCount": 1,
        "content": "C is right."
      },
      {
        "date": "2023-05-11T07:01:00.000Z",
        "voteCount": 1,
        "content": "D seems right choice considering 1 TB size and LEAST downtime req. Without these specific hints, snapshot is good (option B)."
      },
      {
        "date": "2023-05-11T07:02:00.000Z",
        "voteCount": 1,
        "content": "I meant to say C (not D...sorry for fat finger) is right."
      },
      {
        "date": "2023-04-16T17:59:00.000Z",
        "voteCount": 3,
        "content": "This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime. \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"
      },
      {
        "date": "2023-03-26T09:16:00.000Z",
        "voteCount": 2,
        "content": "No downtime. Dms"
      },
      {
        "date": "2023-04-04T13:49:00.000Z",
        "voteCount": 2,
        "content": "The question requirement is \"with the LEAST amount of downtime and no data loss\".\nThus I will choose C.\nOr B could work, but DOWN TIME is needed."
      },
      {
        "date": "2023-03-23T03:23:00.000Z",
        "voteCount": 1,
        "content": "1 TB Amazon RDS for PostgreSQL DB  would take ages with B. So I would do it using DMS. Therefore, C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 306,
    "url": "https://www.examtopics.com/discussions/amazon/view/115124-exam-aws-certified-database-specialty-topic-1-question-306/",
    "body": "An advertising company is developing a backend for a bidding platform. The company needs a cost-effective datastore solution that will accommodate a sudden increase in the volume of write transactions. The database also needs to make data changes available in a near real-time data stream.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora MySQL Multi-AZ DB cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Keyspaces (for Apache Cassandra)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB table with DynamoDB auto scaling\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DocumentDB (with MongoDB compatibility) cluster with a replica instance in a second Availability Zone"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-08T04:30:00.000Z",
        "voteCount": 3,
        "content": "C. Amazon DynamoDB table with DynamoDB auto scaling\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"
      },
      {
        "date": "2023-08-18T17:34:00.000Z",
        "voteCount": 2,
        "content": "DynamoDB Streams can provide a near real-time data stream of changes to the data, enabling applications to react to changes in the database in real-time"
      },
      {
        "date": "2023-07-13T23:13:00.000Z",
        "voteCount": 4,
        "content": "RealTime = DynamoDB"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 307,
    "url": "https://www.examtopics.com/discussions/amazon/view/115126-exam-aws-certified-database-specialty-topic-1-question-307/",
    "body": "An ecommerce company uses an Amazon Aurora MySQL DB cluster to process payments. The company\u2019s database specialist notices that Aurora performs database maintenance actions periodically. The database specialist is concerned because the upcoming maintenance window conflicts with a company sales event.<br><br>What should the database specialist do to address this concern with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new Aurora Replica so that the maintenance action occurs on the Aurora Replica first.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefer the maintenance action in the AWS Management Console or by using the AWS CLI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the maintenance action in the AWS Management Console or by using the AWS CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new Aurora standby DB instance so that the maintenance action occurs on the standby DB instance first."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-06T21:32:00.000Z",
        "voteCount": 1,
        "content": "\"If the maintenance value is next window, defer the maintenance items by choosing Defer upgrade from Actions.\" https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html"
      },
      {
        "date": "2023-09-07T00:28:00.000Z",
        "voteCount": 3,
        "content": "B. Defer the maintenance action"
      },
      {
        "date": "2023-07-18T10:42:00.000Z",
        "voteCount": 3,
        "content": "It's B"
      },
      {
        "date": "2023-07-13T23:23:00.000Z",
        "voteCount": 4,
        "content": "Deferring maintenance actions is the least effort"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 308,
    "url": "https://www.examtopics.com/discussions/amazon/view/115086-exam-aws-certified-database-specialty-topic-1-question-308/",
    "body": "A company wants to use AWS Organizations to create isolated accounts for different teams and functionality. The company\u2019s database administrator needs to copy a DB instance from the main account in the us-east-1 Region to a new test account in the us-west-2 Region. The database administrator has already taken a snapshot of the encrypted Amazon RDS for PostgreSQL source DB instance in the main account.<br><br>Which combination of steps must the database administrator take to copy the snapshot to the new account? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Key Management Service (AWS KMS) customer managed key in the main account in us-east-1. Replicate the key ID and key material to the test account in us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS Key Management Service (AWS KMS) customer managed key in the main account in us-east-1. Copy the key to the test account in us-west-2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the snapshot of the source DB instance to us-west-2 by using the AWS Key Management Service (AWS KMS) customer managed key. Enable encryption on the new snapshot. Share the snapshot with the test account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the snapshot of the source DB instance to the test account in us-east-1. Switch to the test account and share the snapshot with us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the test account, copy the shared snapshot to create a final snapshot. Use the final snapshot to create a new RDS for PostgreSQL DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the test account, copy the shared snapshot by using the copied AWS Key Management Service (AWS KMS) key to create a final encrypted snapshot. Use the final snapshot to create a new RDS for PostgreSQL DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "CDF",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T00:06:00.000Z",
        "voteCount": 2,
        "content": "I believe BCF is best"
      },
      {
        "date": "2023-09-20T17:12:00.000Z",
        "voteCount": 1,
        "content": "ACE  multi-Region keys replica keys.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-manage.html"
      },
      {
        "date": "2023-09-20T19:33:00.000Z",
        "voteCount": 1,
        "content": "Correction :ACF"
      },
      {
        "date": "2023-09-14T06:54:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/share-encrypted-rds-snapshot-kms-key"
      },
      {
        "date": "2023-09-08T04:44:00.000Z",
        "voteCount": 3,
        "content": "https://repost.aws/knowledge-center/share-encrypted-rds-snapshot-kms-key"
      },
      {
        "date": "2023-08-14T15:51:00.000Z",
        "voteCount": 2,
        "content": "it is BCF"
      },
      {
        "date": "2023-08-14T15:49:00.000Z",
        "voteCount": 3,
        "content": "it is BCF"
      },
      {
        "date": "2023-07-18T10:41:00.000Z",
        "voteCount": 3,
        "content": "It's BCF"
      },
      {
        "date": "2023-07-13T16:50:00.000Z",
        "voteCount": 1,
        "content": "C,E,F is answer"
      },
      {
        "date": "2023-07-13T23:25:00.000Z",
        "voteCount": 3,
        "content": "BCF is best"
      },
      {
        "date": "2023-07-17T01:32:00.000Z",
        "voteCount": 1,
        "content": "Which one is correct.. CEF or BCF ??"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 309,
    "url": "https://www.examtopics.com/discussions/amazon/view/115087-exam-aws-certified-database-specialty-topic-1-question-309/",
    "body": "A database specialist is working with a company to launch a new website. The website accesses a database on an Amazon Aurora MySQL DB cluster that is configured with several Aurora Replicas. The website will replace an on-premises website that is connected to a legacy relational database. Because of stability issues in the legacy database, the company wants to test the resiliency of the Aurora cluster.<br><br>Which action can the database specialist take to test the resiliency of the Aurora cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSimulate a failover test of the Aurora cluster resiliency by using the failover testing feature from the Resiliency Toolkit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit a fault injection query to one of the Aurora Replica instances by connecting to the endpoint for the Aurora Replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSimulate a failover test of the Aurora cluster by using the PromoteReadReplica API operation to promote one of the read replica DB instances to a standalone Aurora DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS Performance Insights to capture resiliency-related metrics for the Aurora cluster during periods of high load."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-08T13:45:00.000Z",
        "voteCount": 2,
        "content": "You can submit a fault injection query to one of your Aurora Replica instances by connecting to the endpoint for the Aurora Replica"
      },
      {
        "date": "2023-09-07T00:54:00.000Z",
        "voteCount": 2,
        "content": "B. Submit a fault injection query\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html"
      },
      {
        "date": "2023-07-18T10:35:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2023-07-13T16:54:00.000Z",
        "voteCount": 3,
        "content": "B is answer\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html"
      },
      {
        "date": "2023-09-07T00:54:00.000Z",
        "voteCount": 1,
        "content": "agreed, from above link\n\"You can test the fault tolerance of your Aurora MySQL DB cluster by using fault injection queries. Fault injection queries are issued as SQL commands to an Amazon Aurora instance. They let you schedule a simulated occurrence of one of the following events:\nA crash of a writer or reader DB instance\nA failure of an Aurora Replica\nA disk failure\nDisk congestion\""
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 310,
    "url": "https://www.examtopics.com/discussions/amazon/view/115088-exam-aws-certified-database-specialty-topic-1-question-310/",
    "body": "A company needs to troubleshoot its Amazon Aurora Serverless MySQL database. The company selected a db.t3, medium instance class for the database's initial deployment. The database experienced light usage, and performance was normal.<br><br>As the number of client connections increases, the application that is connected to the database is experiencing higher latency and occasional lost connections. A database specialist determines that the database needs to support a maximum of 2,000 simultaneous connections.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the instance class to db.r3.xlarge. Apply the changes immediately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the default parameter group for the MySQL engine that the database uses. Change the max_connections value to 2,000. Reboot the DB instance to apply the new value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new parameter group for the MySQL engine that the database uses. Set the max_connections value to 2,000. Assign the parameter group to the DB instance. Apply the changes immediately.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the instance class to db.t3.large. Apply the changes immediately."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-13T16:55:00.000Z",
        "voteCount": 6,
        "content": "In Amazon Aurora Serverless, the performance of the database is controlled by the instance class. In the current situation, an increase in client connections is causing delays and potential connection loss in the database. To support a maximum of 2,000 concurrent connections, it is necessary to upgrade to a higher-performance instance class.\n\nThe db.r3.xlarge instance class offers better performance compared to db.t3. Therefore, making this change will improve the performance of the database. By applying the changes immediately, you can resolve issues such as delays and connection losses.\n\nHence, the most cost-effective solution that meets the requirements is to change the instance class to db.r3.xlarge and apply the changes immediately."
      },
      {
        "date": "2024-04-07T04:30:00.000Z",
        "voteCount": 1,
        "content": "I agree with you. Additionally, C is definitely incorrect because we have to reboot to apply the new max_connection values. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.setting-capacity.html#aurora-serverless-v2.max-connections"
      },
      {
        "date": "2023-08-21T17:05:00.000Z",
        "voteCount": 1,
        "content": "We recommend using the T DB instance classes only for development, test, or other nonproduction servers. For more detailed recommendations for the T instance classes"
      },
      {
        "date": "2024-01-14T00:13:00.000Z",
        "voteCount": 1,
        "content": "C\nFirst, the question is obviously wrong. In Serverless, you have no instance class or size. \nHowever, since most of the question and the answers talk about specific instance sizes, I will ignore the \"Serverless\" keyword. \nTherefore, \"C\" is the most cost-effective solution. It may not be the most performant or even stable, but it is cost-effective."
      },
      {
        "date": "2023-10-08T13:50:00.000Z",
        "voteCount": 1,
        "content": "the question is wrong? does aurora server less has instance type? it says its running on t3 medium"
      },
      {
        "date": "2023-10-08T06:19:00.000Z",
        "voteCount": 3,
        "content": "Defnitely C:\n\nThe maximum number of connections allowed to an Aurora MySQL DB instance is determined by the max_connections parameter in the instance-level parameter group for the DB instance.\n\nThe following table lists the resulting default value of max_connections for each DB instance class available to Aurora MySQL. You can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\n\nThe db.t3.medium has a default value of 90. So setting the max_connections = 2000 (by creating a new parameter group) should fit the bill. Remember we are looking for a solution that keeps our costs as low as possible. Changing the instance class to db.r3.xlarge would also work but it will cost you more.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html"
      },
      {
        "date": "2023-08-19T10:52:00.000Z",
        "voteCount": 1,
        "content": "None of the answer fulfil the requirements.\n- Aurora Serverless db only support min ACUs to max ACUs. No db class available.\n- Change parameter group required reboot.\nIn short, to meet requirements, we need to:\n1. Change maxACUs to 16\n2. Create new parameter group, change max_connection to 2000, apply, and reboot\nSee:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.setting-capacity.html#aurora-serverless-v2.parameter-groups\nFor legacy Aurora, we need to:\n1. Change Aurora db server class to db.r3.xlarge\n2. Create new parameter group, change max_connection to 2000, and apply\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html\nJust keep in mind where to go in case AWS fix this questions during the real exam."
      },
      {
        "date": "2023-07-29T17:48:00.000Z",
        "voteCount": 4,
        "content": "I'm not sure if we can modify the instance class for Aurora Serverless. So I think A and D are not right. We can not edit the default parameter group. This only leaves C possible. So We will go with C."
      },
      {
        "date": "2023-07-15T04:32:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 311,
    "url": "https://www.examtopics.com/discussions/amazon/view/115089-exam-aws-certified-database-specialty-topic-1-question-311/",
    "body": "A database administrator needs to save a particular automated database snapshot from an Amazon RDS for Microsoft SQL Server DB instance for longer than the maximum number of days.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual copy of the snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the contents of the snapshot to an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the retention period of the snapshot to 45 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a native SQL Server backup. Save the backup to an Amazon S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-26T03:21:00.000Z",
        "voteCount": 1,
        "content": "most operationally efficient solution for saving a particular automated database snapshot for longer than the maximum retention period.\nnot A since it will take more time and additional resources."
      },
      {
        "date": "2023-10-08T13:57:00.000Z",
        "voteCount": 2,
        "content": "operationally efficient = manual snapshot, that's it , no additional procedures or moving to S3 needed."
      },
      {
        "date": "2023-09-28T05:24:00.000Z",
        "voteCount": 2,
        "content": "Ans: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html\n\nCopying the automated backup\n- To retain a particular automated snapshot for longer than 35 days, you can copy the snapshot. \n- The copy is stored as a manual snapshot. \n- You can retain this manual snapshot for as long as you need. \n- Unlike automated backups, manual snapshots aren't subject to the backup retention period. \n- Manual snapshots don't expire."
      },
      {
        "date": "2023-08-19T07:22:00.000Z",
        "voteCount": 3,
        "content": "A.\nManual snapshots can be retained indefinitely, allowing you to keep the snapshot for as long as you need. Same apply to MS SQL Server RDS."
      },
      {
        "date": "2023-08-18T18:13:00.000Z",
        "voteCount": 3,
        "content": "A. Create a manual copy of the snapshot.\n Manual snapshots can be retained indefinitely, allowing you to keep the snapshot for as long as you need."
      },
      {
        "date": "2023-07-19T20:46:00.000Z",
        "voteCount": 1,
        "content": "Not sure on this one, but looks like D maybe."
      },
      {
        "date": "2023-07-19T11:59:00.000Z",
        "voteCount": 1,
        "content": "B\nhttps://repost.aws/knowledge-center/rds-automated-snapshot-retain-longer"
      },
      {
        "date": "2023-07-29T17:28:00.000Z",
        "voteCount": 1,
        "content": "Per the link you provided above, the answer should be A. \"To retain a particular automated snapshot for longer than 35 days, you can copy the snapshot. \"."
      },
      {
        "date": "2023-07-15T04:36:00.000Z",
        "voteCount": 2,
        "content": "I think it is A."
      },
      {
        "date": "2023-07-13T16:56:00.000Z",
        "voteCount": 1,
        "content": "D is answer\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Procedural.Importing.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 312,
    "url": "https://www.examtopics.com/discussions/amazon/view/115090-exam-aws-certified-database-specialty-topic-1-question-312/",
    "body": "A company runs an Amazon Aurora MySQL DB instance for one of its critical applications. The company\u2019s marketing department sends promotional email messages to customers based on the data in this database. A database engineer needs to make the data from all the tables available in the company\u2019s Amazon S3 data lake. The database engineer wants to perform an export from a snapshot to populate the S3 data lake with the contents of the database.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an existing automated snapshot or manual snapshot, or create a manual snapshot of the DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the S3 bucket for export. Provide access to the S3 bucket by using an IAM user. Attach an IAM policy with s3:PutObject*, s3:GetObject*, s3:ListBucket, s3:DeleteObject*, and s3:GetBucketLocation permissions to the IAM user. Attach the IAM role to the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a copy of an existing automated snapshot or manual snapshot of the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a symmetric AWS Key Management Service (AWS KMS) key for server-side encryption. Export the snapshot to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the S3 bucket for export. Provide access to the S3 bucket by using an IAM role. Attach an IAM policy with s3:PutObject*, s3:GetQpject*, s3:ListBucket, s3:DeleteObject*, and s3:GetBucketLocation permissions to the IAM role. Attach the IAM role to the DB instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a symmetric AWS Key Management Service (AWS KMS) key for server-side encryption. Export the snapshot to Amazon S3 Glacier Flexible Retrieval."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "ABC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-19T12:07:00.000Z",
        "voteCount": 6,
        "content": "ADE\nRefer https://www.youtube.com/watch?v=lyNGeDg6EII"
      },
      {
        "date": "2023-09-08T00:42:00.000Z",
        "voteCount": 5,
        "content": "A,D,E is the correct answer.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html"
      },
      {
        "date": "2024-01-13T13:49:00.000Z",
        "voteCount": 2,
        "content": "ADE\nWe need IAM role, not IAM user\nWe need the least overhead"
      },
      {
        "date": "2023-09-14T07:02:00.000Z",
        "voteCount": 3,
        "content": "B: IAM User, this must be a Role so D\nC: Option A is better\nF: Question doesn't mention archiving the data anywhere"
      },
      {
        "date": "2023-08-19T07:40:00.000Z",
        "voteCount": 3,
        "content": "ADE.\nB: No. Shall provide IAM role, not the IAM user.\nC: No. Opt A has better scope.\nF: No. This operation would be cost saving but NOT LEAST operation overhead."
      },
      {
        "date": "2023-07-19T20:43:00.000Z",
        "voteCount": 3,
        "content": "A,D,E is the correct answer."
      },
      {
        "date": "2023-07-13T16:59:00.000Z",
        "voteCount": 2,
        "content": "A,B,C is answer\n\nA. Create an existing automatic snapshot or a manual snapshot of the DB instance. This saves the content of the database as a snapshot.\n\nB. Identify the S3 bucket to export to and provide access to the bucket using an IAM user. Attach an IAM policy to the IAM user with permissions such as s3:PutObject*, s3:GetObject*, s3:ListBucket, s3:DeleteObject*, and s3:GetBucketLocation. Attach an IAM role to the DB instance. This allows the DB instance to have access to the S3 bucket.\n\nC. Create a copy of the existing automatic snapshot or manual snapshot of the DB instance. This creates a copy of the snapshot.\n\nTherefore, the combination of steps that meets these requirements with minimal operational overhead is A, B, and C."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 313,
    "url": "https://www.examtopics.com/discussions/amazon/view/115091-exam-aws-certified-database-specialty-topic-1-question-313/",
    "body": "A large financial services company is using AWS Database Migration Service (AWS DMS) to migrate databases from on-premises to the AWS Cloud. During the migration of one of the databases, the AWS DMS replication instance entered a storage-full status. A database administrator needs to troubleshoot and fix the issue.<br><br>Which options would help the database administrator resolve this issue? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the size of the replication instance to a larger supported instance type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Management Console to modify the replication task settings to the limited large binary object (LOB) mode and set the value to 16.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to modify the replication task settings with \u2018{\u201cLogging\u201d: {\u201cDeleteTaskLogs\u201d: true}}\u2019.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to modify the replication task settings with \u2018{\u201cLogging\u201d: {\u201cCloudWatchLogGroup\u201d: null}}\u2019.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the modify-replication-instance API to increase the amount of storage allocated to the replication instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-14T04:40:00.000Z",
        "voteCount": 1,
        "content": "Base on comments"
      },
      {
        "date": "2024-04-01T06:09:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/dms-replication-instance-storage-full"
      },
      {
        "date": "2024-04-07T11:42:00.000Z",
        "voteCount": 1,
        "content": "video clearly states that we can't DeleteTaskLogs if instance is in \"storage-full\" status.  Has to be in \"Available\" status."
      },
      {
        "date": "2024-04-07T11:43:00.000Z",
        "voteCount": 2,
        "content": "thanks for sharing the video link"
      },
      {
        "date": "2024-01-13T13:51:00.000Z",
        "voteCount": 2,
        "content": "C+E\nWe need to provide more storage, not bigger instances. Also, deleting the logs will help"
      },
      {
        "date": "2023-10-08T14:10:00.000Z",
        "voteCount": 1,
        "content": "If the replication DB instance is in a storage-full status, you can\u2019t delete logs."
      },
      {
        "date": "2023-10-08T06:34:00.000Z",
        "voteCount": 2,
        "content": "While C is a good action to prevent full storage you simply can't delete logs when you've already recieved the status of full stroage. So C is out.\n\n\"If the replication DB instance is in a storage-full status, you can't delete logs. You must increase the allocated storage size, or delete the replication task. To increase the storage size of a replication DB instance:\"\n\nIt's A and E: Just check:\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full"
      },
      {
        "date": "2023-09-14T07:07:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/dms-replication-instance-storage-full\nhttps://docs.aws.amazon.com/cli/latest/reference/dms/modify-replication-instance.html"
      },
      {
        "date": "2023-08-16T23:52:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/dms-replication-instance-storage-full"
      },
      {
        "date": "2024-02-03T08:25:00.000Z",
        "voteCount": 2,
        "content": "in the link you provided it is mentionned : \"If the replication DB instance is in a storage-full status, you can't delete logs. You must increase the allocated storage size, or delete the replication task. \" and they specifically say you should Increase the storage size of the replication DB instance in that case.\nRead carefully so you don't mislead people"
      },
      {
        "date": "2023-07-19T20:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is C,E."
      },
      {
        "date": "2023-07-19T12:17:00.000Z",
        "voteCount": 2,
        "content": "CE\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full\nhttps://docs.aws.amazon.com/cli/latest/reference/dms/modify-replication-instance.html"
      },
      {
        "date": "2023-07-29T16:56:00.000Z",
        "voteCount": 3,
        "content": "In the first link you provided, it says that \"If the replication DB instance is in a storage-full status, you can't delete logs. You must increase the allocated storage size, or delete the replication task\". This statement makes C not possible because the replication instance entered storage-full status."
      },
      {
        "date": "2023-07-30T08:07:00.000Z",
        "voteCount": 1,
        "content": "Thanks Windy. I now feel A,E is the right answer."
      },
      {
        "date": "2024-01-13T13:54:00.000Z",
        "voteCount": 1,
        "content": "Not really. Once you provide more storage (coming from E), you can also delete some logs (this is C). So C+E is the correct answer.\nThe instance size will add CPU and RAM, not storage."
      },
      {
        "date": "2024-04-06T04:07:00.000Z",
        "voteCount": 1,
        "content": "But this quotation asks which option works and not a combination of options. So A and E is right because they basically do the same."
      },
      {
        "date": "2024-04-06T04:41:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind. As MultiAZ says, changing instance type by itself does nothing with the problem. Modifying MemoryLimitTotal and MemoryKeepTime values is also needed to avoid swapping. So C and E are more feasible. But the question actually looks for a help to solve, we don't have to resolve directly. The wording is too vague."
      },
      {
        "date": "2023-07-18T09:30:00.000Z",
        "voteCount": 1,
        "content": "The answer is AE."
      },
      {
        "date": "2023-07-13T17:00:00.000Z",
        "voteCount": 4,
        "content": "A,E is answer\n\nA. Change the size of the replication instance to a larger instance type supported.\nIf the replication instance has reached the storage full status, it indicates that the size of the replication instance may be insufficient for the storage capacity. By changing the instance size to a larger supported type, you can increase the storage capacity.\n\nE. Increase the amount of storage allocated to the replication instance using the modify-replication-instance API.\nIf the replication instance has reached the storage full status, it suggests that the allocated storage for the replication instance might be insufficient. By using the modify-replication-instance API, you can increase the amount of storage to address the issue.\n\nTherefore, the options that would be helpful to resolve this issue are A and E."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 314,
    "url": "https://www.examtopics.com/discussions/amazon/view/115093-exam-aws-certified-database-specialty-topic-1-question-314/",
    "body": "A company wants to implement a design that includes multiple AWS Regions to support disaster recovery for its application. The company currently has a static website that is hosted on Amazon S3 and Amazon CloudFront. The application connects to an existing Amazon DynamoDB database in the us-east-1 Region. The DynamoDB table was recently created and was initialized with a large amount of company data. The company wants to replicate the database in real time to the us-west-2 Region.<br><br>A database specialist needs to perform the replication, which must include all existing table data and any new data that is added in the future, in an automated way.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB streams. Configure streams for new and old images. Create a global table replica in us-west-2. Monitor the progress of the replication until the status changes to Active.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate global table replica in us-west-2. Monitor the progress of the replication until the status changes to Active.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB streams. Configure streams for new and old images. Create a global table replica in us-west-2. Copy all existing data from us-east-1 to us-west-2 by using an export and batch import.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global table replica in us-west-2. Copy all existing data from us-east-1 to us-west-2 by using an export and batch import."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T14:04:00.000Z",
        "voteCount": 2,
        "content": "The answer is A\nStreams (with old+new images) are a prerequisite for Global Table."
      },
      {
        "date": "2023-09-23T06:22:00.000Z",
        "voteCount": 3,
        "content": "A. Enable DynamoDB streams. Configure streams for new and old images. Create a global table replica in us-west-2. Monitor the progress of the replication until the status changes to Active.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html"
      },
      {
        "date": "2023-09-14T07:10:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/"
      },
      {
        "date": "2023-07-29T16:47:00.000Z",
        "voteCount": 1,
        "content": "I changed the answer to A. We must enable streams for the DynamoDB global table. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html"
      },
      {
        "date": "2023-07-18T07:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is B."
      },
      {
        "date": "2023-07-13T17:01:00.000Z",
        "voteCount": 2,
        "content": "By enabling DynamoDB Streams and configuring both new and old images in the stream, you ensure that all changes to the DynamoDB table are captured. Creating a global table replica in the us-west-2 region allows for real-time replication of the table data. By monitoring the progress of replication and ensuring the status is active, you can verify the successful replication of the data.\n\nThis solution ensures that all existing table data and future additions are included in an automated manner, replicating the data from the us-east-1 region to the us-west-2 region.\n\nTherefore, the solution that meets these requirements is option A."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 315,
    "url": "https://www.examtopics.com/discussions/amazon/view/115095-exam-aws-certified-database-specialty-topic-1-question-315/",
    "body": "A company provisioned a three-tier application by using AWS CloudFormation and an Amazon RDS DB instance. During a test, a database administrator accidentally deleted the CloudFormation stack. The results were a deletion of all the resources, including the DB instance, and a loss of critical data. The company wants to prevent accidental deletion of a DB instance from happening in the future.<br><br>Which solutions will meet this requirement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deletion policy of the stack to Retain.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deletion policy of the RDS resource to Retain.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deletion policy of the stack to Snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable termination protection for the RDS resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable termination protection for the stack.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-18T04:30:00.000Z",
        "voteCount": 7,
        "content": "The answer is BE. There is no termination protection for RDS. There is deletion protection for RDS."
      },
      {
        "date": "2023-09-07T13:07:00.000Z",
        "voteCount": 3,
        "content": "The answer is B and E.\n\nB. Set the deletion policy of the RDS resource to Retain.\nhttps://repost.aws/knowledge-center/delete-cf-stack-retain-resources\n\nE. Enable termination protection for the stack. \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html"
      },
      {
        "date": "2023-08-15T02:13:00.000Z",
        "voteCount": 1,
        "content": "The answer is BE"
      },
      {
        "date": "2023-07-13T17:03:00.000Z",
        "voteCount": 1,
        "content": "D,E is answer\n\nBy implementing these solutions, you can prevent accidental deletion of the DB instance and ensure the protection of important data. Enabling termination protection adds an extra layer of security and safeguards the availability of the application.\n\nTherefore, the solutions that meet these requirements are D and E."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 316,
    "url": "https://www.examtopics.com/discussions/amazon/view/115096-exam-aws-certified-database-specialty-topic-1-question-316/",
    "body": "A company that is located in the United States wants to expand its operations in Asia. The company\u2019s data in the us-west-2 Region is stored in an Amazon DynamoD8 table. The company\u2019s development team in the ap-northeast-1 Region needs to perform user acceptance testing (UAT) and several other performance feasibility tests with a copy of production data from us-west-2. The feasibility tests do not need to be run on data that is updated in real time.<br><br>Which solution will make data available from us-west-2 to ap-northeast-1 MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DynamoDB table in ap-northeast-1. Create an AWS Glue job to perform a data export from the DynamoDB table in us-west-2. Import the same data into the DynamoDB table in ap-northeast-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB Streams on the DynamoDB table in us-west-2. Create a new DynamoDB table in ap-northeast-1. Create an AWS Lambda function to poll the DynamoDB table stream in us-west-2 and to deliver batch records from the stream to the new DynamoDB table in ap-northeast-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse point-in-time recovery to restore the DynamoDB table from us-west-2 lo ap-northeast-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DynamoDB Streams on the DynamoDB table in us-west-2. Add ap-northeast-1 to the DynamoDB global tables setting in us-west-2."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-18T04:26:00.000Z",
        "voteCount": 5,
        "content": "I think C is the cost-effective. Streams cost money."
      },
      {
        "date": "2023-10-16T09:31:00.000Z",
        "voteCount": 3,
        "content": "The point-in-time recovery process restores to a new table.\n[..]\nYou can restore the table to the same AWS Region or to a different Region from where the source table resides.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.Tutorial.html"
      },
      {
        "date": "2023-09-11T13:19:00.000Z",
        "voteCount": 3,
        "content": "C. Use point-in-time recovery to restore the DynamoDB table"
      },
      {
        "date": "2023-09-04T01:43:00.000Z",
        "voteCount": 2,
        "content": "CCCCCCCCC"
      },
      {
        "date": "2023-08-19T09:17:00.000Z",
        "voteCount": 3,
        "content": "Also vote to C. Tested in AWS DDB console."
      },
      {
        "date": "2023-07-13T17:06:00.000Z",
        "voteCount": 1,
        "content": "By enabling DynamoDB Streams on the DynamoDB table in us-west-2 and adding ap-northeast-1 as a region in the DynamoDB Global Table configuration, you can efficiently replicate the data from us-west-2 to ap-northeast-1 in real-time. DynamoDB Streams captures changes made to the table, and the Global Table configuration ensures automatic replication to multiple regions.\n\nThis solution eliminates the need for manual data export and import processes and allows for continuous replication of the data from us-west-2 to ap-northeast-1. It ensures that the ap-northeast-1 development team has access to up-to-date data for user acceptance testing (UAT) and performance feasibility testing.\n\nTherefore, the solution that provides the most cost-efficient utilization of data from us-west-2 to ap-northeast-1 is option D."
      },
      {
        "date": "2023-10-16T09:28:00.000Z",
        "voteCount": 1,
        "content": "The question asked for the most cost-efficient solution, not the most operationally efficient one. Streams cost money."
      },
      {
        "date": "2023-09-04T15:38:00.000Z",
        "voteCount": 2,
        "content": "The question said: The feasibility tests do not need to be run on data that is updated in real time"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 317,
    "url": "https://www.examtopics.com/discussions/amazon/view/115097-exam-aws-certified-database-specialty-topic-1-question-317/",
    "body": "A company uses multiple AWS accounts in AWS Organizations to separate development teams that work on different applications. Each AWS account contains multiple applications that run in the default VPC with interface endpoints. The applications need access to the same underlying data in an Amazon Aurora PostgreSQL DB cluster in one of the AWS accounts.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager (AWS RAM) to share the subnet that contains the database. Create an Amazon RDS Proxy endpoint for the other applications to access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VPC peering to connect the VPCs of the other AWS accounts to the subnet that contains the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket that stores database backups. Configure replication to S3 buckets in the other accounts. Restore the backups in the other AWS accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an interface VPC endpoint for the Amazon RDS API. Attach an endpoint policy that grants the other AWS accounts access to the database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T13:35:00.000Z",
        "voteCount": 1,
        "content": "A. Use AWS Resource Access Manager (AWS RAM) and RDS Proxy endpoint\n\nhttps://aws.amazon.com/blogs/database/use-amazon-rds-proxy-to-provide-access-to-rds-databases-across-aws-accounts/"
      },
      {
        "date": "2023-08-27T02:52:00.000Z",
        "voteCount": 4,
        "content": "Answer A:\nUsing RAM u do not need to expose all your resources and below blogs explained in details using RAM to achieve Operation Excellence\n\nhttps://aws.amazon.com/blogs/database/use-amazon-rds-proxy-to-provide-access-to-rds-databases-across-aws-accounts/"
      },
      {
        "date": "2023-10-15T06:11:00.000Z",
        "voteCount": 1,
        "content": "Exactly - A:\n\"For example, your organization might have multiple applications that access the same database resources and each application might be in its own VPC.\n\nRDS Proxy makes it easy to address this pattern by sharing the VPC subnet as a resource using AWS Resource Access Manager (RAM).\""
      },
      {
        "date": "2023-08-19T23:52:00.000Z",
        "voteCount": 3,
        "content": "A.\nB: No, VPC peering is for 1-to-1 hub-and-spoke topology. In other words, 2x (n-1) times operations (one side requested and another side accepted) are needed for n teams. Opt A only needs n-1 times."
      },
      {
        "date": "2023-07-13T17:08:00.000Z",
        "voteCount": 1,
        "content": "By using VPC peering, you can establish private network communication between VPCs in different AWS accounts. You can add the subnet containing the database to the peering connection and peer it with the VPCs of the other AWS accounts.\n\nThis solution allows for secure network connectivity for the applications to access the same underlying data. Each application in the respective AWS accounts can utilize the database through the peering connection.\n\nTherefore, the most operationally efficient solution to meet these requirements is option B."
      },
      {
        "date": "2023-10-15T06:10:00.000Z",
        "voteCount": 1,
        "content": "B is possible, but not the most operationally efficient option.\n\"You can use VPC peering, AWS Transit Gateway, or an AWS PrivateLink custom solution. Each of these options has potential drawbacks.\"\n\nhttps://aws.amazon.com/blogs/database/use-amazon-rds-proxy-to-provide-access-to-rds-databases-across-aws-accounts/"
      },
      {
        "date": "2023-11-27T12:22:00.000Z",
        "voteCount": 1,
        "content": "All uses default VPC so Overlapping IP addresses wouldn't allow peering"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 318,
    "url": "https://www.examtopics.com/discussions/amazon/view/115098-exam-aws-certified-database-specialty-topic-1-question-318/",
    "body": "A company hosts an online gaming application on AWS. A single Amazon DynamoDB table contains one item for each registered user. The partition key for each item is the user's ID.<br><br>A daily report generator computes the sum totals of two well-known attributes for all items in the table that contain a dimension attribute. As the number of users grows, the report generator takes more time to generate the report.<br><br>Which combination of steps will minimize the time it takes to generate the report? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global secondary index (GSI) that uses the user ID as the partition key and the dimension attribute as the sort key. Use the GSI to project the two attributes that the report generator uses to compute the sum totals.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a local secondary index (LSI) that uses the user ID as the partition key and the dimension attribute as the sort key. Use the LSI to project the two attributes that the report generator uses to compute the sum totals.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the report generator to query the index instead of the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the report generator to scan the index instead of the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the report generator to call the BatchGetItem operation."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-10T20:34:00.000Z",
        "voteCount": 1,
        "content": "A,C query faster then scan"
      },
      {
        "date": "2024-01-13T23:42:00.000Z",
        "voteCount": 2,
        "content": "AD\nThe daily report generator computes the \"sum totals\" of the attributes. In order to do this, you have to go over all the items (scan) and not query some of them."
      },
      {
        "date": "2023-12-10T15:51:00.000Z",
        "voteCount": 2,
        "content": "Cannot query the index as you need to provide the partition key, and the use case needs to scan all items. The index will help because it will serve as a sparse index (only items with attribute) and at the same time having only two attributes reduces the size, thus also the size that the scan needs."
      },
      {
        "date": "2023-10-08T07:04:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html"
      },
      {
        "date": "2023-10-02T08:16:00.000Z",
        "voteCount": 1,
        "content": "A: By creating a GSI with the appropriate partition and sort keys, the report generator can efficiently query the index for the required attributes, minimizing the time required for calculations.\n\nD: It need to sum all the items with the dimension attribute, which means all items will be retrieved in the GSI table. So scan will be better than query."
      },
      {
        "date": "2023-09-04T01:48:00.000Z",
        "voteCount": 1,
        "content": "I Think AC"
      },
      {
        "date": "2023-09-01T02:20:00.000Z",
        "voteCount": 1,
        "content": "Scan runs on table\nQuery runs on index"
      },
      {
        "date": "2023-09-04T16:16:00.000Z",
        "voteCount": 2,
        "content": "Changed to A,C since we can't add LSI to existing table : \"Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.\"\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"
      },
      {
        "date": "2023-08-20T00:19:00.000Z",
        "voteCount": 3,
        "content": "B: No. An LSI is an index that is only spread across the same partition as the table, which can improve read performance for queries that scan a small number of items. In this case, the LSI will be scanned to compute the sum totals of the two attributes, so it is not as effective as creating a GSI.\nD: No, because scanning an index is not as efficient as querying an index.\nE: No, because the BatchGetItem operation is used to get multiple items from a table or index. The report generator only needs to get one item from the index, so the BatchGetItem operation would be overkill.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html"
      },
      {
        "date": "2023-08-17T00:32:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html"
      },
      {
        "date": "2023-07-17T17:44:00.000Z",
        "voteCount": 1,
        "content": "The Answer: BE"
      },
      {
        "date": "2023-08-07T07:28:00.000Z",
        "voteCount": 2,
        "content": "Any Reason for BE.. any amazon doc for reference ??"
      },
      {
        "date": "2023-07-13T17:15:00.000Z",
        "voteCount": 1,
        "content": "By creating a GSI with the appropriate partition and sort keys, the report generator can efficiently query the index for the required attributes, minimizing the time required for calculations. Additionally, using the BatchGetItem operation allows the report generator to retrieve multiple items in a single request, further reducing the processing time.\n\nTherefore, the combination of steps A and E would help minimize the time taken for report generation in this scenario."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 319,
    "url": "https://www.examtopics.com/discussions/amazon/view/115099-exam-aws-certified-database-specialty-topic-1-question-319/",
    "body": "A company has an existing system that uses a single-instance Amazon DocumentDB (with MongoDB compatibility) cluster. Read requests account for 75% of the system queries. Write requests are expected to increase by 50% after an upcoming global release. A database specialist needs to design a solution that improves the overall database performance without creating additional application overhead.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the cluster with a shared cluster volume. Add two instances to serve both read requests and write requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd one read replica instance. Activate a shared cluster volume. Route all read queries to the read replica instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd one read replica instance. Set the read preference to secondary preferred.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd one read replica instance. Update the application to route all read queries to the read replica instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T23:45:00.000Z",
        "voteCount": 1,
        "content": "C\nB and D would require application change."
      },
      {
        "date": "2023-10-08T07:10:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/database/building-resilient-applications-with-amazon-documentdb-with-mongodb-compatibility-part-2-exception-handling/#:~:text=Amazon%20DocumentDB%20can%20scale%20reads%20by%20adding%20read,primary%20if%20all%20replicas%20are%20unavailable%20Weitere%20Elemente"
      },
      {
        "date": "2023-08-30T00:55:00.000Z",
        "voteCount": 1,
        "content": "C. Add one read replica instance. Set the read preference to secondary preferred.\n--- below copy / paste from mongodb web link ---\nBy default, an application directs its read operations to the primary member in a replica set (i.e. read preference mode \"primary\"). But, clients can specify a read preference to send read operations to secondaries.\n\n\nhttps://www.mongodb.com/docs/manual/core/read-preference/"
      },
      {
        "date": "2023-08-20T01:37:00.000Z",
        "voteCount": 2,
        "content": "A: No. DDB uses a shared cluster volume by default. Adding 2 instances without setting which one is primary and read replica won't be most efficient for the load.\nB: No. DDB uses a shared cluster volume by default, no need to activate. Routing all read queries to a read replicas required application logic change, so No.\nC: Yes. Setting the read preference to \"secondary preferred\" means that read operations will be routed to read replicas when they're available. If not, they'll go to the primary. This change doesn't require any changes in the application logic, so it doesn't introduce application overhead.\nD: No. Required application change for read-write operation.\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/storage.html\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/db-cluster-read-preference.html"
      },
      {
        "date": "2023-08-10T17:02:00.000Z",
        "voteCount": 2,
        "content": "Amazon DocumentDB  already has a cluster volume where data is shared across instances.  So Answer is D.  Any thoughts on this ?"
      },
      {
        "date": "2023-07-13T17:19:00.000Z",
        "voteCount": 1,
        "content": "In this solution, a read replica instance is added and the shared cluster volume is activated. This allows read queries to be distributed to the read replica instance, improving performance. At the same time, write queries are processed on the primary instance.\n\nThis architecture scales the processing of read requests and can handle the anticipated increase in write requests. Additionally, since there is no need to modify the application, there is no added overhead.\n\nTherefore, solution B fulfills the requirements."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 320,
    "url": "https://www.examtopics.com/discussions/amazon/view/115100-exam-aws-certified-database-specialty-topic-1-question-320/",
    "body": "A database specialist needs to enable IAM authentication on an existing Amazon Aurora PostgreSQL DB cluster. The database specialist already has modified the DB cluster settings, has created IAM and database credentials, and has distributed the credentials to the appropriate users.<br><br>What should the database specialist do next to establish the credentials for the users to use to log in to the DB cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users' IAM credentials to the Aurora cluster parameter group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the generate-db-auth-token command with the user names to generate a temporary password for the users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users' IAM credentials to the default credential profile, Use the AWS Management Console to access the DB cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Security Token Service (AWS STS) token by sending the IAM access key and secret key as headers to the DB cluster API endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-13T17:20:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.AWSCLI.html"
      },
      {
        "date": "2023-08-30T09:20:00.000Z",
        "voteCount": 2,
        "content": "B is the right one"
      },
      {
        "date": "2023-07-17T15:30:00.000Z",
        "voteCount": 2,
        "content": "The answer is B"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 321,
    "url": "https://www.examtopics.com/discussions/amazon/view/115101-exam-aws-certified-database-specialty-topic-1-question-321/",
    "body": "A database specialist is designing a disaster recovery (DR) strategy for a highly available application that is in development. The application uses an Amazon DynamoDB table as its data store. The application requires an RTO of 1 minute and an RPO of 2 minutes.<br><br>Which DR strategy for the DynamoDB table will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a DynamoDB stream and an AWS Lambda function. Configure the Lambda function to process the stream and copy the data to a table in another AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a DynamoDB global table replica in another AWS Region. Activate point-in-time recovery for both tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a DynamoDB Accelerator (DAX) table in another AWS Region. Activate point-in-time recovery for the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Backup plan. Assign the DynamoDB table as a resource."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T19:35:00.000Z",
        "voteCount": 1,
        "content": "Answer B, Dynamodb Global Table"
      },
      {
        "date": "2023-09-08T06:13:00.000Z",
        "voteCount": 2,
        "content": "B. DynamoDB global table replica\n\nhttps://aws.amazon.com/dynamodb/global-tables/\n\n\"Amazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provide you 99.999% availability, increased application resiliency, and improved business continuity. As global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions, you can achieve fast, local read and write performance.\""
      },
      {
        "date": "2023-08-30T09:22:00.000Z",
        "voteCount": 1,
        "content": "B is the right one"
      },
      {
        "date": "2023-07-17T15:00:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2023-07-13T17:22:00.000Z",
        "voteCount": 1,
        "content": "B is answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 322,
    "url": "https://www.examtopics.com/discussions/amazon/view/115102-exam-aws-certified-database-specialty-topic-1-question-322/",
    "body": "A company is using an Amazon RDS for MySQL DB instance for a production application. During the company\u2019s upcoming scheduled maintenance window, a database specialist will perform a major version upgrade to the DB instance. The application is critical, so the company wants to minimize the maintenance time and allow for a rollback if a problem occurs.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the automatic upgrade option by using the AWS Management Console. Amazon RDS will apply the upgrade, which will occur during the scheduled maintenance window with no downtime.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DB instance that has the desired version. Configure AWS Database Migration Service (AWS DMS) to migrate the existing data to the new DB instance. Change the DNS records to point to the new DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate read replica of the DB instance. Upgrade the version on the read replica. Promote the read replica to be the primary DB instance. Direct the application to use the read replica endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the DB instance. Configure a policy to fall over to the read replica if failure occurs during the upgrade. Upgrade the version on the primary DB instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-02T08:02:00.000Z",
        "voteCount": 1,
        "content": "It needs to minimize the maintenance time and allow for a rollback if a problem occurs.\nPromote read replica doesn't support rollback. But DMS does."
      },
      {
        "date": "2023-10-02T08:06:00.000Z",
        "voteCount": 1,
        "content": "Sorry. I misunderstand the rollback meaning. \nRollback after failure to upgrade from MySQL 5.7 to 8.0 is supported.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.Major.RollbackAfterFailure"
      },
      {
        "date": "2023-09-08T06:29:00.000Z",
        "voteCount": 3,
        "content": "C is correct\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime"
      },
      {
        "date": "2023-09-04T16:36:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MariaDB.html#USER_UpgradeDBInstance.MariaDB.Major"
      },
      {
        "date": "2023-09-04T16:37:00.000Z",
        "voteCount": 2,
        "content": "create a read replica and upgrade to new version, then promote it to primary instance"
      },
      {
        "date": "2023-08-30T09:28:00.000Z",
        "voteCount": 3,
        "content": "C is the right one"
      },
      {
        "date": "2023-07-17T15:00:00.000Z",
        "voteCount": 2,
        "content": "It's C."
      },
      {
        "date": "2023-07-13T17:23:00.000Z",
        "voteCount": 3,
        "content": "C is answer"
      },
      {
        "date": "2023-07-13T23:55:00.000Z",
        "voteCount": 2,
        "content": "D is better"
      },
      {
        "date": "2024-02-03T08:53:00.000Z",
        "voteCount": 1,
        "content": "Failover to a read replica is a manual process"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 323,
    "url": "https://www.examtopics.com/discussions/amazon/view/115103-exam-aws-certified-database-specialty-topic-1-question-323/",
    "body": "A company uses an Amazon RDS for PostgreSQL database in the us-east-2 Region. The company wants to have a copy of the database available in the us-west-2 Region as part of a new disaster recovery strategy.<br><br>A database architect needs to create the new database. There can be little to no downtime to the source database. The database architect has decided to use AWS Database Migration Service (AWS DMS) to replicate the database across Regions. The database architect will use full load mode and then will switch to change data capture (CDC) mode.<br><br>Which parameters must the database architect configure to support CDC mode for the RDS for PostgreSQL database? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet wal_level = logical.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet wal_level = replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet max_replication_slots to 1 or more, depending on the number of DMS tasks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet max_replication_slots to 0 to support dynamic allocation of slots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet wal_sender_timeout to 20,000 milliseconds.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet wal_sender_timeout to 5,000 milliseconds."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T00:51:00.000Z",
        "voteCount": 2,
        "content": "ACE\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Prerequisites.CDC"
      },
      {
        "date": "2023-10-16T10:01:00.000Z",
        "voteCount": 1,
        "content": "Evidence:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Prerequisites.CDC"
      },
      {
        "date": "2023-08-27T03:28:00.000Z",
        "voteCount": 2,
        "content": "ACE,\nAnswer below, cannot set wal_sender_timeout  to 5000 coz it needs minimum 10,000 (10 seconds).\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html"
      },
      {
        "date": "2023-08-16T22:25:00.000Z",
        "voteCount": 2,
        "content": "When setting wal_sender_timeout to a non-zero value, DMS requires a minimum of 10000 milliseconds (10 seconds), and fails if the value is less than 10000. Keep the value less than 5 minutes to avoid causing a delay during a Multi-AZ failover of a DMS replication instance.\n\nACE"
      },
      {
        "date": "2023-07-17T14:54:00.000Z",
        "voteCount": 2,
        "content": "The answer is ACE"
      },
      {
        "date": "2023-07-13T17:25:00.000Z",
        "voteCount": 2,
        "content": "ACE ACE"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 324,
    "url": "https://www.examtopics.com/discussions/amazon/view/115104-exam-aws-certified-database-specialty-topic-1-question-324/",
    "body": "A company has a 250 GB Amazon RDS Multi-AZ DB instance. The company\u2019s disaster recovery policy requires an RPO of 6 hours in a second AWS Region.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse RDS automated snapshots. Create an AWS Lambda function to copy the snapshot to a second Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse RDS automated snapshots every 6 hours. Use Amazon S3 Cross-Region Replication to copy the snapshot to a second Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to take an RDS snapshot every 6 hours and to copy the snapshot to a second Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS cross-Region read replica in a second Region. Use AWS Backup to take an automated snapshot of the read replica every 6 hours."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-16T10:12:00.000Z",
        "voteCount": 2,
        "content": "\"As part of your backup plan, you can optionally create a backup copy in another AWS Region.\"\n\nhttps://aws.amazon.com/getting-started/hands-on/amazon-rds-backup-restore-using-aws-backup/"
      },
      {
        "date": "2023-07-17T13:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is C."
      },
      {
        "date": "2023-07-13T17:26:00.000Z",
        "voteCount": 1,
        "content": "C is answer"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 325,
    "url": "https://www.examtopics.com/discussions/amazon/view/115105-exam-aws-certified-database-specialty-topic-1-question-325/",
    "body": "A database administrator is reviewing the deployment of an application that uses Amazon DynamoDB. A fleet of Amazon EC2 application instances accesses the database.<br><br>The database administrator notices that EC2 instances are using public IP addresses to access the database and that the database is available to the internet. Company policy requires that all corporate data must be accessed privately and that external access from the internet is not allowed.<br><br>Which combination of steps will ensure that the DynamoDB database meets these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the DynamoDB security group and network ACLs to block external access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink VPC endpoint for DynamoDUpdate the VPC route table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway VPC endpoint for DynamoDB. Update the VPC route table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a NAT gateway to access DynamoDB. Update the VPC route table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aws:sourceVpce condition for all the IAM roles that provision access to the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-05T20:57:00.000Z",
        "voteCount": 3,
        "content": "C. Create a gateway VPC endpoint \nE. Use the aws:sourceVpce condition \n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html"
      },
      {
        "date": "2023-08-17T09:21:00.000Z",
        "voteCount": 1,
        "content": "CE.\nA: DDB can only assign security group during cluster creation, and binding with VPC. It seems no document stated DDB can change VPC later on.\nB: No. DDB doesn't have privatelink vpc endpoint.\nC: Yes. DDB only have gateway vpc endpoint.\nD: No. Not relevant.\nE: Yes.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices-security-preventative.html"
      },
      {
        "date": "2023-07-28T17:26:00.000Z",
        "voteCount": 2,
        "content": "After the second thought. I change my anwer to CE. There is no security group and ACL for a DynamoDB table. So A is not correct."
      },
      {
        "date": "2023-07-17T12:44:00.000Z",
        "voteCount": 1,
        "content": "The answer is AC"
      },
      {
        "date": "2023-07-13T17:29:00.000Z",
        "voteCount": 2,
        "content": "In Option A, configure the DynamoDB security group and network ACL to block external access. This restricts access to the database from the internet, ensuring that all company data is accessed privately.\n\nIn Option B, create an AWS PrivateLink VPC endpoint for DynamoDB and update the VPC route table. Using AWS PrivateLink allows accessing DynamoDB through a private network, completely blocking access from the internet and keeping database access private."
      },
      {
        "date": "2023-10-16T10:19:00.000Z",
        "voteCount": 1,
        "content": "No. You would use a VPC Endpoint to access DynamoDB, not a PrivateLink endpoint. SO it would be C.\n\nTo access the Virtual Gateway, you can restrict client usage with the aws:sourceVpce condition = E.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 326,
    "url": "https://www.examtopics.com/discussions/amazon/view/115106-exam-aws-certified-database-specialty-topic-1-question-326/",
    "body": "A company recently created a snapshot of an Amazon RDS for PostgreSQL DB instance that hosts a production database. The company created a new DB instance from the snapshot to test a new application feature while providing isolation from the production database.<br><br>During testing of the new application feature, the company noticed that read latency on the new database was higher than normal. A database specialist needs to resolve the latency issue.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLog in to the database by using the PostgreSQL administration tool. Issue a SELECT * command against each table in the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new parameter group and set the max_connections parameter to 100. Assign the parameter group to the new database. Apply the changes immediately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the default parameter group for the matching PostgreSQL engine. Set the max_connections parameter to 100. Reboot the new database to pick up the changes to the parameter group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLogin to the database by using the PostgreSQL administration tool. Issue the VACUUM (ANALYZE, DISABLE_PAGE_SKIPPING) command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-09T04:32:00.000Z",
        "voteCount": 1,
        "content": "A to mitigate the lazy loading impact to the performance."
      },
      {
        "date": "2024-01-13T08:08:00.000Z",
        "voteCount": 1,
        "content": "A will work, but D is more operationally efficient"
      },
      {
        "date": "2023-10-09T04:18:00.000Z",
        "voteCount": 1,
        "content": "to avoid first touch penalty we can do select *  for all the tables when the read replica is created, VACCUM can be done during the DB running not so effective for first time running."
      },
      {
        "date": "2023-09-11T12:54:00.000Z",
        "voteCount": 1,
        "content": "D. run VACUUM (ANALYZE, DISABLE_PAGE_SKIPPING) command\n\nhttps://www.enterprisedb.com/blog/postgresql-vacuum-and-analyze-best-practice-tips"
      },
      {
        "date": "2023-09-04T02:06:00.000Z",
        "voteCount": 1,
        "content": "I THINK D"
      },
      {
        "date": "2023-08-27T03:39:00.000Z",
        "voteCount": 4,
        "content": "https://stackoverflow.com/questions/47545414/aws-rds-instance-created-from-snapshot-very-slow"
      },
      {
        "date": "2023-07-17T10:05:00.000Z",
        "voteCount": 1,
        "content": "it is D."
      },
      {
        "date": "2023-07-15T06:22:00.000Z",
        "voteCount": 2,
        "content": "VACUUM is effective to improve the query performance and to reduce the latency."
      },
      {
        "date": "2023-07-13T17:30:00.000Z",
        "voteCount": 1,
        "content": "In option C, editing the default parameter group of the PostgreSQL engine and setting the max_connections parameter to 100 is a way to increase the limit on simultaneous connections. By restarting the new database to reflect the changes in the parameter group, it is possible to increase the number of concurrently executed queries and potentially alleviate read latency."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 327,
    "url": "https://www.examtopics.com/discussions/amazon/view/115107-exam-aws-certified-database-specialty-topic-1-question-327/",
    "body": "A legal research company wants to build a recommendation engine on AWS that connects datasets to help lawyers create legal arguments. The recommendation engine will collect millions of unstructured text documents from third-party sources to identify connections between documents without users needing to manually compare the documents.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a graph-based recommendation engine by using Amazon Neptune. Search the documents for vertices with relationships among the different sources to connect.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda application in which the documents are uploaded into Amazon S3. Populate Amazon DynamoDB tables with the metadata of the documents for users to search.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a serverless document scanner by using Amazon Textract to analyze the text from the various sources. Store the detected text in an Amazon Aurora database for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine the data sources in an Amazon S3 data lake. Analyze the documents by using AWS Glue. Query the documents for relationships by using Amazon Athena."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T13:32:00.000Z",
        "voteCount": 2,
        "content": "This can be implemented using an RDF graph in Neptune"
      },
      {
        "date": "2023-09-21T06:19:00.000Z",
        "voteCount": 2,
        "content": "this is the last question 327 ?  not 335 ?"
      },
      {
        "date": "2023-09-05T18:42:00.000Z",
        "voteCount": 2,
        "content": "A. Amazon Neptune.\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/intro.html"
      },
      {
        "date": "2023-09-04T02:08:00.000Z",
        "voteCount": 2,
        "content": "AAAAAA"
      },
      {
        "date": "2023-08-30T09:51:00.000Z",
        "voteCount": 2,
        "content": "GraphDB"
      },
      {
        "date": "2023-08-24T14:37:00.000Z",
        "voteCount": 3,
        "content": "Its A. Graph database for relationships. also Amazon Athena doesn't work with unstructured documents !"
      },
      {
        "date": "2023-07-17T09:36:00.000Z",
        "voteCount": 3,
        "content": "It's A."
      },
      {
        "date": "2023-07-13T17:32:00.000Z",
        "voteCount": 2,
        "content": "Option D, defining data sources in an Amazon S3 data lake, analyzing the documents using AWS Glue, and querying the relationships between documents using Amazon Athena, is the recommended solution. It allows for the storage and analysis of unstructured text documents in the data lake, while leveraging AWS Glue and Amazon Athena for efficient querying of document relationships. This approach minimizes operational overhead while meeting the stated requirements."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 328,
    "url": "https://www.examtopics.com/discussions/amazon/view/127215-exam-aws-certified-database-specialty-topic-1-question-328/",
    "body": "A database specialist needs to move a table from a database that is running on an Amazon Aurora PostgreSQL DB cluster into a new and distinct database cluster. The new table in the new database must be updated with any changes to the original table that happen while the migration is in progress.<br><br>The original table contains a column to store data as large as 2 GB in the form of large binary objects (LOBs). A few records are large in size, but most of the LOB data is smaller than 32 KB.<br><br>What is the FASTEST way to replicate all the data from the original table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) with ongoing replication in full LOB mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the database. Create a new DB instance by using the snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) with ongoing replication in limited LOB mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) with ongoing replication in inline LOB mode.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T18:05:00.000Z",
        "voteCount": 5,
        "content": "I think D.\nInline LOB mode \u2013 In inline LOB mode, you set the maximum LOB size that DMS transfers inline. LOBs smaller than the specified size are transferred inline. LOBs larger than the specified size are replicated using full LOB mode. You can select this option to replicate both small and large LOBs when most of the LOBs are small. DMS doesn\u2019t support inline LOB mode for endpoints that don\u2019t support Full LOB mode, like S3 and Redshift.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"
      },
      {
        "date": "2024-01-13T13:38:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. \nSet InlineLobMaxSize to 32KB to migrate the small documents fast."
      },
      {
        "date": "2023-12-18T20:48:00.000Z",
        "voteCount": 3,
        "content": "Full LOB Mode - Full LOB mode can be quite slow, \nLimited LOB Mode-  LOBs that exceed the maximum LOB size are truncated, and a warning is issued to the log file\nInline LOB mode - LOBs larger than the specified size are replicated using full LOB mode."
      },
      {
        "date": "2023-12-11T02:53:00.000Z",
        "voteCount": 1,
        "content": "The fastest will always be LIMITED, but we loose data. Also, INLINE is only supported during full load (not CDC), so if the ongoing replication means only CDC can be an indicator to choose LIMITED."
      },
      {
        "date": "2023-12-11T02:57:00.000Z",
        "voteCount": 1,
        "content": "Actually I'm chaning to inline. The question says all data, so cannot be limited. Also, as marll88 shows, when the most of the LOBs are small it makes sense to use INLINE."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 329,
    "url": "https://www.examtopics.com/discussions/amazon/view/127216-exam-aws-certified-database-specialty-topic-1-question-329/",
    "body": "A company is using an Amazon Aurora PostgreSQL database for a project with a government agency. All database communications must be encrypted in transit. All non-SSL/TLS connection requests must be rejected.<br><br>What should a database specialist do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl parameter in the DB cluster parameter group to default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl parameter in the DB cluster parameter group to 1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the rds.force_ssl parameter in the DB cluster parameter group to 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the SQLNET.SSL_VERSION option in the DB cluster option group to 1.2."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T13:39:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nValue of 1 will force SSL to all connections"
      },
      {
        "date": "2023-12-15T21:40:00.000Z",
        "voteCount": 2,
        "content": "The default rds.force_ssl parameter is set to 1 (on) for RDS for PostgreSQL version 15."
      },
      {
        "date": "2023-12-02T21:48:00.000Z",
        "voteCount": 1,
        "content": "B is correct \nAll database communications must be encrypted in transit. All non-SSL/TLS connection requests must be rejected.\nSet the rds.force_ssl parameter in the DB cluster parameter group to 1 to enforce Enable \nSSL for connections to your DB instance. and rds.force_ssl parameter is dynamic parameters don't require a reboot after changing their settings.\nA: is wrong , Can not be change default parameter group\nC:  is wrong , rds.force_ssl parameter set to 0 \"off SSL for connections to your DB instance\" \nD: is not related  SSL for connections to your DB instance\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Concepts.General.SSL.html"
      },
      {
        "date": "2023-11-25T18:12:00.000Z",
        "voteCount": 1,
        "content": "I think B.\nYou can require that connections to your PostgreSQL DB instance use SSL by using the rds.force_ssl parameter. The default rds.force_ssl parameter is set to 1 (on) for RDS for PostgreSQL version 15. All other RDS for PostgreSQL major version 14 and older have the default value for rds.force_ssl parameter set to 0 (off). You can set the rds.force_ssl parameter to 1 (on) to require SSL for connections to your DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Concepts.General.SSL.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 330,
    "url": "https://www.examtopics.com/discussions/amazon/view/127217-exam-aws-certified-database-specialty-topic-1-question-330/",
    "body": "An ecommerce company is running Amazon RDS for Microsoft SQL Server. The company is planning to perform testing in a development environment with production data. The development environment and the production environment are in separate AWS accounts. Both environments use AWS Key Management Service (AWS KMS) encrypted databases with both manual and automated snapshots. A database specialist needs to share a KMS encrypted production RDS snapshot with the development account.<br><br>Which combination of steps should the database specialist take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an automated snapshot. Share the snapshot from the production account to the development account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual snapshot. Share the snapshot from the production account to the development account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the snapshot that is encrypted by using the development account default KMS encryption key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the snapshot that is encrypted by using the production account custom KMS encryption key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow the development account to access the production account KMS encryption key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow the production account to access the development account KMS encryption key."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T21:45:00.000Z",
        "voteCount": 3,
        "content": "Correct answer"
      },
      {
        "date": "2023-11-25T18:20:00.000Z",
        "voteCount": 2,
        "content": "I think BDE.\nNote\nTo share an automated DB snapshot, create a manual DB snapshot by copying the automated snapshot, and then share that copy. This process also applies to AWS Backup\u2013generated resources.\nhttps://docs.aws.amazon.com/ja_jp/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html"
      },
      {
        "date": "2023-11-25T18:21:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html"
      },
      {
        "date": "2024-01-20T00:50:00.000Z",
        "voteCount": 1,
        "content": "It is BDE. To share an automated DB snapshot, create a manual DB snapshot by copying the automated snapshot, and then share that copy. This process also applies to AWS Backup\u2013generated resources.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 331,
    "url": "https://www.examtopics.com/discussions/amazon/view/127677-exam-aws-certified-database-specialty-topic-1-question-331/",
    "body": "A news portal is looking for a data store to store 120 GB of metadata about its posts and comments. The posts and comments are not frequently looked up or updated. However, occasional lookups are expected to be served with single-digit millisecond latency on average.<br><br>What is the MOST cost-effective solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB with on-demand capacity mode. Purchase reserved capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ElastiCache for Redis for data storage. Turn off cluster mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for data storage and use Amazon Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB with on-demand capacity mode. Switch the table class to DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\nhttps://aws.amazon.com/dynamodb/standard-ia/"
      },
      {
        "date": "2024-01-04T23:02:00.000Z",
        "voteCount": 3,
        "content": "D is the answer. sub digit milisecond latecy = DynamoDB"
      },
      {
        "date": "2023-12-26T03:48:00.000Z",
        "voteCount": 1,
        "content": "most cost efficient while others require some more cost for provision and manage the DB"
      },
      {
        "date": "2023-12-24T07:46:00.000Z",
        "voteCount": 3,
        "content": "DynamoDB = single digit ms latency and IA for cost-efficienct"
      },
      {
        "date": "2023-12-15T21:50:00.000Z",
        "voteCount": 1,
        "content": "Correct C- S3 is a cost-effective storage"
      },
      {
        "date": "2023-12-02T21:09:00.000Z",
        "voteCount": 2,
        "content": "I think not there is an answer that  match the question requirements.\nBecause  A and B support single-digit millisecond latency but cost expensive \nand C: S3 is a cost-effective storage dose not support single-digit millisecond\nKeyword : \"single-digit millisecond latency on average\" and \"MOST cost-effective solution\" \nA: Only DynamoDB it can provide single-digit millisecond latency . But on-demand capacity mode doesn't Purchase reserved capacity.\nB:  Amazon ElastiCache for Redis is fast in-memory data store that provides sub-millisecond latency.  If choose node size memory more 120 Gg is cost expensive \nC: Use Amazon S3 Standard-Infrequent Access . S3 is a cost-effective storage solution but  Amazon Athena to queries and get results in seconds but may not meet the single-digit millisecond latency requirement\nxD: is wrong. DynamoDB not has Standard-Infrequent Access"
      },
      {
        "date": "2023-12-03T20:57:00.000Z",
        "voteCount": 6,
        "content": "Sorry I Change to D: is correct. DynamoDB has Standard-IA \nKeyword: \n- Single-digit millisecond latency = DynamoDB\n- Most cost-effective = DynamoDB has Standard-IA  \nAmazon DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class helps you reduce your DynamoDB costs by up to 60% for tables that store data that is infrequently accessed \nhttps://aws.amazon.com/dynamodb/standard-ia/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 332,
    "url": "https://www.examtopics.com/discussions/amazon/view/127218-exam-aws-certified-database-specialty-topic-1-question-332/",
    "body": "A company is using AWS CloudFormation to provision and manage infrastructure resources, including a production database. During a recent CloudFormation stack update, a database specialist observed that changes were made to a database resource that is named ProductionDatabase. The company wants to prevent changes to only ProductionDatabase during future stack updates.<br><br>Which stack policy will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-database-specialty/image1.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-database-specialty/image2.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-database-specialty/image3.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-database-specialty/image4.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T21:54:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-12-02T22:06:00.000Z",
        "voteCount": 2,
        "content": "I think A\nB is wrong so deny all  \nAWS IAM Policy  Implicit Deny: If there is no matching \"Allow\" statement and no matching \"Deny\" statement, access is denied by default."
      },
      {
        "date": "2023-11-27T15:43:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html#stack-policy-intro-example"
      },
      {
        "date": "2023-11-25T18:32:00.000Z",
        "voteCount": 2,
        "content": "I think B.\nIf it is only a prohibition of modification of ProductionDatabase, the requirement can be satisfied with B."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 333,
    "url": "https://www.examtopics.com/discussions/amazon/view/127219-exam-aws-certified-database-specialty-topic-1-question-333/",
    "body": "A company needs to deploy an Amazon Aurora PostgreSQL DB instance into multiple accounts. The company will initiate each DB instance from an existing Aurora PostgreSQL DB instance that runs in a shared account. The company wants the process to be repeatable in case the company adds additional accounts in the future. The company also wants to be able to verify if manual changes have been made to the DB instance configurations after the company deploys the DB instances.<br><br>A database specialist has determined that the company needs to create an AWS CloudFormation template with the necessary configuration to create a DB instance in an account by using a snapshot of the existing DB instance to initialize the DB instance. The company will also use the CloudFormation template's parameters to provide key values for the DB instance creation (account ID, etc.).<br><br>Which final step will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bash script to compare the configuration to the current DB instance configuration and to report any changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CloudFormation drift detection feature to check if the DB instance configurations have changed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up CloudFormation to use drift detection to send notifications if the DB instance configurations have been changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to compare the configuration to the current DB instance configuration and to report any changes."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-24T07:57:00.000Z",
        "voteCount": 3,
        "content": "Is it a poorly worded question?   Notifications can be automated using other supporting services such as config, sns...https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/"
      },
      {
        "date": "2023-12-15T22:01:00.000Z",
        "voteCount": 1,
        "content": "Use the CloudFormation drift detection feature to check if the DB instance configurations have changed."
      },
      {
        "date": "2023-11-25T18:39:00.000Z",
        "voteCount": 1,
        "content": "I think B.\n\nA. do not want to use handmade shells\nB. use CloudFormation features\nC. I don't think CloudFormation's functionality can do automatic verification; AWS Config might be able to do it\nD. Don't want to use handmade lambda"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 334,
    "url": "https://www.examtopics.com/discussions/amazon/view/127220-exam-aws-certified-database-specialty-topic-1-question-334/",
    "body": "A company runs online transaction processing (OLTP) workloads on an Amazon RDS for PostgreSQL Multi-AZ DB instance. The company recently conducted tests on the database after business hours, and the tests generated additional database logs. As a result, free storage of the DB instance is low and is expected to be exhausted in 2 days.<br><br>The company wants to recover the free storage that the additional logs consumed. The solution must not result in downtime for the database.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the rds.log_retention_period parameter to 0. Reboot the DB instance to save the changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the rds.log_retention_period parameter to 1440. Wait up to 24 hours for database logs to be deleted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the temp_file_limit parameter to a smaller value to reclaim space on the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the rds.log_retention_period parameter to 1440. Reboot the DB instance to save the changes."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T22:05:00.000Z",
        "voteCount": 1,
        "content": "B Correct, https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.Parameters.html"
      },
      {
        "date": "2023-12-02T22:19:00.000Z",
        "voteCount": 2,
        "content": "I thnik B\nrds.log_retention_period is Sets log retention such that Amazon RDS deletes PostgreSQL logs that are older than 1440 minutes = 1 Day.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.Parameters.html   \ndynamic parameter doesn't need to reboot."
      },
      {
        "date": "2023-11-28T06:02:00.000Z",
        "voteCount": 3,
        "content": "parameter is dynamic so it doesn't need to reboot. Closet answer seems B"
      },
      {
        "date": "2023-11-25T18:47:00.000Z",
        "voteCount": 1,
        "content": "I think B.\n\nA and D cannot be selected because downtime must not occur.\nPlease give me information that C can be done."
      },
      {
        "date": "2023-11-28T06:02:00.000Z",
        "voteCount": 2,
        "content": "C has nothing to do with logs\nhttps://postgresqlco.nf/doc/en/param/temp_file_limit/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 335,
    "url": "https://www.examtopics.com/discussions/amazon/view/127106-exam-aws-certified-database-specialty-topic-1-question-335/",
    "body": "A company runs an ecommerce application on premises on Microsoft SQL Server. The company is planning to migrate the application to the AWS Cloud. The application code contains complex T-SQL queries and stored procedures.<br><br>The company wants to minimize database server maintenance and operating costs after the migration is completed. The company also wants to minimize the need to rewrite code as part of the migration effort.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon Aurora PostgreSQL. Turn on Babelfish.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon S3. Use Amazon Redshift Spectrum for query processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Amazon RDS for SQL Server. Turn on Kerberos authentication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to an Amazon EMR cluster that includes multiple primary nodes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T13:20:00.000Z",
        "voteCount": 1,
        "content": "A.\n\nhttps://aws.amazon.com/rds/aurora/babelfish/"
      },
      {
        "date": "2024-01-13T07:49:00.000Z",
        "voteCount": 3,
        "content": "C\nGives minimal rewrite of the complex T-SQL code"
      },
      {
        "date": "2023-12-18T21:11:00.000Z",
        "voteCount": 3,
        "content": "C is Correct, The company also wants to minimize the need to rewrite code as part of the migration effort"
      },
      {
        "date": "2023-12-15T22:08:00.000Z",
        "voteCount": 2,
        "content": "C is Correct, The company also wants to minimize the need to rewrite code as part of the migration effort"
      },
      {
        "date": "2023-12-02T22:40:00.000Z",
        "voteCount": 3,
        "content": "I think C ***\nMigrate SQL Server to AWS and minimize database  maintenance operating costs = RDS SQL Server\nT-SQL queries and stored procedures =  Support RDS SQL Server\nMinimize the need to rewrite code as part of the migration effort = RDS SQL Server"
      },
      {
        "date": "2023-11-25T21:11:00.000Z",
        "voteCount": 3,
        "content": "I think C.\n\nAmazon RDS for SQL Server with T-SQL compatibility is better."
      },
      {
        "date": "2023-11-24T03:19:00.000Z",
        "voteCount": 3,
        "content": "i think C!"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 336,
    "url": "https://www.examtopics.com/discussions/amazon/view/127421-exam-aws-certified-database-specialty-topic-1-question-336/",
    "body": "A company is running critical applications on AWS. Most of the application deployments use Amazon Aurora MySQL for the database stack. The company uses AWS CloudFormation to deploy the DB instances.<br><br>The company's application team recently implemented a CI/CD pipeline. A database engineer needs to integrate the database deployment CloudFormation stack with the newly built CI/CD platform. Updates to the CloudFormation stack must not update existing production database resources.<br><br>Which CloudFormation stack policy action should the database engineer implement to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Deny statement for the Update:Modify action on the production database resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Deny statement for the Update:* action on the production database resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Deny statement for the Update:Delete action on the production database resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Deny statement for the Update:Replace action on the production database resources."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T22:11:00.000Z",
        "voteCount": 3,
        "content": "B Correct"
      },
      {
        "date": "2023-11-28T06:39:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 337,
    "url": "https://www.examtopics.com/discussions/amazon/view/127108-exam-aws-certified-database-specialty-topic-1-question-337/",
    "body": "An online bookstore uses Amazon Aurora MySQL as its backend database. After the online bookstore added a popular book to the online catalog, customers began reporting intermittent timeouts on the checkout page. A database specialist determined that increased load was causing locking contention on the database. The database specialist wants to automatically detect and diagnose database performance issues and to resolve bottlenecks faster.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Performance Insights for the Aurora MySQL database. Configure and turn on Amazon DevOps Guru for RDS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CPU usage alarm. Select the CPU utilization metric for the DB instance. Create an Amazon Simple Notification Service (Amazon SNS) topic to notify the database specialist when CPU utilization is over 75%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon RDS query editor to get the process ID of the query that is causing the database to lock. Run a command to end the process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the SELECT INTO OUTFILE S3 statement to query data from the database. Save the data directly to an Amazon S3 bucket. Use Amazon Athena to analyze the files for long-running queries."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T22:13:00.000Z",
        "voteCount": 3,
        "content": "Turn on Performance Insights for the Aurora MySQL database."
      },
      {
        "date": "2023-11-28T06:46:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/rds/performance-insights/"
      },
      {
        "date": "2023-11-25T19:02:00.000Z",
        "voteCount": 2,
        "content": "Not B.\nI think CPU utilization is irrelevant because of lock contention."
      },
      {
        "date": "2023-11-24T03:20:00.000Z",
        "voteCount": 2,
        "content": "perhaps C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 338,
    "url": "https://www.examtopics.com/discussions/amazon/view/127021-exam-aws-certified-database-specialty-topic-1-question-338/",
    "body": "A company has a web application that uses Amazon API Gateway to route HTTPS requests to AWS Lambda functions. The application uses an Amazon Aurora MySQL database for its data storage. The application has experienced unpredictable surges in traffic that overwhelm the database with too many connection requests. The company needs to implement a scalable solution that is more resilient to database failures as quickly as possible.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Aurora MySQL database to Amazon Aurora Serverless by restoring a snapshot. Change the endpoint in the Lambda functions to use the new database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Aurora MySQL database to Amazon DynamoDB tables by using AWS Database Migration Service (AWS DMS). Change the endpoint in the Lambda functions to use the new database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that invokes a Lambda function. Code the function to iterate over all existing connections and to call MySQL queries to end any connections in the sleep state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance class for the Aurora database with more memory. Set a larger value for the max_connections parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-15T06:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\n\nIncrease the maximum connections to your DB instance\nIncrease the maximum number of connections to your DB instance using the following methods:\n\nScale the instance up to a DB instance class with more memory. Note: Scaling the DB instance class causes an outage.\nSet a larger value for the max_connections parameter using a custom instance-level parameter group. Increasing the max_connections parameter doesn't cause an outage, but if your DB instance is using a default parameter group, then change the parameter group to a custom parameter group. Changing the parameter group causes an outage. For more information, see Working with DB parameter groups.\n\nhttps://repost.aws/knowledge-center/aurora-mysql-max-connection-errors"
      },
      {
        "date": "2024-04-10T23:37:00.000Z",
        "voteCount": 1,
        "content": "Amazon Aurora Serverless is an on-demand, auto-scaling version of Amazon Aurora, where the database will automatically start up, shut down, and scale capacity up or down based on your application\u2019s needs. It enables you to run your database in the cloud without managing any database instances. It\u2019s a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n\nGiven the unpredictable surges in traffic that overwhelm the database with too many connection requests, a serverless database would be able to handle the variability in demand more effectively. Additionally, Aurora Serverless is more resilient to database failures as it automatically scales compute capacity and can quickly recover from physical storage failures."
      },
      {
        "date": "2024-04-03T16:32:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect. Manually modifying parameter isn't scalable but Aurora Severless is. It is hence A."
      },
      {
        "date": "2024-02-20T01:05:00.000Z",
        "voteCount": 2,
        "content": "I think the correct answer is D. Because A's serverless scales slowly."
      },
      {
        "date": "2024-02-03T09:33:00.000Z",
        "voteCount": 2,
        "content": "\"The company needs to implement a scalable solution that is more resilient to database failures\" i would go for D"
      },
      {
        "date": "2024-02-03T09:34:00.000Z",
        "voteCount": 1,
        "content": "Sorry meant A"
      },
      {
        "date": "2024-01-13T07:53:00.000Z",
        "voteCount": 2,
        "content": "D will allow for more connections"
      },
      {
        "date": "2023-12-15T22:16:00.000Z",
        "voteCount": 1,
        "content": "A is True, for aurora serverless"
      },
      {
        "date": "2023-11-25T21:51:00.000Z",
        "voteCount": 2,
        "content": "I think A.\n\nWhich solution meets cost-effectively? As it says, Amazon Aurora Serverless is good for unmeasurable traffic spikes."
      },
      {
        "date": "2023-11-25T18:00:00.000Z",
        "voteCount": 3,
        "content": "Why not D?\nIf you increase the size of the instance class, max_connections can be increased."
      },
      {
        "date": "2023-11-23T07:28:00.000Z",
        "voteCount": 2,
        "content": "may be A ?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 339,
    "url": "https://www.examtopics.com/discussions/amazon/view/127225-exam-aws-certified-database-specialty-topic-1-question-339/",
    "body": "A company has an Amazon Redshift cluster with database audit logging enabled. A security audit shows that raw SQL statements that run against the Redshift cluster are being logged to an Amazon S3 bucket. The security team requires that authentication logs are generated for use in an intrusion detection system (IDS), but the security team does not require SQL queries.<br><br>What should a database specialist do to remediate this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the use_fips_ssl parameter to true in the database parameter group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off the query monitoring rule in the Redshift cluster's workload management (WLM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the enable_user_activity_logging parameter to false in the database parameter group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable audit logging on the Redshift cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T22:19:00.000Z",
        "voteCount": 2,
        "content": "Set the enable_user_activity_logging parameter to false in the database parameter group."
      },
      {
        "date": "2023-11-28T11:19:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html#db-auditing-enable-logging"
      },
      {
        "date": "2023-11-25T21:21:00.000Z",
        "voteCount": 1,
        "content": "I think B.\n\nThe security team does not need SQL queries.\nTurn off query monitoring."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 340,
    "url": "https://www.examtopics.com/discussions/amazon/view/127221-exam-aws-certified-database-specialty-topic-1-question-340/",
    "body": "A company performs an audit on various data stores and discovers that an Amazon S3 bucket is storing a credit card number. The S3 bucket is the target of an AWS Database Migration Service (AWS DMS) continuous replication task that uses change data capture (CDC). The company determines that this field is not needed by anyone who uses the target data. The company has manually removed the existing credit card data from the S3 bucket.<br><br>What is the MOST operationally efficient way to prevent new credit card data from being written to the S3 bucket?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a transformation rule to the DMS task to ignore the column from the source data endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a transformation rule to the DMS task to mask the column by using a simple SQL query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the target S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the credit card number column from the data source so that the DMS task does not need to be altered."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-10T23:50:00.000Z",
        "voteCount": 1,
        "content": "Option A: Add a transformation rule to the DMS task to ignore the column from the source data endpoint.\n\nAWS Database Migration Service (DMS) allows you to specify transformation rules that change the schema and table definitions of the source data before it is migrated to the target. By adding a transformation rule to ignore the specific column (in this case, the credit card number), you can ensure that this data is not written to the S3 bucket during the continuous replication process.\n\nThis approach does not require changes to the source database schema (as in Option D), does not involve masking the data (as in Option B), and does not rely on encryption to protect the data (as in Option C). It simply prevents the unwanted data from being written to the S3 bucket, which is the most direct and operationally efficient solution to the problem."
      },
      {
        "date": "2023-12-11T04:13:00.000Z",
        "voteCount": 4,
        "content": "A for sure.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Transformations.html"
      },
      {
        "date": "2023-11-25T19:09:00.000Z",
        "voteCount": 3,
        "content": "Not C.\nKMS encryption is secure, but it is not a procedure to prevent S3 writes"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 341,
    "url": "https://www.examtopics.com/discussions/amazon/view/127226-exam-aws-certified-database-specialty-topic-1-question-341/",
    "body": "A large financial services company uses Amazon ElastiCache for Redis for its new application that has a global user base. A database administrator must develop a caching solution that will be available across AWS Regions and include low-latency replication and failover capabilities for disaster recovery (DR). The company's security team requires the encryption of cross-Region data transfers.<br><br>Which solution meets these requirements with the LEAST amount of operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable cluster mode in ElastiCache for Redis. Then create multiple clusters across Regions and replicate the cache data by using AWS Database Migration Service (AWS DMS). Promote a cluster in the failover Region to handle production traffic when DR is required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global datastore in ElastiCache for Redis. Then create replica clusters in two other Regions. Promote one of the replica clusters as primary when DR is required.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable cluster mode in ElastiCache for Redis. Then create multiple replication groups across Regions and replicate the cache data by using AWS Database Migration Service (AWS DMS). Promote a replication group in the failover Region to primary when DR is required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of ElastiCache for Redis in the primary Region and copy it to the failover Region. Use the snapshot to restore the cluster from the failover Region when DR is required."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T10:07:00.000Z",
        "voteCount": 1,
        "content": "For sure B"
      },
      {
        "date": "2024-01-13T07:24:00.000Z",
        "voteCount": 2,
        "content": "B\nElasticache Global DataStore allows for one master and two read replica regions, and manual failover in case of disaster. RPO &lt;1sec and RTO &lt;1 min"
      },
      {
        "date": "2023-11-28T11:26:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Redis-Global-Datastore.html"
      },
      {
        "date": "2023-11-25T21:29:00.000Z",
        "voteCount": 3,
        "content": "I think B.\n\nIn the unlikely event of regional degradation, one of the healthy cross-region replica clusters can be promoted to become the primary cluster with full read/write capabilities. Once initiated, the promotion typically completes in less than a minute, allowing your applications to remain available. \nhttps://aws.amazon.com/about-aws/whats-new/2020/03/amazon-elasticache-for-redis-announces-global-datastore/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 342,
    "url": "https://www.examtopics.com/discussions/amazon/view/127222-exam-aws-certified-database-specialty-topic-1-question-342/",
    "body": "A healthcare company is running an application on Amazon EC2 in a public subnet and using Amazon DocumentDB (with MongoDB compatibility) as the storage layer. An audit reveals that the traffic between the application and Amazon DocumentDB is not encrypted and that the DocumentDB cluster is not encrypted at rest. A database specialist must correct these issues and ensure that the data in transit and the data at rest are encrypted.<br><br>Which actions should the database specialist take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the SSH RSA public key for Amazon DocumentDB. Update the application configuration to use the instance endpoint instead of the cluster endpoint and run queries over SSH.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the SSL .pem public key for Amazon DocumentDAdd the key to the application package and make sure the application is using the key while connecting to the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the unencrypted cluster. Restore the unencrypted snapshot as a new cluster with the --storage-encrypted parameter set to true. Update the application to point to the new cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DocumentDB VPC endpoint to prevent the traffic from going to the Amazon DocumentDB public endpoint. Set a VPC endpoint policy to allow only the application instance's security group to connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate encryption at rest using the modify-db-cluster command with the --storage-encrypted parameter set to true. Set the security group of the cluster to allow only the application instance's security group to connect."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-03T14:46:00.000Z",
        "voteCount": 1,
        "content": "A and C are answer.\n\nSSL/TLS doesn't require public key being installed on client, which is application server in this case. So B is incorrect. https://docs.aws.amazon.com/documentdb/latest/developerguide/security.encryption.ssl.html\n\nA is SSH tunneling. And SSH connection establishes between client and EC2 instance. So client should choose EC2 instance end point. Therefore, A is correct. https://docs.aws.amazon.com/documentdb/latest/developerguide/connect-from-outside-a-vpc.html"
      },
      {
        "date": "2024-01-13T07:27:00.000Z",
        "voteCount": 2,
        "content": "BC\nB for data in transit, C for data at rest"
      },
      {
        "date": "2023-11-28T11:38:00.000Z",
        "voteCount": 3,
        "content": "B &amp;C \nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/connect_programmatically.html\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html"
      },
      {
        "date": "2023-11-25T19:19:00.000Z",
        "voteCount": 1,
        "content": "Correction. \nA. I did not understand the reason for instance endpoints.\nD. Secure, but data in transit and data in storage seem unrelated to encryption.\nE. Can you encrypt dynamically?"
      },
      {
        "date": "2023-11-25T19:17:00.000Z",
        "voteCount": 1,
        "content": "I think B and C.\n\nA. I did not understand the reason for instance endpoints.\nC. Secure, but data in transit and data in storage seem unrelated to encryption.\nD. Can you encrypt dynamically?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 343,
    "url": "https://www.examtopics.com/discussions/amazon/view/127227-exam-aws-certified-database-specialty-topic-1-question-343/",
    "body": "A database specialist is planning to migrate a 4 TB Microsoft SQL Server DB instance from on premises to Amazon RDS for SQL Server. The database is primarily used for nightly batch processing.<br><br>Which RDS storage option meets these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGeneral Purpose SSD storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvisioned IOPS storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMagnetic storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThroughput Optimized hard disk drives (HDD)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:29:00.000Z",
        "voteCount": 3,
        "content": "A is most cost-effective (GP storage)\nMagnetic disks are not allowed on RDS"
      },
      {
        "date": "2023-12-25T09:25:00.000Z",
        "voteCount": 4,
        "content": "Magnetic is only up to 3TB\nGeneral purpose SSD is the cost effective option from the list\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html#CHAP_Storage.Magnetic"
      },
      {
        "date": "2023-12-24T08:41:00.000Z",
        "voteCount": 3,
        "content": "General purpose SSD cost-effective for nightly batch"
      },
      {
        "date": "2023-11-25T21:45:00.000Z",
        "voteCount": 2,
        "content": "I think A.\n\nSince it is a night batch, we do not think the speed is necessary.\nC. Magnetic Storage is capped at 3 TiB\nD. Not selectable\nhttps://aws.amazon.com/rds/sqlserver/pricing/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 344,
    "url": "https://www.examtopics.com/discussions/amazon/view/127662-exam-aws-certified-database-specialty-topic-1-question-344/",
    "body": "A coffee machine manufacturer is equipping all of its coffee machines with IoT sensors. The IoT core application is writing measurements for each record to Amazon Timestream. The records have multiple dimensions and measures. The measures include multiple measure names and values.<br><br>An analysis application is running queries against the Timestream database and is focusing on data from the current week. A database specialist needs to optimize the query costs of the analysis application.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that queries contain whole records over the relevant time range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse time range, measure name, and dimensions in the WHERE clause of the query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvoid canceling any query after the query starts running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement exponential backoff in the application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:31:00.000Z",
        "voteCount": 3,
        "content": "B is mandatory for efficiency"
      },
      {
        "date": "2023-12-30T05:21:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/timestream/latest/developerguide/queries-bp.html\nWhere possible, use the equality operator when comparing dimensions and measures in the WHERE clause of a query. An equality predicate on dimensions and measure names allows for improved query performance and reduced query costs."
      },
      {
        "date": "2023-12-25T09:35:00.000Z",
        "voteCount": 1,
        "content": "Either A or B.\nB is the best option if selecting a subset of the measures.\nSince in A it gets the complete record, I think it may be faster. But did not see any documentation supporting that."
      },
      {
        "date": "2023-12-02T16:29:00.000Z",
        "voteCount": 2,
        "content": "B: It' s work.\nuse SELECT clause and WHERE clause as applicable to improve query performance and reduce cost.\nhttps://docs.aws.amazon.com/timestream/latest/developerguide/queries-bp.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 345,
    "url": "https://www.examtopics.com/discussions/amazon/view/127023-exam-aws-certified-database-specialty-topic-1-question-345/",
    "body": "A database specialist needs to replace the encryption key for an Amazon RDS DB instance. The database specialist needs to take immediate action to ensure security of the database.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DB instance to update the encryption key. Perform this update immediately without waiting for the next scheduled maintenance window.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the database to an Amazon S3 bucket. Import the data to an existing DB instance by using the export file. Specify a new encryption key during the import process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual snapshot of the DB instance. Create an encrypted copy of the snapshot by using a new encryption key. Create a new DB instance from the encrypted snapshot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual snapshot of the DB instance. Restore the snapshot to a new DB instance. Specify a new encryption key during the restoration process."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:32:00.000Z",
        "voteCount": 1,
        "content": "Only C will work"
      },
      {
        "date": "2023-12-24T08:43:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/update-encryption-key-rds"
      },
      {
        "date": "2023-12-02T16:37:00.000Z",
        "voteCount": 1,
        "content": "C: It's work\n1.Create a manual snapshot for your DB instance.\n2. In the navigation pane, choose Snapshots.\n3. Select the manual snapshot that you created.\n4. Choose Actions, and then choose Copy Snapshot.\n5. Under Encryption, select Enable Encryption.\n6. For AWS KMS Key, choose the new encryption key that you want to use.\n7. Choose Copy snapshot.\n8. Restore the copied snapshot Create new DB Instance.\nhttps://repost.aws/knowledge-center/update-encryption-key-rds"
      },
      {
        "date": "2023-11-25T19:26:00.000Z",
        "voteCount": 2,
        "content": "I think C.\n\nYou can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created. However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"
      },
      {
        "date": "2023-11-23T07:36:00.000Z",
        "voteCount": 1,
        "content": "may be C ?"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 346,
    "url": "https://www.examtopics.com/discussions/amazon/view/127228-exam-aws-certified-database-specialty-topic-1-question-346/",
    "body": "A gaming company is building a mobile game that will have as many as 25,000 active concurrent users in the first 2 weeks after launch. The game has a leaderboard that shows the 10 highest scoring players over the last 24 hours. The leaderboard calculations are processed by an AWS Lambda function, which takes about 10 seconds. The company wants the data on the leaderboard to be no more than 1 minute old.<br><br>Which architecture will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeliver the player data to an Amazon Timestream database. Create an Amazon ElastiCache for Redis cluster. Configure the Lambda function to store the results in Redis. Create a scheduled event with Amazon EventBridge to invoke the Lambda function once every minute. Reconfigure the game server to query the Redis cluster for the leaderboard data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeliver the player data to an Amazon Timestream database. Create an Amazon DynamoDB table. Configure the Lambda function to store the results in DynamoDCreate a scheduled event with Amazon EventBridge to invoke the Lambda function once every minute. Reconfigure the game server to query the DynamoDB table for the leaderboard data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeliver the player data to an Amazon Aurora MySQL database. Create an Amazon DynamoDB table. Configure the Lambda function to store the results in MySQL. Create a scheduled event with Amazon EventBridge to invoke the Lambda function once every minute. Reconfigure the game server to query the DynamoDB table for the leaderboard data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeliver the player data to an Amazon Neptune database. Create an Amazon ElastiCache for Redis cluster. Configure the Lambda function to store the results in Redis. Create a scheduled event with Amazon EventBridge to invoke the Lambda function once every minute. Reconfigure the game server to query the Redis cluster for the leaderboard data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:17:00.000Z",
        "voteCount": 1,
        "content": "The most operationally efficient architecture for this use case would be Option A.\n\nHere\u2019s why:"
      },
      {
        "date": "2024-04-11T00:17:00.000Z",
        "voteCount": 1,
        "content": "Amazon Timestream is a time series database service that makes it easy to store and analyze trillions of events per day at 1/10th the cost of relational databases. It\u2019s ideal for processing high-volume, time-stamped data like gaming leaderboard scores."
      },
      {
        "date": "2024-04-11T00:18:00.000Z",
        "voteCount": 1,
        "content": "Amazon ElastiCache for Redis offers fully managed Redis and provides sub-millisecond latency to power real-time applications. It\u2019s perfect for storing the leaderboard data and serving it quickly to the game server."
      },
      {
        "date": "2024-04-11T00:18:00.000Z",
        "voteCount": 1,
        "content": "AWS Lambda is a serverless compute service that lets you run your code without provisioning or managing servers. It\u2019s suitable for processing the leaderboard calculations.\n    Amazon EventBridge is a serverless event bus that makes it easy to connect applications together. It can be used to schedule the invocation of the Lambda function every minute, ensuring the leaderboard data is no more than 1 minute old.\n    The game server querying the Redis cluster for the leaderboard data ensures fast response times, given Redis\u2019 in-memory data structure store capabilities.\n\nThis architecture meets the requirements of having up-to-date leaderboard data (no more than 1 minute old), handling high concurrent users, and doing so in an operationally efficient manner. The other options involve technologies that may not be as efficient for this specific use case. For example, DynamoDB and Aurora MySQL are not as fast as Redis for read-heavy workloads like a gaming leaderboard, and Neptune is a graph database, which is not necessary for this use case."
      },
      {
        "date": "2024-02-18T01:59:00.000Z",
        "voteCount": 2,
        "content": "A: sounds good to me\nB: Use EventBridge to schedule Lambda, not DynamoDCreate (never heard of it)\nC: Why creating the DynamoDb? not operationally efficient\nD: Neptune is not for this use case"
      },
      {
        "date": "2024-01-13T07:37:00.000Z",
        "voteCount": 2,
        "content": "All options are dumb. In any case, both A and B will do the work. However, B is more operationally efficient, as you do not need a cluster."
      },
      {
        "date": "2023-12-26T07:45:00.000Z",
        "voteCount": 2,
        "content": "Redis for leaderboard scores + Neptune for player data. Player data is not time series."
      },
      {
        "date": "2023-12-24T09:25:00.000Z",
        "voteCount": 3,
        "content": "Elasticache + persistent store (Timestream in this case)... https://aws.amazon.com/blogs/gametech/rethinking-game-leaderboards-with-amazon-memory-db-for-redis/"
      },
      {
        "date": "2023-12-26T07:45:00.000Z",
        "voteCount": 1,
        "content": "Sorry D, Redis + Neptune for player data.  Player data is not time series."
      },
      {
        "date": "2023-11-25T21:59:00.000Z",
        "voteCount": 2,
        "content": "I think A.\n\nhttps://aws.amazon.com/blogs/news/building-a-real-time-gaming-leaderboard-with-amazon-elasticache-for-redis/"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 347,
    "url": "https://www.examtopics.com/discussions/amazon/view/127230-exam-aws-certified-database-specialty-topic-1-question-347/",
    "body": "A retail company uses Amazon Redshift for its 1 PB data warehouse. Several analytical workloads run on a Redshift cluster. The tables within the cluster have grown rapidly. End users are reporting poor performance of daily reports that run on the transaction fact tables.<br><br>A database specialist must change the design of the tables to improve the reporting performance. All the changes must be applied dynamically. The changes must have the least possible impact on users and must optimize the overall table size.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the STL_SCAN view to understand how the tables are getting scanned. Identify the columns that are used in filter and group by conditions. Create a temporary table with the identified columns as sort keys and compression as Zstandard (ZSTD) by copying the data from the original table. Drop the original table. Give the temporary table the same name that the original table had.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an explain plan to analyze the queries on the tables. Consider recommendations from Amazon Redshift Advisor. Identify the columns that are used in filter and group by conditions. Convert the recommended columns from Redshift Advisor into sort keys with compression encoding set to RAW. Set the rest of the column compression encoding to AZ64.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an explain plan to analyze the queries on the tables. Consider recommendations from Amazon Redshift Advisor. Identify the columns that are used in filter and group by conditions. Convert the recommended columns from Redshift Advisor into sort keys with compression encoding set to LZO. Set the rest of the column compression encoding to Zstandard (ZSTD).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an explain plan to analyze the queries on the tables. Consider recommendations from Amazon Redshift Advisor. Identify the columns that are used in filter and group by conditions. Create a deep copy of the table with the identified columns as sort keys and compression for all columns as Zstandard (ZSTD) by using a bulk insert. Drop the original table. Give the copy table the same name that the original table had."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:23:00.000Z",
        "voteCount": 1,
        "content": "Running an explain plan to analyze the queries on the tables. It helps to understand how the queries are being executed and where the performance bottlenecks might be.\n Considering recommendations from Amazon Redshift Advisor is a smart move. Redshift Advisor analyzes all the clusters in your account to generate tailored recommendations on how to optimize performance and decrease operating costs.\n    Identifying the columns that are used in filter and group by conditions and converting them into sort keys can significantly improve query performance. Sort keys determine the order in which the data is physically stored in a table and can greatly reduce the amount of data that needs to be read from disk during query execution.\n    Setting the compression encoding for the recommended columns to RAW and the rest of the column compression encoding to AZ64 is a good strategy. RAW encoding disables compression and can be useful for frequently accessed columns, while AZ64 is a high-performance compression encoding optimized for analytic queries and is suitable for the rest of the columns."
      },
      {
        "date": "2024-04-10T14:46:00.000Z",
        "voteCount": 1,
        "content": "B\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/query-best-practices-redshift/best-practices-tables.html\n\nAvoid compressing the sort key column."
      },
      {
        "date": "2024-04-10T14:53:00.000Z",
        "voteCount": 1,
        "content": "You can compress the distribution key, &lt;b&gt;but you must avoid compressing the sort key column (especially the first column of the sort key). &lt;/b&gt;"
      },
      {
        "date": "2024-01-13T07:40:00.000Z",
        "voteCount": 1,
        "content": "D\nYou cannot change the compression of a column, without recreating the table"
      },
      {
        "date": "2024-02-03T10:23:00.000Z",
        "voteCount": 1,
        "content": "wrong https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-redshift-supports-modifying-column-comprression-encodings-to-optimize-storage-utilization-query-performance/?nc1=h_ls"
      },
      {
        "date": "2023-12-24T09:30:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-redshift-supports-modifying-column-comprression-encodings-to-optimize-storage-utilization-query-performance/\n\nAZ64, a new compression encoding that consumes 5-10% less storage than ZSTD and enables queries to run 70% faster. Previously, customers who wanted to take advantage of new encoding algorithms such as AZ64 needed to recreate the entire table. Since Redshift recommends that columns defined as SORT keys should not be compressed, previously customers who apply sort keys to existing tables needed to recreate the entire table."
      },
      {
        "date": "2023-12-06T03:41:00.000Z",
        "voteCount": 2,
        "content": "B. Redshift now supports modifying column compression encodings to optimize storage utilization and query performance.  https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-redshift-supports-modifying-column-comprression-encodings-to-optimize-storage-utilization-query-performance/"
      },
      {
        "date": "2023-11-26T00:16:00.000Z",
        "voteCount": 1,
        "content": "I think D.\n\nNot B and C\nRedShift does not allow ALTER TABLE to change column compression type"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 348,
    "url": "https://www.examtopics.com/discussions/amazon/view/138107-exam-aws-certified-database-specialty-topic-1-question-348/",
    "body": "A company has a reporting application that runs on an Amazon EC2 instance in an isolated developer account on AWS. The application needs to retrieve data during non-peak company hours from an Amazon Aurora PostgreSQL database that runs in the company\u2019s production account. The company's security team requires that access to production resources complies with AWS best security practices.<br><br>A database administrator needs to provide the reporting application with access to the production database. The company has already configured VPC peering between the production account and developer account. The company has also updated the route tables in both accounts with the necessary entries to correctly set up VPC peering.<br><br>What must the database administrator do to finish providing connectivity to the reporting application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. Add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an outbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. Add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on all TCP ports. Add an inbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. Add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on all TCP ports."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:31:00.000Z",
        "voteCount": 1,
        "content": "The database administrator should add an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. This port is the default port for PostgreSQL databases.\n\nAdditionally, the administrator should add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432. This will allow the reporting application running on the EC2 instance to reach the database in the production account."
      },
      {
        "date": "2024-04-07T17:06:00.000Z",
        "voteCount": 1,
        "content": "A.\n\nAs long as the DB permits traffic in, return traffic is permitted. Security Groups are stateful.\n\nSecurity groups are stateful. For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 349,
    "url": "https://www.examtopics.com/discussions/amazon/view/134129-exam-aws-certified-database-specialty-topic-1-question-349/",
    "body": "A global company is creating an application. The application must be highly available. The company requires an RTO and an RPO of less than 5 minutes. The company needs a database that will provide the ability to set up an active-active configuration and near real-time synchronization of data across tables in multiple AWS Regions.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS for MariaDB with cross-Region read replicas",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon RDS with a Multi-AZ deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB global tables\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB with a global secondary index (GSI)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:35:00.000Z",
        "voteCount": 1,
        "content": "Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution. When you create a global table, you specify the AWS regions where you want the table to be available. DynamoDB performs all of the necessary tasks to create identical tables in these regions, and propagate ongoing data changes to all of them.\n\nThe other options (A, B, and D) do not meet all the requirements:\n\n    A. Amazon RDS for MariaDB with cross-Region read replicas: This option does not support active-active configuration. The read replicas are read-only.\n    B. Amazon RDS with a Multi-AZ deployment: This option is for high availability and failover support for DB instances within a single region. It does not support multi-region, active-active configuration.\n    D. Amazon DynamoDB with a global secondary index (GSI): GSIs are used to speed up access to data in DynamoDB, but they do not provide multi-region, active-active configuration."
      },
      {
        "date": "2024-02-18T03:15:00.000Z",
        "voteCount": 1,
        "content": "similar to #249. So, I pick C"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 350,
    "url": "https://www.examtopics.com/discussions/amazon/view/135099-exam-aws-certified-database-specialty-topic-1-question-350/",
    "body": "A company has a hybrid environment in which a VPC connects to an on-premises network through an AWS Site-to-Site VPN connection. The VPC contains an application that is hosted on Amazon EC2 instances. The EC2 instances run in private subnets behind an Application Load Balancer (ALB) that is associated with multiple public subnets. The EC2 instances need to securely access an Amazon DynamoDB table.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the internet gateway of the VPC to access the DynamoDB table. Use the ALB to route the traffic to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a NAT gateway in one of the public subnets of the VPC. Configure the security groups of the EC2 instances to access the DynamoDB table through the NAT gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Site-to-Site VPN connection to route all DynamoDB network traffic through the on-premises network infrastructure to access the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC endpoint for DynamoDB. Assign the endpoint to the route table of the private subnets that contain the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:38:00.000Z",
        "voteCount": 1,
        "content": "A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet. The EC2 instances do not require public IP addresses, and internet gateway, NAT device, VPN connection, or AWS Direct Connect connection is not required to communicate with DynamoDB."
      },
      {
        "date": "2024-03-03T02:16:00.000Z",
        "voteCount": 1,
        "content": "D using VPC endpoint"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 351,
    "url": "https://www.examtopics.com/discussions/amazon/view/135100-exam-aws-certified-database-specialty-topic-1-question-351/",
    "body": "A web-based application uses Amazon DocumentDB (with MongoDB compatibility) as its underlying data store. Sufficient access control is in place, but a database specialist wants to be able to review logs if the primary DocumentDB database is deleted.<br><br>Which combination of steps should the database specialist take to meet this requirement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the audit_logs cluster parameter to enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable DocumentDB log export to Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Enhanced Monitoring for DocumentDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail for DocumentDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config to monitor the state of DocumentDB."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html"
      },
      {
        "date": "2024-03-03T02:21:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 352,
    "url": "https://www.examtopics.com/discussions/amazon/view/135101-exam-aws-certified-database-specialty-topic-1-question-352/",
    "body": "A marketing company is developing an application to track responses to email message campaigns. The company needs a database storage solution that is optimized to work with highly connected data. The database needs to limit connections and programmatic access to the data by using IAM policies.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon ElastiCache for Redis cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Aurora MySQL DB cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Neptune DB cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:49:00.000Z",
        "voteCount": 1,
        "content": "Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. It is optimized for storing billions of relationships and querying the graph with milliseconds latency."
      },
      {
        "date": "2024-04-07T15:57:00.000Z",
        "voteCount": 1,
        "content": "D\n\nConnected data = Neptune"
      },
      {
        "date": "2024-03-03T02:23:00.000Z",
        "voteCount": 2,
        "content": "Amazon Neptune is a fully managed graph database service that is optimized for highly connected data. It is ideal for scenarios where data relationships are complex and require efficient querying of interconnected data."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 353,
    "url": "https://www.examtopics.com/discussions/amazon/view/135103-exam-aws-certified-database-specialty-topic-1-question-353/",
    "body": "A company uses a large, growing, and high performance on-premises Microsoft SQL Server instance with an Always On availability group cluster size of 120 TiB. The company uses a third-party backup product that requires system-level access to the databases. The company will continue to use this third-party backup product in the future.<br><br>The company wants to move the DB cluster to AWS with the least possible downtime and data loss. The company needs a 2 Gbps connection to sustain Always On asynchronous data replication between the company\u2019s data center and AWS.<br><br>Which combination of actions should a database specialist take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish an AWS Direct Connect hosted connection between the company\u2019s data center and AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Site-to-Site VPN connection between the company's data center and AWS over the internet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate the on-premises SQL Server databases to Amazon RDS for SQL Server. Configure Always On availability groups for SQL Server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new SQL Server Always On availability group DB cluster on Amazon EC2. Configure Always On distributed availability groups between the on-premises DB cluster and the AWS DB cluster. Fail over to the AWS DB cluster when it is time to migrate.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant system-level access to the third-party backup product to perform backups of the Amazon RDS for SQL Server DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the third-party backup product to perform backups of the DB cluster on Amazon EC2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:55:00.000Z",
        "voteCount": 1,
        "content": "AWS Direct Connect provides a dedicated network connection from your premises to AWS, which can provide more bandwidth throughput and a more consistent network experience than internet-based connections. This would help sustain the 2 Gbps connection needed for Always On asynchronous data replication.\n\nDeploying a new SQL Server Always On availability group DB cluster on Amazon EC2 and configuring Always On distributed availability groups between the on-premises DB cluster and the AWS DB cluster would allow for minimal downtime and data loss during migration. The failover to the AWS DB cluster can be done when it is time to migrate.\n\n Since the company wants to continue using the third-party backup product, configuring it to perform backups of the DB cluster on Amazon EC2 would meet this requirement. Amazon EC2 instances provide the system-level access required by the third-party backup product."
      },
      {
        "date": "2024-03-23T12:00:00.000Z",
        "voteCount": 1,
        "content": "Amazon RDS for SQL server supports Always ON : https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-rds-for-sql-server-now-supports-alwayson-availability-groups/#:~:text=Amazon%20RDS%20for%20SQL%20Server%20now%20offers%20Always%20On%20Availability,production%20workloads%20on%20SQL%20Server."
      },
      {
        "date": "2024-03-22T23:20:00.000Z",
        "voteCount": 3,
        "content": "it needs ec2"
      },
      {
        "date": "2024-03-03T02:30:00.000Z",
        "voteCount": 1,
        "content": "A. Establish an AWS Direct Connect hosted connection between the company\u2019s data center and AWS.\nC. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises SQL Server databases to Amazon RDS for SQL Server. Configure Always On availability groups for SQL Server.\nE. Grant system-level access to the third-party backup product to perform backups of the Amazon RDS for SQL Server DB instance."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 354,
    "url": "https://www.examtopics.com/discussions/amazon/view/135104-exam-aws-certified-database-specialty-topic-1-question-354/",
    "body": "A company has an application environment that deploys Amazon Aurora PostgreSQL databases as part of its CI/CD process that uses AWS CloudFormation. The company's database administrator has received reports of performance issues from the resulting database but has no way to investigate the issues.<br><br>Which combination of changes must the database administrator make to the database deployment to automate the collection of performance data? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Amazon DevOps Guru for the Aurora database resources in the CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail in each AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on and configure AWS Config for all Aurora PostgreSQL databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to enable Amazon CloudWatch monitoring on the Aurora PostgreSQL DB instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to turn on Performance Insights for Aurora PostgreSQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T00:58:00.000Z",
        "voteCount": 1,
        "content": "D. Update the CloudFormation template to enable Amazon CloudWatch monitoring on the Aurora PostgreSQL DB instances.\n\n    Amazon CloudWatch provides monitoring for AWS resources and applications, collecting and tracking metrics, collecting and monitoring log files, and setting alarms. It can be used to gain system-wide visibility into resource utilization, application performance, and operational health.\n\n    E. Update the CloudFormation template to turn on Performance Insights for Aurora PostgreSQL.\n\n    Performance Insights is a database performance tuning and monitoring feature of AWS RDS (including Aurora). It helps you quickly assess the load on your database, and determine when and where to take action."
      },
      {
        "date": "2024-03-03T02:32:00.000Z",
        "voteCount": 1,
        "content": "D. Enabling Amazon CloudWatch monitoring on the Aurora PostgreSQL DB instances allows for the collection of performance metrics such as CPU utilization, disk I/O, and database connections. This data can help diagnose performance issues and monitor the health of the databases.\n\nE. Turning on Performance Insights for Aurora PostgreSQL provides a detailed view of database performance metrics such as SQL statements, waits, and load patterns. This can help identify bottlenecks and optimize database performance."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 355,
    "url": "https://www.examtopics.com/discussions/amazon/view/135105-exam-aws-certified-database-specialty-topic-1-question-355/",
    "body": "A social media company recently launched a new feature that gives users the ability to share live feeds of their daily activities with their followers. The company has an Amazon RDS for MySQL DB instance that stores data about follower engagement.<br><br>After the new feature launched, the company noticed high CPU utilization and high database latency during reads and writes. The company wants to implement a solution that will identify the source of the high CPU utilization.<br><br>Which solution will meet these requirements with the LEAST administrative oversight?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DevOps Guru insights.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora Database Activity Streams."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T01:12:00.000Z",
        "voteCount": 1,
        "content": "Amazon DevOps Guru is a fully managed operations service that uses machine learning to automatically detect operational issues and recommend specific actions for remediation. By using DevOps Guru, the company can identify the source of the high CPU utilization with minimal administrative oversight.\n\nWhile AWS CloudTrail (Option B) can track user activity and API usage, it\u2019s not designed to monitor performance issues like high CPU utilization. Amazon CloudWatch Logs (Option C) can monitor, store, and access log files, but it would require more administrative effort to set up and analyze. Amazon Aurora Database Activity Streams (Option D) provides a near real-time stream of database activity in your relational database, but it\u2019s not designed to identify performance issues and would require significant administrative effort to analyze the streams. Therefore, these options would not meet the requirements with the least administrative oversight."
      },
      {
        "date": "2024-04-10T13:48:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/devops-guru/latest/userguide/working-with-rds.overview.supported-engines.html\n\nWorks with Aurora RDS (both flavors) and with RDS PostgreSQL only"
      },
      {
        "date": "2024-04-10T13:49:00.000Z",
        "voteCount": 1,
        "content": "DevOps Guru only works with the above engines as per Doc\n\nShould be C"
      },
      {
        "date": "2024-03-03T02:36:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/devops-guru/latest/userguide/working-with-rds.overview.definitions.html"
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 356,
    "url": "https://www.examtopics.com/discussions/amazon/view/135107-exam-aws-certified-database-specialty-topic-1-question-356/",
    "body": "A company has more than 100 AWS accounts that need Amazon RDS instances. The company wants to build an automated solution to deploy the RDS instances with specific compliance parameters. The data does not need to be replicated. The company needs to create the databases within 1 day.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate RDS resources by using AWS CloudFormation. Share the CloudFormation template with each account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RDS snapshot. Share the snapshot with each account. Deploy the snapshot into each account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to create RDS instances in each account. Run AWS Database Migration Service (AWS DMS) replication to each of the created instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script by using the AWS CLI to copy the RDS instance into the other accounts from a template account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T01:14:00.000Z",
        "voteCount": 1,
        "content": "AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. It allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment."
      },
      {
        "date": "2024-03-03T02:41:00.000Z",
        "voteCount": 3,
        "content": "A\n\nOption C - Wrong: (Use AWS CloudFormation to create RDS instances and run AWS DMS replication) introduces unnecessary complexity by involving AWS DMS replication, which is not needed according to the requirements. It may also prolong the deployment process."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 357,
    "url": "https://www.examtopics.com/discussions/amazon/view/135108-exam-aws-certified-database-specialty-topic-1-question-357/",
    "body": "A database specialist needs to reduce the cost of an application's database. The database is running on a Multi-AZ deployment of an Amazon RDS for Microsoft SQL Server DB instance. The application requires the database to support stored procedures, SQL Server Wire Protocol (TDS), and T-SQL. The database must also be highly available. The database specialist is using AWS Database Migration Service (AWS DMS) to migrate the database to a new data store.<br><br>Which solution will reduce the cost of the database with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (DMS) to migrate to an RDS for MySQL Multi-AZ database. Update the application code to use the features of MySQL that correspond to SQL Server. Update the application to use the MySQL port.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (DMS) to migrate to an RDS for PostgreSQL Multi-AZ database. Turn on the SQL_COMPAT optional extension within the database to allow the required features. Update the application to use the PostgreSQL port.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (DMS) to migrate to an RDS for SQL Server Single-AZ database. Update the application to use the new database endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (DMS) to migrate the database to Amazon Aurora PostgreSQL. Turn on Babelfish for Aurora PostgreSQL. Update the application to use the Babelfish TDS port.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T01:15:00.000Z",
        "voteCount": 1,
        "content": "Amazon Aurora PostgreSQL with Babelfish provides a new way to run SQL Server applications on PostgreSQL. Babelfish understands the SQL Server wire protocol (TDS) and T-SQL, the SQL Server query language. By using Babelfish, you can switch from SQL Server to Aurora PostgreSQL with minimal effort and reduce the cost because Aurora PostgreSQL is less expensive than SQL Server."
      },
      {
        "date": "2024-03-22T19:39:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/cn/blogs/database/migrate-from-sql-server-to-amazon-aurora-using-babelfish/"
      },
      {
        "date": "2024-03-03T02:46:00.000Z",
        "voteCount": 2,
        "content": "D. Use AWS Database Migration Service (DMS) to migrate the database to Amazon Aurora PostgreSQL. Turn on Babelfish for Aurora PostgreSQL. Update the application to use the Babelfish TDS port."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 358,
    "url": "https://www.examtopics.com/discussions/amazon/view/135109-exam-aws-certified-database-specialty-topic-1-question-358/",
    "body": "A company's application team needs to select an AWS managed database service to store application and user data. The application team is familiar with MySQL but is open to new solutions. The application and user data is stored in 10 tables and is de-normalized. The application will access this data through an API layer using a unique ID in each table. The company expects the traffic to be light at first, but the traffic will increase to thousands of transactions each second within the first year. The database service must support active reads and writes in multiple AWS Regions at the same time. Query response times need to be less than 100 ms.<br><br>Which AWS database solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon RDS for MySQL environment in each Region and leverage AWS Database Migration Service (AWS DMS) to set up a multi-Region bidirectional replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon Aurora MySQL global database with write forwarding turned on.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon DynamoDB database with global tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Amazon DocumentDB global cluster across multiple Regions."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-05T19:15:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2024-03-03T02:49:00.000Z",
        "voteCount": 2,
        "content": "C. Deploy an Amazon DynamoDB database with global tables."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 359,
    "url": "https://www.examtopics.com/discussions/amazon/view/135110-exam-aws-certified-database-specialty-topic-1-question-359/",
    "body": "A manufacturing company stores its inventory details in an Amazon DynamoDB table in the us-east-2 Region. According to new compliance and regulatory policies, the company is required to back up all of its tables nightly and store these backups in the us-west-2 Region for disaster recovery for 1 year.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the existing DynamoDB table into a global table and create a global table replica in the us-west-2 Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to create a backup plan. Configure cross-Region replication in the plan and assign the DynamoDB table to this plan.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an on-demand backup of the DynamoDB table and restore this backup in the us-west-2 Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon S3 Cross-Region Replication (CRR) on the S3 bucket where DynamoDB on-demand backups are stored."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-11T22:16:00.000Z",
        "voteCount": 1,
        "content": "answer :B"
      },
      {
        "date": "2024-03-03T02:53:00.000Z",
        "voteCount": 2,
        "content": "Option B suggests using AWS Backup to create a backup plan, which allows for centralized management of backups across multiple AWS services, including DynamoDB. By configuring cross-Region replication in the backup plan, the backups of the DynamoDB table will be automatically replicated to the us-west-2 Region as required by the compliance and regulatory policies."
      }
    ],
    "examNameCode": "aws-certified-database-specialty",
    "topicNumber": "1"
  }
]