[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/133045-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is configuring an AWS Glue job to read data from an Amazon S3 bucket. The data engineer has set up the necessary AWS Glue connection details and an associated IAM role. However, when the data engineer attempts to run the AWS Glue job, the data engineer receives an error message that indicates that there are problems with the Amazon S3 VPC gateway endpoint.<br>The data engineer must resolve the error and connect the AWS Glue job to the S3 bucket.<br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the AWS Glue security group to allow inbound traffic from the Amazon S3 VPC gateway endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an S3 bucket policy to explicitly grant the AWS Glue job permissions to access the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the AWS Glue job code to ensure that the AWS Glue connection details include a fully qualified domain name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the VPC's route table includes inbound and outbound routes for the Amazon S3 VPC gateway endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T22:20:00.000Z",
        "voteCount": 1,
        "content": "D is valid "
      },
      {
        "date": "2024-09-23T02:25:00.000Z",
        "voteCount": 3,
        "content": "Although there is no such thing as \"inbound and outbound routes\" when we talk about VPC route table, when we define a S3 gateway endpoint we must have proper routes in place. I will go with D."
      },
      {
        "date": "2024-09-23T02:25:00.000Z",
        "voteCount": 4,
        "content": "A - wrong - AWS glue - are serverless service, so it don't have any security groups\nB - wrong - Because we have error with VPC, not with S3 itself\nC - wrong - Becuase with S3 - we always have only FQDN for buckets"
      },
      {
        "date": "2024-07-13T14:55:00.000Z",
        "voteCount": 1,
        "content": "they most certainly can have SGs."
      },
      {
        "date": "2024-09-20T14:03:00.000Z",
        "voteCount": 2,
        "content": "Be sure that the subnet configured for your AWS Glue connection has an Amazon S3 VPC gateway endpoint or a route to a NAT gateway in the subnet's route table.\n"
      },
      {
        "date": "2024-09-07T07:13:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-07-17T19:57:00.000Z",
        "voteCount": 1,
        "content": "I think D.  We check \"VPC's route table\""
      },
      {
        "date": "2024-07-03T02:18:00.000Z",
        "voteCount": 1,
        "content": "A - wrong - AWS glue doesn't have any security groups\nB - wrong - You can\u00b4t give permissions in the S3 to the AWS glue job but to the role\nD. wrong because there has to be a definend route for the S3 gateway endpoint in the subnet assigned to the glue job but not in the VPC's route table and also route tables doesn\u00b4t have inbound and outbound routes."
      },
      {
        "date": "2024-09-06T18:17:00.000Z",
        "voteCount": 1,
        "content": "\"route tables don\u00b4t have inbound and outbound routes.\"? It does. You need to check how the VPC works in AWS."
      },
      {
        "date": "2024-06-02T16:38:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer."
      },
      {
        "date": "2024-05-27T01:06:00.000Z",
        "voteCount": 1,
        "content": "I will go with D, the other options don't seem to be related."
      },
      {
        "date": "2024-05-18T19:35:00.000Z",
        "voteCount": 2,
        "content": "\"problems with the Amazon S3 VPC gateway endpoint\""
      },
      {
        "date": "2024-02-14T03:01:00.000Z",
        "voteCount": 1,
        "content": "Go with A:\nIf you receive an error, check the following:\n\nThe correct privileges are provided to the role selected.\nThe correct Amazon S3 bucket is provided.\nThe security groups and Network ACL allow the required incoming and outgoing traffic.\nThe VPC you specified is connected to an Amazon S3 VPC endpoint."
      },
      {
        "date": "2024-02-06T04:11:00.000Z",
        "voteCount": 1,
        "content": "some relevant info:\nmain: https://docs.aws.amazon.com/glue/latest/dg/connection-VPC-disable-proxy.html\nadditional (glue crawler instead of glue job here, but I think this is relevant for both): https://docs.aws.amazon.com/glue/latest/dg/connection-S3-VPC.html"
      },
      {
        "date": "2024-02-06T04:05:00.000Z",
        "voteCount": 4,
        "content": "Both ChatGPT and I agree with D"
      },
      {
        "date": "2024-04-23T06:46:00.000Z",
        "voteCount": 1,
        "content": ":-)) nice"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/131714-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A retail company has a customer data hub in an Amazon S3 bucket. Employees from many countries use the data hub to support company-wide analytics. A governance team must ensure that the company's data analysts can access data only for customers who are within the same country as the analysts.<br>Which solution will meet these requirements with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate table for each country's customer data. Provide access to each analyst based on the country that the analyst serves.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the S3 bucket as a data lake location in AWS Lake Formation. Use the Lake Formation row-level security features to enforce the company's access policies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data to AWS Regions that are close to the countries where the customers are. Provide access to each analyst based on the country that the analyst serves.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Amazon Redshift. Create a view for each country. Create separate IAM roles for each country to provide access to data from each country. Assign the appropriate roles to the analysts."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-08T21:27:00.000Z",
        "voteCount": 11,
        "content": "AWS Lake Formation: It's specifically designed for managing data lakes on AWS, providing capabilities for securing and controlling access to data.\nRow-Level Security: With Lake Formation, you can define fine-grained access control policies, including row-level security. This means you can enforce policies to restrict access to data based on specific conditions, such as the country associated with each customer.\nLeast Operational Effort: Once the policies are defined within Lake Formation, they can be centrally managed and applied to the data in the S3 bucket without the need for creating separate tables or views for each country, as in options A, C, and D. This reduces operational overhead and complexity."
      },
      {
        "date": "2024-08-22T20:36:00.000Z",
        "voteCount": 1,
        "content": "if the situation is not about least operational effort, D makes sense"
      },
      {
        "date": "2024-07-17T20:13:00.000Z",
        "voteCount": 1,
        "content": "Select B. It means \"with the LEAST operational effort\"."
      },
      {
        "date": "2024-06-02T16:38:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer."
      },
      {
        "date": "2024-04-02T01:56:00.000Z",
        "voteCount": 1,
        "content": "AWS really likes Lakeformation, plus creating separate tables might require some refactoring, and the requirements is about the LEAST operational effor"
      },
      {
        "date": "2024-03-27T10:29:00.000Z",
        "voteCount": 1,
        "content": "Agreed with Bartosz. I think setup DataLake, then integrate it with LakeFormation take a lot of effort than just separate the table"
      },
      {
        "date": "2024-03-07T01:00:00.000Z",
        "voteCount": 1,
        "content": "Keyword \"LEAST operational effort\" - I will go with B"
      },
      {
        "date": "2024-02-08T01:31:00.000Z",
        "voteCount": 2,
        "content": "Creating DataLake takes at least few days to set up and the solution should be LEAST operational. I think B is not correct."
      },
      {
        "date": "2024-01-20T19:16:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/registration-role.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/131706-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A media company wants to improve a system that recommends media content to customer based on user behavior and preferences. To improve the recommendation system, the company needs to incorporate insights from third-party datasets into the company's existing analytics platform.<br>The company wants to minimize the effort and time required to incorporate third-party datasets.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse API calls to access and integrate third-party datasets from AWS Data Exchange.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse API calls to access and integrate third-party datasets from AWS DataSync.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to access and integrate third-party datasets from AWS CodeCommit repositories.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to access and integrate third-party datasets from Amazon Elastic Container Registry (Amazon ECR)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T14:03:00.000Z",
        "voteCount": 8,
        "content": "AWS DataSync is primarily used for data transfer services designed to simplify, automate, and accelerate moving data between on-premises storage systems and AWS storage services, as well as between different AWS storage services. Its primary role is not for accessing third-party datasets but for efficiently transferring large volumes of data.\nIn contrast, AWS Data Exchange is designed specifically for discovering and subscribing to third-party data in the cloud, providing direct API access to these datasets, which aligns perfectly with the company's need to integrate this data into their recommendation systems with minimal overhead."
      },
      {
        "date": "2024-09-24T21:06:00.000Z",
        "voteCount": 1,
        "content": "Should be AWS Data Exchange."
      },
      {
        "date": "2024-07-18T00:07:00.000Z",
        "voteCount": 1,
        "content": "I will go with A. Kinesis Data Stram is more operational overhead."
      },
      {
        "date": "2024-07-10T23:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-06-03T15:12:00.000Z",
        "voteCount": 2,
        "content": "A is correct, DataSync doesn't really rely on API calls."
      },
      {
        "date": "2024-06-02T16:39:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2024-05-22T08:55:00.000Z",
        "voteCount": 1,
        "content": "AWS DataExchange"
      },
      {
        "date": "2024-05-08T21:30:00.000Z",
        "voteCount": 1,
        "content": "options B, C, and D involve using Amazon Kinesis Data Streams or other services that may not be directly suited for integrating third-party datasets from external sources like AWS Data Exchange. These options might require additional configurations, data processing steps, or infrastructure management, resulting in higher operational overhead compared to directly leveraging AWS Data Exchange's capabilities through API calls (Option A)."
      },
      {
        "date": "2024-03-18T00:25:00.000Z",
        "voteCount": 3,
        "content": "AWS Data Exchange is a service that makes it easy to share and manage data permissions from other organizations"
      },
      {
        "date": "2024-03-13T05:18:00.000Z",
        "voteCount": 1,
        "content": "I will go with A."
      },
      {
        "date": "2024-03-10T02:40:00.000Z",
        "voteCount": 2,
        "content": "There is no info or guarantee this third-party dataset is available in AWS to be part of a data-share, hence the more assertive answer is B"
      },
      {
        "date": "2024-03-07T01:03:00.000Z",
        "voteCount": 3,
        "content": "A for me. \"You can also discover and subscribe to new third-party data sets available through AWS Data Exchange\" \nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/what-is.html"
      },
      {
        "date": "2024-02-12T23:24:00.000Z",
        "voteCount": 2,
        "content": "A\nData exchange is primarily designed for this purpose."
      },
      {
        "date": "2024-02-01T12:35:00.000Z",
        "voteCount": 3,
        "content": "A\nData exchange is primarily designed for this purpose."
      },
      {
        "date": "2024-01-31T21:19:00.000Z",
        "voteCount": 2,
        "content": "A\nData Exchange is the AWS official third-party datasets repository: https://aws.amazon.com/data-exchange"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/131467-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A financial company wants to implement a data mesh. The data mesh must support centralized data governance, data analysis, and data access control. The company has decided to use AWS Glue for data catalogs and extract, transform, and load (ETL) operations.<br>Which combination of AWS services will implement a data mesh? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora for data storage. Use an Amazon Redshift provisioned cluster for data analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for data storage. Use Amazon Athena for data analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew for centralized data governance and access control.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS for data storage. Use Amazon EMR for data analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lake Formation for centralized data governance and access control.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-18T00:30:00.000Z",
        "voteCount": 7,
        "content": "The answer is B and E.\nThe data mesh implementation uses Amazon S3 and Athena for data storage and analysis, and AWS Lake Formation for centralized data governance and access control. When combined with AWS Glue, you can efficiently manage your data."
      },
      {
        "date": "2024-09-24T21:08:00.000Z",
        "voteCount": 1,
        "content": "S3 is best storage for data lake, and AWS lake formation is best for management."
      },
      {
        "date": "2024-09-23T02:27:00.000Z",
        "voteCount": 2,
        "content": "BE are correct answer."
      },
      {
        "date": "2024-09-23T02:27:00.000Z",
        "voteCount": 3,
        "content": "Sometimes I think examtopics uses us to calibrate the right answers hehehe, by the goal statement and the services outlines and objectives there are no way the answer be different then B,E"
      },
      {
        "date": "2024-09-23T02:27:00.000Z",
        "voteCount": 1,
        "content": "A: Cost-effective storage: Amazon S3 is a highly scalable and cost-effective object storage service perfect for storing large datasets commonly found in financial institutions.\nCentralized data lake: S3 acts as the central data lake where all data from different domains can reside in its raw or processed form.\nEasy data access: Athena provides a serverless interactive query service that allows data analysts to directly query data stored in S3 using standard SQL. This simplifies data exploration and analysis without managing servers.\nB: Data governance: Lake Formation helps establish data ownership, access control, and lineage for data products within the data mesh. It ensures data quality, security, and compliance with regulations.\nFine-grained access control: Lake Formation allows you to define granular access policies for each data domain, ensuring only authorized users can access specific data sets. This aligns with the need for centralized control in a data mesh."
      },
      {
        "date": "2024-07-24T00:44:00.000Z",
        "voteCount": 1,
        "content": "Must be B and E"
      },
      {
        "date": "2024-03-24T19:11:00.000Z",
        "voteCount": 1,
        "content": "i thing so"
      },
      {
        "date": "2024-02-29T16:16:00.000Z",
        "voteCount": 2,
        "content": "B and E . \nC - is not correct \"AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML)\""
      },
      {
        "date": "2024-02-24T09:27:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      },
      {
        "date": "2024-02-01T12:44:00.000Z",
        "voteCount": 2,
        "content": "BE\nGiven the requirements for implementing a data mesh architecture with centralized data governance, data analysis, and data access control, the two better choices from the options provided would be:\n\nB. Use Amazon S3 for data storage. Use Amazon Athena for data analysis.\n\nE. Use AWS Lake Formation for centralized data governance and access control."
      },
      {
        "date": "2024-01-18T01:07:00.000Z",
        "voteCount": 4,
        "content": "Textbook question, the keyword data mesh means S3, the keyword data governance means LakeFormation"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/131707-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer maintains custom Python scripts that perform a data formatting process that many AWS Lambda functions use. When the data engineer needs to modify the Python scripts, the data engineer must manually update all the Lambda functions.<br>The data engineer requires a less manual way to update the Lambda functions.<br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore a pointer to the custom Python scripts in the execution context object in a shared Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore a pointer to the custom Python scripts in environment variables in a shared Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the same alias to each Lambda function. Call reach Lambda function by specifying the function's alias."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T14:19:00.000Z",
        "voteCount": 16,
        "content": "B. Package the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions.\nExplanation:\nLambda layers allow you to centrally manage shared code and dependencies across multiple Lambda functions. By packaging the custom Python scripts into a Lambda layer, you can simply update the layer whenever changes are made to the scripts, and all the Lambda functions that use the layer will automatically inherit the updates. This approach reduces manual effort and ensures consistency across the functions."
      },
      {
        "date": "2024-09-23T02:28:00.000Z",
        "voteCount": 3,
        "content": "Centralized Code Management: Lambda layers allow you to store and manage the custom Python scripts in a central location outside the individual Lambda function code. This eliminates the need to update the script in each Lambda function manually.\nReusable Code: Layers provide a way to share code across multiple Lambda functions. Any changes made to the layer code are automatically reflected in all the functions using that layer, streamlining updates.\nReduced Deployment Size: By separating core functionality into layers, you can keep the individual Lambda function code focused and smaller. This reduces deployment package size and potentially improves Lambda execution times."
      },
      {
        "date": "2024-09-03T05:16:00.000Z",
        "voteCount": 2,
        "content": "Lambda Layers is a feature created with this literal objective in mind."
      },
      {
        "date": "2024-06-22T06:03:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2024-05-27T06:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-05-26T01:12:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-05-16T17:22:00.000Z",
        "voteCount": 1,
        "content": "Lamba layers"
      },
      {
        "date": "2024-03-28T11:12:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2024-03-14T06:17:00.000Z",
        "voteCount": 2,
        "content": "Typical use case for Lambda Layers.\nOption B."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/131469-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company created an extract, transform, and load (ETL) data pipeline in AWS Glue. A data engineer must crawl a table that is in Microsoft SQL Server. The data engineer needs to extract, transform, and load the output of the crawl to an Amazon S3 bucket. The data engineer also must orchestrate the data pipeline.<br>Which AWS service or feature will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Step Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue workflows\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Workflows for Apache Airflow (Amazon MWAA)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-18T01:32:00.000Z",
        "voteCount": 9,
        "content": "Glue workflows are the easiest solution here:\n\nhttps://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/\n\nhttps://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/"
      },
      {
        "date": "2024-09-23T02:28:00.000Z",
        "voteCount": 6,
        "content": "I asked an AI.\nAnalysis of the answers:\nA. AWS Step Functions:\nIt is a good option for orchestrating workflows with steps from different AWS services, but requires additional development to connect to Microsoft SQL Server.\nB. AWS Glue Workflows:\nThis is the best and most profitable option. AWS Glue is designed specifically for ETL on AWS and integrates directly with data sources such as Microsoft SQL Server through connectors. This allows for easier configuration and avoids the need for additional development.\nC. AWS Glue Studio:\nIt is a visual interface for AWS Glue that makes it easy to create and manage ETL jobs. However, the underlying functionality comes from AWS Glue (B) workflows.\nD. Amazon Managed Workflows for Apache Airflow (Amazon MWAA):\nIt's a viable option, but it's generally more expensive than native AWS services like AWS Glue Workflows. Additionally, it requires some Airflow experience for setup and maintenance."
      },
      {
        "date": "2024-09-29T19:38:00.000Z",
        "voteCount": 1,
        "content": "https://community.aws/content/2iBQiAGS4RvEolgSQKu4iF8InTV/choose-the-right-data-orchestration-service-for-your-data-pipeline?lang=en"
      },
      {
        "date": "2024-09-24T21:12:00.000Z",
        "voteCount": 1,
        "content": "Glue is easiest here to choose from."
      },
      {
        "date": "2024-05-03T01:18:00.000Z",
        "voteCount": 3,
        "content": "Agree with B. CRAWLING and ETL are the main functions of a Glue workflow and MS SQL is supported: https://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html"
      },
      {
        "date": "2024-02-24T09:32:00.000Z",
        "voteCount": 1,
        "content": "Is B !"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/131470-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A financial services company stores financial data in Amazon Redshift. A data engineer wants to run real-time queries on the financial data to support a web-based trading application. The data engineer wants to run the queries from within the trading application.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish WebSocket connections to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Redshift Data API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Java Database Connectivity (JDBC) connections to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore frequently accessed data in Amazon S3. Use Amazon S3 Select to run the queries."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T15:13:00.000Z",
        "voteCount": 1,
        "content": "A) Redshift doesn't support WebSockets;\nC) It is way harder to manage DB connections than using Redshift Data API which will offer you the possibility to run SQL queries directly.\nD)"
      },
      {
        "date": "2024-07-07T05:36:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-05-09T16:14:00.000Z",
        "voteCount": 2,
        "content": "Inside application with minimal effort then using API would be correct"
      },
      {
        "date": "2024-05-03T01:39:00.000Z",
        "voteCount": 3,
        "content": "\"The Amazon Redshift Data API enables you to painlessly access data from Amazon Redshift with all types of traditional, cloud-native, and containerized, serverless web service-based applications and event-driven applications.\"\nhttps://aws.amazon.com/de/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-clusters/#:~:text=The%20Amazon%20Redshift%20Data%20API%20is%20not%20a%20replacement%20for,supported%20by%20the%20AWS%20SDK."
      },
      {
        "date": "2024-03-11T06:19:00.000Z",
        "voteCount": 3,
        "content": "Even if you don't know nothing about them, you will still choose B because it seems the \"LEAST operational overhead\" :)"
      },
      {
        "date": "2024-02-24T09:33:00.000Z",
        "voteCount": 1,
        "content": "B. DATA API"
      },
      {
        "date": "2024-02-01T12:59:00.000Z",
        "voteCount": 4,
        "content": "B. Use the Amazon Redshift Data API.\n\nExplanation:\nThe Amazon Redshift Data API is a lightweight, HTTPS-based API that provides an alternative to using JDBC or ODBC drivers for running queries against Amazon Redshift. It allows you to execute SQL queries directly from within your application without the need for managing connections or drivers. This reduces operational overhead as there's no need to manage and maintain WebSocket or JDBC connections."
      },
      {
        "date": "2024-01-18T01:35:00.000Z",
        "voteCount": 4,
        "content": "Real time queries with S3 are obviously BS. B it is:\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/131471-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon Athena for one-time queries against data that is in Amazon S3. The company has several use cases. The company must implement permission controls to separate query processes and access to query history among users, teams, and applications that are in the same AWS account.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket for each use case. Create an S3 bucket policy that grants permissions to appropriate individual IAM users. Apply the S3 bucket policy to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Athena workgroup for each use case. Apply tags to the workgroup. Create an IAM policy that uses the tags to apply appropriate permissions to the workgroup.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role for each use case. Assign appropriate permissions to the role for each use case. Associate the role with Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue Data Catalog resource policy that grants permissions to appropriate individual IAM users for each use case. Apply the resource policy to the specific tables that Athena uses."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-18T01:38:00.000Z",
        "voteCount": 16,
        "content": "Haha they copied this from the old DA Specialty. It's B\n\nhttps://docs.aws.amazon.com/athena/latest/ug/user-created-workgroups.html"
      },
      {
        "date": "2024-02-01T13:05:00.000Z",
        "voteCount": 12,
        "content": "B. Create an Athena workgroup for each use case. Apply tags to the workgroup. Create an IAM policy that uses the tags to apply appropriate permissions to the workgroup.\n\nExplanation:\nAthena workgroups allow you to isolate and manage different workloads, users, and permissions. By creating a separate workgroup for each use case, you can control access to query history, manage permissions, and enforce resource usage limits independently for each workload. Applying tags to workgroups allows you to categorize and organize them based on the use case, which simplifies policy management."
      },
      {
        "date": "2024-07-10T23:47:00.000Z",
        "voteCount": 2,
        "content": "B is correct."
      },
      {
        "date": "2024-05-09T16:19:00.000Z",
        "voteCount": 1,
        "content": "The only other answer that's confusing is C But its not the one. Creating separate IAM roles for each use case and associating them with Athena would not provide the necessary isolation and access control for query processes and query history."
      },
      {
        "date": "2024-03-25T05:01:00.000Z",
        "voteCount": 1,
        "content": "B is more granular"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/132628-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to schedule a workflow that runs a set of AWS Glue jobs every day. The data engineer does not require the Glue jobs to run or finish at a specific time.<br>Which solution will run the Glue jobs in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose the FLEX execution class in the Glue job properties.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Spot Instance type in Glue job properties.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose the STANDARD execution class in the Glue job properties.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose the latest version in the GlueVersion field in the Glue job properties."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-08T08:58:00.000Z",
        "voteCount": 6,
        "content": "The FLEX execution class leverages spare capacity within the AWS infrastructure to run Glue jobs at a discounted price compared to the standard execution class. Since the data engineer doesn't have specific time constraints, utilizing spare capacity is ideal for cost savings.\nToday's date its a checkbox in order to spare capacity and will mean we dont know when is going to finish, which is recommended to increase a timeout"
      },
      {
        "date": "2024-02-01T13:10:00.000Z",
        "voteCount": 5,
        "content": "A. Choose the FLEX execution class in the Glue job properties.\n\nExplanation:\nThe FLEX execution class in AWS Glue allows jobs to use idle resources within the Glue service, which can significantly reduce costs compared to the STANDARD execution class. With FLEX, Glue jobs run when resources are available, which is a cost-effective approach for jobs that don't need to be completed within a specific timeframe."
      },
      {
        "date": "2024-07-24T09:18:00.000Z",
        "voteCount": 1,
        "content": "FLEX is how you lower Glue cost when you dont have urgency to run ETLs."
      },
      {
        "date": "2024-05-09T16:22:00.000Z",
        "voteCount": 3,
        "content": "As its said the FLEX job comes cheaper that hiring a spot instance"
      },
      {
        "date": "2024-04-02T13:58:00.000Z",
        "voteCount": 1,
        "content": "I'd go with A"
      },
      {
        "date": "2024-01-31T21:29:00.000Z",
        "voteCount": 4,
        "content": "A\nFlex allows you to optimize your costs on your non-urgent or non-time sensitive data integration workloads such as testing, and one-time data loads. With Flex, AWS Glue jobs run on spare compute capacity instead of dedicated hardware. The start and runtimes of jobs using Flex can vary because spare compute resources aren\u2019t readily available and can be reclaimed during the run of a job\n\nhttps://aws.amazon.com/blogs/big-data/introducing-aws-glue-flex-jobs-cost-savings-on-etl-workloads/"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/131472-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to create an AWS Lambda function that converts the format of data from .csv to Apache Parquet. The Lambda function must run only if a user uploads a .csv file to an Amazon S3 bucket.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification that has an event type of s3:ObjectTagging:* for objects that have a tag set to .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification that has an event type of s3:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set an Amazon Simple Notification Service (Amazon SNS) topic as the destination for the event notification. Subscribe the Lambda function to the SNS topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-18T01:43:00.000Z",
        "voteCount": 12,
        "content": "\"only if a user uploads data to an Amazon S3 bucket\" that excludes B &amp; C because we need s3:ObjectCreated:*\n\nYou don't need SNS for S3 event notifications so A is easier."
      },
      {
        "date": "2024-02-01T13:15:00.000Z",
        "voteCount": 7,
        "content": "A. Create an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.\n\nExplanation:\nThis solution directly triggers the Lambda function only when a .csv file is uploaded to the S3 bucket, minimizing unnecessary invocations of the Lambda function. It uses a specific event type (s3:ObjectCreated:*) and a filter rule to ensure that the Lambda function is invoked only for relevant events. Additionally, it directly invokes the Lambda function without the need for additional services like Amazon SNS, reducing operational overhead."
      },
      {
        "date": "2024-09-29T21:34:00.000Z",
        "voteCount": 1,
        "content": "s3:ObjectCreated:* instead of s3:*: triggers the Lambda function only when objects are created in the bucket."
      },
      {
        "date": "2024-09-24T01:24:00.000Z",
        "voteCount": 1,
        "content": "A is the answer for least operational. C also correct!"
      },
      {
        "date": "2024-06-08T09:01:00.000Z",
        "voteCount": 1,
        "content": "since is the least operational, the D its a candidate, however add a SNS operation, which in this case is not needed. so A includes S3 and triggering towards the lambda function. 2 services."
      },
      {
        "date": "2024-05-09T16:28:00.000Z",
        "voteCount": 1,
        "content": "S3 event notification to lamba for file prefix with.csv is the least overhead way"
      },
      {
        "date": "2024-05-03T01:53:00.000Z",
        "voteCount": 2,
        "content": "\"You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/131473-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs Amazon Athena queries to finish faster. The data engineer notices that all the files the Athena queries use are currently stored in uncompressed .csv format. The data engineer also notices that users perform most queries by selecting a specific column.<br>Which solution will MOST speed up the Athena query performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the data format from .csv to JSON format. Apply Snappy compression.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the .csv files by using Snappy compression.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the data format from .csv to Apache Parquet. Apply Snappy compression.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the .csv files by using gzip compression."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-18T01:47:00.000Z",
        "voteCount": 10,
        "content": "If the exam would only have these kinds of questions everyone would be blessed"
      },
      {
        "date": "2024-01-20T18:22:00.000Z",
        "voteCount": 1,
        "content": "Hahahaha! I believe that this kind of question is only for the beta calibration purpose. They won't be in the final exam version."
      },
      {
        "date": "2024-02-01T13:17:00.000Z",
        "voteCount": 6,
        "content": "C. Change the data format from .csv to Apache Parquet. Apply Snappy compression.\n\nExplanation:\nApache Parquet is a columnar storage format optimized for analytical queries. It is highly efficient for query performance, especially when queries involve selecting specific columns, as it allows for column pruning and predicate pushdown optimizations."
      },
      {
        "date": "2024-07-24T09:22:00.000Z",
        "voteCount": 1,
        "content": "C is the way to do It based on best practices recommended by AWS (https://aws.amazon.com/pt/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)"
      },
      {
        "date": "2024-05-11T23:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-05-09T16:40:00.000Z",
        "voteCount": 1,
        "content": "switching to Apache Parquet format with Snappy compression offers the most significant improvement in Athena query performance, especially for queries that select specific columns"
      },
      {
        "date": "2024-05-05T19:23:00.000Z",
        "voteCount": 1,
        "content": "Parquet is columnar storage and the question specifies that users performs most queries by selecting a specific column."
      },
      {
        "date": "2024-04-08T06:46:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/jp/blogs/news/top-10-performance-tuning-tips-for-amazon-athena/"
      },
      {
        "date": "2024-02-24T09:37:00.000Z",
        "voteCount": 1,
        "content": "C easy"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/131474-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A manufacturing company collects sensor data from its factory floor to monitor and enhance operational efficiency. The company uses Amazon Kinesis Data Streams to publish the data that the sensors collect to a data stream. Then Amazon Kinesis Data Firehose writes the data to an Amazon S3 bucket.<br>The company needs to display a real-time view of operational efficiency on a large screen in the manufacturing facility.<br>Which solution will meet these requirements with the LOWEST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Use a connector for Apache Flink to write data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the S3 bucket to send a notification to an AWS Lambda function when any new object is created. Use the Lambda function to publish the data to Amazon Aurora. Use Aurora as a source to create an Amazon QuickSight dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Create a new Data Firehose delivery stream to publish data directly to an Amazon Timestream database. Use the Timestream database as a source to create an Amazon QuickSight dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue bookmarks to read sensor data from the S3 bucket in real time. Publish the data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-23T06:16:00.000Z",
        "voteCount": 10,
        "content": "https://aws.amazon.com/blogs/database/near-real-time-processing-with-amazon-kinesis-amazon-timestream-and-grafana/\n\nLook at the architecture diagram"
      },
      {
        "date": "2024-08-04T23:02:00.000Z",
        "voteCount": 1,
        "content": "No! :-)\nBecause: The company needs to display a REAL-TIME view of operational efficiency on a large screen in the manufacturing facility.\n\nSo it's for sure C"
      },
      {
        "date": "2024-09-29T22:02:00.000Z",
        "voteCount": 1,
        "content": "Grafana:\nReal-time Performance:\nGrafana is known for its excellent real-time data visualization capabilities.\nIt's often used for operational dashboards that require frequent updates.\n\nIntegration:\nWorks well with time-series databases and streaming data sources. [2]"
      },
      {
        "date": "2024-08-19T22:12:00.000Z",
        "voteCount": 2,
        "content": "Firehose cannot use Timestream as destination. Answer is A"
      },
      {
        "date": "2024-08-14T01:36:00.000Z",
        "voteCount": 1,
        "content": "Option A is for processing data in Flink and then sending it to Timestream. This is advantageous when complex data processing is required in Flink, but the processing step where complex analytics are processed can handle additional latency.\n\nOption C performs data processing in Flink, sends the data directly to Timestream without any additional steps, and provides dashboards via QuickSight. Since data can be started immediately after arriving in Timestream, latency is likely to be higher.\n\nTherefore, option C is preferable because it can handle latency by performing data processing, publishing data directly to Timestream, and provides fast dashboards using QuickSight."
      },
      {
        "date": "2024-08-13T07:39:00.000Z",
        "voteCount": 2,
        "content": "Amazon QuickSight is primarily designed for business intelligence and data visualization, and it can provide near real-time views depending on the data refresh rate. However, it is not typically used for real-time streaming data visualization with very low latency. For real-time dashboards with very low latency, services like Grafana are more suitable.\nYou can use Amazon Managed Grafana to setup the dashboard so you're using an AWS service which is always preferible on these exams."
      },
      {
        "date": "2024-08-04T23:02:00.000Z",
        "voteCount": 1,
        "content": "Because: The company needs to display a real-time view of operational efficiency on a large screen in the manufacturing facility."
      },
      {
        "date": "2024-08-01T15:58:00.000Z",
        "voteCount": 2,
        "content": "Based on this it should be C, why use an open source app when you can an AWS Service\n\nhttps://community.amazonquicksight.com/t/real-time-data-visualization-capabilities-of-amazon-quicksight/24007"
      },
      {
        "date": "2024-05-07T05:11:00.000Z",
        "voteCount": 1,
        "content": "The Question is: Which solution will meet these requirements with the LOWEST latency?\nSo just A can be the right answer \"lowest latency!!!!\""
      },
      {
        "date": "2024-04-08T18:54:00.000Z",
        "voteCount": 1,
        "content": "I go with Option A. Kinesis Data Firehose can connect to 3 AWS destinations so far S3, Redshift and OpenSearch."
      },
      {
        "date": "2024-03-19T11:16:00.000Z",
        "voteCount": 1,
        "content": "Option A:\n- Involves additional steps: Option A requires writing data to Amazon Timestream after processing with Apache Flink, potentially introducing additional latency compared to a more direct approach like Option C.\n- Grafana integration: While Grafana is a powerful visualization tool, setting up and configuring Grafana dashboards might require additional effort compared to using Amazon QuickSight, which offers more straightforward integration with AWS services like Amazon Timestream."
      },
      {
        "date": "2024-03-19T11:13:00.000Z",
        "voteCount": 2,
        "content": "C. - **Processing Sensor Data with Amazon Flink**: Similar to option A, this approach uses Amazon Managed Service for Apache Flink to process sensor data, providing real-time analytics or transformation capabilities.\n   - **Data Firehose Delivery Stream to Timestream**: Sets up a new Amazon Data Firehose delivery stream to publish processed data directly to Amazon Timestream. Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as data lakes, databases, and analytics services.\n   - **Timestream Database as a Source for QuickSight Dashboard**: Similar to option B, the data stored in Amazon Timestream serves as the data source for creating an Amazon QuickSight dashboard."
      },
      {
        "date": "2024-03-19T11:13:00.000Z",
        "voteCount": 1,
        "content": "A. - **Processing Sensor Data**: Utilizes Amazon Managed Service for Apache Flink, a fully managed service for real-time data processing. This service is used to process the sensor data, which likely involves real-time analysis or transformation of incoming data streams.\n   - **Connector for Apache Flink to Amazon Timestream**: Integrates a connector for Apache Flink to write processed data into Amazon Timestream, a fully managed time-series database. Timestream is optimized for IoT and time-series data.\n   - **Timestream Database as a Source for Grafana Dashboard**: The data stored in Timestream serves as the data source for creating a Grafana dashboard. Grafana is a popular open-source analytics and monitoring platform that visualizes time-series data."
      },
      {
        "date": "2024-03-19T11:08:00.000Z",
        "voteCount": 1,
        "content": "Considerations:\n\nOption A utilizes Amazon Managed Service for Apache Flink to process sensor data and then writes the processed data to Amazon Timestream. From there, the Timestream database serves as a source to create a Grafana dashboard.\nThus the data goes through Apache Flink for processing, then to Timestream, and finally to Grafana. \"Each additional step introduces potential latency\".\n\nOption C processes sensor data using Amazon Managed Service for Apache Flink and then publishes data directly to Amazon Timestream via a Data Firehose delivery stream. Finally, it uses Timestream as a source to create an Amazon QuickSight dashboard.\n\nSo, in terms of latency, both options involve processing data in real-time using Apache Flink. However, Option C has a more direct data flow by publishing data directly to Timestream, potentially reducing latency compared to Option A, where the data has to go through an additional step of writing to Timestream."
      },
      {
        "date": "2024-02-01T13:24:00.000Z",
        "voteCount": 2,
        "content": "A. Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Use a connector for Apache Flink to write data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard.\n\nExplanation:\nAmazon Managed Service for Apache Flink provides real-time stream processing capabilities, which can process sensor data with low latency. By using Apache Flink connectors, the processed data can be efficiently written to Amazon Timestream, which is optimized for time-series data storage and querying."
      },
      {
        "date": "2024-01-18T01:51:00.000Z",
        "voteCount": 4,
        "content": "real time -&gt; no Quicksight. And bookmarks to read sensor data real time is just as stupid as the flat earth theory. A it is."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/131709-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores daily records of the financial performance of investment portfolios in .csv format in an Amazon S3 bucket. A data engineer uses AWS Glue crawlers to crawl the S3 data.<br>The data engineer must make the S3 data accessible daily in the AWS Glue Data Catalog.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that includes the AmazonS3FullAccess policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Configure the output destination to a new path in the existing S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Specify a database name for the output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that includes the AmazonS3FullAccess policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Allocate data processing units (DPUs) to run the crawler every day. Specify a database name for the output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Allocate data processing units (DPUs) to run the crawler every day. Configure the output destination to a new path in the existing S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T14:12:00.000Z",
        "voteCount": 7,
        "content": "B. Create an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Specify a database name for the output.\n\nExplanation:\nOption B correctly sets up the IAM role with the necessary permissions using the AWSGlueServiceRole policy, which is designed for use with AWS Glue. It specifies the S3 bucket path of the source data as the crawler's data store and creates a daily schedule to run the crawler. Additionally, it specifies a database name for the output, ensuring that the crawled data is properly cataloged in the AWS Glue Data Catalog."
      },
      {
        "date": "2024-05-09T17:27:00.000Z",
        "voteCount": 3,
        "content": "Glue Crawlers are serverless. Assigning DPUs is the point where i decided it option B"
      },
      {
        "date": "2024-03-07T04:52:00.000Z",
        "voteCount": 3,
        "content": "A,C are wrong because you use don't need full S3 access. D is wrong because you don't need to provision DPU and the destination should be a database, not an s3 bucket. so it's B"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/131675-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company loads transaction data for each day into Amazon Redshift tables at the end of each day. The company wants to have the ability to track which tables have been loaded and which tables still need to be loaded.<br>A data engineer wants to store the load statuses of Redshift tables in an Amazon DynamoDB table. The data engineer creates an AWS Lambda function to publish the details of the load statuses to DynamoDB.<br>How should the data engineer invoke the Lambda function to write load statuses to the DynamoDB table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a second Lambda function to invoke the first Lambda function based on Amazon CloudWatch events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Redshift Data API to publish an event to Amazon EventBridge. Configure an EventBridge rule to invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Redshift Data API to publish a message to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queue to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a second Lambda function to invoke the first Lambda function based on AWS CloudTrail events."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T03:48:00.000Z",
        "voteCount": 11,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/data-api-monitoring-events.html"
      },
      {
        "date": "2024-02-01T13:54:00.000Z",
        "voteCount": 8,
        "content": "The most appropriate way for the data engineer to invoke the Lambda function to write load statuses to the DynamoDB table is:\n\nB. Use the Amazon Redshift Data API to publish an event to Amazon EventBridge. Configure an EventBridge rule to invoke the Lambda function.\n\nExplanation:\nOption B leverages the Amazon Redshift Data API to publish events to Amazon EventBridge, which provides a serverless event bus service for handling events across AWS services. By configuring an EventBridge rule to invoke the Lambda function in response to events published by the Redshift Data API, the data engineer can ensure that the Lambda function is triggered whenever there is a new transaction data load in Amazon Redshift. This approach offers a straightforward and scalable solution for tracking table load statuses without relying on additional Lambda functions or services."
      },
      {
        "date": "2024-08-19T00:31:00.000Z",
        "voteCount": 1,
        "content": "This job doesn\u2019t need a real time check"
      },
      {
        "date": "2024-06-22T07:40:00.000Z",
        "voteCount": 1,
        "content": "Why not used SQS to keep API change in the Queue ?"
      },
      {
        "date": "2024-06-08T18:30:00.000Z",
        "voteCount": 1,
        "content": "Im not 100% sure of B or C, this is a tricky question. The reason is due to either SQS or EventBridge has not direct connection natively speaking to Redshift Data API. There is no way to publish events by itself. So, this means either SQS / EventBridge eventually need a \"proxy\" (e.g lambda function) in order to publish events or process events to this 2 sources. In both services we need something to publish those events from Redshift. so Yes, we need a lamda function between Redshift Data API and (SQS|EB). so either B,C doesnt seem to be 100% right. I think this question its a good candidate to be \"Choose two options\" but none has 100% right. Both are valid considering that there is an adapter function between 2 solutions."
      },
      {
        "date": "2024-08-20T01:01:00.000Z",
        "voteCount": 1,
        "content": "It seems that Redshift Data API could directly publishing events in EventBridge (see first comment). For monitoring the Redshift Data API, you could use both EventBridge (near-real-time) or CloudTrail (stored in S3): https://docs.aws.amazon.com/redshift/latest/mgmt/data-api-monitoring.html\nBut both services are related to \"Data API\" not Redshift database itself. So it is really tricky."
      },
      {
        "date": "2024-08-20T01:04:00.000Z",
        "voteCount": 1,
        "content": "So, you could use the Redshift table STV_LOAD_STATE,\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_STV_LOAD_STATE.html\nand running a \"select\" query on that table for getting status of tables (filtering by timestamp) and add the result to EventBridge, applying a rule on those events to invoke the lambda function. I guess that B is the most appropiate answer."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/131676-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to securely transfer 5 TB of data from an on-premises data center to an Amazon S3 bucket. Approximately 5% of the data changes every day. Updates to the data need to be regularly proliferated to the S3 bucket. The data includes files that are in multiple formats. The data engineer needs to automate the transfer process and must schedule the process to run periodically.<br>Which AWS service should the data engineer use to transfer the data in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS DataSync\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Direct Connect",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon S3 Transfer Acceleration"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-01T13:57:00.000Z",
        "voteCount": 10,
        "content": "A. AWS DataSync\n\nExplanation:\nAWS DataSync is a managed data transfer service that simplifies and accelerates moving large amounts of data online between on-premises storage and Amazon S3, EFS, or FSx for Windows File Server. DataSync is optimized for efficient, incremental, and reliable transfers of large datasets, making it suitable for transferring 5 TB of data with daily updates."
      },
      {
        "date": "2024-08-20T01:13:00.000Z",
        "voteCount": 1,
        "content": "Aseems correct.\n\nAWS Direct Connect is a networking service, nothing to be realted to sync data between on-premises and cloud storage services, as DataSync does (\" online service that automates and accelerates moving data between on premises and AWS Storage services.\")."
      },
      {
        "date": "2024-06-08T18:39:00.000Z",
        "voteCount": 2,
        "content": "DataSync, locations, tasks, is all what you need."
      },
      {
        "date": "2024-05-16T17:41:00.000Z",
        "voteCount": 1,
        "content": "is datasync"
      },
      {
        "date": "2024-04-26T01:35:00.000Z",
        "voteCount": 2,
        "content": "A. AWS DataSync \nAWS DataSync is a data transfer service specifically designed to simplify and accelerate moving large volumes of data between on-premises storage systems and AWS storage services like S3."
      },
      {
        "date": "2024-04-19T00:47:00.000Z",
        "voteCount": 1,
        "content": "That's the job of DataSync"
      },
      {
        "date": "2024-04-11T15:56:00.000Z",
        "voteCount": 1,
        "content": "A - DataSync is build for this use case"
      },
      {
        "date": "2024-01-20T03:52:00.000Z",
        "voteCount": 2,
        "content": "Typical DataSync use case"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/131677-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses an on-premises Microsoft SQL Server database to store financial transaction data. The company migrates the transaction data from the on-premises database to AWS at the end of each month. The company has noticed that the cost to migrate data from the on-premises database to an Amazon RDS for SQL Server database has increased recently.<br>The company requires a cost-effective solution to migrate the data to AWS. The solution must cause minimal downtown for the applications that access the database.<br>Which AWS service should the company use to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Lambda",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Database Migration Service (AWS DMS)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Direct Connect",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS DataSync"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T03:54:00.000Z",
        "voteCount": 20,
        "content": "Whoever is the admin that pre-marks the answers, it's time to go"
      },
      {
        "date": "2024-02-01T14:00:00.000Z",
        "voteCount": 12,
        "content": "B. AWS Database Migration Service (AWS DMS)\n\nExplanation:\nAWS Database Migration Service (DMS) is specifically designed for migrating data from various sources, including on-premises databases, to AWS with minimal downtime and disruption to applications. It supports homogeneous migrations (e.g., SQL Server to SQL Server) as well as heterogeneous migrations (e.g., SQL Server to Amazon RDS for SQL Server)."
      },
      {
        "date": "2024-09-06T19:36:00.000Z",
        "voteCount": 1,
        "content": "Hahaha, I loved the \"downtown\" typo in the question. I always say the same instead of \"downtime\".."
      },
      {
        "date": "2024-08-20T03:26:00.000Z",
        "voteCount": 1,
        "content": "D could be OK.\nI mean, it is talking about migrating to a cloud database and switching-off the current on-premise database. So you could use AWS Snowball Edge Storage to move the backup of the on-premises database, and when it is in the edge storage, copy the data to a new cloud-based SQL server instance using AWS DataSync\n\nhttps://aws.amazon.com/es/blogs/storage/seamlessly-migrate-large-sql-databases-using-aws-snowball-and-aws-datasync/"
      },
      {
        "date": "2024-09-06T19:34:00.000Z",
        "voteCount": 1,
        "content": "Right, but Snowball isn't mentioned in Option D. Thus, you can't consider it as OK."
      },
      {
        "date": "2024-06-08T18:44:00.000Z",
        "voteCount": 2,
        "content": "AWS DMS offers a cost-effective solution for database migrations compared to replicating data to a fully managed RDS instance.\nYou only pay for the resources used during the migration, making it ideal for infrequent, monthly transfers"
      },
      {
        "date": "2024-04-30T04:41:00.000Z",
        "voteCount": 1,
        "content": "AWS Database Migration Service (DMS)"
      },
      {
        "date": "2024-03-30T16:37:00.000Z",
        "voteCount": 2,
        "content": "B Since it's for Migration porpouse, typical for DMS"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/131679-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is building a data pipeline on AWS by using AWS Glue extract, transform, and load (ETL) jobs. The data engineer needs to process data from Amazon RDS and MongoDB, perform transformations, and load the transformed data into Amazon Redshift for analytics. The data updates must occur every hour.<br>Which combination of tasks will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Glue triggers to run the ETL jobs every hour.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew to clean and prepare the data for analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda functions to schedule and run the ETL jobs every hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Redshift Data API to load transformed data into Amazon Redshift."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T23:09:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: AC\nA. because the question is saying that the jobs are build in Glue, and must run every hour.\nC. because you can run the jobs as Lambda functions every hour.\n\nB. discarted, because the question is saying that \"DE\" is using Glue, DataBrew is for cleaning data without code, but it seems that the \"DE\" is writing code for transforming the data.\nD. Discarted, because the connections are not directly related to the question, that it is saying that you should run every hour Glue jobs, and the connections doesn't seem relevant.\nE. Discarted, because is saying that the data source is RDS and MongoDB, not Redshift, so you cannot use the Redshift Data API for getting the data and transform it."
      },
      {
        "date": "2024-08-11T10:44:00.000Z",
        "voteCount": 1,
        "content": "AE \nD is not valid. as it shoyld be \nUse AWS Glue connections to establish connectivity between the data sources (including Amazon Redshift) and Glue Job"
      },
      {
        "date": "2024-08-14T02:24:00.000Z",
        "voteCount": 1,
        "content": "An AWS Glue connection is a setting that allows an AWS Glue job to access a data source. This allows you to connect to databases such as RDS, MongoDB, etc. However, this opinion states that this connection is not used to load data directly into Redshift, and that Glue jobs must use the COPY command to load data into Redshift, which is inappropriate. However, since Glue jobs can process data and load it directly into Redshift, it is a bit of a stretch to consider option D as unconditionally wrong."
      },
      {
        "date": "2024-06-08T18:49:00.000Z",
        "voteCount": 2,
        "content": "A. Configure AWS Glue triggers to run the ETL jobs every hour.\n    Reduced Code Complexity: Glue triggers eliminate the need to write custom code for scheduling ETL jobs. This simplifies the pipeline and reduces maintenance overhead.\n    Scalability and Integration: Glue triggers work seamlessly with Glue ETL jobs, ensuring efficient scheduling and execution within the Glue ecosystem.\nD. Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.\n    Pre-Built Connectors: Glue connections offer pre-built connectors for various data sources like RDS and Redshift. This eliminates the need for manual configuration and simplifies data source access within the ETL jobs.\n    Centralized Management: Glue connections are centrally managed within the Glue service, streamlining connection management and reducing operational overhead."
      },
      {
        "date": "2024-05-03T03:12:00.000Z",
        "voteCount": 2,
        "content": "I was not sure about A - But in AWS console =&gt; Glue =&gt; Triggers =&gt; Add Trigger I have found the Trigger type: \"Schedule - Fire the trigger on a timer.\""
      },
      {
        "date": "2024-03-31T13:24:00.000Z",
        "voteCount": 1,
        "content": "I found this question actually confusing. In which step the transformation would be implemented itself? I can be wrong, but with Glue triggers we would only run the job, but not the transformation logic itself. In this way, I would go in C and D"
      },
      {
        "date": "2024-03-18T08:28:00.000Z",
        "voteCount": 2,
        "content": "Not a clear question - B would kinda make sense - but AD seems to be more correct"
      },
      {
        "date": "2024-03-07T05:16:00.000Z",
        "voteCount": 3,
        "content": "A - this is obvious and D -https://docs.aws.amazon.com/glue/latest/dg/console-connections.html"
      },
      {
        "date": "2024-02-01T14:05:00.000Z",
        "voteCount": 3,
        "content": "A. Configure AWS Glue triggers to run the ETL jobs every hour.\nD. Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.\n\nExplanation:\n\nOption A: Configuring AWS Glue triggers allows the ETL jobs to be scheduled and run automatically every hour without the need for manual intervention. This reduces operational overhead by automating the data processing pipeline.\n\nOption D: Using AWS Glue connections simplifies connectivity between the data sources (Amazon RDS and MongoDB) and Amazon Redshift. Glue connections abstract away the details of connection configuration, making it easier to manage and maintain the data pipeline."
      },
      {
        "date": "2024-01-21T22:58:00.000Z",
        "voteCount": 4,
        "content": "AWS Glue triggers provide a simple and integrated way to schedule ETL jobs. By configuring these triggers to run hourly, the data engineer can ensure that the data processing and updates occur as required without the need for external scheduling tools or custom scripts. This approach is directly integrated with AWS Glue, reducing the complexity and operational overhead.\nAWS Glue supports connections to various data sources, including Amazon RDS and MongoDB. By using AWS Glue connections, the data engineer can easily configure and manage the connectivity between these data sources and Amazon Redshift. This method leverages AWS Glue\u2019s built-in capabilities for data source integration, thus minimizing operational complexity and ensuring a seamless data flow from the sources to the destination (Amazon Redshift)."
      },
      {
        "date": "2024-01-20T03:59:00.000Z",
        "voteCount": 2,
        "content": "Lambda triggers for Glue jobs make me dizzy"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/131680-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses an Amazon Redshift cluster that runs on RA3 nodes. The company wants to scale read and write capacity to meet demand. A data engineer needs to identify a solution that will turn on concurrency scaling.<br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on concurrency scaling in workload management (WLM) for Redshift Serverless workgroups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on concurrency scaling in the settings during the creation of any new Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on concurrency scaling for the daily usage quota for the Redshift cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-01T14:09:00.000Z",
        "voteCount": 8,
        "content": "B. Turn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster.\n\nExplanation:\nConcurrency scaling in Amazon Redshift allows the cluster to automatically add and remove compute resources in response to workload demands. Enabling concurrency scaling at the workload management (WLM) queue level allows you to specify which queues can benefit from concurrency scaling based on the query workload."
      },
      {
        "date": "2024-08-22T03:57:00.000Z",
        "voteCount": 2,
        "content": "Selected answer: B\nB. According to documentation, the \"concurrency scaling\" is set up in workload management queue (see comment below).\n\nA. discarted, because redshift serverless scales automatically (it doesn't need enable \"concurrency scaling\")."
      },
      {
        "date": "2024-05-05T20:54:00.000Z",
        "voteCount": 1,
        "content": "B. Turn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster."
      },
      {
        "date": "2024-04-26T22:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nB. Turn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster."
      },
      {
        "date": "2024-01-20T04:02:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/131683-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer must orchestrate a series of Amazon Athena queries that will run every day. Each query can run for more than 15 minutes.<br>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions workflow and add two states. Add the first state before the Lambda function. Configure the second state as a Wait state to periodically check whether the Athena query has finished using the Athena Boto3 get_query_execution API call. Configure the workflow to invoke the next query when the current query has finished running.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue Python shell job and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue Python shell script to run a sleep timer that checks every 5 minutes to determine whether the current Athena query has finished running successfully. Configure the Python shell script to invoke the next query when the current query has finished running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate the Athena queries in AWS Batch."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T20:13:00.000Z",
        "voteCount": 7,
        "content": "AWS Lambda can be effectively used to trigger Athena queries. By using the start_query_execution API from the Athena Boto3 client, you can programmatically start Athena queries. Lambda functions are cost-effective as they charge based on the compute time used, and there's no charge when the code is not running. However, Lambda has a maximum execution timeout of 15 minutes, which means it's not suitable for long-running operations but can be used to trigger or start queries.\nAWS Step Functions can orchestrate multiple AWS services in workflows. By using a Wait state, the workflow can periodically check the status of the Athena query, and proceed to the next step once the query is complete. This approach is more scalable and reliable compared to continuously running a Lambda function, as Step Functions can handle long-running processes better and can maintain the state of each step in the workflow."
      },
      {
        "date": "2024-08-22T04:08:00.000Z",
        "voteCount": 2,
        "content": "Lambda max timeout is 15 minutes, and the query takes more than 15 minutes. So Lambda should be ended prior the Athena query."
      },
      {
        "date": "2024-03-07T05:42:00.000Z",
        "voteCount": 7,
        "content": "B - because\n https://docs.aws.amazon.com/step-functions/latest/dg/sample-athena-query.html\nE - because \nhttps://aws.amazon.com/blogs/big-data/orchestrate-amazon-emr-serverless-spark-jobs-with-amazon-mwaa-and-data-validation-using-amazon-athena/"
      },
      {
        "date": "2024-08-22T04:09:00.000Z",
        "voteCount": 1,
        "content": "I discarted E because Airflow is more expensive than Glue/Step-Functions. So B (step-function) and D (glue python shell)."
      },
      {
        "date": "2024-05-13T22:18:00.000Z",
        "voteCount": 3,
        "content": "The question is about a \"combination of steps\" - MWAA and Step Functions are different options, so I would prefer AB"
      },
      {
        "date": "2024-08-11T10:54:00.000Z",
        "voteCount": 1,
        "content": "BE is right. A is only giving option to envoke the athena query. how about the response. if the execution is beyong 15 mins"
      },
      {
        "date": "2024-08-22T04:07:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: BD\n\nLambda maximum timeout is 15 minutes. So the query takes more than Lambda could manage. So you cannot use lambda. Use Step-Function (answer B) or glue python (answer D)\nAirflow is more expensive than Glue/Step-Functions, so E is discarted also."
      },
      {
        "date": "2024-08-04T23:14:00.000Z",
        "voteCount": 1,
        "content": "It should be AB"
      },
      {
        "date": "2024-06-30T04:03:00.000Z",
        "voteCount": 1,
        "content": "Since the Athena API supports async/await, users are able to separate the steps into trigger queries and get results after 15 minutes."
      },
      {
        "date": "2024-06-08T19:03:00.000Z",
        "voteCount": 1,
        "content": "tricky, A is valid. Still, cost effective:\nB no one doubt on it. then why E?\nMWAA offers a managed Apache Airflow environment for orchestrating complex workflows.\nIt can handle long-running tasks like Athena queries efficiently.\nBatch Processing: Leveraging AWS Batch within the Airflow workflow allows for distributed and scalable execution of the Athena queries, improving overall processing efficiency."
      },
      {
        "date": "2024-08-22T04:10:00.000Z",
        "voteCount": 1,
        "content": "A could be not valid, as queries takes more than 15 minutes, and Lambda maximum timeout is 15 minutes. Lambda would be ended prior than the query is finished."
      },
      {
        "date": "2024-05-25T10:12:00.000Z",
        "voteCount": 1,
        "content": "my opinian"
      },
      {
        "date": "2024-05-22T03:45:00.000Z",
        "voteCount": 1,
        "content": "I would prefer AB"
      },
      {
        "date": "2024-05-18T21:52:00.000Z",
        "voteCount": 2,
        "content": "Lambda for kick start Athena\nStep Functions for orchestration"
      },
      {
        "date": "2024-05-04T04:07:00.000Z",
        "voteCount": 1,
        "content": "Option C and D involve using an AWS Glue Python shell script to run a sleep timer and periodically check whether the current Athena query has finished running. While this approach might seem cost-effective in terms of using AWS Glue, it's not the most efficient way to manage the execution of Athena queries. AWS Glue is primarily designed for ETL (Extract, Transform, Load) tasks rather than orchestrating long-running query execution.\n\nTherefore, while both options B, C and D could technically work, they might not be the most cost-effective or efficient solutions for orchestrating long-running Athena queries. Instead, options A and E would likely be more cost-effective and suitable for this scenario."
      },
      {
        "date": "2024-05-04T04:07:00.000Z",
        "voteCount": 1,
        "content": "Option B, utilizing AWS Step Functions, can be a cost-effective solution for orchestrating the execution of Athena queries, but it might not be the most cost-effective in this scenario because Step Functions are billed based on state transitions and the duration of state execution. Since each query can run for more than 15 minutes, using Step Functions to wait and periodically check the status of the queries could potentially result in higher costs, especially if the queries frequently take a long time to complete."
      },
      {
        "date": "2024-04-13T23:23:00.000Z",
        "voteCount": 3,
        "content": "Lambda call Athena query;\nStep function orchestrate query workflow"
      },
      {
        "date": "2024-03-30T06:02:00.000Z",
        "voteCount": 3,
        "content": "A: Lambda is a good option and it only trigger the athena not actually run it. No need 15 min for it.\nB. it mentioned a series of athena queries and it may means that one query should wait until the former one finished. B is the perfect way to do it.\nAnd lambda and step functions are very cost effective."
      },
      {
        "date": "2024-08-22T04:13:00.000Z",
        "voteCount": 1,
        "content": "Yes, lambda could asynchronically run, but in this case, you don't get the status of the query. The current query should be ended before launch the next one."
      },
      {
        "date": "2024-03-24T19:38:00.000Z",
        "voteCount": 2,
        "content": "Remember, anything involves writing codes are gonna be cheaper than automated/UI guided workflows, so that left ACD, and aws Lambda can't run for more than 15 mins so CD.\n\nGuys, go take the associate architect certificate first... this is basic knowledge... stop spamming chatgpt (wrongly) generated answer"
      },
      {
        "date": "2024-08-22T04:14:00.000Z",
        "voteCount": 1,
        "content": "C could be discarted because it not said nothing about waiting for the current query to start a new one. So BD are the most accurate."
      },
      {
        "date": "2024-03-26T22:39:00.000Z",
        "voteCount": 4,
        "content": "The Lambda function for A is only used to start Athena's query and then stop. It would only take around one second to do it.\nThe StepFunction waits for the Athena query to complete; it performs no computing, so it would be cheaper than using AWS Glue Python shell scripts."
      },
      {
        "date": "2024-08-22T04:15:00.000Z",
        "voteCount": 1,
        "content": "But if you stop the lambda before the query ends, you don't get the status of the query, and  it seems that \"orchestrate\" meaning is to run them secuencially."
      },
      {
        "date": "2024-03-20T09:43:00.000Z",
        "voteCount": 1,
        "content": "2. **AWS Glue Documentation**:\n   - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. AWS Glue offers capabilities for running Python shell jobs, which can be used to execute custom scripts for various data processing tasks. The documentation provides details on how to create and manage Python shell jobs, including examples of using scripts to interact with AWS services like Athena.\n   - Reference: [AWS Glue Documentation]https://docs.aws.amazon.com/glue/index.html"
      },
      {
        "date": "2024-03-20T09:42:00.000Z",
        "voteCount": 2,
        "content": "To justify the selection of options B and D as the most cost-effective combination for orchestrating Amazon Athena queries, let's refer to the official AWS documentation:\n\n1. **AWS Step Functions Documentation**:\n   - AWS Step Functions is a fully managed service provided by AWS for coordinating the components of distributed applications and microservices using visual workflows. With Step Functions, you can build workflows that execute a sequence of AWS Lambda functions, API calls, and other AWS services. The documentation provides information on how to create workflows, define states, and configure wait states for checking the status of tasks, which aligns with the requirements of orchestrating Amazon Athena queries.\n   - Reference: [AWS Step Functions Documentation]https://docs.aws.amazon.com/step-functions/index.html"
      },
      {
        "date": "2024-03-20T09:40:00.000Z",
        "voteCount": 2,
        "content": "Option E: Using Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate Athena queries in AWS Batch.\n\nWhile Amazon MWAA provides managed Apache Airflow environments, which can be used for orchestrating workflows, it might not be the most cost-effective option for orchestrating Athena queries due to:\n\n- **Complexity**: Setting up and managing an Amazon MWAA environment can introduce additional complexity and potentially higher costs compared to other options.\n\n- **Resource Allocation**: Amazon MWAA environments come with a minimum cost, regardless of usage, and managing resources in AWS Batch might not be as cost-efficient for this specific use case compared to simpler solutions like Step Functions or Glue Python shell scripts.\n\nReference: [Amazon Managed Workflows for Apache Airflow Documentation]https://docs.aws.amazon.com/mwaa/index.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/131684-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is migrating on-premises workloads to AWS. The company wants to reduce overall operational overhead. The company also wants to explore serverless options.<br>The company's current workloads use Apache Pig, Apache Oozie, Apache Spark, Apache Hbase, and Apache Flink. The on-premises workloads process petabytes of data in seconds. The company must maintain similar or better performance after the migration to AWS.<br>Which extract, transform, and load (ETL) service will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EMR\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Lambda",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Redshift"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T04:20:00.000Z",
        "voteCount": 15,
        "content": "Glue is like the more good-looking one, but weaker brother of EMR. So when it's about petabyte scales, let EMR do the work and have Glue stay away from the action."
      },
      {
        "date": "2024-09-09T12:02:00.000Z",
        "voteCount": 1,
        "content": "B.\nAmazon EMR Serverless is a deployment option for Amazon EMR that provides a serverless runtime environment. This simplifies the operation of analytics applications that use the latest open-source frameworks, such as Apache Spark and Apache Hive. With EMR Serverless, you don\u2019t have to configure, optimize, secure, or operate clusters to run applications with these frameworks."
      },
      {
        "date": "2024-08-28T01:00:00.000Z",
        "voteCount": 2,
        "content": "I think it is A, Glue\n\u2022  Amazon EMR is used for petabyte-scale data collection and data processing.\n\u2022  AWS Glue is used as a serverless and managed ETL service, and also used for managing data quality with AWS Glue Data Quality."
      },
      {
        "date": "2024-08-25T22:42:00.000Z",
        "voteCount": 1,
        "content": "Glue.\nIt talks about \"serverless\" so EMR is discarted. The mention of Spark, Hbase, etc is for confusing you, because it doesn't say that they wanted to keep using them. Glue can run Spark using \"glueContext\" (similar a SparkContext) for reading tables, files and create frames."
      },
      {
        "date": "2024-08-11T10:58:00.000Z",
        "voteCount": 1,
        "content": "The company also wants to explore serverless options. ? Glue (A). or EMR Serverless"
      },
      {
        "date": "2024-08-04T23:17:00.000Z",
        "voteCount": 1,
        "content": "Serverless: AWS Glue is a fully managed, serverless ETL service that automates the process of data discovery, preparation, and transformation, helping minimize operational overhead.Integration with Big Data Tools: It integrates well with various AWS services and supports Spark jobs for ETL purposes, which aligns well with Apache Spark workloads.Performance: AWS Glue can handle large-scale ETL workloads, and it is designed to manage petabytes of data efficiently, comparable to the performance of on-premises solutions.While B. Amazon EMR could also be considered for its flexibility in handling big data workloads using tools like Apache Spark, it requires more management and doesn't fit the serverless requirement as closely as AWS Glue. Therefore, AWS Glue is the most suitable choice given the constraints and requirements."
      },
      {
        "date": "2024-06-08T19:06:00.000Z",
        "voteCount": 3,
        "content": "EMR provides a managed Hadoop framework that natively supports Apache Pig,\nOozie, Spark, and Flink. This allows the company to migrate their existing workloads with minimal code changes, reducing development effort"
      },
      {
        "date": "2024-06-01T23:27:00.000Z",
        "voteCount": 2,
        "content": "That's exactly the purpose of EMR. \n\n\"Amazon EMR is the industry-leading cloud big data solution for petabyte-scale data processing, interactive analytics, and machine learning using open-source frameworks such as Apache Spark, Apache Hive, and Presto.\"\n\nhttps://aws.amazon.com/emr/"
      },
      {
        "date": "2024-05-07T10:28:00.000Z",
        "voteCount": 2,
        "content": "Glue is Serverless :)"
      },
      {
        "date": "2024-04-08T16:55:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/ja_jp/emr/latest/ManagementGuide/emr-what-is-emr.html"
      },
      {
        "date": "2024-03-20T10:21:00.000Z",
        "voteCount": 2,
        "content": "- While AWS Glue is a fully managed ETL service and offers serverless capabilities, it might not provide the same level of performance and flexibility as Amazon EMR for handling petabyte-scale workloads with complex processing requirements.\n   - AWS Glue is optimized for data integration, cataloging, and ETL jobs but may not be as well-suited for heavy-duty processing tasks that require frameworks like Apache Spark, Apache Flink, etc., which are commonly used for large-scale data processing.\n   - Documentation on AWS Glue can be found in the AWS Glue Developer Guide https://docs.aws.amazon.com/glue/index.html."
      },
      {
        "date": "2024-03-20T10:20:00.000Z",
        "voteCount": 2,
        "content": "A. AWS Glue:\nAWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services (AWS). It allows users to prepare and load data for analytics purposes\n\nB. Amazon EMR:\nAmazon Elastic MapReduce (EMR) is a cloud-based big data platform provided by AWS. It allows users to process and analyze large amounts of data using popular frameworks such as Apache Hadoop, Apache Spark, Apache Hive, Apache HBase, and more. \n\nhttps://docs.aws.amazon.com/emr/index.html\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-best-practices.html\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-manage.html\nhttps://docs.aws.amazon.com/emr/latest/DeveloperGuide/emr-developer-guide.html\n\nAs per the AWS/Amazon docs, option B specifically calls out it out with the specific features/options that the question asked directly about."
      },
      {
        "date": "2024-03-07T05:48:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html"
      },
      {
        "date": "2024-02-01T14:30:00.000Z",
        "voteCount": 1,
        "content": "A. AWS Glue"
      },
      {
        "date": "2024-01-20T18:37:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/emr/features/"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/132653-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer must use AWS services to ingest a dataset into an Amazon S3 data lake. The data engineer profiles the dataset and discovers that the dataset contains personally identifiable information (PII). The data engineer must implement a solution to profile the dataset and obfuscate the PII.<br>Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon Kinesis Data Firehose delivery stream to process the dataset. Create an AWS Lambda transform function to identify the PII. Use an AWS SDK to obfuscate the PII. Set the S3 data lake as the target for the delivery stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Detect PII transform in AWS Glue Studio to identify the PII. Obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Detect PII transform in AWS Glue Studio to identify the PII. Create a rule in AWS Glue Data Quality to obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the dataset into Amazon DynamoDB. Create an AWS Lambda function to identify and obfuscate the PII in the DynamoDB table and to transform the data. Use the same Lambda function to ingest the data into the S3 data lake."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-18T09:04:00.000Z",
        "voteCount": 7,
        "content": "How does Data Quality obfuscate PII? You can do this directly in Glue Studio: https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html"
      },
      {
        "date": "2024-09-11T16:01:00.000Z",
        "voteCount": 1,
        "content": "The keyt"
      },
      {
        "date": "2024-08-07T19:30:00.000Z",
        "voteCount": 1,
        "content": "B provides a streamlined, mostly visual approach using purpose-built tools for data processing and PII handling, making it the solution with the least operational effort."
      },
      {
        "date": "2024-07-27T13:46:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/automated-data-governance-with-aws-glue-data-quality-sensitive-data-detection-and-aws-lake-formation/"
      },
      {
        "date": "2024-07-27T13:58:00.000Z",
        "voteCount": 1,
        "content": "Actually it is B"
      },
      {
        "date": "2024-07-12T00:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html"
      },
      {
        "date": "2024-06-30T17:20:00.000Z",
        "voteCount": 1,
        "content": "anwser is C"
      },
      {
        "date": "2024-06-22T12:31:00.000Z",
        "voteCount": 4,
        "content": "Option C involves additional steps and complexity with creating rules in AWS Glue Data Quality, which adds more operational effort compared to directly using AWS Glue Studio's capabilities."
      },
      {
        "date": "2024-06-15T18:54:00.000Z",
        "voteCount": 3,
        "content": "I don't think we need to use much more services to fulfill these requirements. Just AWS Glue is enough, it can detect and obfuscate PII data already.\nSource: https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html#choose-action-pii"
      },
      {
        "date": "2024-05-18T23:39:00.000Z",
        "voteCount": 3,
        "content": "We cannot directly handle PII with Glue Studio, and Glue Data Quality can be used to handle PII."
      },
      {
        "date": "2024-05-07T21:55:00.000Z",
        "voteCount": 1,
        "content": "A very easy was is to use the SDK to identify PII.\n\nhttps://docs.aws.amazon.com/code-library/latest/ug/comprehend_example_comprehend_DetectPiiEntities_section.html"
      },
      {
        "date": "2024-05-04T06:39:00.000Z",
        "voteCount": 3,
        "content": "The transform Detect PII in AWS Glue Studio is specifically used to identify personally identifiable information (PII) within the data. It can detect and flag this information, but on its own, it does not perform the obfuscation or removal of these details.\n\nTo effectively obfuscate or alter the identified PII, an additional transformation would be necessary. This could be accomplished in several ways, such as:\n\nWriting a custom script within the same AWS Glue job using Python or Scala to modify the PII data as needed.\nUsing AWS Glue Data Quality, if available, to create rules that automatically obfuscate or modify the data identified as PII. AWS Glue Data Quality is a newer tool that helps improve data quality through rules and transformations, but whether it's needed will depend on the functionality's availability and the specificity of the obfuscation requirements"
      },
      {
        "date": "2024-04-13T12:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is option C. Period"
      },
      {
        "date": "2024-03-30T06:13:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nC: glue data quality cannot obfuscate the PII\nD: need to write code but the question is the \"LEAST operational effort\""
      },
      {
        "date": "2024-03-20T11:44:00.000Z",
        "voteCount": 2,
        "content": "In python ---\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"Example Glue Job\") \\\n    .getOrCreate()\n\n# Initialize Glue context\nglueContext = GlueContext(SparkContext.getOrCreate())\n\n# Retrieve Glue job arguments\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\n# Define your EMR step\nemr_step = [\n    {\n        \"Name\": \"My EMR Step\",\n        \"ActionOnFailure\": \"CONTINUE\",\n        \"HadoopJarStep\": {\n            \"Jar\": \"s3://your-bucket/emr-scripts/your_script.jar\",\n            \"Args\": [\n                \"arg1\",\n                \"arg2\"\n            ]\n        }\n    }\n]\n\n# Execute the EMR step\nresponse = glueContext.start_job_run(args['JOB_NAME'], job_run_args={'--extra-py-files': 'your_script.py'})\nprint(response)"
      },
      {
        "date": "2024-03-20T11:14:00.000Z",
        "voteCount": 2,
        "content": "B. Utilizes AWS Glue Studio for PII detection, AWS Step Functions for orchestration, and S3 for storage. Glue Studio simplifies PII detection, and Step Functions can streamline the data pipeline orchestration, potentially reducing operational effort compared to option A.\n\nC. Similar to option B, but it additionally includes AWS Glue Data Quality for obfuscating PII. This might add a bit more complexity but can also streamline the process if Glue Data Quality offers convenient features for PII obfuscation."
      },
      {
        "date": "2024-03-17T20:57:00.000Z",
        "voteCount": 4,
        "content": "AWS Glue Data Quality is a feature that automatically validates the quality of the data during a Glue job run, but it's not typically used for data obfuscation."
      },
      {
        "date": "2024-03-07T05:58:00.000Z",
        "voteCount": 1,
        "content": "https://dev.to/awscommunity-asean/validating-data-quality-with-aws-glue-databrew-4df4\nhttps://docs.aws.amazon.com/glue/latest/dg/detect-PII.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/131710-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company maintains multiple extract, transform, and load (ETL) workflows that ingest data from the company's operational databases into an Amazon S3 based data lake. The ETL workflows use AWS Glue and Amazon EMR to process data.<br>The company wants to improve the existing architecture to provide automated orchestration and to require minimal manual effort.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue workflows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Step Functions tasks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Lambda functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Workflows for Apache Airflow (Amazon MWAA) workflows"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-22T03:44:00.000Z",
        "voteCount": 10,
        "content": "Glue Workflow only orchestrate crawlers and glue jobs"
      },
      {
        "date": "2024-05-03T04:18:00.000Z",
        "voteCount": 6,
        "content": "For me it's B because I did not found a possibility how Glue can trigger/orchestrate EMR processes OOTB.\nBut with StepFunction there is a way: https://aws.amazon.com/blogs/big-data/orchestrate-amazon-emr-serverless-jobs-with-aws-step-functions/"
      },
      {
        "date": "2024-10-01T22:59:00.000Z",
        "voteCount": 1,
        "content": "glue workflows is part of the glue ecosystem so its provides seamless integration with minimal changes"
      },
      {
        "date": "2024-10-01T18:39:00.000Z",
        "voteCount": 1,
        "content": "Answer A, Glue workflows"
      },
      {
        "date": "2024-08-24T12:47:00.000Z",
        "voteCount": 1,
        "content": "Glue workflows are managed services and best for considering least operational overhead."
      },
      {
        "date": "2024-08-04T23:22:00.000Z",
        "voteCount": 1,
        "content": "AWS Glue Workflows are specifically designed for orchestrating ETL jobs in AWS Glue. They allow you to define and manage complex workflows that include multiple jobs and triggers, all within the AWS Glue environment.Integration: AWS Glue workflows seamlessly integrate with other AWS Glue components, making it easier to manage ETL processes without the need for external orchestration tools.Minimal Operational Overhead: Since AWS Glue is a fully managed service, using Glue workflows will reduce the operational overhead compared to managing separate orchestrators or building custom solutions.While D. Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is also a good choice for more complex orchestration, it may involve more management overhead compared to the more straightforward AWS Glue workflows. Thus, AWS Glue workflows provide the least operational overhead given the context of this scenario."
      },
      {
        "date": "2024-07-03T21:48:00.000Z",
        "voteCount": 1,
        "content": "B - because AWS Glue can't trigger EMR"
      },
      {
        "date": "2024-05-26T06:29:00.000Z",
        "voteCount": 3,
        "content": "EMR in workflows , i dont think so"
      },
      {
        "date": "2024-05-19T00:00:00.000Z",
        "voteCount": 4,
        "content": "There is no way for Glue Workflow to trigger EMR"
      },
      {
        "date": "2024-04-29T10:11:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/"
      },
      {
        "date": "2024-03-31T14:47:00.000Z",
        "voteCount": 6,
        "content": "Since it seems to me that this pipeline is complex, with multiple workflows, I would go for Glue workflows."
      },
      {
        "date": "2024-03-28T08:07:00.000Z",
        "voteCount": 3,
        "content": "Yo me voy por la D) Amazon MWAA porque Glue Workflows solo admite Jobs de Glue y Step Function puede fucionar pero no son workflows de datos. Amazon MWAA son workflows de datos y esta integrado tanto con Glue como EMR:  https://aws.amazon.com/blogs/big-data/simplify-aws-glue-job-orchestration-and-monitoring-with-amazon-mwaa/"
      },
      {
        "date": "2024-03-20T11:44:00.000Z",
        "voteCount": 2,
        "content": "Here's an example of how you can use AWS Glue to initiate an EMR (Elastic MapReduce) job:\n\nLet's assume you have an AWS Glue job that performs ETL tasks on data stored in Amazon S3. You want to leverage EMR for a specific task within this job, such as running a complex Spark job.\n\n1. Define a Glue Job: Create an AWS Glue job using the AWS Glue console, SDK, or CLI. Define the input and output data sources, as well as the transformations you want to apply.\n\n2. Incorporate EMR Step: Within the Glue job script, include a section where you define an EMR step. An EMR step is a unit of work that performs a specific task on an EMR cluster.\n\nCode follows in the next entry..."
      },
      {
        "date": "2024-03-10T23:32:00.000Z",
        "voteCount": 4,
        "content": "orchestrating = step function"
      },
      {
        "date": "2024-02-04T05:24:00.000Z",
        "voteCount": 3,
        "content": "Option A, AWS Glue Workflows, seems to be the best solution to meet the requirements with the least operational overhead. It offers a seamless integration with the company's existing AWS Glue and Amazon EMR setup, providing a managed and straightforward way to orchestrate their ETL workflows without extensive additional setup or manual intervention."
      },
      {
        "date": "2024-03-05T15:21:00.000Z",
        "voteCount": 3,
        "content": "Can you provide an example of Glue initiating an EMR job? Or somewhere in the documents? AFAIK, Glue workflows are only to be used for Glue related things e.g. pull data, transform it, and store it somewhere else (ETL). Executing commands on behalf of other services can be done using boto in glue, but it feels weird using Glue like that when you have step functions which are designed for orchestrating different services."
      },
      {
        "date": "2024-02-01T14:59:00.000Z",
        "voteCount": 2,
        "content": "Glue Work flows"
      },
      {
        "date": "2024-01-20T18:40:00.000Z",
        "voteCount": 4,
        "content": "Orchestrating different AWS services is a typical use case for Step Functions: https://docs.aws.amazon.com/step-functions/latest/dg/connect-emr.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-glue.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/132654-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company currently stores all of its data in Amazon S3 by using the S3 Standard storage class.<br>A data engineer examined data access patterns to identify trends. During the first 6 months, most data files are accessed several times each day. Between 6 months and 2 years, most data files are accessed once or twice each month. After 2 years, data files are accessed only once or twice each year.<br>The data engineer needs to use an S3 Lifecycle policy to develop new data storage rules. The new storage solution must continue to provide high availability.<br>Which solution will meet these requirements in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Deep Archive after 2 years.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months. Transfer objects to S3 Glacier Deep Archive after 2 years."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T14:07:00.000Z",
        "voteCount": 10,
        "content": "\"S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously\"\n\nSource: https://aws.amazon.com/s3/storage-classes/glacier/"
      },
      {
        "date": "2024-10-03T07:09:00.000Z",
        "voteCount": 1,
        "content": "It's C, the request is about \"High availability\" not \"Less time to retrieve the data\". The other request is \"most cost effective\" so deleting A and D for the HA, remains B and C that both satisfy the HA. Now choose the most cost effective --&gt; C"
      },
      {
        "date": "2024-09-24T02:55:00.000Z",
        "voteCount": 1,
        "content": "Galcier Deep Archive take longer retrieval time (hours to days) than Glacier Flexible Retrieval."
      },
      {
        "date": "2024-09-17T18:18:00.000Z",
        "voteCount": 1,
        "content": "Should be B. The questions asks for high availability, when a file is in Glacier Deep Archive, it takes more time to be available to use."
      },
      {
        "date": "2024-09-06T20:02:00.000Z",
        "voteCount": 1,
        "content": "To retrieve data from the S3 Glacier Deep Archive, you need more than 12 hours! The scenario mentions that we still need to access data instantly once or twice yearly. S3 Glacier Flexible Retrieval is more appropriate in this case."
      },
      {
        "date": "2024-08-12T21:54:00.000Z",
        "voteCount": 1,
        "content": "\u2022\tS3 Glacier Instant Retrieval delivers the lowest cost storage, up to 68% lower cost (than S3 Standard-Infrequent Access), for long-lived data that is accessed once per quarter and requires millisecond retrieval.\n\u2022\tS3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously. \n\u2022\tS3 Glacier Deep Archive delivers the lowest cost storage, up to 75% lower cost (than S3 Glacier Flexible Retrieval), for long-lived archive data that is accessed less than once per year and is retrieved asynchronously."
      },
      {
        "date": "2024-07-21T03:02:00.000Z",
        "voteCount": 2,
        "content": "Based on this link https://aws.amazon.com/s3/storage-classes/glacier/\nGlacier Flexible Retrieval is cheaper that Instant Retrieval"
      },
      {
        "date": "2024-07-11T15:05:00.000Z",
        "voteCount": 2,
        "content": "questions doesnt provide clairty on when data is accessed does it need to made available instantly or not. Deep archive times are longer."
      },
      {
        "date": "2024-07-08T22:23:00.000Z",
        "voteCount": 1,
        "content": "B https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-s3-glacier-flexible-retrieval/"
      },
      {
        "date": "2024-06-22T12:37:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C.\nB: While this option transitions to S3 Glacier Flexible Retrieval after 2 years, which provides quicker retrieval times than Glacier Deep Archive, it is more expensive. Given the infrequent access pattern after 2 years, the additional cost is not justified."
      },
      {
        "date": "2024-06-12T04:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is B, because Object needs to be retrieved once / twice monthly , hence GFR\nS3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously\n\nhttps://aws.amazon.com/s3/storage-classes/glacier/"
      },
      {
        "date": "2024-05-27T02:24:00.000Z",
        "voteCount": 1,
        "content": "I will go with C because Glacier Flexible Retrieval is way more expensive than Glacier Deep Archive."
      },
      {
        "date": "2024-05-07T22:09:00.000Z",
        "voteCount": 2,
        "content": "HA and cost effective. \nHere is no hint in the question for instant access.."
      },
      {
        "date": "2024-04-12T15:42:00.000Z",
        "voteCount": 1,
        "content": "HA - C"
      },
      {
        "date": "2024-03-31T15:41:00.000Z",
        "voteCount": 4,
        "content": "Since it was requested high availability, then can't be Standard One Zone. And in the end of 2 years it was asked for the most cost effective, than Glacier Deep Archive."
      },
      {
        "date": "2024-03-30T06:24:00.000Z",
        "voteCount": 2,
        "content": "2 requirements: 1)highly available 2) cost-effective. no mention about load time so C is correct."
      },
      {
        "date": "2024-03-29T07:29:00.000Z",
        "voteCount": 2,
        "content": "The question mention \"the most cost-effective way\". C is most cost-effective and still highly available. The requirement doesn't indicate the retrieval time requirement."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/131711-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company maintains an Amazon Redshift provisioned cluster that the company uses for extract, transform, and load (ETL) operations to support critical analysis tasks. A sales team within the company maintains a Redshift cluster that the sales team uses for business intelligence (BI) tasks.<br>The sales team recently requested access to the data that is in the ETL Redshift cluster so the team can perform weekly summary analysis tasks. The sales team needs to join data from the ETL cluster with data that is in the sales team's BI cluster.<br>The company needs a solution that will share the ETL cluster data with the sales team without interrupting the critical analysis tasks. The solution must minimize usage of the computing resources of the ETL cluster.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the sales team BI cluster as a consumer of the ETL cluster by using Redshift data sharing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate materialized views based on the sales team's requirements. Grant the sales team direct access to the ETL cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate database views based on the sales team's requirements. Grant the sales team direct access to the ETL cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnload a copy of the data from the ETL cluster to an Amazon S3 bucket every week. Create an Amazon Redshift Spectrum table based on the content of the ETL cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T22:31:00.000Z",
        "voteCount": 1,
        "content": "Seems that the performance of the critical ETL cluster should not be affected when using data sharing, so the answer is likely A:\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\n\nSupporting different kinds of business-critical workloads \u2013 Use a central extract, transform, and load (ETL) cluster that shares data with multiple business intelligence (BI) or analytic clusters. This approach provides read workload isolation and chargeback for individual workloads. You can size and scale your individual workload compute according to the workload-specific requirements of price and performance.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/considerations.html\nThe performance of the queries on shared data depends on the compute capacity of the consumer clusters."
      },
      {
        "date": "2024-09-03T16:56:00.000Z",
        "voteCount": 1,
        "content": "A as Redshift data sharing allows you to share live data across Redshift clusters without having to duplicate the data. This feature enables the sales team to access the data from the ETL cluster directly without interrupting the critical analysis tasks or overloading the ETL cluster's resources. The sales team can join this shared data with their own data in the BI cluster efficiently."
      },
      {
        "date": "2024-08-28T00:17:00.000Z",
        "voteCount": 1,
        "content": "\"The solution must minimize usage of the computing resources of the ETL cluster.\" That is key. You shouldn't use ETL cluster, so unload data to S3 and run queries in a separate Redshift Spectrum database. ETL cluster do nothing meanwhile."
      },
      {
        "date": "2024-05-19T00:14:00.000Z",
        "voteCount": 2,
        "content": "Typetical Redshift data sharing use case"
      },
      {
        "date": "2024-05-10T07:00:00.000Z",
        "voteCount": 2,
        "content": "key words: \"weekly\"\n\"The solution must minimize usage of the computing resources of the ETL cluster.\"\n\nAnswer:D"
      },
      {
        "date": "2024-05-06T21:25:00.000Z",
        "voteCount": 3,
        "content": "Typical usecase of datasharing in Redshift. \n\nThe question mentions that - 'team needs to join data from the ETL cluster with data that is in the sales team's BI cluster.' This is possible with datashare."
      },
      {
        "date": "2024-03-31T17:03:00.000Z",
        "voteCount": 4,
        "content": "In my opinion using Redshift Data Sharing will consume less resources. 'D' envolves using a S3 bucket."
      },
      {
        "date": "2024-03-31T17:04:00.000Z",
        "voteCount": 4,
        "content": "Sorry I wanted to select A but did D"
      },
      {
        "date": "2024-03-30T06:33:00.000Z",
        "voteCount": 4,
        "content": "A: redshift data sharing:\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\nWith data sharing, you can securely and easily share live data across Amazon Redshift clusters.\nB: materialized view is only within 1 redshift cluster, across different tables"
      },
      {
        "date": "2024-03-28T08:30:00.000Z",
        "voteCount": 3,
        "content": "The spectrum table is accessed from the sales cluster with zero impact on the ETL cluster."
      },
      {
        "date": "2024-03-20T18:56:00.000Z",
        "voteCount": 1,
        "content": "Options A, B, and C involve granting the sales team direct access to the ETL cluster, which could potentially impact the performance of the ETL cluster and interfere with its critical analysis tasks. Option D provides a more isolated and scalable approach by leveraging Amazon S3 and Redshift Spectrum for data sharing while minimizing the usage of the ETL cluster's computing resources.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum-sharing-data.html \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-design-tables.html"
      },
      {
        "date": "2024-03-20T18:56:00.000Z",
        "voteCount": 2,
        "content": "Overall, while both options offer ways to share data between the ETL and BI clusters, Option D offers a more robust and scalable solution that minimizes the impact on the ETL cluster's resources and provides greater flexibility and independence for the sales team's analysis tasks.\n\nBy unloading a copy of the data from the ETL cluster to Amazon S3 and leveraging Redshift Spectrum for querying, the solution aligns with AWS best practices for managing data and resource usage in Amazon Redshift clusters. It ensures that critical analysis tasks are not interrupted while providing the sales team with the necessary access to perform their analysis tasks efficiently."
      },
      {
        "date": "2024-03-10T23:41:00.000Z",
        "voteCount": 4,
        "content": "Initially I would go with B but that definitely will use more resource."
      },
      {
        "date": "2024-01-20T18:47:00.000Z",
        "voteCount": 3,
        "content": "To share data between Redshift clusters and meet the requirements of sharing ETL cluster data with the sales team without interrupting critical analysis tasks and minimizing the usage of the ETL cluster's computing resources, Redshift Data Sharing is the way to go\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\n\n\"Supporting different kinds of business-critical workloads \u2013 Use a central extract, transform, and load (ETL) cluster that shares data with multiple business intelligence (BI) or analytic clusters. This approach provides read workload isolation and chargeback for individual workloads. You can size and scale your individual workload compute according to the workload-specific requirements of price and performance\""
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/131712-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to join data from multiple sources to perform a one-time analysis job. The data is stored in Amazon DynamoDB, Amazon RDS, Amazon Redshift, and Amazon S3.<br>Which solution will meet this requirement MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon EMR provisioned cluster to read from all sources. Use Apache Spark to join the data and perform the analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the data from DynamoDB, Amazon RDS, and Amazon Redshift into Amazon S3. Run Amazon Athena queries directly on the S3 files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena Federated Query to join the data from all data sources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Redshift Spectrum to query data from DynamoDB, Amazon RDS, and Amazon S3 directly from Redshift."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-31T17:37:00.000Z",
        "voteCount": 6,
        "content": "I would go for C because Federated Query is typical for this porpouse. Besides, we don't need to add/duplicate resources in S3. But I see that, becasuse Athena is more optimized for S3, it can be considered a tricky question, since there can be more trade-offs to consider, such as data governance that are easier if data is centralized in S3 in my opinion."
      },
      {
        "date": "2024-06-08T19:46:00.000Z",
        "voteCount": 4,
        "content": "Serverless Processing: Athena is a serverless query service, meaning you only pay for the queries you run. This eliminates the need to provision and manage compute resources like in EMR clusters,\nmaking it ideal for one-time jobs.\nFederated Query Capability: Athena Federated Query allows you to directly query data from various sources like DynamoDB, RDS, Redshift, and S3 without physically moving the data. This eliminates data movement costs and simplifies the analysis process.\nReduced Cost for Large Datasets: Compared to copying data to S3, which can be expensive for large datasets, Athena Federated Query avoids unnecessary data movement, reducing overall costs."
      },
      {
        "date": "2024-03-20T20:26:00.000Z",
        "voteCount": 2,
        "content": "Amazon Athena Federated Query allows you to query data from multiple federated data sources including relational databases, NoSQL databases, and object stores directly from Athena. While this might seem like an efficient way to join data from different sources without the need for copying data into Amazon S3, it's essential to consider the cost implications.\n\nAWS documentation on Amazon Athena Federated Query [1] explains that while Federated Query enables you to query data from external data sources without data movement, it does not eliminate data transfer costs. Depending on the data sources involved (such as Amazon RDS, DynamoDB, etc.), there might be data transfer costs associated with querying data directly from these sources.\n\n[1] Amazon Athena Federated Query Documentation: https://docs.aws.amazon.com/athena/latest/ug/federated-data-sources.html"
      },
      {
        "date": "2024-03-20T20:25:00.000Z",
        "voteCount": 1,
        "content": "1. Data Storage Costs: Storing data in Amazon S3 is generally cheaper compared to the other AWS storage options like Amazon Redshift or Amazon RDS.\n\n2. Compute Costs: Amazon: Athena is a serverless query service that allows you to query data directly from S3 without the need for provisioning or managing infrastructure. You only pay for the queries you run, which can be more cost-effective compared to provisioning an EMR cluster (option A) or using Redshift Spectrum (option D), both of which involve compute resources that you might not fully utilize.\n\n3. Data Transfer Costs: Option B involves copying the data once into S3, and then there are no additional data transfer costs for querying the data using Athena. In contrast, options A and D would involve data transfer costs as data is moved between different services.\n\nAmazon Athena Pricing: https://aws.amazon.com/athena/pricing/ \nAmazon S3 Pricing: https://aws.amazon.com/s3/pricing/"
      },
      {
        "date": "2024-03-20T20:24:00.000Z",
        "voteCount": 1,
        "content": "Point:\n\"perform a one-time analysis job\"\n\nOption C (Amazon Athena Federated Query) might seem appealing, but it's generally more suited for querying data from external sources without copying the data into S3. However, since the data is already within AWS services, copying it to S3 and using Athena directly would likely be more cost-effective."
      },
      {
        "date": "2024-01-20T18:53:00.000Z",
        "voteCount": 4,
        "content": "You can query these sources by using Federated Queries, which is a native feature of Athena. The other options may increase costs and operational overhead, as they use more than one service to achieve the same result\n\nhttps://docs.aws.amazon.com/athena/latest/ug/connectors-available.html"
      },
      {
        "date": "2024-03-11T08:11:00.000Z",
        "voteCount": 2,
        "content": "Agree. C"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/131713-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is planning to use a provisioned Amazon EMR cluster that runs Apache Spark jobs to perform big data analysis. The company requires high reliability. A big data team must follow best practices for running cost-optimized and long-running workloads on Amazon EMR. The team must find a solution that will maintain the company's current level of performance.<br>Which combination of resources will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Hadoop Distributed File System (HDFS) as a persistent data store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 as a persistent data store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse x86-based instances for core nodes and task nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Graviton instances for core nodes and task nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spot Instances for all primary nodes."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T19:03:00.000Z",
        "voteCount": 6,
        "content": "HDFS is not recommended for persistent storage because once a cluster is terminated, all HDFS data is lost. Also, long-running workloads can fill the disk space quickly. Thus, S3 is the best option since it's highly available, durable, and scalable.\n\nAWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon\nEC2 instances: https://aws.amazon.com/ec2/graviton/"
      },
      {
        "date": "2024-02-11T03:17:00.000Z",
        "voteCount": 1,
        "content": "If you are using instance storage this is true, but you can use EBS instead of instance storage.\nEBS has better performance than s3 for HDFS. This is the keyword from question, so EBS &gt; S3\n\nI would rather select AD."
      },
      {
        "date": "2024-06-08T19:51:00.000Z",
        "voteCount": 3,
        "content": "s3 no question.\nGraviton=&gt; Cost-Effectiveness: Graviton instances are ARM-based instances specifically designed for cloud workloads.\nThey offer significant cost savings compared to x86-based instances while delivering comparable or better performance for many Apache Spark workloads.\nPerformance: Graviton instances are optimized for Spark workloads and can deliver the same level of performance as x86-based instances in many cases. Additionally, EMR offers performance-optimized versions of Spark built for Graviton instances."
      },
      {
        "date": "2024-04-13T12:15:00.000Z",
        "voteCount": 1,
        "content": "My answer is BE"
      },
      {
        "date": "2024-04-19T02:22:00.000Z",
        "voteCount": 3,
        "content": "E is incorrect, Spot instances does not provide high reliability as required by the company."
      },
      {
        "date": "2024-03-20T20:43:00.000Z",
        "voteCount": 2,
        "content": "A. - AWS recommends using Amazon S3 as a persistent data store for Amazon EMR due to its scalability, durability, and cost-effectiveness. Storing data in HDFS would require managing and maintaining additional infrastructure, which may incur higher costs in terms of storage, management, and scalability compared to using Amazon S3. AWS documentation emphasizes the benefits of integrating Amazon EMR with Amazon S3 for cost optimization and efficiency.\n\nD. - While Graviton instances may offer cost savings in certain scenarios, they might not always be the most cost-effective option depending on the specific workload requirements and availability of compatible software. x86-based instances are more commonly supported by a broader range of software and frameworks, which could result in better performance and compatibility in some cases. Additionally, AWS documentation on instance types and pricing can provide insights into the cost-effectiveness of Graviton instances compared to x86-based instances."
      },
      {
        "date": "2024-03-11T00:53:00.000Z",
        "voteCount": 3,
        "content": "B and D."
      },
      {
        "date": "2024-04-04T00:55:00.000Z",
        "voteCount": 1,
        "content": "yes BD is answer"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/133048-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company wants to implement real-time analytics capabilities. The company wants to use Amazon Kinesis Data Streams and Amazon Redshift to ingest and process streaming data at the rate of several gigabytes per second. The company wants to derive near real-time insights by using existing business intelligence (BI) and analytics tools.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Streams to stage data in Amazon S3. Use the COPY command to load data from Amazon S3 directly into Amazon Redshift to make the data immediately available for real-time analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess the data from Kinesis Data Streams by using SQL queries. Create materialized views directly on top of the stream. Refresh the materialized views regularly to query the most recent stream data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external schema in Amazon Redshift to map the data from Kinesis Data Streams to an Amazon Redshift object. Create a materialized view to read data from the stream. Set the materialized view to auto refresh.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Kinesis Data Streams to Amazon Kinesis Data Firehose. Use Kinesis Data Firehose to stage the data in Amazon S3. Use the COPY command to load the data from Amazon S3 to a table in Amazon Redshift."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-15T19:15:00.000Z",
        "voteCount": 6,
        "content": "Key word here is near real-time. If it's involve S3 and COPY, it's not gonna be near real-time"
      },
      {
        "date": "2024-09-14T11:16:00.000Z",
        "voteCount": 2,
        "content": "Redshift cannot create external schemas that map directly to Kinesis Data Streams. You would still need an intermediary step, such as Firehose or S3, to handle data ingestion. Additionally, maintaining auto-refreshing materialized views directly from a stream isn't feasible with Redshift."
      },
      {
        "date": "2024-09-14T11:17:00.000Z",
        "voteCount": 2,
        "content": "Here\u2019s why D is the best choice:\n\nKinesis Data Firehose is a fully managed service that automatically handles the ingestion of data from Kinesis Data Streams and stages it in S3, which significantly reduces operational overhead compared to managing custom data ingestion pipelines.\nS3 as a staging area: Using Amazon S3 as a staging location allows for flexible data management, high durability, and direct loading into Redshift without needing to manage complex buffering or data handling processes.\nCOPY command: The COPY command in Amazon Redshift is highly optimized for loading large datasets efficiently, making it a common and effective method to load bulk data from S3 into Redshift for near real-time analysis.\nFirehose to Redshift: Firehose can automatically buffer, batch, and transform data before loading it into Redshift, reducing manual intervention and ensuring data is readily available for real-time analytics."
      },
      {
        "date": "2024-09-06T20:22:00.000Z",
        "voteCount": 2,
        "content": "Option C has an issue: Redshift does not natively support direct querying or mapping of Kinesis Data Streams. D is the only correct option."
      },
      {
        "date": "2024-08-05T00:03:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2024-07-04T03:56:00.000Z",
        "voteCount": 1,
        "content": "Option A (using Kinesis Data Streams to stage data in Amazon S3 and loading it directly into Amazon Redshift) is the most straightforward and efficient approach. It minimizes operational overhead and ensures immediate availability of data for analysis.\nOptions B and C introduce additional complexity and may not provide the same level of efficiency"
      },
      {
        "date": "2024-05-06T21:40:00.000Z",
        "voteCount": 1,
        "content": "MVs in Redshift with auto refresh is the best option for near real time."
      },
      {
        "date": "2024-04-12T15:55:00.000Z",
        "voteCount": 2,
        "content": "Using materialized views with auto-refresh directly on a Redshift external schema of Kinesis Data Stream offers the most streamlined and efficient approach for near real-time insights using existing BI tools."
      },
      {
        "date": "2024-03-29T07:50:00.000Z",
        "voteCount": 4,
        "content": "The answer is C. It can provide near real-time insight analysis. Refer the article from AWS - https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/"
      },
      {
        "date": "2024-03-23T10:03:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html\n\nC is correct. (KDS -&gt; Redshift)\nD is wrong as it has more operational overhead (KDS -&gt; KDF -&gt; S3 -&gt; Redshift)"
      },
      {
        "date": "2024-03-21T06:58:00.000Z",
        "voteCount": 2,
        "content": "1. Amazon Kinesis Data Firehose: It's designed to reliably load streaming data into data lakes and data stores with minimal configuration and management overhead. It handles tasks like buffering, scaling, and delivering data to destinations like Amazon S3 and Amazon Redshift automatically.\n\n2. Amazon S3 as a staging area: Storing data in Amazon S3 provides a scalable and durable solution for data storage without needing to manage infrastructure. It also allows for easy integration with other AWS services and existing BI and analytics tools.\n\n3. Amazon Redshift: While Redshift requires some setup and management, loading data from Amazon S3 using the COPY command is a straightforward process. Once data is loaded into Redshift, existing BI and analytics tools can query the data directly, enabling near real-time insights.\n\n4. Minimal operational overhead: This solution minimizes operational overhead because much of the management tasks, such as scaling, buffering, and delivery of data, are handled by Amazon Kinesis Data Firehose. Additionally, using Amazon S3 as a staging area simplifies data storage and integration with other services."
      },
      {
        "date": "2024-03-20T20:57:00.000Z",
        "voteCount": 1,
        "content": "By considering the characteristics and capabilities of each AWS service and approach, along with insights from AWS documentation, it becomes evident that option D offers the most streamlined and operationally efficient solution for the scenario described.\n\nThis idea/concept is also straight out of the Amazon Solutions Architect course material."
      },
      {
        "date": "2024-03-20T20:56:00.000Z",
        "voteCount": 1,
        "content": "Point: \"Which solution will meet these requirements with the LEAST operational overhead?\"\n\nC.   - This approach involves creating an external schema in Amazon Redshift to map data from Kinesis Data Streams, which adds complexity compared to directly loading data from Amazon S3 using Amazon Kinesis Data Firehose.\n   - While materialized views with auto-refresh can provide near real-time insights, managing them and ensuring proper synchronization with the streaming data source may require more operational effort.\n   - AWS documentation for Amazon Redshift primarily focuses on traditional data loading methods and querying, with limited guidance on integrating with real-time data sources like Kinesis Data Streams."
      },
      {
        "date": "2024-03-11T01:00:00.000Z",
        "voteCount": 3,
        "content": "I think D. It could be C but because of \"LEAST operational overhead\" I will go with D."
      },
      {
        "date": "2024-02-06T04:18:00.000Z",
        "voteCount": 1,
        "content": "Both ChatGPT and I are thinking D is correct (100%)"
      },
      {
        "date": "2024-02-11T03:31:00.000Z",
        "voteCount": 1,
        "content": "I think this is true. \nI could not find any sources that AWS Kinesis Data Stream can stream data directly into s3 without a middle step with AWS Kinesis Data Firehose. \n\nThe AWS Kinesis Data Firehose is near real-time service.\n\nAnyway, I think the answer is D because the other 3 options are not better at all."
      },
      {
        "date": "2024-02-11T03:42:00.000Z",
        "voteCount": 4,
        "content": "However, after some investigation, I found out that Amazon Kinesis Data Streams provides a way to ingest stream data directly into an Amazon Redshift cluster.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n\nMaterialized views with auto-refresh enabled will continuously ingest new data from the stream as it arrives, keeping the view updated with the latest data in real time.\n\nSo I think the correct answer is C. \n\n\n\nThe COPY command also supports loading data from streaming sources like Kinesis Data Streams or Kinesis Data Firehose. When used with these services, COPY provides a way to ingest real-time streaming data into Redshift tables. \nBut this solution is not an option for this question."
      },
      {
        "date": "2024-07-11T18:52:00.000Z",
        "voteCount": 1,
        "content": "thank you, i was leaning towards D but this article helps"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/132734-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses an Amazon QuickSight dashboard to monitor usage of one of the company's applications. The company uses AWS Glue jobs to process data for the dashboard. The company stores the data in a single Amazon S3 bucket. The company adds new data every day.<br>A data engineer discovers that dashboard queries are becoming slower over time. The data engineer determines that the root cause of the slowing queries is long-running AWS Glue jobs.<br>Which actions should the data engineer take to improve the performance of the AWS Glue jobs? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data that is in the S3 bucket. Organize the data by year, month, and day.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the AWS Glue instance size by scaling up the worker type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the AWS Glue schema to the DynamicFrame schema class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust AWS Glue job scheduling frequency so the jobs run half as many times each day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM role that grants access to AWS glue to grant access to all S3 features."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T02:14:00.000Z",
        "voteCount": 10,
        "content": "A. Partition the data that is in the S3 bucket. Organize the data by year, month, and day.\n\n\t\u2022\tPartitioning data in Amazon S3 can significantly improve query performance. By organizing the data by year, month, and day, AWS Glue and Amazon QuickSight can scan only the relevant partitions of data, which reduces the amount of data read and processed. This approach is particularly effective for time-series data, where queries often target specific time ranges.\n\nB. Increase the AWS Glue instance size by scaling up the worker type.\n\n\t\u2022\tScaling up the worker type can provide more computational resources to the AWS Glue jobs, enabling them to process data faster. This can be especially beneficial when dealing with large datasets or complex transformations. It\u2019s important to monitor the performance improvements and cost implications of scaling up."
      },
      {
        "date": "2024-04-11T12:57:00.000Z",
        "voteCount": 1,
        "content": "I would also go for A, B.\nBut there are no worker types in AWS Glue. You can only increase the DPU."
      },
      {
        "date": "2024-05-27T23:15:00.000Z",
        "voteCount": 2,
        "content": "It looks like there are various worker types in AWS Glue actually. I'll go with AB as well. \n\n\"With AWS Glue, you only pay for the time your ETL job takes to run. There are no resources to manage, no upfront costs, and you are not charged for startup or shutdown time. You are charged an hourly rate based on the number of Data Processing Units (or DPUs) used to run your ETL job. A single Data Processing Unit (DPU) is also referred to as a worker. AWS Glue comes with three worker types to help you select the configuration that meets your job latency and cost requirements. Workers come in Standard, G.1X, G.2X, and G.025X configurations.\"\n\nhttps://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html"
      },
      {
        "date": "2024-05-03T05:50:00.000Z",
        "voteCount": 2,
        "content": "Here you can find 5 different Worker types:\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html"
      },
      {
        "date": "2024-03-20T21:06:00.000Z",
        "voteCount": 2,
        "content": "1. **Partition the Data in Amazon S3**: \n   - AWS documentation on optimizing Amazon S3 performance: https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\n   - AWS Glue documentation on partitioning data for AWS Glue jobs: https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html#how-partitioning-works\n   - Best practices for partitioning in Amazon S3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/best-practices-partitioning.html\n\n2. **Optimizing AWS Glue Job Settings**:\n   - AWS Glue documentation on optimizing job performance: https://docs.aws.amazon.com/glue/latest/dg/best-practices.html\n   - AWS Glue documentation on scaling AWS Glue job resources: https://docs.aws.amazon.com/glue/latest/dg/monitor-profile-glue-job-cloudwatch-metrics.html\n\nBy referring to these documentation resources, the data engineer can gain insights into best practices and recommendations provided by AWS for optimizing AWS Glue jobs, thereby justifying the suggested actions to address the issue of slowing job performance."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/132773-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to use AWS Step Functions to design an orchestration workflow. The workflow must parallel process a large collection of data files and apply a specific transformation to each file.<br>Which Step Functions state should the data engineer use to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParallel state",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoice state",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMap state\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWait state"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-28T19:40:00.000Z",
        "voteCount": 1,
        "content": "Clearly is mapping state"
      },
      {
        "date": "2024-06-08T20:11:00.000Z",
        "voteCount": 3,
        "content": "The Map state allows you to define a single execution path for processing a collection of data items in parallel.\nThis aligns perfectly with the data engineer's requirement of parallel processing a large collection of data files"
      },
      {
        "date": "2024-06-04T18:59:00.000Z",
        "voteCount": 1,
        "content": "to execute in parallel"
      },
      {
        "date": "2024-05-22T18:34:00.000Z",
        "voteCount": 3,
        "content": "C is Correct\nTo meet the requirement of parallel processing a large collection of data files and applying a specific transformation to each file, the data engineer should use the Map state in AWS Step Functions.\nThe Map state is specifically designed to run a set of tasks in parallel for each element in a collection or array. Each element (in this case, each data file) is processed independently and in parallel, allowing the workflow to take advantage of parallel processing."
      },
      {
        "date": "2024-03-31T19:09:00.000Z",
        "voteCount": 1,
        "content": "C, Map state is correct"
      },
      {
        "date": "2024-02-06T04:36:00.000Z",
        "voteCount": 1,
        "content": "With Step Functions, you can orchestrate large-scale parallel workloads to perform tasks, such as on-demand processing of semi-structured data. These parallel workloads let you concurrently process large-scale data sources stored in Amazon S3. For example, you might process a single JSON or CSV file that contains large amounts of data. Or you might process a large set of Amazon S3 objects.\n\nTo set up a large-scale parallel workload in your workflows, include a Map state in Distributed mode."
      },
      {
        "date": "2024-02-06T04:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\nMap state is designed precisely for the requirement described. It allows you to iterate over a collection of items, processing each item individually. The Map state can automatically manage the iteration and execute the specified transformation on each item in parallel, making it the perfect choice for parallel processing of a large collection of data files."
      },
      {
        "date": "2024-02-04T05:46:00.000Z",
        "voteCount": 2,
        "content": "The Map state is specifically designed for processing a collection of items (like data files) in parallel. It allows you to apply a transformation or a set of steps to each item in the input array independently.\nThe Map state automatically iterates over each item in the array and performs the defined steps. This makes it ideal for scenarios where you need to process a large number of files in a similar manner, as in your requirement."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/132774-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is migrating a legacy application to an Amazon S3 based data lake. A data engineer reviewed data that is associated with the legacy application. The data engineer found that the legacy data contained some duplicate information.<br>The data engineer must identify and remove duplicate information from the legacy application data.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom extract, transform, and load (ETL) job in Python. Use the DataFrame.drop_duplicates() function by importing the Pandas library to perform data deduplication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an AWS Glue extract, transform, and load (ETL) job. Use the FindMatches machine learning (ML) transform to transform the data to perform data deduplication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom extract, transform, and load (ETL) job in Python. Import the Python dedupe library. Use the dedupe library to perform data deduplication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an AWS Glue extract, transform, and load (ETL) job. Import the Python dedupe library. Use the dedupe library to perform data deduplication."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T05:56:00.000Z",
        "voteCount": 5,
        "content": "Option B, writing an AWS Glue ETL job with the FindMatches ML transform, is likely to meet the requirements with the least operational overhead. This solution leverages a managed service (AWS Glue) and incorporates a built-in ML transform specifically designed for deduplication, thus minimizing the need for manual setup, maintenance, and machine learning expertise."
      },
      {
        "date": "2024-08-04T23:39:00.000Z",
        "voteCount": 1,
        "content": "100 % B"
      },
      {
        "date": "2024-03-11T01:08:00.000Z",
        "voteCount": 4,
        "content": "B. https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html\n\"Find matches\nFinds duplicate records in the source data. You teach this machine learning transform by labeling example datasets to indicate which rows match. The machine learning transform learns which rows should be matches the more you teach it with example labeled data.\""
      },
      {
        "date": "2024-02-06T05:21:00.000Z",
        "voteCount": 1,
        "content": "Remove duplicates from already migrated data - probably D.\nRemove duplicates from data before migration - A is preferable."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/132737-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is building an analytics solution. The solution uses Amazon S3 for data lake storage and Amazon Redshift for a data warehouse. The company wants to use Amazon Redshift Spectrum to query the data that is in Amazon S3.<br>Which actions will provide the FASTEST queries? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gzip compression to compress individual files to sizes that are between 1 GB and 5 GB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a columnar storage file format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data based on the most common query predicates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the data into files that are less than 10 KB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse file formats that are not splittable."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-11T01:12:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html"
      },
      {
        "date": "2024-02-03T07:05:00.000Z",
        "voteCount": 5,
        "content": "B. Use a columnar storage file format: This is an excellent approach. Columnar storage formats like Parquet and ORC are highly recommended for use with Redshift Spectrum. They store data in columns, which allows Spectrum to scan only the needed columns for a query, significantly improving query performance and reducing the amount of data scanned.\n\nC. Partition the data based on the most common query predicates: Partitioning data in S3 based on commonly used query predicates (like date, region, etc.) allows Redshift Spectrum to skip large portions of data that are irrelevant to a particular query. This can lead to substantial performance improvements, especially for large datasets."
      },
      {
        "date": "2024-07-09T21:48:00.000Z",
        "voteCount": 1,
        "content": "Partioning helps filter the data and columnar storage is optimised for analytical (OLAP) queries"
      },
      {
        "date": "2024-06-09T09:34:00.000Z",
        "voteCount": 3,
        "content": "Redshift Spectrum is optimized for querying data stored in columnar formats like Parquet or ORC.\n These formats store each data column separately, allowing Redshift Spectrum to only scan the relevant columns for a specific query, significantly improving performance compared to row-oriented formats\nPartitioning organizes data files in S3 based on specific column values (e.g., date,\n region). When your queries filter or join data based on these partitioning columns (common query predicates), Redshift Spectrum can quickly locate the relevant data files, minimizing the amount of data scanned and accelerating query execution"
      },
      {
        "date": "2024-05-06T21:52:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/"
      },
      {
        "date": "2024-03-21T09:18:00.000Z",
        "voteCount": 1,
        "content": "2. **Partitioning**:\n   AWS documentation for Amazon Redshift Spectrum highlights the importance of partitioning data based on commonly used query predicates to improve query performance. By partitioning data, Redshift Spectrum can prune unnecessary partitions during query execution, reducing the amount of data scanned and improving overall query performance. This guidance can be found in the AWS documentation for Amazon Redshift Spectrum under \"Using Partitioning to Improve Query Performance\": https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum-partitioning.html"
      },
      {
        "date": "2024-03-21T09:18:00.000Z",
        "voteCount": 1,
        "content": "1. **Columnar Storage File Format**:\n   According to AWS documentation, columnar storage file formats like Apache Parquet and Apache ORC are recommended for optimizing query performance with Amazon Redshift Spectrum. They state that these formats are highly efficient for selective column reads, which aligns with the way analytical queries typically operate. This can be found in the AWS documentation for Amazon Redshift Spectrum under \"Choosing Data Formats\": https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#spectrum-columnar-storage"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/132738-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon RDS to store transactional data. The company runs an RDS DB instance in a private subnet. A developer wrote an AWS Lambda function with default settings to insert, update, or delete data in the DB instance.<br>The developer needs to give the Lambda function the ability to connect to the DB instance privately without using the public internet.<br>Which combination of steps will meet this requirement with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the public access setting for the DB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the security group of the DB instance to allow only Lambda function invocations on the database port.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Lambda function to run in the same subnet that the DB instance uses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the same security group to the Lambda function and the DB instance. Include a self-referencing rule that allows access through the database port.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the network ACL of the private subnet to include a self-referencing rule that allows access through the database port."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-28T19:40:00.000Z",
        "voteCount": 5,
        "content": "This solution only modifies the inbound rules of the security group of the DB instance, but it does not modify the outbound rules of the security group of the Lambda function. Additionally, this solution does not facilitate a private connection from the Lambda function to the DB instance, hence, the Lambda function would still need to use the public internet to access the DB instance. Therefore, this option does not fulfill the requirements."
      },
      {
        "date": "2024-10-07T02:55:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D \nC is wrong\nWhile you want the Lambda function to access the RDS instance privately, it does not need to run in the same subnet. As long as both are in the same VPC, the Lambda function can connect."
      },
      {
        "date": "2024-06-01T23:56:00.000Z",
        "voteCount": 2,
        "content": "I will go with C and D on this one, because in my opinion B is not correctly phrased. \n\n The correct way to phrase it would be something like:\n\nUpdate the security group of the RDS instance to allow inbound traffic on the database port (3306) only from the security group associated with the Lambda function."
      },
      {
        "date": "2024-05-05T02:18:00.000Z",
        "voteCount": 1,
        "content": "While placing the Lambda function in the same subnet as the DB instance would technically allow them to communicate privately within the same network, it introduces additional complexity and operational overhead. Lambda functions typically run in AWS-managed VPCs, and configuring them to run in a specific subnet might require manual intervention and ongoing maintenance."
      },
      {
        "date": "2024-05-05T02:19:00.000Z",
        "voteCount": 1,
        "content": "Moreover, running a Lambda function within a subnet does not inherently ensure private connectivity to the RDS instance. Additional networking configurations would still be needed to allow the Lambda function to access the RDS instance securely, such as configuring the appropriate security groups and potentially adjusting network ACLs.\nHence C can't be the answer"
      },
      {
        "date": "2024-04-28T18:23:00.000Z",
        "voteCount": 2,
        "content": "bbb ddd"
      },
      {
        "date": "2024-04-03T07:36:00.000Z",
        "voteCount": 1,
        "content": "I would go with CD, since it's less operational effort, in my opinion"
      },
      {
        "date": "2024-03-30T19:04:00.000Z",
        "voteCount": 3,
        "content": "B: need update security group. and there there may be other application need to access db except for lambda function\nD: it works and reuse security group which has less operational overhead"
      },
      {
        "date": "2024-03-30T08:19:00.000Z",
        "voteCount": 2,
        "content": "A is not an option as it exposes the data to public\nB is not an option as we don't want the lambda to be the only entity accessing the db, there can be many other apps. doing this is not scalable"
      },
      {
        "date": "2024-03-21T09:58:00.000Z",
        "voteCount": 1,
        "content": "B. - While updating the security group of the DB instance to allow only Lambda function invocations on the database port may seem like a viable solution, it's not the most efficient approach. This option overlooks the need for the Lambda function to be able to communicate securely with the DB instance within the same VPC/subnet.\n   - Reference: [Amazon RDS documentation on security groups](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html)"
      },
      {
        "date": "2024-03-21T09:50:00.000Z",
        "voteCount": 1,
        "content": "- AWS Lambda supports VPC configurations, allowing you to run Lambda functions within your own VPC. This enables private connectivity between Lambda functions and resources within the VPC, such as RDS DB instances.\nReference AWS Lambda documentation on VPC configurations: [AWS Lambda VPC Settings]https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html \n\n- AWS security groups provide a flexible and scalable way to control traffic to your instances or resources. By attaching the same security group to both the Lambda function and the RDS DB instance, you can ensure they share the same set of rules for inbound and outbound traffic.\n- Self-referencing rules within security groups enable instances within the same security group to communicate with each other over specified ports.\n- Reference AWS documentation on security groups and self-referencing rules: [Security Groups for Your VPC]https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
      },
      {
        "date": "2024-03-21T09:46:00.000Z",
        "voteCount": 4,
        "content": "So, there coudl be a justified argument for the following:\n\nC. Configure the Lambda function to run in the same subnet that the DB instance uses:\nBy running the Lambda function in the same subnet as the RDS DB instance, you enable them to communicate privately within the same network, eliminating the need for public internet access and reducing operational overhead.\n\nD. Attach the same security group to the Lambda function and the DB instance. Include a self-referencing rule that allows access through the database port:\nBy attaching the same security group to both the Lambda function and the RDS DB instance, and including a self-referencing rule that allows access through the database port, you ensure secure communication between them within the same VPC without exposing the database to the public internet. This approach minimizes operational overhead by centralizing security management and simplifying access control."
      },
      {
        "date": "2024-03-21T09:29:00.000Z",
        "voteCount": 1,
        "content": "Here's how you would implement this:\n\n1. **Attach the same security group to both the Lambda function and the RDS DB instance**: Ensure that both resources are associated with the same security group.\n\n2. **Create an inbound rule in the security group**: Configure the security group to allow inbound traffic on the database port (e.g., 3306 for MySQL) from the security group itself.\n\nFor example, if the security group ID is sg-1234567890 and the database port is 3306, the inbound rule would look something like this:\n\nType: Custom TCP Rule\nProtocol: TCP\nPort Range: 3306 (or the port your database uses)\nSource: sg-1234567890 (the security group ID itself)\n\n\nThis rule allows the Lambda function, which is also part of the same security group, to communicate with the RDS DB instance through the specified port. It effectively creates a loopback or self-referencing rule within the security group, allowing internal communication between resources while maintaining security boundaries."
      },
      {
        "date": "2024-03-21T09:28:00.000Z",
        "voteCount": 2,
        "content": "The phrase \"Include a self-referencing rule that allows access through the database port\" refers to configuring the security group associated with the resources (in this case, the Lambda function and the RDS DB instance) to allow inbound traffic from the resources themselves on a specific port, typically the port used for database communication.\n\nIn AWS security groups, a self-referencing rule means allowing traffic from the security group itself. This setup is often used to facilitate communication between resources within the same security group or VPC without needing to specify individual IP addresses."
      },
      {
        "date": "2024-08-19T16:51:00.000Z",
        "voteCount": 1,
        "content": "Thank you. You always help me solve my problems."
      },
      {
        "date": "2024-03-11T01:20:00.000Z",
        "voteCount": 1,
        "content": "When you want Lambda to \"privately\" connect to a resource (RDS in this case) that sits inside a VPC, then you deploy Lambda inside VPC. = C\nThen you attach a proper IAM role to lambda. \nThen, to be more secure you open the RDS security group only on the specific port:\nMySQL/Aurora MySQL: 3306\nSQL Server: 1433\nPostgreSQL: 5432\nOracle: 1521"
      },
      {
        "date": "2024-02-11T08:12:00.000Z",
        "voteCount": 1,
        "content": "what does \"Include a self-referencing rule that allows access through the database port.\" mean?"
      },
      {
        "date": "2024-02-03T07:08:00.000Z",
        "voteCount": 1,
        "content": "B. Update the security group of the DB instance to allow only Lambda function invocations on the database port: Modifying the security group of the RDS instance to allow incoming connections on the database port (e.g., port 3306 for MySQL, 5432 for PostgreSQL) from the Lambda function is a crucial step. This ensures that the RDS instance can accept connections from the Lambda function.\n\nC. Configure the Lambda function to run in the same subnet that the DB instance uses: Placing the Lambda function in the same VPC and subnet as the RDS instance ensures private connectivity. AWS Lambda needs to be configured with a VPC configuration that includes the subnet(s) and security group(s) that allow access to the RDS instance."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/132630-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has a frontend ReactJS website that uses Amazon API Gateway to invoke REST APIs. The APIs perform the functionality of the website. A data engineer needs to write a Python script that can be occasionally invoked through API Gateway. The code must return results to API Gateway.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a custom Python script on an Amazon Elastic Container Service (Amazon ECS) cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda Python function with provisioned concurrency.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a custom Python script that can integrate with API Gateway on Amazon Elastic Kubernetes Service (Amazon EKS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function. Ensure that the function is warm by scheduling an Amazon EventBridge rule to invoke the Lambda function every 5 minutes by using mock events."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T09:49:00.000Z",
        "voteCount": 3,
        "content": "B and D are both ok. Still, since it says LEAST operational overhead, then keep it simple. B then."
      },
      {
        "date": "2024-04-30T03:23:00.000Z",
        "voteCount": 2,
        "content": "B - simple and clear"
      },
      {
        "date": "2024-04-01T05:01:00.000Z",
        "voteCount": 1,
        "content": "I would go in B"
      },
      {
        "date": "2024-03-11T01:30:00.000Z",
        "voteCount": 1,
        "content": "Although D seems a good choice but the questions asks for \"LEAST operational overhead\" will result in B"
      },
      {
        "date": "2024-03-01T13:18:00.000Z",
        "voteCount": 1,
        "content": "Answ. B\nYou can create a web API with an HTTP endpoint for your Lambda function by using Amazon API Gateway. API Gateway provides tools for creating and documenting web APIs that route HTTP requests to Lambda functions. You can secure access to your API with authentication and authorization controls. Your APIs can serve traffic over the internet or can be accessible only within your VPC.\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html"
      },
      {
        "date": "2024-01-31T23:31:00.000Z",
        "voteCount": 2,
        "content": "B.\nAWS Lambda functions can be easily integrated with Amazon API Gateway to create RESTful APIs. This integration allows API Gateway to directly invoke the Lambda function when the API endpoint is hit."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/133056-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has a production AWS account that runs company workloads. The company's security team created a security AWS account to store and analyze security logs from the production AWS account. The security logs in the production AWS account are stored in Amazon CloudWatch Logs.<br>The company needs to use Amazon Kinesis Data Streams to deliver the security logs to the security AWS account.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination data stream in the production AWS account. In the security AWS account, create an IAM role that has cross-account permissions to Kinesis Data Streams in the production AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination data stream in the security AWS account. Create an IAM role and a trust policy to grant CloudWatch Logs the permission to put data into the stream. Create a subscription filter in the security AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination data stream in the production AWS account. In the production AWS account, create an IAM role that has cross-account permissions to Kinesis Data Streams in the security AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a destination data stream in the security AWS account. Create an IAM role and a trust policy to grant CloudWatch Logs the permission to put data into the stream. Create a subscription filter in the production AWS account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T16:15:00.000Z",
        "voteCount": 3,
        "content": "Cross-Account Delivery: Kinesis Data Streams in the security account ensures the logs reside in the designated security-focused environment.\nCloudWatch Logs Integration: Granting CloudWatch Logs permissions to put records into the Kinesis Data Stream directly establishes a streamlined and secure data flow from the production account.\nFiltering Controls: The subscription filter in the production account provides precise control over which log events are sent to the security account."
      },
      {
        "date": "2024-03-21T11:26:00.000Z",
        "voteCount": 1,
        "content": "1. **Cross-Account Access:**\n   - AWS Documentation: [Cross-Account Access]\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html  \n   - This documentation provides detailed instructions on how to set up cross-account access using IAM roles and trust policies, which is essential for allowing CloudWatch Logs in one AWS account to put data into a Kinesis Data Stream in another AWS account.\n\n2. **Configuring CloudWatch Logs Subscription Filters:**\n   - AWS Documentation: [Subscription Filters for Amazon CloudWatch Logs]\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html \n   - This documentation explains how to create subscription filters for CloudWatch Logs, which enable you to route log data to various destinations, including Kinesis Data Streams. Placing the subscription filter in the production AWS account ensures that only the relevant security logs are sent to the Kinesis Data Stream in the security AWS account."
      },
      {
        "date": "2024-03-11T01:36:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions-Kinesis.html"
      },
      {
        "date": "2024-02-06T05:24:00.000Z",
        "voteCount": 1,
        "content": "Both ChatGPT and me agree with anser D"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/131705-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon S3 to store semi-structured data in a transactional data lake. Some of the data files are small, but other data files are tens of terabytes.<br>A data engineer must perform a change data capture (CDC) operation to identify changed data from the data source. The data source sends a full snapshot as a JSON file every day and ingests the changed data into the data lake.<br>Which solution will capture the changed data MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to identify the changes between the previous data and the current data. Configure the Lambda function to ingest the changes into the data lake.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Amazon RDS for MySQL. Use AWS Database Migration Service (AWS DMS) to write the changed data to the data lake.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an open source data lake format to merge the data source with the S3 data lake to insert the new data and update the existing data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into an Amazon Aurora MySQL DB instance that runs Aurora Serverless. Use AWS Database Migration Service (AWS DMS) to write the changed data to the data lake."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T00:36:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/choosing-an-open-table-format-for-your-transactional-data-lake-on-aws/"
      },
      {
        "date": "2024-05-26T06:50:00.000Z",
        "voteCount": 2,
        "content": "Ill go with Delta or something like that. is C"
      },
      {
        "date": "2024-03-21T19:55:00.000Z",
        "voteCount": 2,
        "content": "Relative to cost, here are docs for the reason for option C:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html \nhttps://aws.amazon.com/blogs/big-data/ \nhttps://docs.aws.amazon.com/glue/latest/dg/welcome.html \nhttps://docs.aws.amazon.com/emr/ \n\nHere are docs for reasons the others are not correct:\nhttps://aws.amazon.com/lambda/pricing/ \nhttps://aws.amazon.com/rds/pricing/\nhttps://aws.amazon.com/dms/pricing/"
      },
      {
        "date": "2024-03-11T01:42:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/big-data/implement-a-cdc-based-upsert-in-a-data-lake-using-apache-iceberg-and-aws-glue/"
      },
      {
        "date": "2024-03-01T13:25:00.000Z",
        "voteCount": 1,
        "content": "Answ. D\nYou can migrate data from any MySQL-compatible database (MySQL, MariaDB, or Amazon Aurora MySQL) using AWS Database Migration Service. \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html"
      },
      {
        "date": "2024-03-11T01:41:00.000Z",
        "voteCount": 6,
        "content": "\"other data files are tens of terabytes\" - good luck with DMS on that :) I think it's C"
      },
      {
        "date": "2024-01-20T18:13:00.000Z",
        "voteCount": 4,
        "content": "This is a tricky one. Although option A seems like the best choice since it uses an AWS service, I believe using Delta/Iceberg APIs would be easier than writing custom code on Lambda"
      },
      {
        "date": "2024-02-13T05:39:00.000Z",
        "voteCount": 4,
        "content": "If all files were small I believe it would be a great idea. However, you wouldn't be able to compare heavy files with lambda due to its memory/capacity and runtime constraints"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/131708-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer runs Amazon Athena queries on data that is in an Amazon S3 bucket. The Athena queries use AWS Glue Data Catalog as a metadata table.<br>The data engineer notices that the Athena query plans are experiencing a performance bottleneck. The data engineer determines that the cause of the performance bottleneck is the large number of partitions that are in the S3 bucket. The data engineer must resolve the performance bottleneck and reduce Athena query planning time.<br>Which solutions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue partition index. Enable partition filtering.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBucket the data based on a column that the data have in common in a WHERE clause of the user query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Athena partition projection based on the S3 bucket prefix.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform the data that is in the S3 bucket to Apache Parquet format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon EMR S3DistCP utility to combine smaller objects in the S3 bucket into larger objects."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-31T23:55:00.000Z",
        "voteCount": 5,
        "content": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nOptimizing Partition Processing using partition projection\nProcessing partition information can be a bottleneck for Athena queries when you have a very large number of partitions and aren\u2019t using AWS Glue partition indexing. You can use partition projection in Athena to speed up query processing of highly partitioned tables and automate partition management. Partition projection helps minimize this overhead by allowing you to query partitions by calculating partition information rather than retrieving it from a metastore. It eliminates the need to add partitions\u2019 metadata to the AWS Glue table."
      },
      {
        "date": "2024-06-09T17:13:00.000Z",
        "voteCount": 1,
        "content": "Creating an AWS Glue partition index and enabling partition filtering can significantly improve query performance when dealing with large datasets with many partitions. The partition index allows Athena to quickly identify the relevant partitions for a query, reducing the time spent scanning unnecessary data. Partition filtering further optimizes the query by only scanning the partitions that match the filter conditions.\nAthena partition projection based on the S3 bucket prefix is another effective technique to improve query performance. By leveraging the bucket prefix structure, Athena can prune partitions that are not relevant to the query, reducing the amount of data that needs to be scanned and processed. This approach is particularly useful when the data is organized in a hierarchical structure within the S3 bucket."
      },
      {
        "date": "2024-05-19T05:40:00.000Z",
        "voteCount": 1,
        "content": "D is not correct because the issue is related to partitioning."
      },
      {
        "date": "2024-04-30T21:51:00.000Z",
        "voteCount": 1,
        "content": "I guess A / C, beucase we faced with - query plans performance bottleneck, so indexing should be improved"
      },
      {
        "date": "2024-04-27T03:20:00.000Z",
        "voteCount": 2,
        "content": "A. Creating an AWS Glue partition index and enabling partition filtering can help improve query performance by allowing Athena to prune unnecessary partitions from the query plan. This can reduce the number of partitions that need to be scanned, resulting in faster query planning times.\n\nC. Athena partition projection allows you to define a partition scheme based on the S3 bucket prefix. This can help reduce the number of partitions that need to be scanned, as Athena can use the prefix to determine which partitions are relevant to the query. This can also help improve query performance and reduce planning times."
      },
      {
        "date": "2024-04-13T12:30:00.000Z",
        "voteCount": 1,
        "content": "The right answer is BD"
      },
      {
        "date": "2024-04-12T16:27:00.000Z",
        "voteCount": 3,
        "content": "A. Create an AWS Glue partition index. Enable partition filtering.\nTargeted Optimization: Partition indexes within the Glue Data Catalog help Athena efficiently identify the relevant partitions, significantly reducing query planning time. Partition filtering further refines the search during query execution.\nD. Transform the data that is in the S3 bucket to Apache Parquet format.\nEfficient Columnar Format: Parquet's columnar storage and built-in metadata often allow Athena to skip over large portions of data irrelevant to the query, leading to faster query planning and execution."
      },
      {
        "date": "2024-03-23T22:18:00.000Z",
        "voteCount": 3,
        "content": "Keyword: Athena query planning time\n\nSee explanation in the link:\nhttps://www.myexamcollection.com/Data-Engineer-Associate-vce-questions.htm\n\nB &amp; D are related to analytical queries performance, not about \"query planning\" performance."
      },
      {
        "date": "2024-03-14T06:45:00.000Z",
        "voteCount": 2,
        "content": "Just finished the exam and I went with AD. I agree with GiorgioGss, but the reason why I picked A over C was becaues the table is already using Glue catalog.\nIf we use the indexes, there's no reason to use C as we already have the partitions indexed. \nNo reason to pick B if we have C selected.\nThus I picked D with this to optimize the query e.g. if I'm only selecting a subset of the columns."
      },
      {
        "date": "2024-03-11T08:17:00.000Z",
        "voteCount": 1,
        "content": "Strange questions.... it can be ABCD"
      },
      {
        "date": "2024-01-31T23:56:00.000Z",
        "voteCount": 1,
        "content": "If your table stored in an AWS Glue Data Catalog has tens and hundreds of thousands and millions of partitions, you can enable partition indexes on the table. With partition indexes, only the metadata for the partition value in the query\u2019s filter is retrieved from the catalog instead of retrieving all the partitions\u2019 metadata. The result is faster queries for such highly partitioned tables. The following table compares query runtimes between a partitioned table with no partition indexing and with partition indexing. The table contains approximately 100,000 partitions and uncompressed text data. The orders table is partitioned by the o_custkey column."
      },
      {
        "date": "2024-01-20T18:25:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/132739-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer must manage the ingestion of real-time streaming data into AWS. The data engineer wants to perform real-time analytics on the incoming streaming data by using time-based aggregations over a window of up to 30 minutes. The data engineer needs a solution that is highly fault tolerant.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function that includes both the business and the analytics logic to perform time-based aggregations over a window of up to 30 minutes for the data in Amazon Kinesis Data Streams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to analyze the data that might occasionally contain duplicates by using multiple types of aggregations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function that includes both the business and the analytics logic to perform aggregations for a tumbling window of up to 30 minutes, based on the event timestamp.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to analyze the data by using multiple types of aggregations to perform time-based analytics over a window of up to 30 minutes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T07:40:00.000Z",
        "voteCount": 5,
        "content": "D. Amazon Managed Service for Apache Flink for Time-Based Analytics over 30 Minutes: This option correctly identifies the use of Amazon Managed Service for Apache Flink for performing time-based analytics over a window of up to 30 minutes. Apache Flink is adept at handling such scenarios, providing capabilities for complex event processing, time-windowed aggregations, and maintaining state over time. This option would offer high fault tolerance and minimal operational overhead due to the managed nature of the service."
      },
      {
        "date": "2024-07-26T18:48:00.000Z",
        "voteCount": 1,
        "content": "This link is not AWS documents but I think you guys can take a look.\nhttps://amandeep-singh-johar.medium.com/real-time-stream-processing-with-apache-flink-153992840f16"
      },
      {
        "date": "2024-05-16T07:12:00.000Z",
        "voteCount": 2,
        "content": "Show the Docs"
      },
      {
        "date": "2024-05-05T23:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/managed-flink/latest/java/how-operators.html#how-operators-agg"
      },
      {
        "date": "2024-03-30T09:23:00.000Z",
        "voteCount": 2,
        "content": "this is crazy, the answers by bot are wrong, please don't rely on them. please care to open discussions and look for reasoning"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/132762-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is planning to upgrade its Amazon Elastic Block Store (Amazon EBS) General Purpose SSD storage from gp2 to gp3. The company wants to prevent any interruptions in its Amazon EC2 instances that will cause data loss during the migration to the upgraded storage.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate snapshots of the gp2 volumes. Create new gp3 volumes from the snapshots. Attach the new gp3 volumes to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new gp3 volumes. Gradually transfer the data to the new gp3 volumes. When the transfer is complete, mount the new gp3 volumes to the EC2 instances to replace the gp2 volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the volume type of the existing gp2 volumes to gp3. Enter new values for volume size, IOPS, and throughput.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS DataSync to create new gp3 volumes. Transfer the data from the original gp2 volumes to the new gp3 volumes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-23T22:24:00.000Z",
        "voteCount": 6,
        "content": "Option C: Check section under \"To modify an Amazon EBS volume using the AWS Management Console\u201c in GiorgioGss's link\nAmazon EBS Elastic Volumes enable you to modify your volume type from gp2 to gp3 without detaching volumes or restarting instances (requirements for modification), which means that there are no interruptions to your applications during modification."
      },
      {
        "date": "2024-03-19T00:47:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/"
      },
      {
        "date": "2024-02-03T21:51:00.000Z",
        "voteCount": 2,
        "content": "Option C is the most straightforward and efficient approach to upgrading from gp2 to gp3 EBS volumes, providing an in-place upgrade path with minimal operational overhead and no interruption in service."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/132742-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is migrating its database servers from Amazon EC2 instances that run Microsoft SQL Server to Amazon RDS for Microsoft SQL Server DB instances. The company's analytics team must export large data elements every day until the migration is complete. The data elements are the result of SQL joins across multiple tables. The data must be in Apache Parquet format. The analytics team must store the data in Amazon S3.<br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule SQL Server Agent to run a daily SQL query that selects the desired data elements from the EC2 instance-based SQL Server databases. Configure the query to direct the output .csv objects to an S3 bucket. Create an S3 event that invokes an AWS Lambda function to transform the output format from .csv to Parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T21:33:00.000Z",
        "voteCount": 6,
        "content": "Leveraging SQL Views: Creating a view on the source database simplifies the data extraction process and keeps your SQL logic centralized.\nGlue Crawler Efficiency: Using a Glue crawler to automatically discover and catalog the view's metadata reduces manual setup.\nGlue Job for ETL: A dedicated Glue job is well-suited for the data transformation (to Parquet) and loading into S3. Glue jobs offer built-in scheduling capabilities.\nOperational Efficiency: This approach minimizes custom code and leverages native AWS services for data movement and cataloging."
      },
      {
        "date": "2024-08-25T09:43:00.000Z",
        "voteCount": 1,
        "content": "Glue crawler is used to catalog and find the schema. In this requirement the data was already stored in MS SQL server which a relational database. Hence I think A is correct"
      },
      {
        "date": "2024-07-01T07:08:00.000Z",
        "voteCount": 1,
        "content": "Option A involves creating a view in the EC2 instance-based SQL Server databases that contains the required data elements. An AWS Glue job is then created to select the data directly from the view and transfer the data in Parquet format to an S3 bucket. This job is scheduled to run every day. This approach is operationally efficient as it leverages managed services (AWS Glue) and does not require additional transformation steps.\n\nOption D involves creating an AWS Lambda function that queries the EC2 instance-based databases using JDBC. The Lambda function is configured to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. This approach could work, but managing and scheduling Lambda functions could add operational overhead compared to using managed services like AWS Glue."
      },
      {
        "date": "2024-03-19T00:56:00.000Z",
        "voteCount": 2,
        "content": "Just beacuse it decouples the whole architecture I will go with C"
      },
      {
        "date": "2024-03-18T18:35:00.000Z",
        "voteCount": 4,
        "content": "Choice A) is almost the same approach, but it doesn't use the AWS Glue crawler, so have to manage the view's metadata manually."
      },
      {
        "date": "2024-03-01T21:18:00.000Z",
        "voteCount": 1,
        "content": "Option C seems to be the most operationally efficient:\nIt leverages Glue for both schema discovery (via the crawler) and data transfer (via the Glue job).\nThe Glue job can directly handle the Parquet format conversion.\nScheduling the Glue job ensures regular data export without manual intervention."
      },
      {
        "date": "2024-03-15T20:10:00.000Z",
        "voteCount": 1,
        "content": "you're right: https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/"
      },
      {
        "date": "2024-03-18T18:28:00.000Z",
        "voteCount": 1,
        "content": "Is this right?\nhttps://aws.amazon.com/jp/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/"
      },
      {
        "date": "2024-02-04T06:18:00.000Z",
        "voteCount": 1,
        "content": "Option A (Creating a view in the EC2 instance-based SQL Server databases and creating an AWS Glue job that selects data from the view, transfers it in Parquet format to S3, and schedules the job to run every day) seems to be the most operationally efficient solution. It leverages AWS Glue\u2019s ETL capabilities for direct data extraction and transformation, minimizes manual steps, and effectively automates the process."
      },
      {
        "date": "2024-02-03T08:48:00.000Z",
        "voteCount": 1,
        "content": "A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day. This solution is operationally efficient for exporting data in the required format."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/132660-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineering team is using an Amazon Redshift data warehouse for operational reporting. The team wants to prevent performance issues that might result from long- running queries. A data engineer must choose a system table in Amazon Redshift to record anomalies when a query optimizer identifies conditions that might indicate performance issues.<br>Which table views should the data engineer use to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSTL_USAGE_CONTROL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSTL_ALERT_EVENT_LOG\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSTL_QUERY_METRICS",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSTL_PLAN_INFO"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T17:29:00.000Z",
        "voteCount": 1,
        "content": "this table records alerts that are generated by the Amazon Redshift system when it detects certain conditions that might indicate performance issues. These alerts are triggered by the query optimizer when it detects suboptimal query plans or other issues that could affect performance."
      },
      {
        "date": "2024-03-19T00:58:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html"
      },
      {
        "date": "2024-03-01T21:22:00.000Z",
        "voteCount": 4,
        "content": "B\nSTL_ALERT_EVENT_LOG records any alerts/notifications related to queries or user-defined performance thresholds. This would capture optimizer alerts about potential performance issues.\n\nSTL_PLAN_INFO provides detailed info on execution plans. The optimizer statistics and warnings provide insight into problematic query plans.\n\nSTL_USAGE_CONTROL limits user activity but does not log anomalies.\n\nSTL_QUERY_METRICS has execution stats but no plan diagnostics.\n\nBy enabling alerts and checking STL_ALERT_EVENT_LOG and STL_PLAN_INFO, the data engineer can best detect and troubleshoot queries flagged by the optimizer as problematic before they impair performance. This meets the requirement to catch potential long running queries."
      },
      {
        "date": "2024-02-01T20:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/cm_chap_system-tables.html\nSTL_ALERT_EVENT_LOG table view to meet this requirement. This system table in Amazon Redshift is designed to record anomalies when a query optimizer identifies conditions that might indicate performance issues"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/132349-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer must ingest a source of structured data that is in .csv format into an Amazon S3 data lake. The .csv files contain 15 columns. Data analysts need to run Amazon Athena queries on one or two columns of the dataset. The data analysts rarely query the entire file.<br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue PySpark job to ingest the source data into the data lake in .csv format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue extract, transform, and load (ETL) job to read from the .csv structured data source. Configure the job to ingest the data into the data lake in JSON format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue PySpark job to ingest the source data into the data lake in Apache Avro format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue extract, transform, and load (ETL) job to read from the .csv structured data source. Configure the job to write the data into the data lake in Apache Parquet format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T17:34:00.000Z",
        "voteCount": 2,
        "content": "Athena is optimized for querying data stored in Parquet format. It can efficiently scan only the necessary columns for a specific query,\nreducing the amount of data processed. This translates to faster query execution times and lower query costs for data analysts who primarily focus on one or two columns"
      },
      {
        "date": "2024-05-26T07:02:00.000Z",
        "voteCount": 2,
        "content": "Cost effectively, and they are going to use only one or two columns, columnar."
      },
      {
        "date": "2024-03-19T01:00:00.000Z",
        "voteCount": 3,
        "content": "MOST cost-effectively = parquet"
      },
      {
        "date": "2024-01-28T13:39:00.000Z",
        "voteCount": 2,
        "content": "Glue +  Parquet for cost efectiveness"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/132348-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has five offices in different AWS Regions. Each office has its own human resources (HR) department that uses a unique IAM role. The company stores employee records in a data lake that is based on Amazon S3 storage.<br>A data engineering team needs to limit access to the records. Each HR department should be able to access records for only employees who are within the HR department's Region.<br>Which combination of steps should the data engineering team take to meet this requirement with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse data filters for each Region to register the S3 paths as data locations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the S3 path as an AWS Lake Formation location.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM roles of the HR departments to add a data filter for each department's Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable fine-grained access control in AWS Lake Formation. Add a data filter for each Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate S3 bucket for each Region. Configure an IAM policy to allow S3 access. Restrict access based on Region."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T17:47:00.000Z",
        "voteCount": 3,
        "content": "Registering the S3 path as an AWS Lake Formation location is the first step in leveraging Lake Formation's data governance and access control capabilities. This allows the data engineering team to centrally manage and govern the data stored in the S3 data lake.\nEnabling fine-grained access control in AWS Lake Formation and adding a data filter for each Region is the key step to achieve the desired access control. Data filters in Lake Formation allow you to define row-level and column-level access policies based on specific conditions or attributes, such as the Region in this case"
      },
      {
        "date": "2024-02-01T21:06:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/access-control-fine-grained.html"
      },
      {
        "date": "2024-01-31T23:55:00.000Z",
        "voteCount": 1,
        "content": "If your table stored in an AWS Glue Data Catalog has tens and hundreds of thousands and millions of partitions, you can enable partition indexes on the table. With partition indexes, only the metadata for the partition value in the query\u2019s filter is retrieved from the catalog instead of retrieving all the partitions\u2019 metadata. The result is faster queries for such highly partitioned tables. The following table compares query runtimes between a partitioned table with no partition indexing and with partition indexing. The table contains approximately 100,000 partitions and uncompressed text data. The orders table is partitioned by the o_custkey column."
      },
      {
        "date": "2024-01-28T13:38:00.000Z",
        "voteCount": 1,
        "content": "BD makes sense"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/132353-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses AWS Step Functions to orchestrate a data pipeline. The pipeline consists of Amazon EMR jobs that ingest data from data sources and store the data in an Amazon S3 bucket. The pipeline also includes EMR jobs that load the data to Amazon Redshift.<br>The company's cloud infrastructure team manually built a Step Functions state machine. The cloud infrastructure team launched an EMR cluster into a VPC to support the EMR jobs. However, the deployed Step Functions state machine is not able to run the EMR jobs.<br>Which combination of steps should the company take to identify the reason the Step Functions state machine is not able to run the EMR jobs? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to automate the Step Functions state machine deployment. Create a step to pause the state machine during the EMR jobs that fail. Configure the step to wait for a human user to send approval through an email message. Include details of the EMR task in the email message for further analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the Step Functions state machine code has all IAM permissions that are necessary to create and run the EMR jobs. Verify that the Step Functions state machine code also includes IAM permissions to access the Amazon S3 buckets that the EMR jobs use. Use Access Analyzer for S3 to check the S3 access properties.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck for entries in Amazon CloudWatch for the newly created EMR cluster. Change the AWS Step Functions state machine code to use Amazon EMR on EKS. Change the IAM access policies and the security group configuration for the Step Functions state machine code to reflect inclusion of Amazon Elastic Kubernetes Service (Amazon EKS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the flow logs for the VPC. Determine whether the traffic that originates from the EMR cluster can successfully reach the data providers. Determine whether any security group that might be attached to the Amazon EMR cluster allows connections to the data source servers on the informed ports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the retry scenarios that the company configured for the EMR jobs. Increase the number of seconds in the interval between each EMR task. Validate that each fallback state has the appropriate catch for each decision state. Configure an Amazon Simple Notification Service (Amazon SNS) topic to store the error messages."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-03T08:26:00.000Z",
        "voteCount": 2,
        "content": "I'd go in BD"
      },
      {
        "date": "2024-03-19T01:09:00.000Z",
        "voteCount": 3,
        "content": "Permissions of course and we need to see if the traffic is blocked at any hops because they mention that EMR is IN vpc so... flow-logs"
      },
      {
        "date": "2024-03-14T06:50:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D. \nE is not an option to identify the failure reason."
      },
      {
        "date": "2024-02-01T21:31:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/step-functions/latest/dg/procedure-create-iam-role.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/service-integration-iam-templates.html"
      },
      {
        "date": "2024-01-28T14:39:00.000Z",
        "voteCount": 2,
        "content": "BE. In others are are redflag keywords"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/132354-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is developing an application that runs on Amazon EC2 instances. Currently, the data that the application generates is temporary. However, the company needs to persist the data, even if the EC2 instances are terminated.<br>A data engineer must launch new EC2 instances from an Amazon Machine Image (AMI) and configure the instances to preserve the data.<br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch new EC2 instances by using an AMI that is backed by an EC2 instance store volume that contains the application data. Apply the default settings to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch new EC2 instances by using an AMI that is backed by a root Amazon Elastic Block Store (Amazon EBS) volume that contains the application data. Apply the default settings to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch new EC2 instances by using an AMI that is backed by an EC2 instance store volume. Attach an Amazon Elastic Block Store (Amazon EBS) volume to contain the application data. Apply the default settings to the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch new EC2 instances by using an AMI that is backed by an Amazon Elastic Block Store (Amazon EBS) volume. Attach an additional EC2 instance store volume to contain the application data. Apply the default settings to the EC2 instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-27T03:34:00.000Z",
        "voteCount": 8,
        "content": "CCCCCCC - you need to attach an extra EBS volume\n\nWhen an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume.\nref: https://repost.aws/knowledge-center/deleteontermination-ebs"
      },
      {
        "date": "2024-10-09T05:56:00.000Z",
        "voteCount": 1,
        "content": "By using an AMI backed by an Amazon EBS root volume, you ensure that the application data is preserved, even if the EC2 instances are terminated, because EBS volumes persist independently of the EC2 lifecycle."
      },
      {
        "date": "2024-10-01T11:04:00.000Z",
        "voteCount": 2,
        "content": "This is because Amazon EBS volumes are persistent, meaning the data is preserved even if the EC2 instance is terminated, which meets the requirement to persist the data. C is incorrect because it suggests launching instances using an EC2 instance store volume, which is ephemeral. Even though it proposes attaching an Amazon EBS volume for data, the root volume remains an instance store."
      },
      {
        "date": "2024-08-06T13:37:00.000Z",
        "voteCount": 1,
        "content": "Using default setting means B won\u2019t work."
      },
      {
        "date": "2024-07-14T01:46:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/preserving-volumes-on-termination.html\n\nRoot volume\nBy default, when you launch an instance the DeleteOnTermination attribute for the root volume of an instance is set to true. Therefore, the default is to delete the root volume of the instance when the instance terminates.\n\nNon-root volume\nBy default, when you attach a non-root EBS volume to an instance, its DeleteOnTermination attribute is set to false. Therefore, the default is to preserve these volumes.\n\nAnswer is C"
      },
      {
        "date": "2024-07-10T04:27:00.000Z",
        "voteCount": 1,
        "content": "its C!!! B with default setting will delete the EBS volume on termination"
      },
      {
        "date": "2024-06-09T18:22:00.000Z",
        "voteCount": 3,
        "content": "Amazon EBS volumes provide persistent block storage for EC2 instances. Data written to an EBS volume is independent of the EC2 instance lifecycle. Even if the EC2 instance is terminated, ***the data on the EBS volume remains intact***. Launching new EC2 instances from an AMI backed by an EBS volume containing the application data ensures the data persists across instance restarts or terminations"
      },
      {
        "date": "2024-05-22T09:04:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2024-05-19T06:43:00.000Z",
        "voteCount": 1,
        "content": "launch EC2 using AMI with root EBS that contains data"
      },
      {
        "date": "2024-05-15T23:20:00.000Z",
        "voteCount": 4,
        "content": "B: the root EBS volume will be deleted on termination by default.\nC: the EBS is independent from EC2 Termination"
      },
      {
        "date": "2024-05-06T21:05:00.000Z",
        "voteCount": 3,
        "content": "C - Looks better, because it will save data in all cases"
      },
      {
        "date": "2024-05-06T21:07:00.000Z",
        "voteCount": 4,
        "content": "And \"Delete on Termination\" flag by defaults sets to true, so better to use additional volume for application data"
      },
      {
        "date": "2024-04-13T22:53:00.000Z",
        "voteCount": 3,
        "content": "ccccccc"
      },
      {
        "date": "2024-04-04T07:46:00.000Z",
        "voteCount": 2,
        "content": "Can someone explain why C is NOT right?"
      },
      {
        "date": "2024-03-19T01:11:00.000Z",
        "voteCount": 3,
        "content": "This question is more for practitioner exam :)"
      },
      {
        "date": "2024-02-02T01:20:00.000Z",
        "voteCount": 2,
        "content": "Amazon EBS volumes are network-attached, and they persist independently of the life of an EC2 instance. By using an AMI backed by an Amazon EBS volume, the root device for the instance is an EBS volume, which means the data will persist."
      },
      {
        "date": "2024-01-28T14:46:00.000Z",
        "voteCount": 1,
        "content": "Voting for B"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/132667-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon Athena to run SQL queries for extract, transform, and load (ETL) tasks by using Create Table As Select (CTAS). The company must use Apache Spark instead of SQL to generate analytics.<br>Which solution will give the company the ability to use Spark to access Athena?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAthena query settings",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAthena workgroup\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAthena data source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAthena query editor"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T01:18:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\n\"To use Apache Spark in Amazon Athena, you create an Amazon Athena workgroup that uses a Spark engine.\""
      },
      {
        "date": "2024-07-10T21:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\nTo get started with Apache Spark on Amazon Athena, you must first create a Spark enabled workgroup. After you switch to the workgroup, you can create a notebook or open an existing notebook. When you open a notebook in Athena, a new session is started for it automatically and you can work with it directly in the Athena notebook editor."
      },
      {
        "date": "2024-06-09T18:30:00.000Z",
        "voteCount": 3,
        "content": "The Athena data source acts as a bridge between Athena and other analytics engines, such as Apache Spark. By using the Athena data source connector, you can access data stored in various formats (e.g., CSV, JSON, Parquet) and locations (e.g., Amazon S3, Apache Hive Metastore) through Spark applications"
      },
      {
        "date": "2024-05-28T04:19:00.000Z",
        "voteCount": 2,
        "content": "C. Athena data source\n\nThe Athena data source is a specific connector or library that allows Apache Spark to interact with data stored in Amazon Athena. This connector enables Spark to read data from Athena tables directly into Spark DataFrames or RDDs (Resilient Distributed Datasets), allowing you to perform analytics and transformations using Spark's capabilities."
      },
      {
        "date": "2024-03-30T06:02:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html"
      },
      {
        "date": "2024-03-13T23:37:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer.\nhttps://aws.amazon.com/blogs/big-data/explore-your-data-lake-using-amazon-athena-for-apache-spark/\nYou need an Athena workgroup as a prerequisite to use Apache Spark."
      },
      {
        "date": "2024-03-06T12:00:00.000Z",
        "voteCount": 3,
        "content": "B. is the correct answer.\nTo use Apache Spark in Amazon Athena, you create an Amazon Athena workgroup that uses a Spark engine.\nhttps://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html"
      },
      {
        "date": "2024-02-02T01:33:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/132364-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company needs to partition the Amazon S3 storage that the company uses for a data lake. The partitioning will use a path of the S3 object keys in the following format: s3://bucket/prefix/year=2023/month=01/day=01.<br>A data engineer must ensure that the AWS Glue Data Catalog synchronizes with the S3 storage when the company adds new partitions to the bucket.<br>Which solution will meet these requirements with the LEAST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule an AWS Glue crawler to run every morning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually run the AWS Glue CreatePartition API twice each day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition API call.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the MSCK REPAIR TABLE command from the AWS Glue console."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T01:40:00.000Z",
        "voteCount": 6,
        "content": "Use code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition API call. This approach ensures that the Data Catalog is updated as soon as new data is written to S3, providing the least latency in reflecting new partitions."
      },
      {
        "date": "2024-06-09T18:35:00.000Z",
        "voteCount": 2,
        "content": "By embedding the Boto3 create_partition API call within the code that writes data to S3, you achieve near real-time synchronization. The Data Catalog is updated immediately after a new partition is created in S3."
      },
      {
        "date": "2024-05-31T22:34:00.000Z",
        "voteCount": 1,
        "content": "The explanation could be more precise regarding the interaction with Amazon S3 and AWS Glue. The key point is that the process should be triggered immediately when new data is added to S3. This can be achieved through event-driven architecture, which indeed makes the solution intuitive and efficient."
      },
      {
        "date": "2024-05-24T07:10:00.000Z",
        "voteCount": 1,
        "content": "add partition after writing the data in s3"
      },
      {
        "date": "2024-05-14T02:07:00.000Z",
        "voteCount": 1,
        "content": "It's about \"synchronizing AWS Glue Data Catalog with S3\". So for me it's D - using MSCK REPAIR TABLE for existing S3 partitions (https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html)"
      },
      {
        "date": "2024-05-15T05:03:00.000Z",
        "voteCount": 1,
        "content": "Least latency"
      },
      {
        "date": "2024-04-13T12:42:00.000Z",
        "voteCount": 2,
        "content": "The answer is D"
      },
      {
        "date": "2024-03-19T01:23:00.000Z",
        "voteCount": 1,
        "content": "It's pure event-driven so... C"
      },
      {
        "date": "2024-01-28T16:30:00.000Z",
        "voteCount": 1,
        "content": "C. Least latency"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/132669-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A media company uses software as a service (SaaS) applications to gather data by using third-party tools. The company needs to store the data in an Amazon S3 bucket. The company will use Amazon Redshift to perform analytics based on the data.<br>Which AWS service or feature will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Streaming for Apache Kafka (Amazon MSK)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon AppFlow\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue Data Catalog",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Kinesis"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T18:39:00.000Z",
        "voteCount": 4,
        "content": "the media company can leverage a fully managed service that simplifies the process of ingesting data from their third-party SaaS applications into an Amazon S3 bucket, with minimal operational overhead. Additionally, AppFlow can integrate with Amazon Redshift, allowing the company to load the ingested data directly into their analytics environment for further processing and analysis"
      },
      {
        "date": "2024-05-31T22:39:00.000Z",
        "voteCount": 4,
        "content": "That's exactly the purpose of AppFlow: \"fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift. For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.\"\n\nhttps://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html"
      },
      {
        "date": "2024-03-19T01:27:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/appflow/latest/userguide/flow-tutorial.html"
      },
      {
        "date": "2024-03-13T23:41:00.000Z",
        "voteCount": 1,
        "content": "B seems the right choice here."
      },
      {
        "date": "2024-02-02T01:49:00.000Z",
        "voteCount": 2,
        "content": "https://d1.awsstatic.com/solutions/guidance/architecture-diagrams/integrating-third-party-saas-data-using-amazon-appflow.pdf\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software as a Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. It can store the raw data pulled from SaaS applications in Amazon S3, and integrates with AWS Glue Data Catalog to catalog and store metadata"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/132672-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is using Amazon Athena to analyze sales data that is in Amazon S3. The data engineer writes a query to retrieve sales amounts for 2023 for several products from a table named sales_data. However, the query does not return results for all of the products that are in the sales_data table. The data engineer needs to troubleshoot the query to resolve the issue.<br>The data engineer's original query is as follows:<br>SELECT product_name, sum(sales_amount)<br><br>FROM sales_data -<br><br>WHERE year = 2023 -<br><br>GROUP BY product_name -<br>How should the data engineer modify the Athena query to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace sum(sales_amount) with count(*) for the aggregation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd HAVING sum(sales_amount) &gt; 0 after the GROUP BY clause.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the GROUP BY clause."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T01:32:00.000Z",
        "voteCount": 10,
        "content": "\"SELECT product_name, sum(sales_amount)\nFROM sales_data\nWHERE extract(year FROM sales_date) = 2023\nGROUP BY product_name;\"\nA. This would change the query to count the number of rows instead of summing sales.\nC. This would filter out products with zero sales amounts.\nD. Removing the GROUP BY clause would result in a single sum of all sales amounts without grouping by product_name."
      },
      {
        "date": "2024-10-02T04:18:00.000Z",
        "voteCount": 1,
        "content": "C, query in the question is correct you just need to get amounts grater than Zero"
      },
      {
        "date": "2024-05-24T07:17:00.000Z",
        "voteCount": 3,
        "content": "year should be the partition in s3 so its necessary to extract. its not a column"
      },
      {
        "date": "2024-05-19T07:25:00.000Z",
        "voteCount": 1,
        "content": "No need to extract the year again"
      },
      {
        "date": "2024-05-16T08:04:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sql-reference-having-clause.html"
      },
      {
        "date": "2024-04-28T23:04:00.000Z",
        "voteCount": 1,
        "content": "Wrong answers \n\nA. Replace sum(sales_amount) with count(*) for the aggregation. This option will return the count of records for each product, not the sum of sales amounts, which is the desired result.\n\nB. Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023. The year column likely stores the year value directly, so there's no need to extract it from a date or timestamp column.\n\nD. Remove the GROUP BY clause. Removing the GROUP BY clause will cause an error because the sum(sales_amount) aggregation function requires a GROUP BY clause to specify the grouping column (product_name in this case)."
      },
      {
        "date": "2024-04-27T22:17:00.000Z",
        "voteCount": 2,
        "content": "B\nB. Change `WHERE year = 2023` to `WHERE extract(year FROM sales_data) = 2023`.\n\nThe issue with the original query is that it assumes there is a column named `year` in the `sales_data` table. However, it's more likely that the date or timestamp information is stored in a single column, for example, a column named `sales_date`.\n\nTo extract the year from a date or timestamp column, you need to use the `extract()` function in Athena SQL."
      },
      {
        "date": "2024-04-17T22:49:00.000Z",
        "voteCount": 2,
        "content": "None of the answer makes senses. Option C will exclude any amount that is 0. This option would be correct if it is: Add HAVING sum(sales_amount) &gt;= 0 after the GROUP BY clause."
      },
      {
        "date": "2024-04-12T22:08:00.000Z",
        "voteCount": 2,
        "content": "Gemini: C. Add HAVING sum(sales_amount) &gt; 0 after the GROUP BY clause.\n\nZero Sales Products: The original query is likely missing products that had zero sales amount in 2023. This modification filters the grouped results, ensuring only products with positive sales are displayed.\nWhy Other Options Don't Address the Core Issue:\n\nA. Replace sum(sales_amount) with count(*) for the aggregation. This would show how many sales transactions a product had, but not if it generated any revenue. It wouldn't solve the issue of missing products.\nB. Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023. This is functionally equivalent to the original WHERE clause if the year column is already an integer type. It wouldn't fix missing products.\nD. Remove the GROUP BY clause. This would aggregate all sales for 2023 with no product breakdown, losing the granularity needed."
      },
      {
        "date": "2024-03-13T23:48:00.000Z",
        "voteCount": 4,
        "content": "Not A because the engineer wants a sum not the total count.\nNot C because it will filter out the data with sales_amount zero.\nNot D because it will return just one result and the engineer wants the sales for multiple products.\n\nB should be the right answer if the sales_data is a date field."
      },
      {
        "date": "2024-05-06T01:49:00.000Z",
        "voteCount": 1,
        "content": "But this is the table name..."
      },
      {
        "date": "2024-03-15T15:46:00.000Z",
        "voteCount": 1,
        "content": "Add HAVING sum(sales_amount) &gt; 0  this does NOT filter out the data with sales_mount zero. it can not be any negative value. \nThe original query\u2019s WHERE year = 2023 condition is already appropriate for filtering data by the year 2023, so that B is unnecessary."
      },
      {
        "date": "2024-03-23T08:33:00.000Z",
        "voteCount": 3,
        "content": "&gt;0 filters out every product which is not sold. The question was about \"some products are not displayed\" so using the having argument can not be the right choice"
      },
      {
        "date": "2024-02-02T02:23:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/select.html"
      },
      {
        "date": "2024-04-04T03:45:00.000Z",
        "voteCount": 1,
        "content": "answer is C!"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/132673-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer has a one-time task to read data from objects that are in Apache Parquet format in an Amazon S3 bucket. The data engineer needs to query only one column of the data.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to load data from the S3 bucket into a pandas dataframe. Write a SQL SELECT statement on the dataframe to query the required column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Select to write a SQL SELECT statement to retrieve the required column from the S3 objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrepare an AWS Glue DataBrew project to consume the S3 objects and to query the required column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an AWS Glue crawler on the S3 objects. Use a SQL SELECT statement in Amazon Athena to query the required column."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-13T14:09:00.000Z",
        "voteCount": 1,
        "content": "Amazon S3 Select is no longer available to new customers. Existing customers of Amazon S3 Select can continue to use the feature as usual\n\nBut with it you can only query one object xD. Glue + athena"
      },
      {
        "date": "2024-06-16T06:38:00.000Z",
        "voteCount": 3,
        "content": "but s3 select can only select one object"
      },
      {
        "date": "2024-06-14T16:08:00.000Z",
        "voteCount": 2,
        "content": "omly once"
      },
      {
        "date": "2024-05-23T18:59:00.000Z",
        "voteCount": 2,
        "content": "if is one-time task"
      },
      {
        "date": "2024-03-19T01:37:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-select.html"
      },
      {
        "date": "2024-02-02T02:29:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory-athena-query.html\nS3 Select allows you to retrieve a subset of data from an object stored in S3 using simple SQL expressions. It is capable of working directly with objects in Parquet format."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/132674-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon Redshift for its data warehouse. The company must automate refresh schedules for Amazon Redshift materialized views.<br>Which solution will meet this requirement with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Airflow to refresh the materialized views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda user-defined function (UDF) within Amazon Redshift to refresh the materialized views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the query editor v2 in Amazon Redshift to refresh the materialized views.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue workflow to refresh the materialized views."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T18:55:00.000Z",
        "voteCount": 1,
        "content": "the company can automate the refresh schedules for materialized views with minimal effort. This approach leverages the built-in capabilities of Amazon Redshift, reducing the need for additional services, configurations, or custom code. It aligns with the principle of using the simplest and most straightforward solution that meets the requirements, minimizing operational overhead and complexity"
      },
      {
        "date": "2024-05-07T21:32:00.000Z",
        "voteCount": 3,
        "content": "We can schedule the refresh using query scheduler from Query Editor V2."
      },
      {
        "date": "2024-04-12T22:14:00.000Z",
        "voteCount": 2,
        "content": "Amazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created with or altered to have the autorefresh option. For more details, refer to the documentation here, https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html."
      },
      {
        "date": "2024-03-31T06:46:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html"
      },
      {
        "date": "2024-03-23T08:38:00.000Z",
        "voteCount": 2,
        "content": "You can set autorefresh for materialized views using CREATE MATERIALIZED VIEW. You can also use the AUTO REFRESH clause to refresh materialized views automatically."
      },
      {
        "date": "2024-03-19T01:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html"
      },
      {
        "date": "2024-03-13T23:55:00.000Z",
        "voteCount": 1,
        "content": "You can set AUTO REFRESH option on creation. So I will vote with C."
      },
      {
        "date": "2024-03-13T06:22:00.000Z",
        "voteCount": 1,
        "content": "Lambda requires code and configuring permissions. A and D are additional overheads as well. Vote C"
      },
      {
        "date": "2024-03-06T12:21:00.000Z",
        "voteCount": 1,
        "content": "B.\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-UDFs.html"
      },
      {
        "date": "2024-02-02T02:39:00.000Z",
        "voteCount": 2,
        "content": "AWS Lambda allows running code in response to triggers without needing to provision or manage servers. However, creating a UDF within Amazon Redshift to call a Lambda function for this purpose involves writing custom code and managing permissions between Lambda and Redshift."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/132676-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer must orchestrate a data pipeline that consists of one AWS Lambda function and one AWS Glue job. The solution must integrate with AWS services.<br>Which solution will meet these requirements with the LEAST management overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Step Functions workflow that includes a state machine. Configure the state machine to run the Lambda function and then the AWS Glue job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Apache Airflow workflow that is deployed on an Amazon EC2 instance. Define a directed acyclic graph (DAG) in which the first task is to call the Lambda function and the second task is to call the AWS Glue job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue workflow to run the Lambda function and then the AWS Glue job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Apache Airflow workflow that is deployed on Amazon Elastic Kubernetes Service (Amazon EKS). Define a directed acyclic graph (DAG) in which the first task is to call the Lambda function and the second task is to call the AWS Glue job."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T19:17:00.000Z",
        "voteCount": 5,
        "content": "Step Functions is a managed service for building serverless workflows. You define a state machine that orchestrates the execution sequence.\nThis eliminates the need to manage and maintain your own workflow orchestration server like Airflow."
      },
      {
        "date": "2024-08-20T03:58:00.000Z",
        "voteCount": 2,
        "content": "AWS Glue is a fully managed ETL (extract, transform, load) service that makes it easy to orchestrate data pipelines. Using the AWS Glue workflow to run Lambda functions and glue jobs is the easiest and least expensive option because it's a fully managed service that requires no additional workflow tools or infrastructure to configure and manage. Other options require additional tools or resources to configure and manage, and are therefore more expensive to manage."
      },
      {
        "date": "2024-05-31T22:46:00.000Z",
        "voteCount": 2,
        "content": "Step Functions can handle both Lambda and Glue in this scenario, making it the best choice."
      },
      {
        "date": "2024-05-13T00:10:00.000Z",
        "voteCount": 3,
        "content": "B and D require additional effort\nC Glue workflows do not have a direct integration with lambda\nhence the best choice is A"
      },
      {
        "date": "2024-03-24T01:44:00.000Z",
        "voteCount": 3,
        "content": "Key word orchestrating is most likely step functions"
      },
      {
        "date": "2024-02-02T02:46:00.000Z",
        "voteCount": 4,
        "content": "Option A, using AWS Step Functions, is the best solution to meet the requirement with the least management overhead. Step Functions is designed for easy integration with AWS services like Lambda and Glue, providing a managed, low-code approach to orchestrate workflows. This allows for a more straightforward setup and less ongoing management compared to the other options."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/132677-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company needs to set up a data catalog and metadata management for data sources that run in the AWS Cloud. The company will use the data catalog to maintain the metadata of all the objects that are in a set of data stores. The data stores include structured sources such as Amazon RDS and Amazon Redshift. The data stores also include semistructured sources such as JSON files and .xml files that are stored in Amazon S3.<br>The company needs a solution that will update the data catalog on a regular basis. The solution also must detect changes to the source metadata.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora as the data catalog. Create AWS Lambda functions that will connect to the data catalog. Configure the Lambda functions to gather the metadata information from multiple sources and to update the Aurora data catalog. Schedule the Lambda functions to run periodically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog as the central metadata repository. Use AWS Glue crawlers to connect to multiple data stores and to update the Data Catalog with metadata changes. Schedule the crawlers to run periodically to update the metadata catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB as the data catalog. Create AWS Lambda functions that will connect to the data catalog. Configure the Lambda functions to gather the metadata information from multiple sources and to update the DynamoDB data catalog. Schedule the Lambda functions to run periodically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog as the central metadata repository. Extract the schema for Amazon RDS and Amazon Redshift sources, and build the Data Catalog. Use AWS Glue crawlers for data that is in Amazon S3 to infer the schema and to automatically update the Data Catalog."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T19:25:00.000Z",
        "voteCount": 7,
        "content": "The AWS Glue Data Catalog is a purpose-built, fully managed service designed to serve as a central metadata repository for your data sources. It provides a unified view of your data across various sources, including structured databases (like Amazon RDS and Amazon Redshift) and semi-structured data formats (like JSON and XML files in Amazon S3)."
      },
      {
        "date": "2024-05-24T08:01:00.000Z",
        "voteCount": 3,
        "content": "glue data catalog with crawlers"
      },
      {
        "date": "2024-05-13T01:40:00.000Z",
        "voteCount": 1,
        "content": "B is the obvious answer"
      },
      {
        "date": "2024-05-31T22:50:00.000Z",
        "voteCount": 3,
        "content": "Even though you picked A."
      },
      {
        "date": "2024-05-16T10:23:00.000Z",
        "voteCount": 1,
        "content": "Sorry there is no Aurora Data Catalog :)"
      },
      {
        "date": "2024-03-19T02:23:00.000Z",
        "voteCount": 4,
        "content": "A,C out for obvious reason \nD out because it involves manual schema extract"
      },
      {
        "date": "2024-02-02T02:51:00.000Z",
        "voteCount": 2,
        "content": "Option B, using the AWS Glue Data Catalog with AWS Glue Crawlers, is the best solution to meet the requirements with the least operational overhead. It provides a fully managed, integrated solution for cataloging both structured and semistructured data across various AWS data stores without the need for extensive manual configuration or custom coding."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/132678-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores data from an application in an Amazon DynamoDB table that operates in provisioned capacity mode. The workloads of the application have predictable throughput load on a regular schedule. Every Monday, there is an immediate increase in activity early in the morning. The application has very low usage during weekends.<br>The company must ensure that the application performs consistently during peak usage times.<br>Which solution will meet these requirements in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the provisioned capacity to the maximum capacity that is currently present during peak load times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDivide the table into two tables. Provision each table with half of the provisioned capacity of the original table. Spread queries evenly across both tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Application Auto Scaling to schedule higher provisioned capacity for peak usage times. Schedule lower capacity during off-peak times.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the capacity mode from provisioned to on-demand. Configure the table to scale up and scale down based on the load on the table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-14T02:37:00.000Z",
        "voteCount": 1,
        "content": "C\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html\n\nDynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated or depressed for a sustained period of several minutes. This means that provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually."
      },
      {
        "date": "2024-06-09T19:29:00.000Z",
        "voteCount": 4,
        "content": "app autoscalling allows you to dynamically adjust provisioned capacity based on usage patterns. You only pay for the capacity you utilize, reducing costs compared to keeping a high, fixed capacity throughout the week"
      },
      {
        "date": "2024-04-12T22:23:00.000Z",
        "voteCount": 3,
        "content": "D. Change the capacity mode from provisioned to on-demand... On-demand mode is great for unpredictable workloads. In your case, with predictable patterns, you'd likely pay more with on-demand than with a well-managed, scheduled, provisioned mode."
      },
      {
        "date": "2024-05-28T01:29:00.000Z",
        "voteCount": 1,
        "content": "I agree with you, on-demand tends to be picked when you don't know the workload. While in this scenario they know, so technically the Auto Scaling solution would be much cheaper here."
      },
      {
        "date": "2024-04-01T18:26:00.000Z",
        "voteCount": 1,
        "content": "As I understand, should be D"
      },
      {
        "date": "2024-04-01T18:31:00.000Z",
        "voteCount": 2,
        "content": "But C is also a good choice. Maybe because it is predictable, I'm now intending to choose C"
      },
      {
        "date": "2024-03-24T01:47:00.000Z",
        "voteCount": 3,
        "content": "Obviously better than B because of peak scaling"
      },
      {
        "date": "2024-03-22T00:43:00.000Z",
        "voteCount": 1,
        "content": "D\nExcerpts from documentation: \nThis means that provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually.\nWhereas on-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic and also if you only want to pay exactly for what you use. The on-demand pricing model is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when under-provisioned capacity would impact the user experience.\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html"
      },
      {
        "date": "2024-03-22T00:43:00.000Z",
        "voteCount": 1,
        "content": "selected answer should be D"
      },
      {
        "date": "2024-05-28T01:31:00.000Z",
        "voteCount": 1,
        "content": "Well, as your comment says:\n\nD -  on-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic and also if you only want to pay exactly for what you use.\n\nThat's not the case, they know exactly when they are expecting an increasing. So the most cost-effective solution is C - Auto Scaling."
      },
      {
        "date": "2024-03-06T12:29:00.000Z",
        "voteCount": 2,
        "content": "C.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-dynamodb.html"
      },
      {
        "date": "2024-02-02T02:54:00.000Z",
        "voteCount": 4,
        "content": "Option C, using AWS Application Auto Scaling to schedule higher provisioned capacity for peak usage times and lower capacity during off-peak times, is the most cost-effective solution for the described scenario. It allows the company to align their DynamoDB capacity costs with actual usage patterns, scaling up only when needed and scaling down during low-usage periods."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/132680-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is planning to migrate on-premises Apache Hadoop clusters to Amazon EMR. The company also needs to migrate a data catalog into a persistent storage solution.<br>The company currently stores the data catalog in an on-premises Apache Hive metastore on the Hadoop clusters. The company requires a serverless solution to migrate the data catalog.<br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Database Migration Service (AWS DMS) to migrate the Hive metastore into Amazon S3. Configure AWS Glue Data Catalog to scan Amazon S3 to produce the data catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use AWS Glue Data Catalog to store the company's data catalog as an external data catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an external Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use Amazon Aurora MySQL to store the company's data catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use the new metastore as the company's data catalog."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T22:25:00.000Z",
        "voteCount": 2,
        "content": "Serverless and Cost-Efficient: AWS Glue Data Catalog offers a serverless metadata repository, reducing operational overhead and making it cost-effective. Using it as an external data catalog means you don't have to manage additional database infrastructure.\nSeamless Migration: Migrating your existing Hive metastore to Amazon EMR ensures compatibility with your current Hadoop setup. EMR is designed to run Hadoop workloads, facilitating this process.\nFlexibility: An external data catalog in AWS Glue offers flexibility and separation of concerns. Your metastore remains managed by EMR for your Hadoop workloads, while Glue provides a centralized catalog for broader AWS data sources."
      },
      {
        "date": "2024-04-06T05:18:00.000Z",
        "voteCount": 1,
        "content": "B is answer!\nBy leveraging AWS Glue Data Catalog as an external data catalog and migrating the existing Hive metastore into Amazon EMR, the company can achieve a serverless, persistent, and cost-effective solution for storing and managing their data catalog."
      },
      {
        "date": "2024-04-05T23:35:00.000Z",
        "voteCount": 2,
        "content": "B.  https://aws.amazon.com/jp/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/"
      },
      {
        "date": "2024-04-01T18:42:00.000Z",
        "voteCount": 1,
        "content": "I will go with A. Besides DMS is typical for migration, it's the only choice which explicitly concerns about how the migration itself will be made. Other choices would demand a script or GLUE ETL job if you will. But this logic of migration was never put"
      },
      {
        "date": "2024-03-25T08:24:00.000Z",
        "voteCount": 1,
        "content": "I will go with A"
      },
      {
        "date": "2024-03-22T00:42:00.000Z",
        "voteCount": 1,
        "content": "serverless catalog in AWS == glue"
      },
      {
        "date": "2024-03-06T12:36:00.000Z",
        "voteCount": 1,
        "content": "B.\nSet up an AWS Glue ETL job which extracts metadata from your Hive metastore (MySQL) and loads it into your AWS Glue Data Catalog. This method requires an AWS Glue connection to the Hive metastore as a JDBC source. An ETL script is provided to extract metadata from the Hive metastore and write it to AWS Glue Data Catalog.\nhttps://github.com/aws-samples/aws-glue-samples/blob/master/utilities/Hive_metastore_migration/README.md"
      },
      {
        "date": "2024-02-02T03:00:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/           Option B is likely the most suitable. Migrating the Hive metastore into Amazon EMR and using AWS Glue Data Catalog as an external catalog provides a balance between leveraging the scalable and managed services of AWS (like EMR and Glue Data Catalog) and ensuring a smooth transition from the on-premises setup. This approach leverages the serverless nature of AWS Glue Data Catalog, minimizing operational overhead and potentially reducing costs compared to managing database servers."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/132681-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses an Amazon Redshift provisioned cluster as its database. The Redshift cluster has five reserved ra3.4xlarge nodes and uses key distribution.<br>A data engineer notices that one of the nodes frequently has a CPU load over 90%. SQL Queries that run on the node are queued. The other four nodes usually have a CPU load under 15% during daily operations.<br>The data engineer wants to maintain the current number of compute nodes. The data engineer also wants to balance the load more evenly across all five compute nodes.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the sort key to be the data column that is most often used in a WHERE clause of the SQL SELECT statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the distribution key to the table column that has the largest dimension.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade the reserved node from ra3.4xlarge to ra3.16xlarge.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the primary key to be the data column that is most often used in a WHERE clause of the SQL SELECT statement."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T03:07:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html\nOption B, changing the distribution key, is the most effective solution to balance the load more evenly across all five compute nodes. Selecting an appropriate distribution key that aligns with the query patterns and data characteristics can result in a more uniform distribution of data and workloads, thus reducing the likelihood of one node being overutilized while others are underutilized."
      },
      {
        "date": "2024-06-10T20:35:00.000Z",
        "voteCount": 2,
        "content": "In a Redshift cluster with key distribution, data is distributed across compute nodes based on the values of the distribution key. An uneven distribution can lead to skewed workloads on specific nodes.\nBy choosing the table column with the largest dimension (most distinct values) as the distribution key, you ensure a more even spread of data across all nodes. This balances the processing load on each node when queries access that column."
      },
      {
        "date": "2024-04-28T00:00:00.000Z",
        "voteCount": 2,
        "content": "The correct solution is B. Change the distribution key to the table column that has the largest dimension. This will help to distribute the data more evenly across the nodes, reducing the load on the heavily utilized node."
      },
      {
        "date": "2024-04-12T22:33:00.000Z",
        "voteCount": 1,
        "content": "Gemini result:\nUnderstanding the Problem:\n\nThe scenario describes a Redshift cluster with uneven load distribution. This indicates potential issues with either the distribution style or the sort key.\n\nKey Distribution:\n\nThe problem states that the cluster uses key distribution, meaning a specific column is designated as the distribution key. Data rows with matching distribution key values are placed on the same node.\n\nSort Key:\n\nA sort key determines the order in which data is physically stored within a table's blocks on a node.  A well-chosen sort key can significantly optimize query performance, especially when queries often filter by that column."
      },
      {
        "date": "2024-05-31T22:57:00.000Z",
        "voteCount": 1,
        "content": "The sort key determines the order of data storage and can improve query performance for specific queries, but it does not directly affect the distribution of data across nodes. Therefore, this will not address the uneven CPU load issue."
      },
      {
        "date": "2024-03-06T12:58:00.000Z",
        "voteCount": 2,
        "content": "B.\nWith \"Key distribution\". The rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together. \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/132683-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A security company stores IoT data that is in JSON format in an Amazon S3 bucket. The data structure can change when the company upgrades the IoT devices. The company wants to create a data catalog that includes the IoT data. The company's analytics department will use the data catalog to index the data.<br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create a new AWS Glue workload to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Redshift provisioned cluster. Create an Amazon Redshift Spectrum database for the analytics department to explore the data that is in Amazon S3. Create Redshift stored procedures to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Athena workgroup. Explore the data that is in Amazon S3 by using Apache Spark through Athena. Provide the Athena workgroup schema and tables to the analytics department.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create AWS Lambda user defined functions (UDFs) by using the Amazon Redshift Data API. Create an AWS Step Functions job to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T03:43:00.000Z",
        "voteCount": 9,
        "content": "Option A, creating an AWS Glue Data Catalog with Glue Schema Registry and orchestrating data ingestion into Amazon Redshift Serverless using AWS Glue, appears to be the most cost-effective and suitable solution. It offers a serverless approach to manage the evolving data schema of the IoT data and efficiently supports data analytics needs without the overhead of managing a provisioned database cluster or complex orchestration setups."
      },
      {
        "date": "2024-04-06T05:21:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: A\nAmazon Redshift Serverless is a serverless option for Amazon Redshift, which means you don't have to provision and manage clusters. This makes it a cost-effective choice for the analytics department's use case."
      },
      {
        "date": "2024-05-19T10:17:00.000Z",
        "voteCount": 1,
        "content": "Athena is not able to create new data catalog"
      },
      {
        "date": "2024-05-07T08:52:00.000Z",
        "voteCount": 1,
        "content": "Option C\n\nCost-effectiveness: Amazon Athena allows you to query data directly from Amazon S3 without the need for any infrastructure setup or management. You pay only for the queries you run, making it cost-effective, especially for sporadic or exploratory analysis.\nFlexibility: Since the data structure can change with IoT device upgrades, using Athena allows for flexibility in querying and analyzing the data regardless of its structure. You don't need to define a fixed schema upfront, enabling you to adapt to changes seamlessly.\nApache Spark Support: Athena supports querying data using Apache Spark, which is powerful for processing and analyzing large datasets. This capability ensures that the analytics department can leverage Spark for more advanced analytics if needed.\nhttps://www.youtube.com/watch?v=Q93NZJBFSWw"
      },
      {
        "date": "2024-04-28T00:03:00.000Z",
        "voteCount": 2,
        "content": "The correct solution is A. Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create a new AWS Glue workload to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless.\n\nOption C (Amazon Athena and Apache Spark) is suitable for ad-hoc querying and exploration but may not be the best choice for the analytics department's ongoing data analysis needs, as Athena is designed for interactive querying rather than complex data transformations."
      },
      {
        "date": "2024-04-17T04:19:00.000Z",
        "voteCount": 4,
        "content": "The objective is to create a data catalog that includes the IoT data and AWS Glue Data Catalog is the best option for this requirement.\nhttps://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\n\nC is incorrect. While Athena makes it easy to read from S3 using SQL, it does not crawl the data source and create a data catalog."
      },
      {
        "date": "2024-04-13T01:04:00.000Z",
        "voteCount": 1,
        "content": "Why Option C is the Most Cost-Effective\n\nServerless and Pay-as-you-go: Athena is a serverless query service, meaning you only pay for the queries the analytics department runs. No need to provision and manage always-running clusters.\nFlexible Schema Handling: Athena works well with semi-structured data like JSON and can handle schema evolution on the fly. This is perfect for the scenario where IoT data structures might change.\nSpark Integration: Integrating Apache Spark with Athena provides rich capabilities for data processing and transformation.\nEase of Use for Analytics: Athena's familiar SQL-like interface and ability to directly query S3 data make it convenient for the analytics department."
      },
      {
        "date": "2024-04-01T18:58:00.000Z",
        "voteCount": 1,
        "content": "Options A, B, and D involve setting up additional infrastructure (e.g., AWS Glue, Redshift clusters, Lambda functions) which may incur unnecessary costs and complexity for the given requirements. Option C, on the other hand, utilizes a serverless and scalable solution directly querying data in S3, making it the most cost-effective choice."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/132684-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores details about transactions in an Amazon S3 bucket. The company wants to log all writes to the S3 bucket into another S3 bucket that is in the same AWS Region.<br>Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an S3 Event Notifications rule for all activities on the transactions S3 bucket to invoke an AWS Lambda function. Program the Lambda function to write the event to Amazon Kinesis Data Firehose. Configure Kinesis Data Firehose to write the event to the logs S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trail of management events in AWS CloudTraiL. Configure the trail to receive data from the transactions S3 bucket. Specify an empty prefix and write-only events. Specify the logs S3 bucket as the destination bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an S3 Event Notifications rule for all activities on the transactions S3 bucket to invoke an AWS Lambda function. Program the Lambda function to write the events to the logs S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trail of data events in AWS CloudTraiL. Configure the trail to receive data from the transactions S3 bucket. Specify an empty prefix and write-only events. Specify the logs S3 bucket as the destination bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T03:48:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\nOption D, creating a trail of data events in AWS CloudTrail, is the best solution to meet the requirement with the least operational effort. It directly logs the desired activities to another S3 bucket and does not involve the development and maintenance of additional resources like Lambda functions or Kinesis Data Firehose streams."
      },
      {
        "date": "2024-05-19T10:32:00.000Z",
        "voteCount": 4,
        "content": "A:  Don't need all activities on the S3 bucket\nB:  Management events include not only the data log but also the admin log\nC:  Don't need all activities on the S3 bucket\nOption D with the LEAST operational effort"
      },
      {
        "date": "2024-04-28T00:07:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D.\n\nOption A or C require writing custom Lambda code to handle the events and write them to the Kinesis or S3 bucket so they are not the LEAST operational effort."
      },
      {
        "date": "2024-04-24T07:56:00.000Z",
        "voteCount": 1,
        "content": "S3 object level activities such as GetObject, DeleteObject, PutObject etc are considered as Data event in cloud trail. Read and Write event be monitored separately."
      },
      {
        "date": "2024-04-22T04:53:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B - CloudTrail of management events includes logging set ups like this"
      },
      {
        "date": "2024-03-19T02:37:00.000Z",
        "voteCount": 3,
        "content": "Although it might be tempting going with C, please keep in mind that if we go with C we must define lambda code, lambda permission, triggers, etc. If we go with D we just enable a trail data events and that's pretty much it."
      },
      {
        "date": "2024-03-02T02:38:00.000Z",
        "voteCount": 2,
        "content": "Other Options were Less Efficient:\nA. Leverage S3 Event Notifications, Lambda function, and Kinesis Data Firehose: While this option works, it involves setting up and managing three services, increasing complexity and operational overhead. Kinesis Data Firehose introduces an unnecessary intermediary step, adding complexity for a straightforward logging task.\nB. Utilize CloudTrail with Management Events: CloudTrail primarily tracks API calls and management activities related to S3 buckets, not data events like writes to objects. Consequently, it wouldn't capture the desired S3 bucket writes.\nD. Employ CloudTrail with Data Events: Similar to option B, CloudTrail with data events doesn't track individual object writes within a bucket. It focuses on object-level changes like creation, deletion, or metadata modification."
      },
      {
        "date": "2024-03-02T02:39:00.000Z",
        "voteCount": 1,
        "content": "Option C is right , by employing S3 Event Notifications with a Lambda function directly writing to the logs S3 bucket, you achieve the desired logging functionality with minimal setup, management, and cost compared to the other options. This approach leverages the built-in integration between these services and avoids unnecessary service dependencies."
      },
      {
        "date": "2024-03-29T13:14:00.000Z",
        "voteCount": 1,
        "content": "Check Amazon S3 object-level actions that are tracked by AWS CloudTrail logging\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html\n\nYou can get CloudTrail logs for object-level Amazon S3 actions. To do this, enable data events for your S3 bucket or all buckets in your account."
      },
      {
        "date": "2024-03-29T13:17:00.000Z",
        "voteCount": 1,
        "content": "Write to S3 means PutObject, CopyObject API"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/132685-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to maintain a central metadata repository that users access through Amazon EMR and Amazon Athena queries. The repository needs to provide the schema and properties of many tables. Some of the metadata is stored in Apache Hive. The data engineer needs to import the metadata from Hive into the central metadata repository.<br>Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR and Apache Ranger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Hive metastore on an EMR cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a metastore on an Amazon RDS for MySQL DB instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T04:06:00.000Z",
        "voteCount": 6,
        "content": "https://aws.amazon.com/blogs/big-data/metadata-classification-lineage-and-discovery-using-apache-atlas-on-amazon-emr/\nOption C, using the AWS Glue Data Catalog, is the best solution to meet the requirements with the least development effort. The AWS Glue Data Catalog is designed to be a central metadata repository that can integrate with various AWS services including EMR and Athena, providing a managed and scalable solution for metadata management with built-in Hive compatibility."
      },
      {
        "date": "2024-06-20T12:42:00.000Z",
        "voteCount": 1,
        "content": "Data Catalog."
      },
      {
        "date": "2024-03-02T03:06:00.000Z",
        "voteCount": 2,
        "content": "Option C, using the AWS Glue Data Catalog, requires the least development effort to meet the requirements for a central metadata repository accessed from EMR and Athena."
      },
      {
        "date": "2024-03-02T03:06:00.000Z",
        "voteCount": 2,
        "content": "Here's an analysis of each option:\n\nA) Amazon EMR and Apache Ranger would require significant coding to build a custom metadata repository solution\n\nB) A Hive metastore provides metadata to EMR, but would require substantial development work to share that metadata with Athena\n\nC) The AWS Glue Data Catalog integrates natively with EMR and Athena, providing a shared schema registry, making it the easiest solution\n\nD) An RDS database metastore would also require building custom integration points with Athena, EMR, and other services to enable metadata sharing\n\nSince AWS Glue provides a fully managed data catalog service purpose built for this metadata management use case across different analytics engines, Option C clearly stands out as the solution requiring the least development effort."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/132686-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company needs to build a data lake in AWS. The company must provide row-level data access and column-level data access to specific teams. The teams will access the data by using Amazon Athena, Amazon Redshift Spectrum, and Apache Hive from Amazon EMR.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for data lake storage. Use S3 access policies to restrict data access by rows and columns. Provide data access through Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for data lake storage. Use Apache Ranger through Amazon EMR to restrict data access by rows and columns. Provide data access by using Apache Pig.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift for data lake storage. Use Redshift security policies to restrict data access by rows and columns. Provide data access by using Apache Spark and Amazon Athena federated queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for data lake storage. Use AWS Lake Formation to restrict data access by rows and columns. Provide data access through AWS Lake Formation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-27T19:31:00.000Z",
        "voteCount": 1,
        "content": "Using Amazon S3 for storage and AWS Lake Formation for fine-grained access control like row-level or column-level access."
      },
      {
        "date": "2024-08-16T04:29:00.000Z",
        "voteCount": 3,
        "content": "this id D"
      },
      {
        "date": "2024-03-02T03:26:00.000Z",
        "voteCount": 3,
        "content": "Option D is the best solution to meet the requirements with the least operational overhead.\n\nUsing Amazon S3 for storage and AWS Lake Formation for access control and data access delivers the following advantages:\n\nS3 provides a highly durable, available, and scalable data lake storage layer\nLake Formation enables fine-grained access control down to column and row-level\nIntegrates natively with Athena, Redshift Spectrum, and EMR for simplified data access\nFully managed service minimizes admin overhead vs self-managing Ranger or piecemeal solutions"
      },
      {
        "date": "2024-03-02T03:26:00.000Z",
        "voteCount": 1,
        "content": "Option A would require custom access control code development and greater ops effort\nOption B still requires managing Ranger integrated with EMR\nOption C does not natively support column-level security policies"
      },
      {
        "date": "2024-02-02T04:18:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/lake-formation/latest/dg/cbac-tutorial.html\nOption D, using Amazon S3 for data lake storage and AWS Lake Formation for access control, is the most suitable solution. It meets the requirements for row-level and column-level access control and integrates well with Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on EMR, all with lower operational overhead compared to the other options."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/132687-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An airline company is collecting metrics about flight activities for analytics. The company is conducting a proof of concept (POC) test to show how analytics can provide insights that the company can use to increase on-time departures.<br>The POC test uses objects in Amazon S3 that contain the metrics in .csv format. The POC test uses Amazon Athena to query the data. The data is partitioned in the S3 bucket by date.<br>As the amount of data increases, the company wants to optimize the storage solution to improve query performance.<br>Which combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a randomized string to the beginning of the keys in Amazon S3 to get more throughput across partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 bucket that is in the same account that uses Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an S3 bucket that is in the same AWS Region where the company runs Athena queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprocess the .csv data to JSON format by fetching only the document keys that the query requires.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprocess the .csv data to Apache Parquet format by fetching only the data blocks that are needed for predicates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T04:36:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html"
      },
      {
        "date": "2024-05-28T01:48:00.000Z",
        "voteCount": 1,
        "content": "I will go with C and E."
      },
      {
        "date": "2024-04-10T23:49:00.000Z",
        "voteCount": 1,
        "content": "C is not mentioned anywhere in the https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html"
      },
      {
        "date": "2024-03-06T14:06:00.000Z",
        "voteCount": 1,
        "content": "Answer C and E"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/135451-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon RDS for MySQL as the database for a critical application. The database workload is mostly writes, with a small number of reads.<br>A data engineer notices that the CPU utilization of the DB instance is very high. The high CPU utilization is slowing down the application. The data engineer must reduce the CPU utilization of the DB Instance.<br>Which actions should the data engineer take to meet this requirement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Performance Insights feature of Amazon RDS to identify queries that have high CPU utilization. Optimize the problematic queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the database schema to include additional tables and indexes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReboot the RDS DB instance once each week.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade to a larger instance size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement caching to reduce the database query load."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-14T01:44:00.000Z",
        "voteCount": 8,
        "content": "Here the issue is with the writes and caching will not solve them.\n\nI will go with A and D."
      },
      {
        "date": "2024-04-01T19:27:00.000Z",
        "voteCount": 5,
        "content": "I will go for A and D, since other options are more likely to improve read performance issues."
      },
      {
        "date": "2024-05-07T09:45:00.000Z",
        "voteCount": 2,
        "content": "A and D\nWith a workload that is mostly writes and a small number of reads, caching will not be as effective in reducing CPU utilization compared to read-heavy workloads. \nhttps://repost.aws/knowledge-center/rds-aurora-postgresql-high-cpu"
      },
      {
        "date": "2024-03-24T03:48:00.000Z",
        "voteCount": 3,
        "content": "A and D.\n\nFor A it is mentioned here https://repost.aws/knowledge-center/rds-instance-high-cpu"
      },
      {
        "date": "2024-03-19T03:13:00.000Z",
        "voteCount": 4,
        "content": "Since the questions states that \"the database workload is mostly writes\" let's eliminate the options that improves the reads."
      },
      {
        "date": "2024-03-07T13:57:00.000Z",
        "voteCount": 3,
        "content": "Ans. AE\nA) Use Amazon RDS Performance Insights to identify the query that's responsible for the database load. Check the SQL tab that corresponds to a particular timeframe.\nE) If there's a query that's repeatedly running, use prepared statements to lower the pressure on your CPU. Repeated running of prepared statements caches the query plan. Because the plan is already in cache for further runs, the time for planning is much less.\nhttps://repost.aws/knowledge-center/rds-aurora-postgresql-high-cpu"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/135091-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has used an Amazon Redshift table that is named Orders for 6 months. The company performs weekly updates and deletes on the table. The table has an interleaved sort key on a column that contains AWS Regions.<br>The company wants to reclaim disk space so that the company will not run out of storage space. The company also wants to analyze the sort key column.<br>Which Amazon Redshift command will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM FULL Orders",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM DELETE ONLY Orders",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM REINDEX Orders\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM SORT ONLY Orders"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T03:21:00.000Z",
        "voteCount": 10,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\n\"A full vacuum doesn't perform a reindex for interleaved tables. To reindex interleaved tables followed by a full vacuum, use the VACUUM REINDEX option.\"\nA - \"A full vacuum doesn't perform a reindex for interleaved tables.\"- from the docs above\nB- \"A DELETE ONLY vacuum operation doesn't sort table data.\" - from the docs above\nD - \"without reclaiming space freed by deleted rows. \" - from the docs above"
      },
      {
        "date": "2024-10-02T05:10:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2024-05-08T08:05:00.000Z",
        "voteCount": 1,
        "content": "VACUUM REINDEX makes an additional pass to analyze the interleaved sort keys. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#r_VACUUM_command-parameters"
      },
      {
        "date": "2024-04-13T01:16:00.000Z",
        "voteCount": 1,
        "content": "Reclaiming Space: After updates and deletes, Redshift tables can retain deleted data blocks, taking up space. The VACUUM REINDEX command:\n\nReclaims the space taken up by the deleted rows.\nRebuilds indexes on the sort key columns.\nAnalyzing the Sort Key: Since the sort key column contains AWS Regions, rebuilding the indexes on this column will help cluster data according to region.  This clustering can improve performance for queries that filter or group by region."
      },
      {
        "date": "2024-04-06T02:44:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C\n\nRequirements:\n1. relcaim the disk space\n2. analyze the sork key column\n\nDocument: https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#vacuum-reindex\nVACUUM FULL: A full vacuum doesn't perform a reindex for interleaved tables. To reindex interleaved tables followed by a full vacuum, use the VACUUM REINDEX option.\nVACUUM REINDEX: Analyzes the distribution of the values in interleaved sort key columns, then performs a full VACUUM operation."
      },
      {
        "date": "2024-04-03T11:59:00.000Z",
        "voteCount": 2,
        "content": "FULL is the only one which claims space and sorts. \nFULL\nSorts the specified table (or all tables in the current database) and reclaims disk space occupied by rows that were marked for deletion by previous UPDATE and DELETE operations. VACUUM FULL is the default.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html"
      },
      {
        "date": "2024-03-15T07:43:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2024-03-14T02:00:00.000Z",
        "voteCount": 1,
        "content": "Option C\n\nAnalyzes the distribution of the values in interleaved sort key columns, then performs a full VACUUM operation. If REINDEX is used, a table name is required.\n\nVACUUM REINDEX takes significantly longer than VACUUM FULL because it makes an additional pass to analyze the interleaved sort keys. The sort and merge operation can take longer for interleaved tables because the interleaved sort might need to rearrange more rows than a compound sort.\n\nIf a VACUUM REINDEX operation terminates before it completes, the next VACUUM resumes the reindex operation before performing the full vacuum operation."
      },
      {
        "date": "2024-03-02T22:06:00.000Z",
        "voteCount": 1,
        "content": "The answer should be B. The VACUUM DELETE ONLY command is used in Amazon Redshift to remove rows that have been marked for deletion due to updates and deletes in a table."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/132688-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A manufacturing company wants to collect data from sensors. A data engineer needs to implement a solution that ingests sensor data in near real time.<br>The solution must store the data to a persistent data store. The solution must store the data in nested JSON format. The company must have the ability to query from the data store with a latency of less than 10 milliseconds.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a self-hosted Apache Kafka cluster to capture the sensor data. Store the data in Amazon S3 for querying.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to process the sensor data. Store the data in Amazon S3 for querying.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to capture the sensor data. Store the data in Amazon DynamoDB for querying.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Simple Queue Service (Amazon SQS) to buffer incoming sensor data. Use AWS Glue to store the data in Amazon RDS for querying."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-11T16:53:00.000Z",
        "voteCount": 2,
        "content": "Amazon Kinesis Data Streams is a fully managed service that allows for seamless integration of diverse data sources, including IoT sensors. By using Kinesis Data Streams as the ingestion mechanism, the company can avoid the overhead of setting up and managing an Apache Kafka cluster or other data ingestion pipelines."
      },
      {
        "date": "2024-05-01T18:20:00.000Z",
        "voteCount": 3,
        "content": "near real time = Kinesis Data streams"
      },
      {
        "date": "2024-07-10T05:53:00.000Z",
        "voteCount": 4,
        "content": "to be more accurate, \nKinesis Data streams = real time\nKinesis Data Firehose = near real time"
      },
      {
        "date": "2024-04-16T15:37:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best solution to meet the requirements"
      },
      {
        "date": "2024-03-02T03:48:00.000Z",
        "voteCount": 3,
        "content": "Option C is the best solution to meet the requirements with the least operational overhead:\n\nUse Amazon Kinesis Data Streams to ingest real-time sensor data\nStore the nested JSON data in Amazon DynamoDB for low latency queries\nThe key advantages of Option C are:\n\nKinesis Data Streams fully manages real-time data ingestion with auto-scaling and persistence\nDynamoDB provides single digit millisecond latency for queries\nDynamoDB natively supports nested JSON data models\nFully managed services minimize operational overhead\nIn contrast:\n\nOption A requires managing Kafka clusters\nOption B uses Lambda which can't provide persistent storage\nOption D requires integrating SQS, Glue, and RDS leading to complexity"
      },
      {
        "date": "2024-02-02T05:00:00.000Z",
        "voteCount": 3,
        "content": "Option C, using Amazon Kinesis Data Streams to capture the sensor data and storing it in Amazon DynamoDB for querying, is the best solution to meet the requirements with the least operational overhead. This solution is well-optimized for real-time data ingestion, supports the desired data format, and provides the necessary query performance."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/132689-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores data in a data lake that is in Amazon S3. Some data that the company stores in the data lake contains personally identifiable information (PII). Multiple user groups need to access the raw data. The company must ensure that user groups can access only the PII that they require.<br>Which solution will meet these requirements with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query the data. Set up AWS Lake Formation and create data filters to establish levels of access for the company's IAM roles. Assign each user to the IAM role that matches the user's PII access requirements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon QuickSight to access the data. Use column-level security features in QuickSight to limit the PII that users can retrieve from Amazon S3 by using Amazon Athena. Define QuickSight access levels based on the PII access requirements of the users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom query builder UI that will run Athena queries in the background to access the data. Create user groups in Amazon Cognito. Assign access levels to the user groups based on the PII access requirements of the users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM roles that have different levels of granular access. Assign the IAM roles to IAM user groups. Use an identity-based policy to assign access levels to user groups at the column level."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-01T19:42:00.000Z",
        "voteCount": 3,
        "content": "Amazon Athena to query the data and setting up AWS Lake Formation with data filters, the company can ensure that user groups can access only the personally identifiable information (PII) that they require. The combination of Athena for querying and Lake Formation for access control provides a comprehensive solution for managing PII access requirements effectively and securely"
      },
      {
        "date": "2024-03-03T14:16:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: A\nThe solution that will meet the requirements with the LEAST effort is:\n\nA. Use Amazon Athena to query the data. Set up AWS Lake Formation and create data filters to establish levels of access for the company\u2019s IAM roles. Assign each user to the IAM role that matches the user\u2019s PII access requirements.\n\nThis option leverages AWS Lake Formation to create data filters and establish access levels for IAM roles, providing a straightforward approach to managing user access based on PII requirements."
      },
      {
        "date": "2024-02-02T05:03:00.000Z",
        "voteCount": 4,
        "content": "Option A, using Amazon Athena with AWS Lake Formation, is the most suitable solution. Lake Formation is designed to provide fine-grained access control to data lakes stored in S3 and integrates well with Athena, thereby meeting the requirements with the least effort.\nhttps://aws.amazon.com/blogs/big-data/anonymize-and-manage-data-in-your-data-lake-with-amazon-athena-and-aws-lake-formation/"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/132744-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer must build an extract, transform, and load (ETL) pipeline to process and load data from 10 source systems into 10 tables that are in an Amazon Redshift database. All the source systems generate .csv, JSON, or Apache Parquet files every 15 minutes. The source systems all deliver files into one Amazon S3 bucket. The file sizes range from 10 MB to 20 GB. The ETL pipeline must function correctly despite changes to the data schema.<br>Which data pipeline solutions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to invoke an AWS Glue crawler when a file is loaded into the S3 bucket. Configure an AWS Glue job to process and load the data into the Amazon Redshift tables. Create a second Lambda function to run the AWS Glue job. Create an Amazon EventBridge rule to invoke the second Lambda function when the AWS Glue crawler finishes running successfully.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to invoke an AWS Glue workflow when a file is loaded into the S3 bucket. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Lambda function to invoke an AWS Glue job when a file is loaded into the S3 bucket. Configure the AWS Glue job to read the files from the S3 bucket into an Apache Spark DataFrame. Configure the AWS Glue job to also put smaller partitions of the DataFrame into an Amazon Kinesis Data Firehose delivery stream. Configure the delivery stream to load data into the Amazon Redshift tables."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T08:16:00.000Z",
        "voteCount": 8,
        "content": "Option B: Amazon EventBridge Rule with AWS Glue Workflow Job Every 15 Minutes - for its streamlined process, automated scheduling, and ability to handle schema changes.\n\nOption D: AWS Lambda to Invoke AWS Glue Workflow When a File is Loaded - for its responsiveness to file arrival and adaptability to schema changes, though it is slightly more complex than option B."
      },
      {
        "date": "2024-03-03T14:27:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect! Options C, D and E have issues like unnecessary complexity, latency due to triggers, or limitations in handling large file sizes. So B and A are the best and most robust options that meet all the requirements."
      },
      {
        "date": "2024-03-29T13:59:00.000Z",
        "voteCount": 5,
        "content": "A is NOT correct. The question said \"The ETL pipeline must function correctly despite changes to the data schema\", therefore run Glue crawler is necessary to handle schema changes."
      },
      {
        "date": "2024-05-25T07:27:00.000Z",
        "voteCount": 1,
        "content": "eventbridge rule or event trigger"
      },
      {
        "date": "2024-04-16T15:47:00.000Z",
        "voteCount": 1,
        "content": "ChatCGT sid A and E"
      },
      {
        "date": "2024-05-31T23:18:00.000Z",
        "voteCount": 1,
        "content": "You should double check your information."
      },
      {
        "date": "2024-05-25T07:26:00.000Z",
        "voteCount": 1,
        "content": "ChatCGT? ahahaha. A is NOT correct and E its too complex"
      },
      {
        "date": "2024-04-13T01:24:00.000Z",
        "voteCount": 1,
        "content": "eventbridge rule or event trigger"
      },
      {
        "date": "2024-04-06T03:02:00.000Z",
        "voteCount": 1,
        "content": "I don't think this pipeline should be triggered by an s3 file upload. However seems A cannot handle the data schema change.\n\nif s3 trigger is good, then C and E are unnessessarily complexed. so I would go with B &amp; D (despite the s3 trigger)"
      },
      {
        "date": "2024-04-02T08:49:00.000Z",
        "voteCount": 3,
        "content": "I will go with BD"
      },
      {
        "date": "2024-03-03T14:25:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: AB\nThe two data pipeline solutions that will meet the requirements are:\n\nA. Use an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.\n\nB. Use an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.\n\nThese solutions leverage AWS Glue to process and load the data from different file formats in the S3 bucket into the Amazon Redshift tables, while also handling changes to the data schema."
      },
      {
        "date": "2024-04-17T23:47:00.000Z",
        "voteCount": 1,
        "content": "A is incorret, it doesn't take care to update the data catalog."
      },
      {
        "date": "2024-02-03T09:21:00.000Z",
        "voteCount": 1,
        "content": "The correct answers are A. Use an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables and B. Use an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables. These solutions automate the ETL pipeline with minimal operational overhead."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/amazon/view/132694-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A financial company wants to use Amazon Athena to run on-demand SQL queries on a petabyte-scale dataset to support a business intelligence (BI) application. An AWS Glue job that runs during non-business hours updates the dataset once every day. The BI application has a standard data refresh frequency of 1 hour to comply with company policies.<br>A data engineer wants to cost optimize the company's use of Amazon Athena without adding any additional infrastructure costs.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon S3 Lifecycle policy to move data to the S3 Glacier Deep Archive storage class after 1 day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the query result reuse feature of Amazon Athena for the SQL queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Amazon ElastiCache cluster between the BI application and Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the format of the files that are in the dataset to Apache Parquet."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T05:31:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\nUse the Query Result Reuse Feature of Amazon Athena. This leverages Athena's built-in feature to reduce redundant data scans and thus lowers query costs."
      },
      {
        "date": "2024-05-06T05:35:00.000Z",
        "voteCount": 1,
        "content": "Yes, seems to be B: https://aws.amazon.com/de/blogs/big-data/reduce-cost-and-improve-query-performance-with-amazon-athena-query-result-reuse/"
      },
      {
        "date": "2024-08-02T01:07:00.000Z",
        "voteCount": 1,
        "content": "D. Because \"query reuse feature\" is reliable only when it's identical but here hourly refresh might be on data related to that hour."
      },
      {
        "date": "2024-08-01T08:05:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2024-04-16T15:52:00.000Z",
        "voteCount": 2,
        "content": "B. Use the query result reuse feature of Amazon Athena for the SQL queries."
      },
      {
        "date": "2024-03-27T06:12:00.000Z",
        "voteCount": 1,
        "content": "It's B: Glacier adds more retrieval time and the other options cost some money"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/amazon/view/132695-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company's data engineer needs to optimize the performance of table SQL queries. The company stores data in an Amazon Redshift cluster. The data engineer cannot increase the size of the cluster because of budget constraints.<br>The company stores the data in multiple tables and loads the data by using the EVEN distribution style. Some tables are hundreds of gigabytes in size. Other tables are less than 10 MB in size.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep using the EVEN distribution style for all tables. Specify primary and foreign keys for all tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ALL distribution style for large tables. Specify primary and foreign keys for all tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ALL distribution style for rarely updated small tables. Specify primary and foreign keys for all tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify a combination of distribution, sort, and partition keys for all tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T05:39:00.000Z",
        "voteCount": 8,
        "content": "Use the ALL Distribution Style for Rarely Updated Small Tables. This approach optimizes the performance of joins involving these smaller tables and is a common best practice in Redshift data warehousing. For the larger tables, maintaining the EVEN distribution style or considering a KEY-based distribution (if there are common join columns) could be more appropriate."
      },
      {
        "date": "2024-06-11T17:23:00.000Z",
        "voteCount": 3,
        "content": "For small tables (less than 10 MB in size) that are rarely updated, using the ALL distribution style can provide better query performance. With the ALL distribution style, each compute node stores a copy of the entire table, eliminating the need for data redistribution or shuffling during certain queries. This can significantly improve query performance, especially for joins and aggregations involving small tables."
      },
      {
        "date": "2024-05-06T05:44:00.000Z",
        "voteCount": 2,
        "content": "\"ALL distribution is appropriate only for relatively slow moving tables; that is, tables that are not updated frequently or extensively.\" (https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html)"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/amazon/view/135424-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company receives .csv files that contain physical address data. The data is in columns that have the following names: Door_No, Street_Name, City, and Zip_Code. The company wants to create a single column to store these values in the following format:<br><img title=\"image1\" src=\"https://img.examtopics.com/aws-certified-data-engineer-associate-dea-c01/image1.png\"><br>Which solution will meet this requirement with the LEAST coding effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew to read the files. Use the NEST_TO_ARRAY transformation to create the new column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew to read the files. Use the NEST_TO_MAP transformation to create the new column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew to read the files. Use the PIVOT transformation to create the new column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Lambda function in Python to read the files. Use the Python data dictionary type to create the new column."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T02:29:00.000Z",
        "voteCount": 11,
        "content": "NEST_TO_ARRAY would result in:\n[  {\"key\": \"key1\", \"value\": \"value1\"},  {\"key\": \"key2\", \"value\": \"value2\"},  {\"key\": \"key3\", \"value\": \"value3\"}]\n\nwhile NEST_TO_MAP results: {\n  \"key1\": \"value1\",\n  \"key2\": \"value2\",\n  \"key3\": \"value3\"\n}\nTherefore go with B"
      },
      {
        "date": "2024-06-11T17:26:00.000Z",
        "voteCount": 3,
        "content": "The NEST_TO_MAP transformation is specifically designed to convert data from nested structures (like rows in a CSV) into key-value pairs,\nperfectly matching the requirement of creating a new column with address components as key-value pairs"
      },
      {
        "date": "2024-04-16T15:58:00.000Z",
        "voteCount": 4,
        "content": "AWS Glue DataBrew is a visual data preparation tool that allows for easy transformation of data without requiring extensive coding. The NEST_TO_MAP transformation in DataBrew allows you to convert columns into a JSON map, which aligns with the desired JSON format for the address data."
      },
      {
        "date": "2024-03-19T04:16:00.000Z",
        "voteCount": 1,
        "content": "Come on guys. That's and array there so..."
      },
      {
        "date": "2024-03-20T10:16:00.000Z",
        "voteCount": 2,
        "content": "I take that back. I will go with B because NEST_TO_ARRAY  is not suitable for the desired JSON format where each attribute has its own key."
      },
      {
        "date": "2024-03-14T03:10:00.000Z",
        "voteCount": 2,
        "content": "Option B:\nNEST_TO_MAP: Converts user-selected columns into key-value pairs, each with a key representing the column name and a value representing the row value. The order of the selected column is not maintained while creating the resultant map. The different column data types are typecast to a common type that supports the data types of all columns.\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_MAP.html\n\nPIVOT: Converts all the row values in a selected column into individual columns with values.\n\nNEST_TO_ARRAY: Converts user-selected columns into array values. The order of the selected columns is maintained while creating the resultant array. The different column data types are typecast to a common type that supports the data types of all columns."
      },
      {
        "date": "2024-03-07T07:01:00.000Z",
        "voteCount": 1,
        "content": "Ans. A\nNEST_TO_ARRAY Converts user-selected columns into array values. The order of the selected columns is maintained while creating the resultant array. \nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_ARRAY.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/amazon/view/132696-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company receives call logs as Amazon S3 objects that contain sensitive customer information. The company must protect the S3 objects by using encryption. The company must also use encryption keys that only specific employees can access.<br>Which solution will meet these requirements with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS CloudHSM cluster to store the encryption keys. Configure the process that writes to Amazon S3 to make calls to CloudHSM to encrypt and decrypt the objects. Deploy an IAM policy that restricts access to the CloudHSM cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with customer-provided keys (SSE-C) to encrypt the objects that contain customer information. Restrict access to the keys that encrypt the objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the KMS keys that encrypt the objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the Amazon S3 managed keys that encrypt the objects."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-16T16:00:00.000Z",
        "voteCount": 3,
        "content": "C. Use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the KMS keys that encrypt the objects.\n\nServer-side encryption with AWS KMS (SSE-KMS) provides strong encryption for S3 objects while allowing fine-grained access control through AWS Key Management Service (KMS). With SSE-KMS, you can control access to encryption keys using IAM policies, ensuring that only specific employees can access them.\n\nThis solution requires minimal effort as it leverages AWS's managed encryption service (SSE-KMS) and integrates seamlessly with S3. Additionally, IAM policies can be easily configured to restrict access to the KMS keys, providing granular control over who can access the encryption keys."
      },
      {
        "date": "2024-04-13T01:56:00.000Z",
        "voteCount": 3,
        "content": "Encryption at Rest: SSE-KMS provides robust encryption of the sensitive call log data while it's stored in S3.\nKey Management and Access Control: AWS KMS offers centralized key management. You can easily create and manage KMS keys (Customer Master Keys - CMKs) and use fine-grained IAM policies to restrict access to specific employees.\nMinimal Effort: SSE-KMS is a built-in S3 feature. Enabling it requires minimal configuration and no custom code for encryption/decryption."
      },
      {
        "date": "2024-03-27T08:23:00.000Z",
        "voteCount": 1,
        "content": "KMS because you can restrict access and of course for pricing ;)"
      },
      {
        "date": "2024-03-19T04:25:00.000Z",
        "voteCount": 4,
        "content": "Least effort = C"
      },
      {
        "date": "2024-02-02T05:47:00.000Z",
        "voteCount": 4,
        "content": "Option D does not provide the ability to restrict access to the encryption keys"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/amazon/view/132697-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores petabytes of data in thousands of Amazon S3 buckets in the S3 Standard storage class. The data supports analytics workloads that have unpredictable and variable data access patterns.<br>The company does not access some data for months. However, the company must be able to retrieve all data within milliseconds. The company needs to optimize S3 storage costs.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Storage Lens standard metrics to determine when to move objects to more cost-optimized storage classes. Create S3 Lifecycle policies for the S3 buckets to move objects to cost-optimized storage classes. Continue to refine the S3 Lifecycle policies in the future to optimize storage costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Storage Lens activity metrics to identify S3 buckets that the company accesses infrequently. Configure S3 Lifecycle rules to move objects from S3 Standard to the S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier storage classes based on the age of the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Intelligent-Tiering. Activate the Deep Archive Access tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Intelligent-Tiering. Use the default access tier.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T04:35:00.000Z",
        "voteCount": 11,
        "content": "Although C is more cost-effective, because of \"must be able to retrieve all data within milliseconds\" will go with D"
      },
      {
        "date": "2024-07-14T02:24:00.000Z",
        "voteCount": 1,
        "content": "Based on this docs https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\nD will be appropriate as it allows for instant retrieval"
      },
      {
        "date": "2024-06-22T11:52:00.000Z",
        "voteCount": 1,
        "content": "Staying with \"D\"... The Amazon S3 Glacier Deep Archive storage class is designed for long-term data archiving where data retrieval times are flexible. It does not offer millisecond retrieval times. Instead, data retrieval from S3 Glacier Deep Archive typically takes 12 hours or more. For millisecond retrieval times, you would use the S3 Standard, S3 Standard-IA, or S3 One Zone-IA storage classes, which are designed for frequent or infrequent access with low latency."
      },
      {
        "date": "2024-04-22T09:05:00.000Z",
        "voteCount": 1,
        "content": "I am confused with C or D"
      },
      {
        "date": "2024-04-18T06:04:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\n\n\"Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds.\"\nhttps://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/"
      },
      {
        "date": "2024-05-28T03:43:00.000Z",
        "voteCount": 2,
        "content": "But C doesn't say anything about Instant Retrieval."
      },
      {
        "date": "2024-04-13T01:58:00.000Z",
        "voteCount": 1,
        "content": "least operation overhead, D"
      },
      {
        "date": "2024-04-06T03:40:00.000Z",
        "voteCount": 2,
        "content": "The correct answer may be D. Intelligent tiering's default access tier is:\n1. accessed less than 30 days: frequent access tier\n2. not accessed in 30-90 days: Infrequent Access tier\n3. not accessed more than 90 days: Archive Instant Access tier\nOther tiers require more retrieve time need activation.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html"
      },
      {
        "date": "2024-03-15T21:58:00.000Z",
        "voteCount": 2,
        "content": "Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds"
      },
      {
        "date": "2024-03-14T03:46:00.000Z",
        "voteCount": 1,
        "content": "A few remarks: Data should be retrieved in ms. This means all the options with Glacier are wrong: BC\n\nFor D how you can set the S3 intelligent-Tiering if the current class is Standard? \nI guess you need a lifecycle policy.\n\nWhich leaves only A as an option.\n\nThoughts?"
      },
      {
        "date": "2024-03-07T09:17:00.000Z",
        "voteCount": 1,
        "content": "D. is correct"
      },
      {
        "date": "2024-03-03T15:01:00.000Z",
        "voteCount": 1,
        "content": "Option C. Use S3 Intelligent-Tiering. Activate the Deep Archive Access tier.\n\n\n\nBy using S3 Intelligent-Tiering and activating the Deep Archive Access tier, the company can optimize S3 storage costs with minimal operational overhead. S3 Intelligent-Tiering automatically moves objects between four access tiers, including the Deep Archive Access tier, based on changing access patterns and cost optimization. This eliminates the need for manual lifecycle policies and constant refinement, as the storage class is adjusted automatically based on data access patterns, resulting in cost savings while ensuring quick access to all data when needed."
      },
      {
        "date": "2024-02-02T05:56:00.000Z",
        "voteCount": 2,
        "content": "Option D, using S3 Intelligent-Tiering with the default access tier, will meet the requirements best. It provides a hands-off approach to storage cost optimization while ensuring that data is available for analytics workloads within the required timeframe."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/amazon/view/132698-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "During a security review, a company identified a vulnerability in an AWS Glue job. The company discovered that credentials to access an Amazon Redshift cluster were hard coded in the job script.<br>A data engineer must remediate the security vulnerability in the AWS Glue job. The solution must securely store the credentials.<br>Which combination of steps should the data engineer take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in the AWS Glue job parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in a configuration file that is in an Amazon S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess the credentials from a configuration file that is in an Amazon S3 bucket by using the AWS Glue job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in AWS Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the AWS Glue job IAM role access to the stored credentials.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T04:37:00.000Z",
        "voteCount": 9,
        "content": "D because it's AWS best practice for securing creds and E because after you put cred in secrets you will need permissions for accesing"
      },
      {
        "date": "2024-03-07T09:19:00.000Z",
        "voteCount": 1,
        "content": "Ans, DE"
      },
      {
        "date": "2024-02-02T06:01:00.000Z",
        "voteCount": 2,
        "content": "D. Store the credentials in AWS Secrets Manager: AWS Secrets Manager is a service that helps you protect access to your applications, services, and IT resources without the upfront investment and on-going maintenance costs of operating your own infrastructure. It's specifically designed for storing and retrieving credentials securely, and therefore, it is an appropriate choice for handling the Redshift cluster credentials.\n\nE. Grant the AWS Glue job IAM role access to the stored credentials: IAM roles for AWS Glue will allow the job to assume a role with the necessary permissions to access the credentials in AWS Secrets Manager. This method avoids embedding credentials directly in the script or a configuration file and allows for centralized management of the credentials."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/amazon/view/132699-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer uses Amazon Redshift to run resource-intensive analytics processes once every month. Every month, the data engineer creates a new Redshift provisioned cluster. The data engineer deletes the Redshift provisioned cluster after the analytics processes are complete every month. Before the data engineer deletes the cluster each month, the data engineer unloads backup data from the cluster to an Amazon S3 bucket.<br>The data engineer needs a solution to run the monthly analytics processes that does not require the data engineer to manage the infrastructure manually.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Step Functions to pause the Redshift cluster when the analytics processes are complete and to resume the cluster to run new processes every month.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift Serverless to automatically process the analytics workload.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to automatically process the analytics workload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation templates to automatically process the analytics workload."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-27T06:00:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-13T02:04:00.000Z",
        "voteCount": 4,
        "content": "Fully Managed, Serverless: Redshift Serverless eliminates the need to manually create, manage, or delete clusters. It automatically scales resources based on the workload, reducing operational overhead significantly.\nCost-Effective for Infrequent Workloads: Since the analytics processes run only once a month, Redshift Serverless's pay-per-use model is ideal for minimizing costs during downtime.\nSeamless S3 Integration: Redshift Serverless natively integrates with S3 for backup and restore operations, ensuring compatibility with the existing process."
      },
      {
        "date": "2024-03-19T04:38:00.000Z",
        "voteCount": 2,
        "content": "\"does not require to manage the infrastructure manually\" = Serverless"
      },
      {
        "date": "2024-03-07T09:45:00.000Z",
        "voteCount": 1,
        "content": "Ans. B"
      },
      {
        "date": "2024-03-03T15:13:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer B: Options A, C and D still involve manual tasks like administering CloudFormation stacks, using AWS CLI commands, or configuring Step Function state machines.\n\nBy leveraging Redshift Serverless, the data engineer avoids all cluster and infrastructure administration effort. This has the least operational overhead to run the monthly"
      },
      {
        "date": "2024-02-02T06:04:00.000Z",
        "voteCount": 2,
        "content": "Use Amazon Redshift Serverless. This option allows the data engineer to focus on the analytics processes themselves without worrying about cluster provisioning, scaling, or management. It provides an on-demand, serverless solution that can handle variable workloads and is cost-effective for intermittent and irregular processing needs like those described."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/amazon/view/132700-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company receives a daily file that contains customer data in .xls format. The company stores the file in Amazon S3. The daily file is approximately 2 GB in size.<br>A data engineer concatenates the column in the file that contains customer first names and the column that contains customer last names. The data engineer needs to determine the number of distinct customers in the file.<br>Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and run an Apache Spark job in an AWS Glue notebook. Configure the job to read the S3 file and calculate the number of distinct customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler to create an AWS Glue Data Catalog of the S3 file. Run SQL queries from Amazon Athena to calculate the number of distinct customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and run an Apache Spark job in Amazon EMR Serverless to calculate the number of distinct customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew to create a recipe that uses the COUNT_DISTINCT aggregate function to calculate the number of distinct customers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T06:07:00.000Z",
        "voteCount": 9,
        "content": "AWS Glue DataBrew: AWS Glue DataBrew is a visual data preparation tool that allows data engineers and data analysts to clean and normalize data without writing code. Using DataBrew, a data engineer could create a recipe that includes the concatenation of the customer first and last names and then use the COUNT_DISTINCT function. This would not require complex code and could be performed through the DataBrew user interface, representing a lower operational effort."
      },
      {
        "date": "2024-06-11T17:42:00.000Z",
        "voteCount": 2,
        "content": "DataBrew supports various transformations,\nincluding the COUNT_DISTINCT function, which is ideal for calculating the number of unique values in a column (combined first and last names in this case)."
      },
      {
        "date": "2024-04-17T05:30:00.000Z",
        "voteCount": 2,
        "content": "go in D"
      },
      {
        "date": "2024-04-02T12:50:00.000Z",
        "voteCount": 2,
        "content": "since it's less operational effort, I would go in D"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/amazon/view/132765-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A healthcare company uses Amazon Kinesis Data Streams to stream real-time health data from wearable devices, hospital equipment, and patient records.<br>A data engineer needs to find a solution to process the streaming data. The data engineer needs to store the data in an Amazon Redshift Serverless warehouse. The solution must support near real-time analytics of the streaming data and the previous day's data.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad data into Amazon Kinesis Data Firehose. Load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the streaming ingestion feature of Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Amazon S3. Use the COPY command to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Aurora zero-ETL integration with Amazon Redshift."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T01:05:00.000Z",
        "voteCount": 7,
        "content": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\nUse the Streaming Ingestion Feature of Amazon Redshift: Amazon Redshift recently introduced streaming data ingestion, allowing Redshift to consume data directly from Kinesis Data Streams in near real-time. This feature simplifies the architecture by eliminating the need for intermediate steps or services, and it is specifically designed to support near real-time analytics. The operational overhead is minimal since the feature is integrated within Redshift."
      },
      {
        "date": "2024-05-27T06:01:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-02T12:53:00.000Z",
        "voteCount": 1,
        "content": "I'd go in B"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/amazon/view/132701-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to use an Amazon QuickSight dashboard that is based on Amazon Athena queries on data that is stored in an Amazon S3 bucket. When the data engineer connects to the QuickSight dashboard, the data engineer receives an error message that indicates insufficient permissions.<br>Which factors could cause to the permissions-related errors? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no connection between QuickSight and Athena.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Athena tables are not cataloged.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuickSight does not have access to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuickSight does not have access to decrypt S3 data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no IAM role assigned to QuickSight."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T04:32:00.000Z",
        "voteCount": 7,
        "content": "C and D\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html\n\nE is incorrect because it will result in authentication/authorization error, not insufficient permission error."
      },
      {
        "date": "2024-02-02T06:16:00.000Z",
        "voteCount": 6,
        "content": "C. QuickSight does not have access to the S3 bucket: Amazon QuickSight needs to have the necessary permissions to access the S3 bucket where the data resides. If QuickSight lacks the permissions to read the data from the S3 bucket, it would result in an error indicating insufficient permissions.\n\nD. QuickSight does not have access to decrypt S3 data: If the data in S3 is encrypted, QuickSight needs permissions to use the necessary keys to decrypt the data. Without access to the decryption keys, typically managed by AWS Key Management Service (KMS), QuickSight cannot read the encrypted data and would give an error."
      },
      {
        "date": "2024-07-02T01:59:00.000Z",
        "voteCount": 1,
        "content": "C. QuickSight does not have access to the S3 bucket. Amazon QuickSight needs to have the necessary permissions to access the Amazon S3 bucket where the data is stored. If these permissions are not correctly configured, QuickSight will not be able to access the data, resulting in an error.\n\nE. There is no IAM role assigned to QuickSight. Amazon QuickSight uses AWS Identity and Access Management (IAM) roles to access AWS resources. If QuickSight is not assigned an IAM role, or if the assigned role does not have the necessary permissions, QuickSight will not be able to access the resources it needs, leading to an error."
      },
      {
        "date": "2024-04-17T05:34:00.000Z",
        "voteCount": 1,
        "content": "C and D"
      },
      {
        "date": "2024-04-13T02:11:00.000Z",
        "voteCount": 3,
        "content": "The two most likely factors causing the permissions-related errors are:\n\nC. QuickSight does not have access to the S3 bucket. To access data from an S3 bucket, QuickSight needs explicit S3 permissions. This is typically handled through an IAM role associated with the QuickSight service.\nD. QuickSight does not have access to decrypt S3 data. If the data in S3 is encrypted (e.g., using KMS), QuickSight must have the necessary permissions to decrypt the data using the relevant KMS key.\nLet's analyze why the other options are less likely the primary culprits:\n\n\nE. There is no IAM role assigned to QuickSight. QuickSight needs an IAM role for overall functionality. A missing role would likely cause broader service failures, not specific data access errors."
      },
      {
        "date": "2024-03-17T15:16:00.000Z",
        "voteCount": 1,
        "content": "I think the assumptions in the problem are insufficient. If the data is encrypted, then D can be the correct answer, but if not, then E is the correct answer."
      },
      {
        "date": "2024-03-07T13:10:00.000Z",
        "voteCount": 1,
        "content": "Ans. CD\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/amazon/view/132702-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores datasets in JSON format and .csv format in an Amazon S3 bucket. The company has Amazon RDS for Microsoft SQL Server databases, Amazon DynamoDB tables that are in provisioned capacity mode, and an Amazon Redshift cluster. A data engineering team must develop a solution that will give data scientists the ability to query all data sources by using syntax similar to SQL.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use Amazon Athena to query the data. Use SQL for structured data sources. Use PartiQL for data that is stored in JSON format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use Redshift Spectrum to query the data. Use SQL for structured data sources. Use PartiQL for data that is stored in JSON format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use AWS Glue jobs to transform data that is in JSON format to Apache Parquet or .csv format. Store the transformed data in an S3 bucket. Use Amazon Athena to query the original and transformed data from the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lake Formation to create a data lake. Use Lake Formation jobs to transform the data from all data sources to Apache Parquet format. Store the transformed data in an S3 bucket. Use Amazon Athena or Redshift Spectrum to query the data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T06:19:00.000Z",
        "voteCount": 6,
        "content": "LEAST operational overhead? query straight with Athena without any intermediate actions or services"
      },
      {
        "date": "2024-06-11T17:54:00.000Z",
        "voteCount": 1,
        "content": "thena natively supports querying JSON data stored in S3 using standard SQL functions.\nThis eliminates the need for additional data transformation steps using Glue jobs (as required in Option C or D)."
      },
      {
        "date": "2024-05-28T04:14:00.000Z",
        "voteCount": 1,
        "content": "As chris_spencer mentioned below, now Athena supports querying with PartiQL which technically makes the answer A correct."
      },
      {
        "date": "2024-05-22T08:10:00.000Z",
        "voteCount": 1,
        "content": "B requires Redshift Spectrum, so A"
      },
      {
        "date": "2024-04-18T06:14:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C.\n\nAmazon Athena does not support querying with PartiQL until 16.04.2024, https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-athena-federated-query-pass-through/\n\nThe DEA01 exam should not have include the latest feature"
      },
      {
        "date": "2024-04-13T02:16:00.000Z",
        "voteCount": 4,
        "content": "A. Unified Querying with Athena: Athena provides a SQL-like interface for querying various data sources, including JSON and CSV in S3, as well as traditional databases.\nPartiQL Support: Athena's PartiQL extension allows querying semi-structured JSON data directly, eliminating the need for a separate query engine.\nServerless and Managed: Both AWS Glue and Athena are serverless, minimizing infrastructure management for the data engineers.\nNo Unnecessary Transformations: Avoiding transformations for JSON data simplifies the pipeline and reduces operational overhead.\nB. Redshift Spectrum: While Spectrum can query external data, it's primarily intended for Redshift data warehouse extensions. It adds complexity for the RDS and DynamoDB data sources."
      },
      {
        "date": "2024-04-03T13:50:00.000Z",
        "voteCount": 4,
        "content": "I will go with B"
      },
      {
        "date": "2024-04-06T05:37:00.000Z",
        "voteCount": 1,
        "content": "B is the best choice:\nAWS Glue Data Catalog: AWS Glue can crawl and catalog the data sources (S3 buckets, RDS databases, DynamoDB tables) and store the metadata in the AWS Glue Data Catalog. This provides a centralized metadata repository for all data sources.\nAmazon Redshift Spectrum: Redshift Spectrum is a feature of Amazon Redshift that allows you to query data directly from various data sources, including S3 buckets, without loading the data into Redshift tables. This means you can query the JSON and CSV files in S3, as well as the RDS and DynamoDB data sources, using standard SQL syntax.\nSQL and PartiQL Support: Redshift Spectrum supports querying structured data sources (like RDS and CSV files) using SQL, and querying semi-structured data sources (like JSON files) using PartiQL, which is a SQL-compatible query language for JSON data."
      },
      {
        "date": "2024-03-30T15:12:00.000Z",
        "voteCount": 4,
        "content": "The answer should be B.\nA is incorrect because Athena does NOT support PartiQL.\nC is NOT the least operational (has the additional step to convert JSON to Parquet or csv)\nD is incorrect because DynamoDB export data to S3 in DynamoDB JSON or Amzone Ion format only (https://aws.amazon.com/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake-amazon-s3/)."
      },
      {
        "date": "2024-03-27T20:09:00.000Z",
        "voteCount": 2,
        "content": "AWS Athena can only query in SQL, not PartiQL, so both A and B are incorrect. LakeFormation can not work directly with DynamoDB, so D is incorrect. \nThe only acceptable answer is C"
      },
      {
        "date": "2024-04-07T09:57:00.000Z",
        "voteCount": 1,
        "content": "similar to SQL, so A"
      },
      {
        "date": "2024-02-02T06:20:00.000Z",
        "voteCount": 3,
        "content": "Option A, using AWS Glue and Amazon Athena, would meet the requirements with the least operational overhead. This solution allows data scientists to directly query data in its original format without the need for additional data transformation steps, making it easier to implement and manage."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/amazon/view/132703-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is configuring Amazon SageMaker Studio to use AWS Glue interactive sessions to prepare data for machine learning (ML) models.<br>The data engineer receives an access denied error when the data engineer tries to prepare the data by using SageMaker Studio.<br>Which change should the engineer make to gain access to SageMaker Studio?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the AWSGlueServiceRole managed policy to the data engineer's IAM user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a policy to the data engineer's IAM user that includes the sts:AssumeRole action for the AWS Glue and SageMaker service principals in the trust policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the AmazonSageMakerFullAccess managed policy to the data engineer's IAM user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a policy to the data engineer's IAM user that allows the sts:AddAssociation action for the AWS Glue and SageMaker service principals in the trust policy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T04:24:00.000Z",
        "voteCount": 6,
        "content": "I don't believe you're supposed to assign a FullAccess policy, so I will go with B."
      },
      {
        "date": "2024-03-19T06:34:00.000Z",
        "voteCount": 5,
        "content": "I will go with B since you can get access denied even with the AmazonSageMakerFullAccess.\n See here: https://stackoverflow.com/questions/64709871/aws-sagemaker-studio-createdomain-access-error"
      },
      {
        "date": "2024-10-07T02:08:00.000Z",
        "voteCount": 1,
        "content": "B. the engineer needs to assume specific roles to allow interaction between these services. The sts:AssumeRole action is necessary for this purpose"
      },
      {
        "date": "2024-08-24T14:56:00.000Z",
        "voteCount": 2,
        "content": "B, this approach involves setting up the trust relationship for roles. It is not a typical requirement for resolving access issues with SageMaker Studio directly."
      },
      {
        "date": "2024-07-15T19:02:00.000Z",
        "voteCount": 1,
        "content": "OPtion A\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-is-security.html"
      },
      {
        "date": "2024-07-15T19:03:00.000Z",
        "voteCount": 1,
        "content": "and You can attach AWSGlueServiceRole to your users, groups, and roles."
      },
      {
        "date": "2024-09-27T08:16:00.000Z",
        "voteCount": 1,
        "content": "Sorry changed my mind option B makes most sense"
      },
      {
        "date": "2024-04-13T02:22:00.000Z",
        "voteCount": 1,
        "content": "SageMaker Permissions: The AmazonSageMakerFullAccess managed policy provides broad permissions for using Amazon SageMaker features, including SageMaker Studio and the ability to interact with other AWS services like AWS Glue.\nLeast Privilege: While this policy is quite permissive, it's the most direct solution to the immediate access issue. After resolving the error, you can refine permissions for a more granular approach."
      },
      {
        "date": "2024-04-03T13:56:00.000Z",
        "voteCount": 3,
        "content": "I will go with C"
      },
      {
        "date": "2024-04-06T05:39:00.000Z",
        "voteCount": 2,
        "content": "Option A (AWSGlueServiceRole managed policy) is not relevant, as this policy is intended for the AWS Glue service itself, not for users accessing SageMaker Studio.\nOption B (adding a policy with sts:AssumeRole action) is not necessary, as SageMaker handles the role assumption process internally.\nOption D (sts:AddAssociation action) is not a valid action and is not required for accessing SageMaker Studio or using AWS Glue interactive sessions."
      },
      {
        "date": "2024-03-24T04:50:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/knowledge-center/sagemaker-featuregroup-troubleshooting"
      },
      {
        "date": "2024-03-07T13:28:00.000Z",
        "voteCount": 2,
        "content": "Ans. C\nhttps://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonSageMakerFullAccess.html"
      },
      {
        "date": "2024-02-16T22:43:00.000Z",
        "voteCount": 2,
        "content": "B. Add a policy to the data engineer\u2019s IAM user that includes the sts:AssumeRole action for the AWS Glue and SageMaker service principals in the trust policy.\n\n\t\u2022\tThis is the most appropriate solution. The sts:AssumeRole action allows the data engineer\u2019s IAM user to assume a role that has the necessary permissions for both AWS Glue and SageMaker. This is a common approach for granting cross-service access in AWS."
      },
      {
        "date": "2024-02-02T06:23:00.000Z",
        "voteCount": 3,
        "content": "Amazon SageMaker requires permissions to perform actions on your behalf. By attaching the AmazonSageMakerFullAccess managed policy to the data engineer\u2019s IAM user, you grant the necessary permissions for SageMaker Studio to access AWS Glue and other related services."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/amazon/view/132706-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company extracts approximately 1 TB of data every day from data sources such as SAP HANA, Microsoft SQL Server, MongoDB, Apache Kafka, and Amazon DynamoDB. Some of the data sources have undefined data schemas or data schemas that change.<br>A data engineer must implement a solution that can detect the schema for these data sources. The solution must extract, transform, and load the data to an Amazon S3 bucket. The company has a service level agreement (SLA) to load the data into the S3 bucket within 15 minutes of data creation.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a PySpark program in AWS Lambda to extract, transform, and load the data into the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stored procedure in Amazon Redshift to detect the schema and to extract, transform, and load the data into a Redshift Spectrum table. Access the table from Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T06:35:00.000Z",
        "voteCount": 6,
        "content": "Least effort = B"
      },
      {
        "date": "2024-02-02T06:28:00.000Z",
        "voteCount": 5,
        "content": "B. Use AWS Glue to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark."
      },
      {
        "date": "2024-04-13T02:26:00.000Z",
        "voteCount": 1,
        "content": "Glue ETL"
      },
      {
        "date": "2024-03-14T04:19:00.000Z",
        "voteCount": 3,
        "content": "The option with the least operational overhead is B."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/amazon/view/132707-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has multiple applications that use datasets that are stored in an Amazon S3 bucket. The company has an ecommerce application that generates a dataset that contains personally identifiable information (PII). The company has an internal analytics application that does not require access to the PII.<br>To comply with regulations, the company must not share PII unnecessarily. A data engineer needs to implement a solution that with redact PII dynamically, based on the needs of each application that accesses the dataset.<br>Which solution will meet the requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket policy to limit the access each application has. Create multiple copies of the dataset. Give each dataset copy the appropriate level of redaction for the needs of the application that accesses the copy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Object Lambda endpoint. Use the S3 Object Lambda endpoint to read data from the S3 bucket. Implement redaction logic within an S3 Object Lambda function to dynamically redact PII based on the needs of each application that accesses the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to transform the data for each application. Create multiple copies of the dataset. Give each dataset copy the appropriate level of redaction for the needs of the application that accesses the copy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API Gateway endpoint that has custom authorizers. Use the API Gateway endpoint to read data from the S3 bucket. Initiate a REST API call to dynamically redact PII based on the needs of each application that accesses the data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-18T22:36:00.000Z",
        "voteCount": 1,
        "content": "It's B based on AWS documentation\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transforming-objects.html"
      },
      {
        "date": "2024-06-11T18:09:00.000Z",
        "voteCount": 2,
        "content": "S3 Object Lambda automatically triggers the Lambda function only when there's a request to access data in the S3 bucket. This eliminates the need for pre-processing or creating multiple data copies with varying levels of redaction (Options A and C)."
      },
      {
        "date": "2024-05-27T06:02:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-03-07T13:40:00.000Z",
        "voteCount": 4,
        "content": "Ans. B\nYou can use an Amazon S3 Object Lambda Access Point to control access to documents with personally identifiable information (PII).\nhttps://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html"
      },
      {
        "date": "2024-02-16T22:56:00.000Z",
        "voteCount": 1,
        "content": "S3 Object Lambda allows you to add custom processing, such as redaction of PII, to data retrieved from S3. This is done dynamically, meaning you don\u2019t need to store multiple copies of the data. It\u2019s a more efficient and operationally simpler approach compared to managing multiple dataset versions."
      },
      {
        "date": "2024-02-02T06:31:00.000Z",
        "voteCount": 3,
        "content": "Amazon S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it is returned to an application. For example, you could use an S3 Object Lambda to dynamically redact personally identifiable information (PII) from data retrieved from S3. This would allow you to control access to sensitive information based on the needs of different applications, without having to create and manage multiple copies of your data."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/amazon/view/132708-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to build an extract, transform, and load (ETL) job. The ETL job will process daily incoming .csv files that users upload to an Amazon S3 bucket. The size of each S3 object is less than 100 MB.<br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom Python application. Host the application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a PySpark ETL script. Host the script on an Amazon EMR cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an AWS Glue PySpark job. Use Apache Spark to transform the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an AWS Glue Python shell job. Use pandas to transform the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-27T20:26:00.000Z",
        "voteCount": 10,
        "content": "AWS Glue Python Shell Job is billed $0.44 per DPU-Hour for each job\nAWS Glue PySpark is billed $0.29 per DPU-Hour for each job with flexible execution and $0.44 per DPU-Hour for each job with standard execution\nSource: https://aws.amazon.com/glue/pricing/"
      },
      {
        "date": "2024-07-11T00:41:00.000Z",
        "voteCount": 2,
        "content": "thats true for the 1 DPU, but thats not good because the minimum DPU for PySpark Job is 1 DPU. But for Python Job the minimum DPU is 0.0625. So the Python job is way more cheaper for small dataset and quick ETL transformation"
      },
      {
        "date": "2024-02-16T23:03:00.000Z",
        "voteCount": 6,
        "content": "Option D: Write an AWS Glue Python shell job and use pandas to transform the data, is the most cost-effective solution for the described scenario.\n\nAWS Glue\u2019s Python shell jobs are a good fit for smaller-scale ETL tasks, especially when dealing with .csv files that are less than 100 MB each. The use of pandas, a powerful and efficient data manipulation library in Python, makes it an ideal tool for processing and transforming these types of files. This approach avoids the overhead and additional costs associated with more complex solutions like Amazon EKS or EMR, which are generally more suited for larger-scale, more complex data processing tasks.\n\nGiven the requirements \u2013 processing daily incoming small-sized .csv files \u2013 this solution provides the necessary functionality with minimal resources, aligning well with the goal of cost-effectiveness."
      },
      {
        "date": "2024-07-15T19:13:00.000Z",
        "voteCount": 2,
        "content": "going with D https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html"
      },
      {
        "date": "2024-06-11T18:19:00.000Z",
        "voteCount": 5,
        "content": "good candidate to be (2 options) for real, either spark and py have similar approaches. I would go with Pandas, although... 50/50.. it could be Spark. I hope not to find this question in the exam"
      },
      {
        "date": "2024-05-21T04:18:00.000Z",
        "voteCount": 3,
        "content": "PySpark with Spark(Flexible Execution): $0.29/hr for 1 DPU\nPySpark with Spark(Standard Execution): $0.44/hr for 1 DPU\nPython Shell with Pandas: $0.44/hr for 1 DPU"
      },
      {
        "date": "2024-05-04T18:50:00.000Z",
        "voteCount": 3,
        "content": "Python Shell is cheaper and can handle small to medium tasks.\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html"
      },
      {
        "date": "2024-04-30T13:03:00.000Z",
        "voteCount": 3,
        "content": "D. \nBecause the pyspark is still being the cheap you have to use minimum of 2 DPU. Which would increase the cost anyway so, i feel that d should be correct"
      },
      {
        "date": "2024-04-28T02:19:00.000Z",
        "voteCount": 3,
        "content": "D.\n\nWhile AWS Glue PySpark jobs are scalable and suitable for large workloads, C may be overkill for processing small .csv files (less than 100 MB each). The overhead of using Apache Spark may not be cost-effective for this specific use case."
      },
      {
        "date": "2024-04-27T01:32:00.000Z",
        "voteCount": 3,
        "content": "Option D:\n\nEven though the Python Shell Job is more expensive on a DPU-Hour basis, you can select the option \"1/16 DPU\" in the Job details for a Python Shell Job, which is definetly cheaper than a Pyspark job."
      },
      {
        "date": "2024-04-02T13:42:00.000Z",
        "voteCount": 6,
        "content": "AWS Glue Python Shell Job is billed $0.44 per DPU-Hour for each job\nAWS Glue PySpark is billed $0.29 per DPU-Hour for each job with flexible execution and $0.44 per DPU-Hour for each job with standard execution\nSource: https://aws.amazon.com/glue/pricing/"
      },
      {
        "date": "2024-03-31T23:35:00.000Z",
        "voteCount": 4,
        "content": "https://medium.com/@navneetsamarth/reduce-aws-cost-using-glue-python-shell-jobs-70a955d4359f#:~:text=The%20cheapest%20Glue%20Spark%20ETL,1%2F16th%20of%20a%20DPU.&amp;text=This%20can%20result%20in%20massive,just%20a%20better%20design%20overall!"
      },
      {
        "date": "2024-03-19T06:55:00.000Z",
        "voteCount": 3,
        "content": "D is more cheaper than C. Not so scalable but is cheaper..."
      },
      {
        "date": "2024-02-02T06:34:00.000Z",
        "voteCount": 4,
        "content": "AWS Glue is a fully managed ETL service, which means you don't need to manage infrastructure, and it automatically scales to handle your data processing needs. This reduces operational overhead and cost.\n\nPySpark, as a part of AWS Glue, is a powerful and widely-used framework for distributed data processing, and it's well-suited for handling data transformations on a large scale."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/amazon/view/142527-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer creates an AWS Glue Data Catalog table by using an AWS Glue crawler that is named Orders. The data engineer wants to add the following new partitions:<br><br>s3://transactions/orders/order_date=2023-01-01<br>s3://transactions/orders/order_date=2023-01-02<br><br>The data engineer must edit the metadata to include the new partitions in the table without scanning all the folders and files in the location of the table.<br><br>Which data definition language (DDL) statement should the data engineer use in Amazon Athena?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALTER TABLE Orders ADD PARTITION(order_date=\u20192023-01-01\u2019) LOCATION \u2018s3://transactions/orders/order_date=2023-01-01\u2019;<br>ALTER TABLE Orders ADD PARTITION(order_date=\u20192023-01-02\u2019) LOCATION \u2018s3://transactions/orders/order_date=2023-01-02\u2019;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMSCK REPAIR TABLE Orders;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tREPAIR TABLE Orders;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALTER TABLE Orders MODIFY PARTITION(order_date=\u20192023-01-01\u2019) LOCATION \u2018s3://transactions/orders/2023-01-01\u2019;<br>ALTER TABLE Orders MODIFY PARTITION(order_date=\u20192023-01-02\u2019) LOCATION \u2018s3://transactions/orders/2023-01-02\u2019;"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-07T12:41:00.000Z",
        "voteCount": 4,
        "content": "Why the Other Options Are Incorrect:\nOption B: MSCK REPAIR TABLE Orders: This command is used to repair the partitions of a table by scanning all the files in the specified location. This is not efficient if you know the specific partitions you want to add, as it will scan the entire table location.\nOption C: REPAIR TABLE Orders: This is not a valid Athena DDL command.\nOption D: ALTER TABLE Orders MODIFY PARTITION: This command is used to modify the location of existing partitions, not to add new partitions. It would not work for adding new partitions."
      },
      {
        "date": "2024-06-14T10:40:00.000Z",
        "voteCount": 4,
        "content": "A is correct as per https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html"
      },
      {
        "date": "2024-06-14T10:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct as per https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html"
      },
      {
        "date": "2024-06-14T06:11:00.000Z",
        "voteCount": 4,
        "content": "A is correct because it uses the appropriate DDL statements to add the new partitions directly without scanning all folders and files, meeting the requirements stated in the question.\nB is incorrect because while it would update the partitions, it would involve scanning all files and folders.\nC is incorrect because REPAIR TABLE is not a valid command.\nD is incorrect because it modifies partitions instead of adding new ones."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/amazon/view/142529-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores 10 to 15 TB of uncompressed .csv files in Amazon S3. The company is evaluating Amazon Athena as a one-time query engine.<br><br>The company wants to transform the data to optimize query runtime and storage costs.<br><br>Which file format and compression solution will meet these requirements for Athena queries?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t.csv format compressed with zip",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON format compressed with bzip2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Parquet format compressed with Snappy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Avro format compressed with LZO"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T08:25:00.000Z",
        "voteCount": 5,
        "content": "Parquet provides efficient columnar storage, enabling Athena to read only the necessary data for queries, which reduces scan times and speeds up query performance.\nSnappy compression offers a good balance between compression speed and efficiency, reducing storage costs without significantly impacting query times."
      },
      {
        "date": "2024-06-14T10:40:00.000Z",
        "voteCount": 3,
        "content": "Parquet + Snappy"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/amazon/view/142558-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Apache Airflow to orchestrate the company's current on-premises data pipelines. The company runs SQL data quality check tasks as part of the pipelines. The company wants to migrate the pipelines to AWS and to use AWS managed services.<br><br>Which solution will meet these requirements with the LEAST amount of refactoring?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup AWS Outposts in the AWS Region that is nearest to the location where the company uses Airflow. Migrate the servers into Outposts hosted Amazon EC2 instances. Update the pipelines to interact with the Outposts hosted EC2 instances instead of the on-premises pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Amazon Machine Image (AMI) that contains the Airflow application and the code that the company needs to migrate. Use the custom AMI to deploy Amazon EC2 instances. Update the network connections to interact with the newly deployed EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the existing Airflow orchestration configuration into Amazon Managed Workflows for Apache Airflow (Amazon MWAA). Create the data quality checks during the ingestion to validate the data quality by using SQL tasks in Airflow.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the pipelines to AWS Step Functions workflows. Recreate the data quality checks in SQL as Python based AWS Lambda functions."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T04:32:00.000Z",
        "voteCount": 2,
        "content": "The solution that will meet these requirements with the least amount of refactoring is Option C: Migrate the existing Airflow orchestration configuration into Amazon Managed Workflows for Apache Airflow (Amazon MWAA). Create the data quality checks during the ingestion to validate the data quality by using SQL tasks in Airflow.\n\nAmazon Managed Workflows for Apache Airflow (MWAA) is a fully managed service that makes it easy to run open-source versions of Apache Airflow on AWS. It allows you to build workflows to design and visualize pipelines, automate complex tasks, and monitor executions. Since the company is already using Apache Airflow for orchestration, migrating to Amazon MWAA would require minimal refactoring."
      },
      {
        "date": "2024-06-20T10:14:00.000Z",
        "voteCount": 3,
        "content": "Amazon MWAA - becuase we already uses Apache Airflow"
      },
      {
        "date": "2024-06-15T01:12:00.000Z",
        "voteCount": 3,
        "content": "Amazon MWAA is a managed service for running Apache Airflow. It allows migrating existing Airflow configurations with minimal changes. Data quality checks can continue to be implemented as SQL tasks in Airflow, similar to the current setup."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/amazon/view/142535-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon EMR as an extract, transform, and load (ETL) pipeline to transform data that comes from multiple sources. A data engineer must orchestrate the pipeline to maximize performance.<br><br>Which AWS service will meet this requirement MOST cost effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EventBridge",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Workflows for Apache Airflow (Amazon MWAA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Step Functions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue Workflows"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T10:44:00.000Z",
        "voteCount": 5,
        "content": "Glue Workflows is for Glue job orchestration. C is for orchestration with different AWS services."
      },
      {
        "date": "2024-08-20T05:23:00.000Z",
        "voteCount": 1,
        "content": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is the best service for orchestrating complex data pipelines, especially for workloads already using Amazon EMR. Airflow is a powerful workflow orchestration tool that can be integrated with various AWS services, including EMR, to provide flexible scheduling, task dependency management, and monitoring capabilities. Using a hosted Airflow service (MWAA) can reduce administrative overhead while maintaining a familiar workflow orchestration environment."
      },
      {
        "date": "2024-07-23T07:31:00.000Z",
        "voteCount": 2,
        "content": "B is not cost effective, D is only to orchestrate Glue Jobs and Crawlers within AWS Glue itself. Hence C is correct, Step functions is cost effective and can link together your different AWS services."
      },
      {
        "date": "2024-07-16T13:23:00.000Z",
        "voteCount": 2,
        "content": "This is EMR not Glue workflows hence step functions\nEventBridge is best for event driven architecture"
      },
      {
        "date": "2024-07-16T11:59:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/big-data/build-a-concurrent-data-orchestration-pipeline-using-amazon-emr-and-apache-livy/"
      },
      {
        "date": "2024-07-02T04:37:00.000Z",
        "voteCount": 2,
        "content": "The most cost-effective AWS service for orchestrating an ETL pipeline that maximizes performance is D. AWS Glue Workflows.\n\nAWS Glue is a fully managed ETL service that makes it easy to move data between your data stores. AWS Glue simplifies and automates the difficult and time-consuming tasks of data discovery, conversion mapping, and job scheduling. AWS Glue Workflows allows you to orchestrate complex ETL jobs involving multiple crawlers, jobs, and triggers.\n\nWhile the other services mentioned (Amazon EventBridge, Amazon MWAA, and AWS Step Functions) can be used for workflow orchestration, they are not specifically designed for ETL workloads and may not be as cost-effective for this use case. AWS Glue is designed for ETL workloads, and its workflows feature is specifically designed for orchestrating ETL jobs, making it the most suitable and cost-effective choice."
      },
      {
        "date": "2024-06-20T10:16:00.000Z",
        "voteCount": 1,
        "content": "C - becuase AWS Glue can be used only for glue based ETL jobs"
      },
      {
        "date": "2024-06-15T01:18:00.000Z",
        "voteCount": 2,
        "content": "While AWS Glue Workflows are excellent for orchestrating Glue-specific ETL tasks, AWS Step Functions is more suitable for orchestrating an Amazon EMR-based ETL pipeline due to its greater flexibility, broader integration capabilities, and effective cost management. Therefore, the correct choice remains [C]"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/amazon/view/142559-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An online retail company stores Application Load Balancer (ALB) access logs in an Amazon S3 bucket. The company wants to use Amazon Athena to query the logs to analyze traffic patterns.<br><br>A data engineer creates an unpartitioned table in Athena. As the amount of the data gradually increases, the response time for queries also increases. The data engineer wants to improve the query performance in Athena.<br><br>Which solution will meet these requirements with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue job that determines the schema of all ALB access logs and writes the partition metadata to AWS Glue Data Catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler that includes a classifier that determines the schema of all ALB access logs and writes the partition metadata to AWS Glue Data Catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to transform all ALB access logs. Save the results to Amazon S3 in Apache Parquet format. Partition the metadata. Use Athena to query the transformed data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Hive to create bucketed tables. Use an AWS Lambda function to transform all ALB access logs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T13:26:00.000Z",
        "voteCount": 1,
        "content": "AWS Crawler with classifiers allow you to determine the schema pattern on files/data that can then be used to partition the data for Athena query optimization"
      },
      {
        "date": "2024-06-21T12:03:00.000Z",
        "voteCount": 4,
        "content": "Creating an AWS Glue crawler (Option B) is the most straightforward and least operationally intensive approach to automatically determine the schema, partition the data, and keep the AWS Glue Data Catalog updated. This ensures Athena queries are optimized without requiring extensive manual management or additional processing steps."
      },
      {
        "date": "2024-06-21T12:03:00.000Z",
        "voteCount": 2,
        "content": "Creating an AWS Glue crawler (Option B) is the most straightforward and least operationally intensive approach to automatically determine the schema, partition the data, and keep the AWS Glue Data Catalog updated. This ensures Athena queries are optimized without requiring extensive manual management or additional processing steps."
      },
      {
        "date": "2024-06-15T01:20:00.000Z",
        "voteCount": 4,
        "content": "An AWS Glue crawler can automatically determine the schema of the logs, infer partitions, and update the Glue Data Catalog. Crawlers can be scheduled to run at intervals, minimizing manual intervention."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/amazon/view/142560-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has a business intelligence platform on AWS. The company uses an AWS Storage Gateway Amazon S3 File Gateway to transfer files from the company's on-premises environment to an Amazon S3 bucket.<br><br>A data engineer needs to setup a process that will automatically launch an AWS Glue workflow to run a series of AWS Glue jobs when each file transfer finishes successfully.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine when the file transfers usually finish based on previous successful file transfers. Set up an Amazon EventBridge scheduled event to initiate the AWS Glue jobs at that time of day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon EventBridge event that initiates the AWS Glue workflow after every successful S3 File Gateway file transfer event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an on-demand AWS Glue workflow so that the data engineer can start the AWS Glue workflow when each file transfer is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Lambda function that will invoke the AWS Glue Workflow. Set up an event for the creation of an S3 object as a trigger for the Lambda function."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T01:25:00.000Z",
        "voteCount": 5,
        "content": "Using EventBridge directly to trigger the AWS Glue workflow upon S3 events is straightforward and leverages AWS's event-driven architecture, requiring minimal maintenance."
      },
      {
        "date": "2024-07-16T13:30:00.000Z",
        "voteCount": 1,
        "content": "Event driven architecture with S3 file creation can only be EventBridge"
      },
      {
        "date": "2024-07-04T15:46:00.000Z",
        "voteCount": 1,
        "content": "Setting up an Amazon EventBridge event that initiates the AWS Glue workflow after every successful S3 File Gateway file transfer event would meet these requirements with the least operational overhead.\n\nThis solution is event-driven and does not require manual intervention or reliance on a schedule that might not align with the actual completion time of the file transfers. The AWS Glue workflow is triggered automatically when a new file is added to the S3 bucket, ensuring that the AWS Glue workflow starts processing the new data as soon as it\u2019s available."
      },
      {
        "date": "2024-07-02T05:01:00.000Z",
        "voteCount": 1,
        "content": "The solution that will meet these requirements with the least operational overhead is Option D.\n\nSetting up an AWS Lambda function that will invoke the AWS Glue Workflow, and setting up an event for the creation of an S3 object as a trigger for the Lambda function, will ensure that the workflow is automatically initiated each time a file transfer is successfully completed. This approach requires minimal operational overhead as it automates the process and does not require manual intervention or scheduling based on estimated completion times.\n\nOptions A and C involve manual intervention or assumptions about transfer times, which could lead to inefficiencies or inaccuracies. Option B is not feasible because Amazon EventBridge does not directly support triggering events based on S3 File Gateway file transfer events. Therefore, Option D is the most suitable solution."
      },
      {
        "date": "2024-06-21T12:04:00.000Z",
        "voteCount": 1,
        "content": "Setting up an Amazon EventBridge event (Option B) to initiate the AWS Glue workflow after every successful S3 File Gateway file transfer event is the most efficient solution. It provides real-time automation with minimal operational overhead, ensuring that the Glue workflow starts immediately after the file transfer is complete."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/amazon/view/142537-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A retail company uses Amazon Aurora PostgreSQL to process and store live transactional data. The company uses an Amazon Redshift cluster for a data warehouse.<br><br>An extract, transform, and load (ETL) job runs every morning to update the Redshift cluster with new data from the PostgreSQL database. The company has grown rapidly and needs to cost optimize the Redshift cluster.<br><br>A data engineer needs to create a solution to archive historical data. The data engineer must be able to run analytics queries that effectively combine data from live transactional data in PostgreSQL, current data in Redshift, and archived historical data. The solution must keep only the most recent 15 months of data in Amazon Redshift to reduce costs.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Amazon Redshift Federated Query feature to query live transactional data that is in the PostgreSQL database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Redshift Spectrum to query live transactional data that is in the PostgreSQL database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a monthly job to copy data that is older than 15 months to Amazon S3 by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Amazon Redshift Spectrum to access historical data in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a monthly job to copy data that is older than 15 months to Amazon S3 Glacier Flexible Retrieval by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Redshift Spectrum to access historical data from S3 Glacier Flexible Retrieval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a materialized view in Amazon Redshift that combines live, current, and historical data from different sources."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-07T02:48:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C: allows exporting Redshift data to Amazon S3 and ability to frequent access"
      },
      {
        "date": "2024-06-20T10:23:00.000Z",
        "voteCount": 1,
        "content": "A / C is a best choice"
      },
      {
        "date": "2024-06-15T04:03:00.000Z",
        "voteCount": 4,
        "content": "AC is correct. D is not correct, because Redshift Spectrum cannot read from S3 Glacier Flexible Retrieval."
      },
      {
        "date": "2024-06-15T01:41:00.000Z",
        "voteCount": 4,
        "content": "Choice A ensures that live transactional data from PostgreSQL can be accessed directly within Redshift queries.\n\nChoice C archives historical data in Amazon S3, reducing storage costs in Redshift while still making the data accessible via Redshift Spectrum.\n\n(to Admin: I can't select multiple answers on the voting comment)"
      },
      {
        "date": "2024-06-14T16:06:00.000Z",
        "voteCount": 4,
        "content": "Option A (A): Configuring Amazon Redshift Federated Query allows Redshift to directly query the live transactional data in the PostgreSQL database without needing to import it. This ensures that you can access the most recent live data efficiently.\n\nOption C (C): Scheduling a monthly job to copy data older than 15 months to Amazon S3 and then using Amazon Redshift Spectrum to access this historical data provides a cost-effective way to manage storage. This ensures that only the most recent 15 months of data are kept in Amazon Redshift, reducing storage costs. The historical data is still accessible via Redshift Spectrum for analytics queries."
      },
      {
        "date": "2024-06-14T12:12:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A and C."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/amazon/view/142562-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A manufacturing company has many IoT devices in facilities around the world. The company uses Amazon Kinesis Data Streams to collect data from the devices. The data includes device ID, capture date, measurement type, measurement value, and facility ID. The company uses facility ID as the partition key.<br><br>The company's operations team recently observed many WriteThroughputExceeded exceptions. The operations team found that some shards were heavily used but other shards were generally idle.<br><br>How should the company resolve the issues that the operations team observed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the partition key from facility ID to a randomly generated key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArchive the data on the producer's side.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the partition key from facility ID to capture date."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T01:43:00.000Z",
        "voteCount": 6,
        "content": "The best solution to resolve the issue of uneven shard usage and WriteThroughputExceeded exceptions is to balance the load more evenly across the shards. This can be effectively achieved by changing the partition key to something that ensures a more uniform distribution of data across the shards."
      },
      {
        "date": "2024-07-04T15:53:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is **A. Change the partition key from facility ID to a randomly generated key.**\n\nAmazon Kinesis Data Streams uses the partition key that you specify to segregate the data records in the stream into shards. If the company uses the facility ID as the partition key, and if some facilities produce more data than others, then the data will be unevenly distributed across the shards. This can lead to some shards being heavily used while others are idle, and can cause `WriteThroughputExceeded` exceptions.\n\nBy changing the partition key to a randomly generated key, the data records are more likely to be evenly distributed across all the shards, which can help to avoid the issue of some shards being heavily used and others being idle. This solution requires the least operational overhead and does not involve increasing costs (as in option B), archiving data (which might not be desirable or feasible, as in option C), or changing to a partition key that might also lead to uneven distribution (as in option D)."
      },
      {
        "date": "2024-07-03T05:29:00.000Z",
        "voteCount": 2,
        "content": "D is not good, because you're effectively making things worse by partitioning by date. My answer is A"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/amazon/view/142563-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer wants to improve the performance of SQL queries in Amazon Athena that run against a sales data table.<br><br>The data engineer wants to understand the execution plan of a specific SQL statement. The data engineer also wants to see the computational cost of each operation in a SQL query.<br><br>Which statement does the data engineer need to run to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXPLAIN SELECT * FROM sales;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXPLAIN ANALYZE FROM sales;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXPLAIN ANALYZE SELECT * FROM sales;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXPLAIN FROM sales;"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-20T10:08:00.000Z",
        "voteCount": 5,
        "content": "use EXPLAIN ANALIZE \nhttps://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html"
      },
      {
        "date": "2024-07-03T21:53:00.000Z",
        "voteCount": 1,
        "content": "explain analyze  + select * from table"
      },
      {
        "date": "2024-06-15T01:46:00.000Z",
        "voteCount": 2,
        "content": "A - Only partially meets the requirements as it does not include computational costs.\nB - Incorrect syntax and does not meet the requirements.\nC - Fully meets the requirements by providing both the execution plan and the computational costs.\nD - Incorrect syntax and does not meet the requirements."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/amazon/view/142564-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company plans to provision a log delivery stream within a VPC. The company configured the VPC flow logs to publish to Amazon CloudWatch Logs. The company needs to send the flow logs to Splunk in near real time for further analysis.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Kinesis Data Streams data stream to use Splunk as the destination. Create a CloudWatch Logs subscription filter to send log events to the data stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination. Create a CloudWatch Logs subscription filter to send log events to the delivery stream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination. Create an AWS Lambda function to send the flow logs from CloudWatch Logs to the delivery stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Kinesis Data Streams data stream to use Splunk as the destination. Create an AWS Lambda function to send the flow logs from CloudWatch Logs to the data stream."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T01:48:00.000Z",
        "voteCount": 6,
        "content": "Kinesis Data Firehose has built-in support for Splunk as a destination, making the integration straightforward. Using a CloudWatch Logs subscription filter directly to Firehose simplifies the data flow, eliminating the need for additional Lambda functions or custom integrations."
      },
      {
        "date": "2024-07-04T16:00:00.000Z",
        "voteCount": 4,
        "content": "Creating an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination and creating a CloudWatch Logs subscription filter to send log events to the delivery stream would meet these requirements with the least operational overhead.\n\nAmazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Splunk.\n\nCloudWatch Logs subscription filters allow you to send real-time log events to Kinesis Data Firehose and are ideal for scenarios where you want to forward the logs to other services for further analysis.\n\nOptions A and D involve Kinesis Data Streams, which would require additional management and operational overhead. Option C involves creating a Lambda function, which also adds operational overhead. Therefore, option B is the best choice."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/amazon/view/142565-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has a data lake on AWS. The data lake ingests sources of data from business units. The company uses Amazon Athena for queries. The storage layer is Amazon S3 with an AWS Glue Data Catalog as a metadata repository.<br><br>The company wants to make the data available to data scientists and business analysts. However, the company first needs to manage fine-grained, column-level data access for Athena based on the user roles and responsibilities.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Lake Formation. Define security policy-based rules for the users and applications by IAM role in Lake Formation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an IAM resource-based policy for AWS Glue tables. Attach the same policy to IAM user groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an IAM identity-based policy for AWS Glue tables. Attach the same policy to IAM roles. Associate the IAM roles with IAM groups that contain the users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a resource share in AWS Resource Access Manager (AWS RAM) to grant access to IAM users."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-07T15:12:00.000Z",
        "voteCount": 5,
        "content": "Correct Solution:\nA. Set up AWS Lake Formation. Define security policy-based rules for the users and applications by IAM role in Lake Formation.\n\nExplanation:\nAWS Lake Formation: This service simplifies and automates the process of securing and managing data lakes. It allows you to define fine-grained access control policies at the database, table, and column levels.\nSecurity Policy-Based Rules: Lake Formation allows you to create policies that specify which users or roles have access to specific data, including column-level access controls. This makes it easier to manage access based on roles and responsibilities."
      },
      {
        "date": "2024-06-20T19:18:00.000Z",
        "voteCount": 1,
        "content": "A - Lake formation"
      },
      {
        "date": "2024-06-15T01:51:00.000Z",
        "voteCount": 4,
        "content": "Lake Formation supports fine-grained access control, including column-level permissions."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/amazon/view/142566-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has developed several AWS Glue extract, transform, and load (ETL) jobs to validate and transform data from Amazon S3. The ETL jobs load the data into Amazon RDS for MySQL in batches once every day. The ETL jobs use a DynamicFrame to read the S3 data.<br><br>The ETL jobs currently process all the data that is in the S3 bucket. However, the company wants the jobs to process only the daily incremental data.<br><br>Which solution will meet this requirement with the LEAST coding effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an ETL job that reads the S3 file status and logs the status in Amazon DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable job metrics for the ETL jobs to help keep track of processed objects in Amazon CloudWatch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ETL jobs to delete processed objects from Amazon S3 after each run."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T01:54:00.000Z",
        "voteCount": 7,
        "content": "AWS Glue job bookmarks are designed to handle incremental data processing by automatically tracking the state."
      },
      {
        "date": "2024-07-16T13:47:00.000Z",
        "voteCount": 1,
        "content": "AWS Glue Bookmarks can be used to pin where the data processing last stopped hence help with incremental processing."
      },
      {
        "date": "2024-07-03T21:52:00.000Z",
        "voteCount": 1,
        "content": "B - bookmarks is a key"
      },
      {
        "date": "2024-07-02T05:27:00.000Z",
        "voteCount": 3,
        "content": "The solution that will meet this requirement with the least coding effort is Option B: Enable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data.\n\nAWS Glue job bookmarks help ETL jobs to keep track of data that has already been processed during previous runs. By enabling job bookmarks, the ETL jobs can skip the processed data and only process the new, incremental data. This feature is designed specifically for this use case and requires minimal coding effort.\n\nOptions A, C, and D would require additional coding and operational effort. Option A would require creating a new ETL job and managing a DynamoDB table. Option C would involve setting up job metrics and CloudWatch, which doesn\u2019t directly address processing incremental data. Option D would involve deleting data from S3 after processing, which might not be desirable if the original data needs to be retained. Therefore, Option B is the most suitable solution."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/amazon/view/142567-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An online retail company has an application that runs on Amazon EC2 instances that are in a VPC. The company wants to collect flow logs for the VPC and analyze network traffic.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish flow logs to Amazon CloudWatch Logs. Use Amazon Athena for analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish flow logs to Amazon CloudWatch Logs. Use an Amazon OpenSearch Service cluster for analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish flow logs to Amazon S3 in text format. Use Amazon Athena for analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish flow logs to Amazon S3 in Apache Parquet format. Use Amazon Athena for analytics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T01:59:00.000Z",
        "voteCount": 5,
        "content": "Flow Logs can be published to S3 in Parquet format: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-s3.html#flow-logs-s3-path"
      },
      {
        "date": "2024-07-19T09:20:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-vpc-flow-logs-parquet-hive-prefixes-partitioned-files/"
      },
      {
        "date": "2024-07-31T02:23:00.000Z",
        "voteCount": 2,
        "content": "The question says clearly most cost effective, so on comparison between C and D, has to be C"
      },
      {
        "date": "2024-07-17T08:08:00.000Z",
        "voteCount": 1,
        "content": "Flow logs acn be published to S3 but then option D sas in Parquet format - it is not automatically converted into parquet....\nhttps://aws.amazon.com/solutions/implementations/centralized-logging-with-opensearch/"
      },
      {
        "date": "2024-06-24T05:12:00.000Z",
        "voteCount": 1,
        "content": "Apache parquet and S3 = most cost-effective solution"
      },
      {
        "date": "2024-06-21T12:06:00.000Z",
        "voteCount": 4,
        "content": "Publishing flow logs to Amazon S3 in Apache Parquet format and using Amazon Athena for analytics (D) is the most cost-effective solution. This approach minimizes storage costs due to the efficient compression of Parquet, and optimizes query performance and cost in Athena due to the reduced data size and optimized columnar storage."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/amazon/view/142568-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A retail company stores transactions, store locations, and customer information tables in four reserved ra3.4xlarge Amazon Redshift cluster nodes. All three tables use even table distribution.<br><br>The company updates the store location table only once or twice every few years.<br><br>A data engineer notices that Redshift queues are slowing down because the whole store location table is constantly being broadcast to all four compute nodes for most queries. The data engineer wants to speed up the query performance by minimizing the broadcasting of the store location table.<br><br>Which solution will meet these requirements in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the distribution style of the store location table from EVEN distribution to ALL distribution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the distribution style of the store location table to KEY distribution based on the column that has the highest dimension.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a join column named store_id into the sort key for all the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade the Redshift reserved node to a larger instance size in the same instance family."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T13:52:00.000Z",
        "voteCount": 2,
        "content": "ALL distribution is optimal for slowly changing dimension tables and generally small in size to allow for optimal joins."
      },
      {
        "date": "2024-07-02T05:40:00.000Z",
        "voteCount": 2,
        "content": "The most cost-effective solution to speed up the query performance by minimizing the broadcasting of the store location table would be:\n\nA. Change the distribution style of the store location table from EVEN distribution to ALL distribution.\n\nIn Amazon Redshift, the ALL distribution style replicates the entire table to all nodes in the cluster, which eliminates the need to redistribute the data when executing a query. This can significantly improve query performance. Given that the store location table is updated only once or twice every few years, the overhead of maintaining the replicated data would be minimal. This makes it a cost-effective solution for improving the query performance."
      },
      {
        "date": "2024-06-21T12:07:00.000Z",
        "voteCount": 4,
        "content": "Changing the distribution style of the store location table to ALL distribution (A) is the most cost-effective solution. It directly addresses the issue of broadcasting by ensuring the entire table is available on each node, significantly improving join performance without incurring substantial additional costs."
      },
      {
        "date": "2024-06-15T02:01:00.000Z",
        "voteCount": 2,
        "content": "Using ALL distribution means the table is replicated to all nodes, eliminating the need for broadcasting during queries. Since the store location table is updated infrequently, this will significantly speed up queries without incurring frequent update costs."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/amazon/view/142569-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has a data warehouse that contains a table that is named Sales. The company stores the table in Amazon Redshift. The table includes a column that is named city_name. The company wants to query the table to find all rows that have a city_name that starts with \"San\" or \"El\".<br><br>Which SQL query will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect * from Sales where city_name ~ \u2018$(San|El)*\u2019;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect * from Sales where city_name ~ \u2018^(San|El)*\u2019;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect * from Sales where city_name ~\u2019$(San&amp;El)*\u2019;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect * from Sales where city_name ~ \u2018^(San&amp;El)*\u2019;"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T08:00:00.000Z",
        "voteCount": 5,
        "content": "Regex Patterns for everyone's reference\n\n. : Matches any single character.\n* : Matches zero or more of the preceding element.\n+ : Matches one or more of the preceding element.\n[abc] : Matches any of the enclosed characters.\n[^abc] : Matches any character not enclosed.\n^ : Matches the start of a string.\n$ : Matches the end of a string.\n| : Logical OR operator.\n(abc) : Matches 'abc' and remembers the match.\n\nAnswer is B"
      },
      {
        "date": "2024-07-16T13:53:00.000Z",
        "voteCount": 1,
        "content": "Regex patterns:\n^ - used to capture the start of the text/string\n| - used as an OR operator"
      },
      {
        "date": "2024-07-02T05:42:00.000Z",
        "voteCount": 1,
        "content": "B. Select * from Sales where city_name ~ \u2018^(San|El)*\u2019;\n\nThis query uses a regular expression pattern with the ~ operator. The caret ^ at the beginning of the pattern indicates that the match must start at the beginning of the string. (San|El) matches either \u201cSan\u201d or \u201cEl\u201d, and * means zero or more of the preceding element. So this query will return all rows where city_name starts with either \u201cSan\u201d or \u201cEl\u201d."
      },
      {
        "date": "2024-06-20T19:23:00.000Z",
        "voteCount": 1,
        "content": "B - becuase of regexp"
      },
      {
        "date": "2024-06-18T16:02:00.000Z",
        "voteCount": 3,
        "content": "^ asserts the position at the start of the string.\n(San|El) matches either \"San\" or \"El\"."
      },
      {
        "date": "2024-06-15T02:02:00.000Z",
        "voteCount": 2,
        "content": "~: This operator indicates the use of a regular expression.\n^: This symbol signifies the start of the string.\n(San|El): This pattern matches strings that start with either \"San\" or \"El\"."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/amazon/view/142571-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company needs to send customer call data from its on-premises PostgreSQL database to AWS to generate near real-time insights. The solution must capture and load updates from operational data stores that run in the PostgreSQL database. The data changes continuously.<br><br>A data engineer configures an AWS Database Migration Service (AWS DMS) ongoing replication task. The task reads changes in near real time from the PostgreSQL source database transaction logs for each table. The task then sends the data to an Amazon Redshift cluster for processing.<br><br>The data engineer discovers latency issues during the change data capture (CDC) of the task. The data engineer thinks that the PostgreSQL source database is causing the high latency.<br><br>Which solution will confirm that the PostgreSQL database is the source of the high latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch to monitor the DMS task. Examine the CDCIncomingChanges metric to identify delays in the CDC from the source database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that logical replication of the source database is configured in the postgresql.conf configuration file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Logs for the DMS endpoint of the source database. Check for error messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch to monitor the DMS task. Examine the CDCLatencySource metric to identify delays in the CDC from the source database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T23:12:00.000Z",
        "voteCount": 1,
        "content": "only D makes sense"
      },
      {
        "date": "2024-06-23T09:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Troubleshooting_Latency.html\nA high CDCLatencySource metric indicates that the process of capturing changes from the source is delayed.\nAnswer is D"
      },
      {
        "date": "2024-06-15T02:11:00.000Z",
        "voteCount": 3,
        "content": "CDCLatencySource Metric: This metric measures the latency between the source database and the DMS task. It shows how long it takes for changes to be read from the source database's transaction logs.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/amazon/view/142561-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A lab uses IoT sensors to monitor humidity, temperature, and pressure for a project. The sensors send 100 KB of data every 10 seconds. A downstream process will read the data from an Amazon S3 bucket every 30 seconds.<br><br>Which solution will deliver the data to the S3 bucket with the LEAST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use the default buffer interval for Kinesis Data Firehose.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams to deliver the data to the S3 bucket. Configure the stream to use 5 provisioned shards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Streams and call the Kinesis Client Library to deliver the data to the S3 bucket. Use a 5 second buffer interval from an application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use a 5 second buffer interval for Kinesis Data Firehose."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T02:36:00.000Z",
        "voteCount": 5,
        "content": "C - This option ensures low latency by using a short buffer interval (5 seconds). The use of KCL allows for customized processing logic and timely delivery of data to S3. This makes it a strong candidate for minimal latency.\n\nD - While this option provides low latency with a 5-second buffer interval, it introduces unnecessary complexity by using Apache Flink for what seems to be a straightforward data ingestion task. This option is overkill for the given use case and may add more operational overhead than necessary."
      },
      {
        "date": "2024-07-23T21:42:00.000Z",
        "voteCount": 2,
        "content": "Use data streams and KCL, option A would be right but the default buffer for Firehose does not allow it to be correct. D adds extra components that are not needed for delivery of data."
      },
      {
        "date": "2024-07-17T09:02:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/"
      },
      {
        "date": "2024-08-05T02:19:00.000Z",
        "voteCount": 1,
        "content": "A can not correct as it's said \"Use the default buffer interval for Kinesis Data Firehose\" wich is 300 secs"
      },
      {
        "date": "2024-07-10T16:55:00.000Z",
        "voteCount": 1,
        "content": "its C - option D uses 1/ Analytics which summarizes data and gence has delay then passses to 2/ Firehose for deliver and Firehose doesnt say its using zero buffering"
      },
      {
        "date": "2024-06-24T09:42:00.000Z",
        "voteCount": 1,
        "content": "Firehose uses multi-part upload for S3 destination when you configure a buffer time interval less than 60 seconds to offer lower latencies. Due to multi-part upload for S3 destination, you will see some increase in S3 PUT API costs if you choose a buffer time interval less than 60 seconds."
      },
      {
        "date": "2024-06-15T04:28:00.000Z",
        "voteCount": 4,
        "content": "Kinesis Data Streams cannot deliver directly to S3. Data has to go through Firehose. A is correct but is not lowest latency. I would go with D, as we can set the buffer interval to a low value. We do not need Flink, tho. That's a bit confusing."
      },
      {
        "date": "2024-06-15T01:31:00.000Z",
        "voteCount": 3,
        "content": "I think the answer is C. Kinesis Data Firehose has a minimum buffer interval of 60 seconds (1 minute) or 1 MB of data."
      },
      {
        "date": "2024-06-16T04:49:00.000Z",
        "voteCount": 3,
        "content": "Fyi, Firehose now supports 0 buffering: https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/"
      },
      {
        "date": "2024-06-24T09:56:00.000Z",
        "voteCount": 2,
        "content": "As per option A, \"Use the default buffer interval for Kinesis Data Firehose\". Default buffer interval for Kinesis Data Firehose is 300seconds where S3 is the destination. \nFlink is not required here. Hence, option D is not suitable."
      },
      {
        "date": "2024-06-24T09:57:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/firehose/latest/dev/buffering-hints.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/amazon/view/142573-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company wants to use machine learning (ML) to perform analytics on data that is in an Amazon S3 data lake. The company has two data transformation requirements that will give consumers within the company the ability to create reports.<br><br>The company must perform daily transformations on 300 GB of data that is in a variety format that must arrive in Amazon S3 at a scheduled time. The company must perform one-time transformations of terabytes of archived data that is in the S3 data lake. The company uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) Directed Acyclic Graphs (DAGs) to orchestrate processing.<br><br>Which combination of tasks should the company schedule in the Amazon MWAA DAGs to meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use AWS Glue crawlers to scan and identify the schema.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use Amazon Athena to scan and identify the schema.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily incoming data, use Amazon Redshift to perform transformations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor daily and archived data, use Amazon EMR to perform data transformations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t For archived data, use Amazon SageMaker to perform data transformations."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T21:45:00.000Z",
        "voteCount": 2,
        "content": "Glue crawlers for identifying the schema, EMR to run batch processing on the data"
      },
      {
        "date": "2024-07-03T22:15:00.000Z",
        "voteCount": 1,
        "content": "A / D - Looks good for me"
      },
      {
        "date": "2024-07-03T07:56:00.000Z",
        "voteCount": 2,
        "content": "According to ChatGPT"
      },
      {
        "date": "2024-07-03T07:55:00.000Z",
        "voteCount": 3,
        "content": "A. For daily incoming data, use AWS Glue crawlers to scan and identify the schema.\nD. For daily and archived data, use Amazon EMR to perform data transformations.\n\nHere's why:\n\nA. AWS Glue crawlers are well-suited for scanning and identifying the schema of data in S3. They are cost-effective and efficient for daily incoming data.\nD. Amazon EMR is a cost-effective solution for performing large-scale data transformations. It can handle both the daily transformations of 300 GB of data and the one-time transformations of terabytes of archived data efficiently."
      },
      {
        "date": "2024-06-15T02:46:00.000Z",
        "voteCount": 3,
        "content": "A. For daily incoming data, use AWS Glue crawlers to scan and identify the schema. This is cost-effective and simplifies the process of managing metadata.\n\nD. For daily and archived data, use Amazon EMR to perform data transformations. EMR is suitable for both large-scale and regular transformations, offering flexibility and cost efficiency."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/amazon/view/142574-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A retail company uses AWS Glue for extract, transform, and load (ETL) operations on a dataset that contains information about customer orders. The company wants to implement specific validation rules to ensure data accuracy and consistency.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue job bookmarks to track the data for accuracy and consistency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate custom AWS Glue Data Quality rulesets to define specific data quality checks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the built-in AWS Glue Data Quality transforms for standard data quality validations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue Data Catalog to maintain a centralized data schema and metadata repository."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T22:18:00.000Z",
        "voteCount": 2,
        "content": "Only B - makes sense"
      },
      {
        "date": "2024-07-03T07:58:00.000Z",
        "voteCount": 4,
        "content": "B. Create custom AWS Glue Data Quality rulesets to define specific data quality checks.\n\nCustom AWS Glue Data Quality rulesets allow you to define precise data quality checks tailored to your specific needs, ensuring that the data meets the required standards of accuracy and consistency. This approach provides flexibility to implement a wide range of validation rules based on your business requirements."
      },
      {
        "date": "2024-06-15T02:49:00.000Z",
        "voteCount": 3,
        "content": "This option provides the necessary flexibility to define and implement custom validation rules tailored to the company's specific requirements for data accuracy and consistency."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/amazon/view/142575-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An insurance company stores transaction data that the company compressed with gzip.<br><br>The company needs to query the transaction data for occasional audits.<br><br>Which solution will meet this requirement in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Amazon Glacier Flexible Retrieval. Use Amazon S3 Glacier Select to query the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Amazon S3. Use Amazon S3 Select to query the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Amazon S3. Use Amazon Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Amazon Glacier Instant Retrieval. Use Amazon Athena to query the data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-16T04:54:00.000Z",
        "voteCount": 6,
        "content": "Actually, I think A makes more sense."
      },
      {
        "date": "2024-10-07T05:53:00.000Z",
        "voteCount": 1,
        "content": "B is the more cost-effective solution for occasional audits. It allows for easier access to the data without incurring high retrieval costs"
      },
      {
        "date": "2024-09-27T17:58:00.000Z",
        "voteCount": 1,
        "content": "gzip compressed data querying -&gt; s3 select -Answer B"
      },
      {
        "date": "2024-09-25T12:46:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/aws/s3-glacier-select/\n\noption B is not cost effective as it is stored in standard S3"
      },
      {
        "date": "2024-09-17T06:23:00.000Z",
        "voteCount": 1,
        "content": "Occasional audits, so go for S3 glacier select"
      },
      {
        "date": "2024-08-14T04:03:00.000Z",
        "voteCount": 3,
        "content": "this is B"
      },
      {
        "date": "2024-08-11T23:16:00.000Z",
        "voteCount": 1,
        "content": "IT is A"
      },
      {
        "date": "2024-08-09T07:38:00.000Z",
        "voteCount": 1,
        "content": "Glacier is an expensive option in cases when you need to access data occasionally"
      },
      {
        "date": "2024-08-03T23:21:00.000Z",
        "voteCount": 1,
        "content": "I am not sure whether to go for B or C. Can anyone comment on this?\nB: No problem, but not available if Parquet is Gzip compressed. But the problem statement doesn't say Parquet is Gzip compressed.\nC: Correct if Parquet is Gzip compressed, but B is more cost-effective if csv or json is Gzip compressed"
      },
      {
        "date": "2024-07-17T21:53:00.000Z",
        "voteCount": 2,
        "content": "I think the solution is either B or D but I would go with B because they mentioned storing the data in gzip and not parquet which is optimised for Athena queries"
      },
      {
        "date": "2024-07-10T17:01:00.000Z",
        "voteCount": 2,
        "content": "there is no such thing as Glacier Flexible Retrieval, so its no A . its either B or D and most likely its D for the cost"
      },
      {
        "date": "2024-08-09T09:38:00.000Z",
        "voteCount": 1,
        "content": "There is glacier flexible retrieval"
      },
      {
        "date": "2024-07-05T02:32:00.000Z",
        "voteCount": 1,
        "content": "B. Store the data in Amazon S3. Use Amazon S3 Select to query the data.\n\nAmazon S3 is a cost-effective object storage service, and S3 Select allows you to retrieve only a subset of data from an object by using simple SQL expressions. S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also supports GZIP and BZIP2 compression formats, which makes it suitable for the given scenario where the data is compressed with gzip.\n\nWhile Amazon Athena is a powerful query service, it can be more expensive than S3 Select for occasional queries. Amazon Glacier and Glacier Select are designed for long-term archival storage and not for frequent access or queries, which might not be suitable for occasional audits. Therefore, option B is the most cost-effective choice for this scenario."
      },
      {
        "date": "2024-07-03T18:53:00.000Z",
        "voteCount": 1,
        "content": "ill go with B, because. of cost to query"
      },
      {
        "date": "2024-07-02T05:55:00.000Z",
        "voteCount": 1,
        "content": "B. Store the data in Amazon S3. Use Amazon S3 Select to query the data.\n\nAmazon S3 is a cost-effective storage service, and S3 Select allows you to retrieve only a subset of data from an object by using simple SQL expressions. S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also supports GZIP compression, which is the format used by the company. This makes it a cost-effective solution for occasional queries needed for audits."
      },
      {
        "date": "2024-06-30T16:40:00.000Z",
        "voteCount": 1,
        "content": "Option B (Amazon S3 with S3 Select) is generally more cost-effective and operationally efficient for occasional audits of gzip-compressed data. It provides faster access to data and lower querying costs, which are critical factors for ad-hoc and timely data retrievals. While Option A (Amazon Glacier Flexible Retrieval with S3 Glacier Select) offers cheaper storage, its longer retrieval times and potential higher querying costs make it less suitable for use cases requiring timely access to data."
      },
      {
        "date": "2024-06-20T19:26:00.000Z",
        "voteCount": 2,
        "content": "Looks like that A - fit better in question requirements"
      },
      {
        "date": "2024-06-15T22:35:00.000Z",
        "voteCount": 2,
        "content": "On the assumptions that querying the audit data is time sensitive and the transaction data is compressed into a single object I would go with using S3 and S3 select to query the data."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/amazon/view/142543-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer finished testing an Amazon Redshift stored procedure that processes and inserts data into a table that is not mission critical. The engineer wants to automatically run the stored procedure on a daily basis.<br><br>Which solution will meet this requirement in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to schedule a cron job to run the stored procedure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule and run the stored procedure by using the Amazon Redshift Data API in an Amazon EC2 Spot Instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse query editor v2 to run the stored procedure on a schedule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule an AWS Glue Python shell job to run the stored procedure."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T04:40:00.000Z",
        "voteCount": 6,
        "content": "This can be achieved with query editor v2 (https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html)"
      },
      {
        "date": "2024-07-17T21:56:00.000Z",
        "voteCount": 1,
        "content": "I go with C because it runs the query within the Redshift instance, B may not be appropriate because it involves other services on top of the Redshift instance and there is movement of data across the services."
      },
      {
        "date": "2024-07-13T12:25:00.000Z",
        "voteCount": 1,
        "content": "given that the table is not mission-critical and requires the \"MOST cost-effective way.\""
      },
      {
        "date": "2024-06-27T12:47:00.000Z",
        "voteCount": 1,
        "content": "I am going with option B, given that the table is not mission-critical and requires the \"MOST cost-effective way.\" \nAWS Spot Instances are Amazon EC2 instances that allow you to utilize spare EC2 capacity at a significantly lower cost than On-Demand instances. These instances are ideal for flexible workloads that can tolerate interruptions, such as batch processing, data analysis, and background processing jobs."
      },
      {
        "date": "2024-06-15T22:38:00.000Z",
        "voteCount": 2,
        "content": "I think all options other than using the query editor will incur additional costs."
      },
      {
        "date": "2024-06-15T02:54:00.000Z",
        "voteCount": 1,
        "content": "AWS Lambda, combined with Amazon CloudWatch Events for scheduling, provides a low-cost, serverless, and reliable way to automatically run the stored procedure daily."
      },
      {
        "date": "2024-06-14T15:23:00.000Z",
        "voteCount": 1,
        "content": "A. Create an AWS Lambda function to schedule a cron job to run the stored procedure."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/amazon/view/142576-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A marketing company collects clickstream data. The company sends the clickstream data to Amazon Kinesis Data Firehose and stores the clickstream data in Amazon S3. The company wants to build a series of dashboards that hundreds of users from multiple departments will use.<br><br>The company will use Amazon QuickSight to develop the dashboards. The company wants a solution that can scale and provide daily updates about clickstream activity.<br><br>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift to store and query the clickstream data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query the clickstream data\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 analytics to query the clickstream data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess the query data through a QuickSight direct SQL query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess the query data through QuickSight SPICE (Super-fast, Parallel, In-memory Calculation Engine). Configure a daily refresh for the dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T08:05:00.000Z",
        "voteCount": 3,
        "content": "B. Use Amazon Athena to query the clickstream data.\nE. Access the query data through QuickSight SPICE (Super-fast, Parallel, In-memory Calculation Engine). Configure a daily refresh for the dataset.\n\nHere's why:\n\nB. Use Amazon Athena to query the clickstream data: Amazon Athena allows you to run SQL queries directly on data stored in Amazon S3 without the need for complex ETL processes. It is a cost-effective solution for querying large datasets on S3.\n\nE. Access the query data through QuickSight SPICE: QuickSight SPICE is designed for fast, in-memory data analysis and can scale to support many users and large datasets. By configuring a daily refresh, you ensure that the dashboards are updated with the latest data while keeping query performance high and costs low."
      },
      {
        "date": "2024-06-15T22:49:00.000Z",
        "voteCount": 2,
        "content": "Agree with B &amp; E. Athena would be cheaper than Redshift. S3 analytics is irrelevant. The functionality in SPICE should be more cost effective than direct SQL by reducing the frequency and volume of queries."
      },
      {
        "date": "2024-06-15T03:00:00.000Z",
        "voteCount": 1,
        "content": "Athena charges based on the amount of data scanned per query, which can be cost-effective for ad-hoc querying and periodic updates.\n\nSPICE can be more cost-effective for frequent access and analysis by multiple users as it reduces the load on the underlying data source."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/amazon/view/142577-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is building a data orchestration workflow. The data engineer plans to use a hybrid model that includes some on-premises resources and some resources that are in the cloud. The data engineer wants to prioritize portability and open source resources.<br><br>Which service should the data engineer use in both the on-premises environment and the cloud-based environment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Data Exchange",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Simple Workflow Service (Amazon SWF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Workflows for Apache Airflow (Amazon MWAA)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T22:01:00.000Z",
        "voteCount": 1,
        "content": "AMWAA is just Apache Airflow managed by AWS. Using it means you can use the same dags for your on-premises solution that you also use on cloud"
      },
      {
        "date": "2024-07-03T08:07:00.000Z",
        "voteCount": 2,
        "content": "C. Amazon Managed Workflows for Apache Airflow (Amazon MWAA)\n\nAmazon MWAA is a managed service for Apache Airflow, which is an open-source workflow automation tool. Apache Airflow can be used both on-premises and in the cloud, making it ideal for hybrid environments. Using Amazon MWAA allows the data engineer to leverage the managed service in the cloud while maintaining the ability to use the same open-source Airflow setup on-premises, ensuring portability and consistency across environments."
      },
      {
        "date": "2024-06-23T09:43:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      },
      {
        "date": "2024-06-15T03:02:00.000Z",
        "voteCount": 2,
        "content": "Airflow can orchestrate workflows that involve both on-premises and cloud resources, making it ideal for hybrid models."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/amazon/view/142542-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A gaming company uses a NoSQL database to store customer information. The company is planning to migrate to AWS.<br><br>The company needs a fully managed AWS solution that will handle high online transaction processing (OLTP) workload, provide single-digit millisecond performance, and provide high availability around the world.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Keyspaces (for Apache Cassandra)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DocumentDB (with MongoDB compatibility)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon DynamoDB\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Timestream"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T13:18:00.000Z",
        "voteCount": 7,
        "content": "provide single-digit millisecond performance =&gt; DynamoDB"
      },
      {
        "date": "2024-07-11T06:30:00.000Z",
        "voteCount": 1,
        "content": "Also RDS and DynamoDB = OLTP"
      },
      {
        "date": "2024-06-20T09:00:00.000Z",
        "voteCount": 1,
        "content": "No doubt - DynamoDB"
      },
      {
        "date": "2024-06-15T03:03:00.000Z",
        "voteCount": 1,
        "content": "Amazon DynamoDB is the most appropriate choice given the company's requirements for high performance, global availability, and minimal operational overhead."
      },
      {
        "date": "2024-06-14T13:17:00.000Z",
        "voteCount": 3,
        "content": "C\nprovide single-digit millisecond performance =&gt; DynamoDB"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/amazon/view/142578-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer creates an AWS Lambda function that an Amazon EventBridge event will invoke. When the data engineer tries to invoke the Lambda function by using an EventBridge event, an AccessDeniedException message appears.<br><br>How should the data engineer resolve the exception?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the trust policy of the Lambda function execution role allows EventBridge to assume the execution role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that both the IAM role that EventBridge uses and the Lambda function's resource-based policy have the necessary permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the subnet where the Lambda function is deployed is configured to be a private subnet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that EventBridge schemas are valid and that the event mapping configuration is correct."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T20:09:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-06-24T05:07:00.000Z",
        "voteCount": 2,
        "content": "Only B - makes sense"
      },
      {
        "date": "2024-06-22T12:53:00.000Z",
        "voteCount": 2,
        "content": "\"B\" is corect because the only way to resolve the AccessDeniedException message is to make sure both the IAM role for EventBridge and the Lambda function's resource-based policy have the necessary permissions."
      },
      {
        "date": "2024-06-17T12:29:00.000Z",
        "voteCount": 4,
        "content": "The lambda resource based policy must allow the events principle to invoke the lambda function. https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html#eb-schedule-create-rule and https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html#eb-schedule-create-rule Amazon SQS, Amazon SNS, Lambda, CloudWatch Logs, and EventBridge bus targets do not use roles, and permissions to EventBridge must be granted via a resource policy."
      },
      {
        "date": "2024-06-15T23:04:00.000Z",
        "voteCount": 2,
        "content": "The trust policy is what grants an AWS service permission to use the role on behalf of the user. Without this trust relationship, EventBridge won\u2019t have the necessary permissions to invoke the Lambda function."
      },
      {
        "date": "2024-07-04T06:16:00.000Z",
        "voteCount": 4,
        "content": "Bro you don't assume the execution role. That's for Lambda to do its thing. EventBridge is just the trigger."
      },
      {
        "date": "2024-06-15T03:06:00.000Z",
        "voteCount": 2,
        "content": "IAM Role for EventBridge: EventBridge needs permission to invoke the Lambda function.\nLambda Resource-Based Policy: The Lambda function must have a resource-based policy that allows EventBridge to invoke it."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/amazon/view/142579-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses a data lake that is based on an Amazon S3 bucket. To comply with regulations, the company must apply two layers of server-side encryption to files that are uploaded to the S3 bucket. The company wants to use an AWS Lambda function to apply the necessary encryption.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse both server-side encryption with AWS KMS keys (SSE-KMS) and the Amazon S3 Encryption Client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with customer-provided keys (SSE-C) before files are uploaded.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse server-side encryption with AWS KMS keys (SSE-KMS)."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-23T09:48:00.000Z",
        "voteCount": 5,
        "content": "Answer is B"
      },
      {
        "date": "2024-08-21T00:30:00.000Z",
        "voteCount": 1,
        "content": "The most crucial objective in the problem is \"Two layers of server-side encryption must be applied.\"\n\nA: SSE-KMS is a single-layer server-side encryption that uses AWS KMS keys to encrypt data.\nThe Amazon S3 Encryption Client performs client-side encryption, not server-side encryption.\nC: SSE-C is server-side encryption that uses customer-provided encryption keys to encrypt data.\nThis does not provide two layers of encryption.\nD: SSE-KMS is a single-layer server-side encryption. It does not meet the encryption requirement of two layers of encryption.\n\nB: DSSE-KMS (dual-layer server-side encryption) uses two layers of encryption to encrypt data using keys managed by AWS KMS. The first layer is used to encrypt the data key, and the second layer is used to encrypt the actual data. This provides the two layers of server-side encryption required to meet compliance requirements."
      },
      {
        "date": "2024-07-03T09:37:00.000Z",
        "voteCount": 2,
        "content": "B. Use dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\n\nDual-layer server-side encryption with AWS KMS keys (DSSE-KMS) is specifically designed to apply two layers of encryption to meet regulatory compliance requirements. This ensures that each object stored in Amazon S3 is encrypted twice, providing the additional security layer that the company needs."
      },
      {
        "date": "2024-07-02T17:40:00.000Z",
        "voteCount": 1,
        "content": "The solution that will meet these requirements is Option A: Use both server-side encryption with AWS KMS keys (SSE-KMS) and the Amazon S3 Encryption Client.\n\nThis approach provides two layers of encryption. The first layer is the server-side encryption with AWS KMS keys (SSE-KMS), which encrypts the data at rest. The second layer is the client-side encryption using the Amazon S3 Encryption Client before the data is uploaded to S3. This way, the data is already encrypted when it arrives at S3 and then it gets encrypted again by S3, thus providing two layers of encryption.\n\nThe other options are not as suitable:\n\nOption B: There\u2019s no such thing as dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\nOption C: Server-side encryption with customer-provided keys (SSE-C) only provides one layer of encryption.\nOption D: Server-side encryption with AWS KMS keys (SSE-KMS) also only provides one layer of encryption"
      },
      {
        "date": "2024-08-21T00:20:00.000Z",
        "voteCount": 1,
        "content": "1) DSSE-KMS is a real feature. In 2021, AWS announced Dual-Layer Server-Side Encryption (DSSE-KMS) for S3. This feature can be used with S3 Managed Keys (SSE-S3) to provide an additional layer of security for your data.\n2) Two-layer encryption generally refers to applying two different encryption mechanisms within the same system or service.\nClient-side encryption encrypts data before it reaches S3, so S3 itself treats data as already encrypted. Therefore, SSE-KMS does not provide an additional layer of encryption for client-side encrypted data; it simply encrypts already encrypted data once more. This may not provide much of a practical security enhancement.\nDSSE-KMS, on the other hand, encrypts data twice within S3 using two different keys, effectively providing two layers of encryption.\nUsing client-side encryption together with SSE-KMS can provide an additional layer of security, but is not two-tier server-side encryption in the strict sense."
      },
      {
        "date": "2024-06-24T10:57:00.000Z",
        "voteCount": 1,
        "content": "Using dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS) applies two layers of encryption to objects when they are uploaded to Amazon S3. DSSE-KMS helps you more easily fulfill compliance standards that require you to apply multilayer encryption to your data and have full control of your encryption keys."
      },
      {
        "date": "2024-06-24T05:17:00.000Z",
        "voteCount": 2,
        "content": "I guess that right answer is - B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingDSSEncryption.html"
      },
      {
        "date": "2024-06-23T09:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2024-06-15T03:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-dsse-encryption.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/amazon/view/142580-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer notices that Amazon Athena queries are held in a queue before the queries run.<br><br>How can the data engineer prevent the queries from queueing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the query result limit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure provisioned capacity for an existing workgroup.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse federated queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users who run the Athena queries to an existing workgroup."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T09:39:00.000Z",
        "voteCount": 2,
        "content": "B. Configure provisioned capacity for an existing workgroup.\n\nProvisioned capacity in Amazon Athena allows you to allocate dedicated query processing capacity to a workgroup. This helps ensure that your queries are run without being held in a queue, providing more consistent and predictable performance."
      },
      {
        "date": "2024-07-02T08:02:00.000Z",
        "voteCount": 1,
        "content": "In my opinion - only B - makes sense"
      },
      {
        "date": "2024-06-29T00:07:00.000Z",
        "voteCount": 3,
        "content": "B is good.\nhttps://aws.amazon.com/blogs/aws/introducing-athena-provisioned-capacity/"
      },
      {
        "date": "2024-06-15T03:10:00.000Z",
        "voteCount": 2,
        "content": "Provisioning capacity ensures that there are sufficient dedicated resources available to handle the query load, thereby preventing queries from being held in a queue. This approach directly addresses the issue by increasing the available processing capacity for your queries."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/amazon/view/143046-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer needs to debug an AWS Glue job that reads from Amazon S3 and writes to Amazon Redshift. The data engineer enabled the bookmark feature for the AWS Glue job.<br>The data engineer has set the maximum concurrency for the AWS Glue job to 1.<br><br>The AWS Glue job is successfully writing the output to Amazon Redshift. However, the Amazon S3 files that were loaded during previous runs of the AWS Glue job are being reprocessed by subsequent runs.<br><br>What is the likely reason the AWS Glue job is reprocessing the files?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe AWS Glue job does not have the s3:GetObjectAcl permission that is required for bookmarks to work correctly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe maximum concurrency for the AWS Glue job is set to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe data engineer incorrectly specified an older version of AWS Glue for the Glue job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe AWS Glue job does not have a required commit statement.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-06T07:40:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data"
      },
      {
        "date": "2024-10-04T04:03:00.000Z",
        "voteCount": 1,
        "content": "Ensure that your job run script ends with the following commit:\n\njob.commit()\n\nWhen you include this object, AWS Glue records the timestamp and path of the job run. If you run the job again with the same path, AWS Glue processes only the new files. If you don't include this object and job bookmarks are enabled, the job reprocesses the already processed files along with the new files and creates redundancy in the job's target data store.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data"
      },
      {
        "date": "2024-09-17T14:09:00.000Z",
        "voteCount": 1,
        "content": "I would go with A option"
      },
      {
        "date": "2024-09-05T20:58:00.000Z",
        "voteCount": 1,
        "content": "A. The AWS Glue job does not have the s3:GetObjectAcl permission that is required for bookmarks to work correctly."
      },
      {
        "date": "2024-08-18T16:33:00.000Z",
        "voteCount": 1,
        "content": "Answer A\n\nthis is a job bookmarks permissions issue"
      },
      {
        "date": "2024-08-08T09:44:00.000Z",
        "voteCount": 4,
        "content": "For AWS Glue bookmarks to function correctly, the job needs the necessary permissions to read and write bookmark data, including the s3:GetObjectAcl permission. If these permissions are not correctly set, the job may not be able to track which files have already been processed, leading to reprocessing of previously processed files."
      },
      {
        "date": "2024-07-17T22:16:00.000Z",
        "voteCount": 1,
        "content": "AWS Glue Job requires the commit statement to save the last successful run/processing"
      },
      {
        "date": "2024-07-02T08:07:00.000Z",
        "voteCount": 2,
        "content": "For me - D looks correct"
      },
      {
        "date": "2024-06-29T18:21:00.000Z",
        "voteCount": 3,
        "content": "The commit statement (Option D) is not required for AWS Glue jobs. AWS Glue commits any open transactions to the database when all the script statements finish running."
      },
      {
        "date": "2024-07-17T22:15:00.000Z",
        "voteCount": 1,
        "content": "It is the commit statement that ensures AWS saves the last successful processing"
      },
      {
        "date": "2024-07-02T22:08:00.000Z",
        "voteCount": 1,
        "content": "I've not found any information that s3:GetObjectACL is necessary for Glue bookmarks, so I'm pretty sure that A is wrong"
      },
      {
        "date": "2024-06-29T00:15:00.000Z",
        "voteCount": 4,
        "content": "D is good\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/amazon/view/143047-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An ecommerce company wants to use AWS to migrate data pipelines from an on-premises environment into the AWS Cloud. The company currently uses a third-party tool in the on-premises environment to orchestrate data ingestion processes.<br><br>The company wants a migration solution that does not require the company to manage servers. The solution must be able to orchestrate Python and Bash scripts. The solution must not require the company to refactor any code.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Lambda",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Workflows for Apache Airflow (Amazon MVVAA)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Step Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAWS Glue"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-04T21:14:00.000Z",
        "voteCount": 2,
        "content": "B - because company want to use same tool on premises and least operational overhead"
      },
      {
        "date": "2024-07-03T12:42:00.000Z",
        "voteCount": 1,
        "content": "\"The company wants a migration solution that does not require the company to manage servers.\". How is it Amazon Managed Workflows for Apache Airflow and not Step Functions when Step Functions is the serverless of the two ?"
      },
      {
        "date": "2024-07-04T06:22:00.000Z",
        "voteCount": 4,
        "content": "\"All of the components contained in the outer box (in the image below) appear as a single Amazon MWAA environment in your account. The Apache Airflow Scheduler and Workers are AWS Fargate (Fargate) containers that connect to the private subnets in the Amazon VPC for your environment. Each environment has its own Apache Airflow metadatabase managed by AWS that is accessible to the Scheduler and Workers Fargate containers via a privately-secured VPC endpoint.\"\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\n\nSo MWAA is hosted on ECS Fargate - serverless. Supports Bash and Python. B is correct."
      },
      {
        "date": "2024-07-03T09:44:00.000Z",
        "voteCount": 3,
        "content": "An ecommerce company wants to use AWS to migrate data pipelines from an on-premises environment into the AWS Cloud. The company currently uses a third-party tool in the on-premises environment to orchestrate data ingestion processes.\n\nThe company wants a migration solution that does not require the company to manage servers. The solution must be able to orchestrate Python and Bash scripts. The solution must not require the company to refactor any code.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. AWS Lambda\nB. Amazon Managed Workflows for Apache Airflow (Amazon MVVAA)\nC. AWS Step Functions\nD. AWS Glue"
      },
      {
        "date": "2024-07-02T08:09:00.000Z",
        "voteCount": 1,
        "content": "B - best fits in task requirements"
      },
      {
        "date": "2024-06-29T00:20:00.000Z",
        "voteCount": 2,
        "content": "My Choice is B"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/amazon/view/143051-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A retail company stores data from a product lifecycle management (PLM) application in an on-premises MySQL database. The PLM application frequently updates the database when transactions occur.<br><br>The company wants to gather insights from the PLM application in near real time. The company wants to integrate the insights with other business datasets and to analyze the combined dataset by using an Amazon Redshift data warehouse.<br><br>The company has already established an AWS Direct Connect connection between the on-premises infrastructure and AWS.<br><br>Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a scheduled AWS Glue extract, transform, and load (ETL) job to get the MySQL database updates by using a Java Database Connectivity (JDBC) connection. Set Amazon Redshift as the destination for the ETL job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a full load plus CDC task in AWS Database Migration Service (AWS DMS) to continuously replicate the MySQL database changes. Set Amazon Redshift as the destination for the task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon AppFlow SDK to build a custom connector for the MySQL database to continuously replicate the database changes. Set Amazon Redshift as the destination for the connector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun scheduled AWS DataSync tasks to synchronize data from the MySQL database. Set Amazon Redshift as the destination for the tasks."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T12:04:00.000Z",
        "voteCount": 1,
        "content": "AWS DMS allows for change data capture that will have the destination updated at near real time with changes from the source database"
      },
      {
        "date": "2024-07-24T10:16:00.000Z",
        "voteCount": 1,
        "content": "Should B. Makes most sense."
      },
      {
        "date": "2024-07-22T08:10:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt\nOption B (AWS DMS) is the most suitable with the least development effort. AWS DMS supports continuous data replication with CDC capabilities, making it well-suited for near real-time data integration from MySQL to Amazon Redshift. It handles schema conversion and simplifies the setup process compared to custom development or scheduled ETL jobs. Given the existing AWS Direct Connect, AWS DMS can efficiently replicate MySQL updates to Redshift with minimal latency, meeting the company's requirement for near real-time insights integration. Therefore, option B is the correct choice."
      },
      {
        "date": "2024-06-29T00:42:00.000Z",
        "voteCount": 3,
        "content": "B is good. DMS+CDC...\n\nhttps://aws.amazon.com/ko/blogs/apn/change-data-capture-from-on-premises-sql-server-to-amazon-redshift-target/"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/amazon/view/143053-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A marketing company uses Amazon S3 to store clickstream data. The company queries the data at the end of each day by using a SQL JOIN clause on S3 objects that are stored in separate buckets.<br><br>The company creates key performance indicators (KPIs) based on the objects. The company needs a serverless solution that will give users the ability to query data by partitioning the data. The solution must maintain the atomicity, consistency, isolation, and durability (ACID) properties of the data.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon S3 Select",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Redshift Spectrum",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Athena\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon EMR"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T22:10:00.000Z",
        "voteCount": 1,
        "content": "C. Amazon Athena"
      },
      {
        "date": "2024-09-06T01:44:00.000Z",
        "voteCount": 1,
        "content": "Amazon Redshift Spectrum is not serverless."
      },
      {
        "date": "2024-07-18T22:01:00.000Z",
        "voteCount": 2,
        "content": "Athena is cost effective as it only charges for queries run"
      },
      {
        "date": "2024-07-03T09:48:00.000Z",
        "voteCount": 4,
        "content": "C. Amazon Athena\n\nHere's why Amazon Athena is suitable:\n\nServerless: Amazon Athena is a serverless query service that allows you to run SQL queries directly on data stored in Amazon S3 without the need to manage infrastructure.\nPartitioning: Athena supports querying data by partitioning, which can significantly improve query performance by limiting the amount of data scanned.\nACID Properties: Although Amazon S3 itself does not provide ACID properties, Amazon Athena ensures consistency in query results and durability of the data stored in S3 through its managed query execution.\nCost-effective: With Amazon Athena, you only pay for the queries you run and the amount of data scanned, making it a cost-effective choice compared to managing infrastructure or using dedicated services like Amazon Redshift Spectrum or Amazon EMR."
      },
      {
        "date": "2024-07-02T08:18:00.000Z",
        "voteCount": 1,
        "content": "C - cheapest solution in this case"
      },
      {
        "date": "2024-06-29T00:52:00.000Z",
        "voteCount": 1,
        "content": "C is good."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/amazon/view/143054-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company wants to migrate data from an Amazon RDS for PostgreSQL DB instance in the eu-east-1 Region of an AWS account named Account_A. The company will migrate the data to an Amazon Redshift cluster in the eu-west-1 Region of an AWS account named Account_B.<br><br>Which solution will give AWS Database Migration Service (AWS DMS) the ability to replicate data between two data stores?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS DMS replication instance in Account_B in eu-west-1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS DMS replication instance in Account_B in eu-east-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS DMS replication instance in a new AWS account in eu-west-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS DMS replication instance in Account_A in eu-east-1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T17:23:00.000Z",
        "voteCount": 1,
        "content": "When you use WS DMS to migrate data between different AWS Regions or accounts, you must remember the following:\n\nThe replication instance must be created in the same Region as the source database.\nThe target endpoint must be created in the Region where the target data store is located.\nYou must set up the required IAM roles and permissions to enable DMS to access the source and target resources."
      },
      {
        "date": "2024-07-17T22:27:00.000Z",
        "voteCount": 4,
        "content": "Redshift needs to be in the same region as the replication instance see docs:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites"
      },
      {
        "date": "2024-07-06T07:56:00.000Z",
        "voteCount": 4,
        "content": "Redshift has to be in the same region as the DMS\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites"
      },
      {
        "date": "2024-07-02T16:12:00.000Z",
        "voteCount": 1,
        "content": "To enable AWS Database Migration Service (AWS DMS) to replicate data between two data stores in different AWS Regions, you should choose option A. Here\u2019s why:\n\nOption A: Set up an AWS DMS replication instance in Account_B in eu-west-1. This approach allows you to configure replication between the Amazon RDS for PostgreSQL DB instance in eu-east-1 and the Amazon Redshift cluster in eu-west-1. By using AWS DMS, you can efficiently migrate data across Regions while minimizing downtime and ensuring data consistency"
      },
      {
        "date": "2024-07-02T09:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html"
      },
      {
        "date": "2024-07-02T09:24:00.000Z",
        "voteCount": 1,
        "content": "The correct solution to replicate data between the Amazon RDS for PostgreSQL DB instance in Account_A (eu-east-1) and the Amazon Redshift cluster in Account_B (eu-west-1) using AWS Database Migration Service (AWS DMS) is:\n\nA. Set up an AWS DMS replication instance in Account_B in eu-west-1."
      },
      {
        "date": "2024-07-02T08:24:00.000Z",
        "voteCount": 2,
        "content": "B - becuase AWS DMS must be in a same region as AWS redshift cluster\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites"
      },
      {
        "date": "2024-06-29T01:11:00.000Z",
        "voteCount": 1,
        "content": "My Choice is D"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/amazon/view/143056-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses Amazon S3 as a data lake. The company sets up a data warehouse by using a multi-node Amazon Redshift cluster. The company organizes the data files in the data lake based on the data source of each data file.<br><br>The company loads all the data files into one table in the Redshift cluster by using a separate COPY command for each data file location. This approach takes a long time to load all the data files into the table. The company must increase the speed of the data ingestion. The company does not want to increase the cost of the process.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a provisioned Amazon EMR cluster to copy all the data files into one folder. Use a COPY command to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad all the data files in parallel into Amazon Aurora. Run an AWS Glue job to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Give job to copy all the data files into one folder. Use a COPY command to load the data into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manifest file that contains the data file locations. Use a COPY command to load the data into Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-18T22:05:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer based on the docs in this page https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2024-07-02T08:27:00.000Z",
        "voteCount": 4,
        "content": "Only D makes sense\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html"
      },
      {
        "date": "2024-06-29T01:34:00.000Z",
        "voteCount": 1,
        "content": "D is good\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html\nhttps://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/amazon/view/143057-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company plans to use Amazon Kinesis Data Firehose to store data in Amazon S3. The source data consists of 2 MB .csv files. The company must convert the .csv files to JSON format. The company must store the files in Apache Parquet format.<br><br>Which solution will meet these requirements with the LEAST development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Firehose to convert the .csv files to JSON. Use an AWS Lambda function to store the files in Parquet format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Firehose to convert the .csv files to JSON and to store the files in Parquet format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON and stores the files in Parquet format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON. Use Kinesis Data Firehose to store the files in Parquet format."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T00:24:00.000Z",
        "voteCount": 6,
        "content": "https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html"
      },
      {
        "date": "2024-07-17T11:20:00.000Z",
        "voteCount": 2,
        "content": "why do you need lambda in the middle, per you link Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3...my choice is B"
      },
      {
        "date": "2024-08-18T16:18:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nthere is need is need to invoke Lambda"
      },
      {
        "date": "2024-09-17T07:07:00.000Z",
        "voteCount": 1,
        "content": "If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information"
      },
      {
        "date": "2024-08-18T16:21:00.000Z",
        "voteCount": 2,
        "content": "Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform source data in Amazon Data Firehose.\nAnswer D"
      },
      {
        "date": "2024-08-10T14:03:00.000Z",
        "voteCount": 2,
        "content": "Kinesis Data Firehose: It has built-in support for data transformation and format conversion. It can directly convert incoming data from .csv to JSON format and then further convert the data to Apache Parquet format before storing it in Amazon S3.\n\nMinimal Development Effort: This option requires the least development effort because Kinesis Data Firehose handles both the transformation (from .csv to JSON) and the format conversion (to Parquet) natively. No additional AWS Lambda functions or custom code are needed."
      },
      {
        "date": "2024-08-02T07:58:00.000Z",
        "voteCount": 4,
        "content": "B. Why? Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3.\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\nWith that LEAST development effort, why do we need to use Lambda additionally? :D"
      },
      {
        "date": "2024-08-18T16:20:00.000Z",
        "voteCount": 1,
        "content": "read to understand:\n\nAmazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform source data in Amazon Data Firehose."
      },
      {
        "date": "2024-07-24T00:59:00.000Z",
        "voteCount": 2,
        "content": "Option D - Need to convert the inout data from .csv to JSON first. Firehose can't do that without the help of a lambda function in this case. After firehose can convert to .parquet and deliver it to s3"
      },
      {
        "date": "2024-07-21T02:54:00.000Z",
        "voteCount": 4,
        "content": "Answer D\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nAmazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform data in Amazon Data Firehose."
      },
      {
        "date": "2024-07-02T08:29:00.000Z",
        "voteCount": 2,
        "content": "B - least development efforts"
      },
      {
        "date": "2024-06-29T18:36:00.000Z",
        "voteCount": 4,
        "content": "By using the built-in transformation and format conversion features of Kinesis Data Firehose, you achieve the desired result with minimal custom development, thereby meeting the requirements efficiently and cost-effectively."
      },
      {
        "date": "2024-06-29T01:35:00.000Z",
        "voteCount": 1,
        "content": "D is good\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html"
      },
      {
        "date": "2024-06-30T15:58:00.000Z",
        "voteCount": 3,
        "content": "\" If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information\""
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/amazon/view/143058-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is using an AWS Transfer Family server to migrate data from an on-premises environment to AWS. Company policy mandates the use of TLS 1.2 or above to encrypt the data in transit.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate new SSH keys for the Transfer Family server. Make the old keys and the new keys available for use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the security group rules for the on-premises network to allow only connections that use TLS 1.2 or above.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the security policy of the Transfer Family server to specify a minimum protocol version of TLS 1.2\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an SSL certificate on the Transfer Family server to encrypt data transfers by using TLS 1.2."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-08T10:29:00.000Z",
        "voteCount": 1,
        "content": "A company is using an AWS Transfer Family server to migrate data from an on-premises environment to AWS. Company policy mandates the use of TLS 1.2 or above to encrypt the data in transit.\n\nWhich solution will meet these requirements?\n\nA. Generate new SSH keys for the Transfer Family server. Make the old keys and the new keys available for use.\nB. Update the security group rules for the on-premises network to allow only connections that use TLS 1.2 or above.\nC. Update the security policy of the Transfer Family server to specify a minimum protocol version of TLS 1.2\nD. Install an SSL certificate on the Transfer Family server to encrypt data transfers by using TLS 1.2."
      },
      {
        "date": "2024-07-02T08:32:00.000Z",
        "voteCount": 2,
        "content": "Only C is good"
      },
      {
        "date": "2024-06-29T01:37:00.000Z",
        "voteCount": 2,
        "content": "C is correct\n\nhttps://docs.aws.amazon.com/transfer/latest/userguide/security-policies.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/amazon/view/143060-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company wants to migrate an application and an on-premises Apache Kafka server to AWS. The application processes incremental updates that an on-premises Oracle database sends to the Kafka server. The company wants to use the replatform migration strategy instead of the refactor strategy.<br><br>Which solution will meet these requirements with the LEAST management overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Kinesis Data Streams",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Streaming for Apache Kafka (Amazon MSK) provisioned cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Kinesis Data Firehose",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Managed Streaming for Apache Kafka (Amazon MSK) Serverless\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T22:43:00.000Z",
        "voteCount": 3,
        "content": "D - becase this is lift-and-shift migration and serveless - because LEAST management overhead"
      },
      {
        "date": "2024-06-29T01:48:00.000Z",
        "voteCount": 2,
        "content": "D is good"
      },
      {
        "date": "2024-06-29T01:49:00.000Z",
        "voteCount": 1,
        "content": "C. Amazon Kinesis Data Firehose: This service delivers real-time data to other AWS destinations, but it's not a direct replacement for Kafka and requires additional configuration for replicating data streams.\nD. Amazon Managed Streaming for Apache Kafka (Amazon MSK) Serverless: This is the best fit as it provides a fully managed Kafka experience with automatic scaling and eliminates the need to manage servers or infrastructure. This aligns perfectly with the replatform strategy and minimizes management overhead."
      },
      {
        "date": "2024-06-29T01:49:00.000Z",
        "voteCount": 1,
        "content": "A. Amazon Kinesis Data Streams: This is a managed service for ingesting and processing real-time streaming data, but it requires separate configuration for message producers and consumers. Not ideal for minimal management overhead.\nB. Amazon Managed Streaming for Apache Kafka (Amazon MSK) provisioned cluster: While MSK offers a familiar Kafka experience, it requires managing the underlying infrastructure like cluster scaling and configuration. Increases management overhead."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/amazon/view/143061-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is building an automated extract, transform, and load (ETL) ingestion pipeline by using AWS Glue. The pipeline ingests compressed files that are in an Amazon S3 bucket. The ingestion pipeline must support incremental data processing.<br><br>Which AWS Glue feature should the data engineer use to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkflows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTriggers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob bookmarks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClassifiers"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-18T22:39:00.000Z",
        "voteCount": 2,
        "content": "C AWS GLue bookmarks are used to implement incremental processing"
      },
      {
        "date": "2024-07-03T10:18:00.000Z",
        "voteCount": 1,
        "content": "C. Job bookmarks\n\nHere's why job bookmarks are the appropriate feature:\n\nIncremental Processing: Job bookmarks in AWS Glue help track the last processed state of data in Amazon S3. They enable the ETL job to resume from where it left off in case of interruptions or subsequent runs, ensuring that only new or modified data since the last successful run is processed (incremental processing).\nAutomated ETL: Job bookmarks work seamlessly within AWS Glue ETL jobs, allowing the job to efficiently manage the state of processed data without the need for manual intervention.\nSupport for Compressed Files: AWS Glue natively supports reading compressed files from Amazon S3, so the ingestion pipeline can handle compressed data formats efficiently."
      },
      {
        "date": "2024-07-02T08:36:00.000Z",
        "voteCount": 1,
        "content": "C - is right"
      },
      {
        "date": "2024-06-29T01:51:00.000Z",
        "voteCount": 4,
        "content": "C is correct answer..\n\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/amazon/view/143062-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A banking company uses an application to collect large volumes of transactional data. The company uses Amazon Kinesis Data Streams for real-time analytics. The company\u2019s application uses the PutRecord action to send data to Kinesis Data Streams.<br><br>A data engineer has observed network outages during certain times of day. The data engineer wants to configure exactly-once delivery for the entire processing pipeline.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the checkpoint configuration of the Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) data collection application to avoid duplicate processing of events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign the data source so events are not ingested into Kinesis Data Streams multiple times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop using Kinesis Data Streams. Use Amazon EMR instead. Use Apache Flink and Apache Spark Streaming in Amazon EMR."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T07:11:00.000Z",
        "voteCount": 1,
        "content": "A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source."
      },
      {
        "date": "2024-07-08T10:41:00.000Z",
        "voteCount": 1,
        "content": "A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\nExplanation:\nExactly-Once Delivery: Ensuring exactly-once delivery is a challenge in distributed systems, especially in the presence of network outages and retries. By embedding a unique ID in each record at the source, you can track and identify duplicate records during processing. This approach allows you to implement idempotent processing, where duplicate records can be detected and discarded, ensuring that each record is processed exactly once.\nDe-duplication Logic: Implementing de-duplication logic based on unique IDs ensures that even if the same record is ingested multiple times due to retries or network issues, it will be processed only once by the downstream applications."
      },
      {
        "date": "2024-07-02T06:37:00.000Z",
        "voteCount": 3,
        "content": "A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\nThis approach ensures that even if a record is sent more than once due to network outages or other issues, it will only be processed once because the unique ID can be used to identify and remove any duplicates. This is a common pattern for achieving exactly-once processing semantics in distributed systems. The other options do not guarantee exactly-once delivery across the entire pipeline. Option B is partially correct but it only avoids duplicate processing within the Amazon Managed Service for Apache Flink, not across the entire pipeline. Option C is not always feasible because network issues and other factors can lead to events being ingested into Kinesis Data Streams multiple times. Option D involves changing the entire technology stack, which is not necessary to achieve the desired outcome and could introduce additional complexity and cost."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/amazon/view/143120-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores logs in an Amazon S3 bucket. When a data engineer attempts to access several log files, the data engineer discovers that some files have been unintentionally deleted.<br><br>The data engineer needs a solution that will prevent unintentional file deletion in the future.<br><br>Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually back up the S3 bucket on a regular basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Versioning for the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure replication for the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon S3 Glacier storage class to archive the data that is in the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T10:30:00.000Z",
        "voteCount": 3,
        "content": "S3 Versioning keeps multiple versions of an object in the same bucket. When you enable versioning, every time an object is overwritten or deleted, a new version of that object is created, and the previous version is retained. This ensures that no data is lost permanently due to accidental deletions or overwrites."
      },
      {
        "date": "2024-07-02T08:39:00.000Z",
        "voteCount": 2,
        "content": "B - is right answer"
      },
      {
        "date": "2024-06-30T15:52:00.000Z",
        "voteCount": 3,
        "content": "B is good.."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/amazon/view/143122-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A telecommunications company collects network usage data throughout each day at a rate of several thousand data points each second. The company runs an application to process the usage data in real time. The company aggregates and stores the data in an Amazon Aurora DB instance.<br><br>Sudden drops in network usage usually indicate a network outage. The company must be able to identify sudden drops in network usage so the company can take immediate remedial actions.<br><br>Which solution will meet this requirement with the LEAST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to query Aurora for drops in network usage. Use Amazon EventBridge to automatically invoke the Lambda function every minute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the processing application to publish the data to an Amazon Kinesis data stream. Create an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to detect drops in network usage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the Aurora database with an Amazon DynamoDB table. Create an AWS Lambda function to query the DynamoDB table for drops in network usage every minute. Use DynamoDB Accelerator (DAX) between the processing application and DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function within the Database Activity Streams feature of Aurora to detect drops in network usage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-10T15:48:00.000Z",
        "voteCount": 1,
        "content": "Regarding D, Database Activity Streams in Aurora are primarily for auditing databases actities, not for analyzing app data."
      },
      {
        "date": "2024-08-08T10:05:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2024-08-06T17:38:00.000Z",
        "voteCount": 2,
        "content": "reduces latency because it is analyze before the data even gets to the Aurora DB"
      },
      {
        "date": "2024-07-14T09:28:00.000Z",
        "voteCount": 2,
        "content": "Option D is the optimal choice because it leverages Aurora's Database Activity Streams to enable real-time monitoring and immediate response to changes in network usage data. This approach ensures the least latency in detecting and responding to sudden drops in network usage, crucial for the telecommunications company to take immediate remedial actions during network outages."
      },
      {
        "date": "2024-07-14T09:27:00.000Z",
        "voteCount": 2,
        "content": "Option D\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html\n\nIn Amazon Aurora, you start a database activity stream at the cluster level. All DB instances within your cluster have database activity streams enabled.\n\nYour Aurora DB cluster pushes activities to an Amazon Kinesis data stream in near real time. The Kinesis stream is created automatically. From Kinesis, you can configure AWS services such as Amazon Data Firehose and AWS Lambda to consume the stream and store the data."
      },
      {
        "date": "2024-09-25T14:34:00.000Z",
        "voteCount": 1,
        "content": "The Database Activity Streams feature of Amazon Aurora can help monitor database activity, but it doesn't directly detect drops in network usage:"
      },
      {
        "date": "2024-07-08T11:59:00.000Z",
        "voteCount": 3,
        "content": "The best solution to identify sudden drops in network usage with the least latency is:\n\nB. Modify the processing application to publish the data to an Amazon Kinesis data stream. Create an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to detect drops in network usage.\n\nThis approach ensures real-time processing with minimal latency and allows immediate detection and response to network usage drops."
      },
      {
        "date": "2024-07-02T08:44:00.000Z",
        "voteCount": 3,
        "content": "I guess D. The question is which solution helps to identitfy sudden drops to take immediate actions"
      },
      {
        "date": "2024-06-30T15:54:00.000Z",
        "voteCount": 2,
        "content": "B is good"
      },
      {
        "date": "2024-07-04T07:09:00.000Z",
        "voteCount": 2,
        "content": "Thank you B master."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/amazon/view/145188-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is processing and analyzing multiple terabytes of raw data that is in Amazon S3. The data engineer needs to clean and prepare the data. Then the data engineer needs to load the data into Amazon Redshift for analytics.<br><br>The data engineer needs a solution that will give data analysts the ability to perform complex queries. The solution must eliminate the need to perform complex extract, transform, and load (ETL) processes or to manage infrastructure.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR to prepare the data. Use AWS Step Functions to load the data into Amazon Redshift. Use Amazon QuickSight to run queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue DataBrew to prepare the data. Use AWS Glue to load the data into Amazon Redshift. Use Amazon Redshift to run queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to prepare the data. Use Amazon Kinesis Data Firehose to load the data into Amazon Redshift. Use Amazon Athena to run queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to prepare the data. Use AWS Database Migration Service (AVVS DMS) to load the data into Amazon Redshift. Use Amazon Redshift Spectrum to run queries."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T00:24:00.000Z",
        "voteCount": 1,
        "content": "It can\u00b4t be D as DMS doesn\u00b4t support S3 as a source, it's B as it achieve all the goals described in the subject."
      },
      {
        "date": "2024-08-21T21:43:00.000Z",
        "voteCount": 1,
        "content": "the LEAST operational overhead ..."
      },
      {
        "date": "2024-08-11T07:58:00.000Z",
        "voteCount": 1,
        "content": "B. They can do the \"complex\" queries in redshift."
      },
      {
        "date": "2024-08-06T17:46:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/amazon/view/145187-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uses an AWS Lambda function to transfer files from a legacy SFTP environment to Amazon S3 buckets. The Lambda function is VPC enabled to ensure that all communications between the Lambda function and other AVS services that are in the same VPC environment will occur over a secure network.<br><br>The Lambda function is able to connect to the SFTP environment successfully. However, when the Lambda function attempts to upload files to the S3 buckets, the Lambda function returns timeout errors. A data engineer must resolve the timeout issues in a secure way.<br><br>Which solution will meet these requirements in the MOST cost-effective way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a NAT gateway in the public subnet of the VPC. Route network traffic to the NAT gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC gateway endpoint for Amazon S3. Route network traffic to the VPC gateway endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC interface endpoint for Amazon S3. Route network traffic to the VPC interface endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a VPC internet gateway to connect to the internet. Route network traffic to the VPC internet gateway."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-07T07:20:00.000Z",
        "voteCount": 6,
        "content": "Option B - VPC Gateway Endpoint for Amazon S3"
      },
      {
        "date": "2024-08-22T20:27:00.000Z",
        "voteCount": 1,
        "content": "The solution that will meet the requirements of resolving the timeout issues when uploading files from the Lambda function to Amazon S3 buckets in a secure and cost-effective way is C. Create a VPC interface endpoint for Amazon S3. Route network traffic to the VPC interface endpoint ."
      },
      {
        "date": "2024-08-06T17:45:00.000Z",
        "voteCount": 4,
        "content": "Option B - VPC Gateway Endpoint for Amazon S3\nWhile interface endpoints is a viable solution, it can be more complex and expensive compared to a gateway endpoint. VPC interface endpoints charge per hour and per gigabyte of data transferred."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/amazon/view/145289-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company reads data from customer databases that run on Amazon RDS. The databases contain many inconsistent fields. For example, a customer record field that iPnamed place_id in one database is named location_id in another database. The company needs to link customer records across different databases, even when customer record fields do not match.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a provisioned Amazon EMR cluster to process and analyze data in the databases. Connect to the Apache Zeppelin notebook. Use the FindMatches transform to find duplicate records in the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler to craw the databases. Use the FindMatches transform to find duplicate records in the data. Evaluate and tune the transform by evaluating the performance and results.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler to craw the databases. Use Amazon SageMaker to construct Apache Spark ML pipelines to find duplicate records in the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a provisioned Amazon EMR cluster to process and analyze data in the databases. Connect to the Apache Zeppelin notebook. Use an Apache Spark ML model to find duplicate records in the data. Evaluate and tune the model by evaluating the performance and results."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-09T16:48:00.000Z",
        "voteCount": 4,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/amazon/view/145291-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A finance company receives data from third-party data providers and stores the data as objects in an Amazon S3 bucket.<br><br>The company ran an AWS Glue crawler on the objects to create a data catalog. The AWS Glue crawler created multiple tables. However, the company expected that the crawler would create only one table.<br><br>The company needs a solution that will ensure the AVS Glue crawler creates only one table.<br><br>Which combination of solutions will meet this requirement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the object format, compression type, and schema are the same for each object.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the object format and schema are the same for each object. Do not enforce consistency for the compression type of each object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the schema is the same for each object. Do not enforce consistency for the file format and compression type of each object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the structure of the prefix for each S3 object name is consistent.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all S3 object names follow a similar pattern."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-09T16:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is AD"
      },
      {
        "date": "2024-08-09T01:25:00.000Z",
        "voteCount": 1,
        "content": "To ensure that the AWS Glue crawler creates only one table and handles the object format, compression type, schema, and prefix structure consistently:\nEnsure Consistent Object Format, Compression Type, Schema, and Prefix Structure\n1. **Consistent Object Format**:\n   - Ensure that all objects in the S3 bucket are in the same format (e.g., CSV, JSON, Parquet).\n\n2. **Consistent Compression Type**:\n   - Ensure that all objects use the same compression type (e.g., GZIP, Snappy).\n\n3. **Consistent Schema**:\n   - Ensure that all objects have the same schema (i.e., the same fields with the same data types).\n\n4. **Consistent Prefix Structure**:\n   - Ensure that all objects follow a consistent naming convention and prefix structure in the S3 bucket (e.g., `s3://your-bucket/path/to/data/`)."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/amazon/view/145713-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An application consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue. The application experiences occasional downtime. As a result of the downtime, messages within the queue expire and are deleted after 1 day. The message deletions cause data loss for the application.<br><br>Which solutions will minimize data loss for the application? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the message retention period\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the visibility timeout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a dead-letter queue (DLQ) to the SQS queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a delay queue to delay message delivery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce message processing time."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T06:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is AC"
      },
      {
        "date": "2024-08-14T05:45:00.000Z",
        "voteCount": 2,
        "content": "To minimize data loss for the application consuming messages from an Amazon SQS queue, the following two solutions are most effective:\n\nA. Increase the message retention period**: By increasing the message retention period, you ensure that messages remain in the queue for a longer duration before being automatically deleted. This provides more time for the application to recover from downtime and process the messages, thereby reducing the chance of data loss due to message expiration.\n\nC. Attach a dead-letter queue (DLQ) to the SQS queue**: A DLQ can be used to capture messages that cannot be processed successfully. When messages fail to be processed after a certain number of attempts (as defined by the redrive policy), they are moved to the DLQ. This allows you to investigate and handle these messages separately, preventing data loss."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/amazon/view/145714-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is creating near real-time dashboards to visualize time series data. The company ingests data into Amazon Managed Streaming for Apache Kafka (Amazon MSK). A customized data pipeline consumes the data. The pipeline then writes data to Amazon Keyspaces (for Apache Cassandra), Amazon OpenSearch Service, and Apache Avro objects in Amazon S3.<br><br>Which solution will make the data available for the data visualizations with the LEAST latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate OpenSearch Dashboards by using the data from OpenSearch Service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena with an Apache Hive metastore to query the Avro objects in Amazon S3. Use Amazon Managed Grafana to connect to Athena and to create the dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query the data from the Avro objects in Amazon S3. Configure Amazon Keyspaces as the data catalog. Connect Amazon QuickSight to Athena to create the dashboards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to catalog the data. Use S3 Select to query the Avro objects in Amazon S3. Connect Amazon QuickSight to the S3 bucket to create the dashboards."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T06:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2024-08-14T05:48:00.000Z",
        "voteCount": 3,
        "content": "Option A: Create OpenSearch Dashboards by using the data from OpenSearch Service is the best choice for achieving the least latency. OpenSearch is designed for low-latency data retrieval and visualization, making it ideal for near real-time dashboards"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/amazon/view/145715-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A media company wants to use Amazon OpenSearch Service to analyze rea-time data about popular musical artists and songs. The company expects to ingest millions of new data events every day. The new data events will arrive through an Amazon Kinesis data stream. The company must transform the data and then ingest the data into the OpenSearch Service domain.<br><br>Which method should the company use to ingest the data with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Logstash pipeline that has prebuilt filters to transform the data and deliver the transformed data to OpenSearch Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function to call the Amazon Kinesis Agent to transform the data and deliver the transformed data OpenSearch Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kinesis Client Library (KCL) to transform the data and deliver the transformed data to OpenSearch Service."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-18T15:54:00.000Z",
        "voteCount": 1,
        "content": "Amazon Kinesis Data Firehose is a fully managed service that reliably loads streaming data into data lakes, data stores and analytics services like OpenSearch Service. It can automatically scale to match the throughput of your data and requires no ongoing administration.\n\nAnswer A"
      },
      {
        "date": "2024-08-14T06:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2024-08-14T05:50:00.000Z",
        "voteCount": 2,
        "content": "Option A: Use Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service is the best choice for achieving the least operational overhead. Kinesis Data Firehose is a managed service that automates the data ingestion process, scales seamlessly, and integrates directly with OpenSearch Service, minimizing the need for manual intervention and infrastructure management."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/amazon/view/145293-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores customer data tables that include customer addresses in an AWS Lake Formation data lake. To comply with new regulations, the company must ensure that users cannot access data for customers who are in Canada.<br><br>The company needs a solution that will prevent user access to rows for customers who are in Canada.<br><br>Which solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a row-level filter to prevent user access to a row where the country is Canada.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that restricts user access to an address where the country is Canada.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a column-level filter to prevent user access to a row where the country is Canada.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a tag to all rows where Canada is the country. Prevent user access where the tag is equal to \u201cCanada\u201d."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-09T16:49:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/amazon/view/145294-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company has implemented a lake house architecture in Amazon Redshift. The company needs to give users the ability to authenticate into Redshift query editor by using a third-party identity provider (IdP).<br><br>A data engineer must set up the authentication mechanism.<br><br>What is the first step the data engineer should take to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the third-party IdP as an identity provider in the configuration settings of the Redshift cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the third-party IdP as an identity provider from within Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the third-party IdP as an identity provider for AVS Secrets Manager. Configure Amazon Redshift to use Secrets Manager to manage user credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the third-party IdP as an identity provider for AWS Certificate Manager (ACM). Configure Amazon Redshift to use ACM to manage user credentials."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T02:33:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html\nregister the identity provider with Amazon Redshift, using SQL statements, which set authentication parameters that are unique to the identity provider."
      },
      {
        "date": "2024-08-18T15:50:00.000Z",
        "voteCount": 3,
        "content": "To enable users to authenticate into the Amazon Redshift query editor using a third-party identity provider (IdP), the data engineer must first register that IdP within the configuration settings of the Redshift cluster itself.\n\nAmazon Redshift natively supports integrating with external identity providers to manage user authentication. By registering the third-party IdP directly in the Redshift cluster settings, it establishes the trust relationship needed for Redshift to rely on that IdP for authenticating users when they log into the query editor. Answer A"
      },
      {
        "date": "2024-08-09T16:49:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/amazon/view/145106-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company currently uses a provisioned Amazon EMR cluster that includes general purpose Amazon EC2 instances. The EMR cluster uses EMR managed scaling between one to five task nodes for the company\u2019s long-running Apache Spark extract, transform, and load (ETL) job. The company runs the ETL job every day.<br><br>When the company runs the ETL job, the EMR cluster quickly scales up to five nodes. The EMR cluster often reaches maximum CPU usage, but the memory usage remains under 30%.<br><br>The company wants to modify the EMR cluster configuration to reduce the EMR costs to run the daily ETL job.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the maximum number of task nodes for EMR managed scaling to 10.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the task node type from general purpose EC2 instances to memory optimized EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the task node type from general purpose Re instances to compute optimized EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the scaling cooldown period for the provisioned EMR cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-07T11:41:00.000Z",
        "voteCount": 4,
        "content": "current situation shows that the EMR cluster is reaching maximum CPU usage, but memory usage remains low (under 30%). This indicates that the workload is CPU-bound rather than memory-bound."
      },
      {
        "date": "2024-08-05T23:33:00.000Z",
        "voteCount": 4,
        "content": "Since the ETL job reaches maximum CPU usage but not memory usage, switching from general-purpose instances to compute-optimized instances (such as C5 or C6g instances) can provide better performance per dollar for CPU-bound workloads."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/amazon/view/145107-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company uploads .csv files to an Amazon S3 bucket. The company\u2019s data platform team has set up an AWS Glue crawler to perform data discovery and to create the tables and schemas.<br><br>An AWS Glue job writes processed data from the tables to an Amazon Redshift database. The AWS Glue job handles column mapping and creates the Amazon Redshift tables in the Redshift database appropriately.<br><br>If the company reruns the AWS Glue job for any reason, duplicate records are introduced into the Amazon Redshift tables. The company needs a solution that will update the Redshift tables without duplicates.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the AWS Glue job to copy the rows into a staging Redshift table. Add SQL commands to update the existing rows with new values from the staging Redshift table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the AWS Glue job to load the previously inserted data into a MySQL database. Perform an upsert operation in the MySQL database. Copy the results to the Amazon Redshift tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Spark\u2019s DataFrame dropDuplicates() API to eliminate duplicates. Write the data to the Redshift tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue ResolveChoice built-in transform to select the value of the column from the most recent record."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-05T23:40:00.000Z",
        "voteCount": 5,
        "content": "Two step approach involving creating a staging table, followed by using Redshift's merge statement to update the target table from staging table and finally truncate/housekeep the staging table."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/amazon/view/145007-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is using Amazon Redshift to build a data warehouse solution. The company is loading hundreds of files into a fact table that is in a Redshift cluster.<br><br>The company wants the data warehouse solution to achieve the greatest possible throughput. The solution must use cluster resources optimally when the company loads data into the fact table.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse multiple COPY commands to load the data into the Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3DistCp to load multiple files into Hadoop Distributed File System (HDFS). Use an HDFS connector to ingest the data into the Redshift cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a number of INSERT statements equal to the number of Redshift cluster nodes. Load the data in parallel into each node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a single COPY command to load the data into the Redshift cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-04T15:54:00.000Z",
        "voteCount": 5,
        "content": "D?\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Loading-data-from-S3.html"
      },
      {
        "date": "2024-08-14T02:21:00.000Z",
        "voteCount": 1,
        "content": "this is D"
      },
      {
        "date": "2024-08-07T11:45:00.000Z",
        "voteCount": 1,
        "content": "A single COPY command automatically parallelizes the load operation across all nodes in the Redshift cluster. This ensures optimal use of cluster resources."
      },
      {
        "date": "2024-08-06T18:02:00.000Z",
        "voteCount": 1,
        "content": "A - Using multiple COPY commands allows parallel loading of data, which maximizes throughput."
      },
      {
        "date": "2024-08-05T23:44:00.000Z",
        "voteCount": 2,
        "content": "Agree with canace; Redshift's copy command uses MPP architecture to read and load in parallel from files into DWH."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/amazon/view/145116-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company ingests data from multiple data sources and stores the data in an Amazon S3 bucket. An AWS Glue extract, transform, and load (ETL) job transforms the data and writes the transformed data to an Amazon S3 based data lake. The company uses Amazon Athena to query the data that is in the data lake.<br><br>The company needs to identify matching records even when the records do not have a common unique identifier.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Macie pattern matching as part of the ETL job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain and use the AWS Glue PySpark Filter class in the ETL job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition tables and use the ETL job to partition the data on a unique identifier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain and use the AWS Lake Formation FindMatches transform in the ETL job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-05T23:52:00.000Z",
        "voteCount": 5,
        "content": "AWS Lake Formation provides machine learning capabilities to create custom transforms to cleanse your data. There is currently one available transform named FindMatches. The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works."
      },
      {
        "date": "2024-09-25T19:04:00.000Z",
        "voteCount": 1,
        "content": ""
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/amazon/view/145607-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer is using an AWS Glue crawler to catalog data that is in an Amazon S3 bucket. The S3 bucket contains both .csv and json files. The data engineer configured the crawler to exclude the .json files from the catalog.<br><br>When the data engineer runs queries in Amazon Athena, the queries also process the excluded .json files. The data engineer wants to resolve this issue. The data engineer needs a solution that will not affect access requirements for the .csv files in the source S3 bucket.<br><br>Which solution will meet this requirement with the SHORTEST query times?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust the AWS Glue crawler settings to ensure that the AWS Glue crawler also excludes .json files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Athena console to ensure the Athena queries also exclude the .json files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRelocate the .json files to a different path within the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 bucket policies to block access to the .json files."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T13:13:00.000Z",
        "voteCount": 1,
        "content": "If the AWS Glue crawler is configured to exclude .json files, then the AWS Glue Data Catalog will not have any metadata related to those .json files. In this case, the Athena table that uses the Glue Data Catalog would not be aware of the .json files at all, and Athena queries would only process the files that are included in the Glue catalog (e.g., .csv files)."
      },
      {
        "date": "2024-09-12T08:38:00.000Z",
        "voteCount": 1,
        "content": "Athena will scan both types of files.\n\nAlthough it may be feasible to adjust Athena query to exclude .json, the SHORTEST query times would be via relocating .json files to different path."
      },
      {
        "date": "2024-08-12T04:48:00.000Z",
        "voteCount": 4,
        "content": "Athena does not recognize exclude patterns that you specify an AWS Glue crawler. For example, if you have an Amazon S3 bucket that contains both .csv and .json files and you exclude the .json files from the crawler, Athena queries both groups of files. To avoid this, place the files that you want to exclude in a different location. \nhttps://docs.aws.amazon.com/athena/latest/ug/troubleshooting-athena.html"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/amazon/view/145725-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer set up an AWS Lambda function to read an object that is stored in an Amazon S3 bucket. The object is encrypted by an AWS KMS key.<br><br>The data engineer configured the Lambda function\u2019s execution role to access the S3 bucket. However, the Lambda function encountered an error and failed to retrieve the content of the object.<br><br>What is the likely cause of the error?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe data engineer misconfigured the permissions of the S3 bucket. The Lambda function could not access the object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Lambda function is using an outdated SDK version, which caused the read failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe S3 bucket is located in a different AWS Region than the Region where the data engineer works. Latency issues caused the Lambda function to encounter an error.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Lambda function\u2019s execution role does not have the necessary permissions to access the KMS key that can decrypt the S3 object.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T06:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2024-08-14T06:06:00.000Z",
        "voteCount": 1,
        "content": "Option D: The Lambda function\u2019s execution role does not have the necessary permissions to access the KMS key that can decrypt the S3 object."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/amazon/view/145726-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer has implemented data quality rules in 1,000 AWS Glue Data Catalog tables. Because of a recent change in business requirements, the data engineer must edit the data quality rules.<br><br>How should the data engineer meet this requirement with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in AWS Glue ETL to edit the rules for each of the 1,000 Data Catalog tables. Use an AWS Lambda function to call the corresponding AWS Glue job for each Data Catalog table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EMR cluster. Run a pipeline on Amazon EMR that edits the rules for each Data Catalog table. Use an AWS Lambda function to run the EMR pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Management Console to edit the rules within the Data Catalog."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T06:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-08-14T06:07:00.000Z",
        "voteCount": 1,
        "content": "Option B: Create an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/amazon/view/145728-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "Two developers are working on separate application releases. The developers have created feature branches named Branch A and Branch B by using a GitHub repository\u2019s master branch as the source.<br><br>The developer for Branch A deployed code to the production system. The code for Branch B will merge into a master branch in the following week\u2019s scheduled application release.<br><br>Which command should the developer for Branch B run before the developer raises a pull request to the master branch?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgit diff branchB master<br>git commit -m <message>\n\t\t\t\t\t\t\t\t\t\t</message>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgit pull master",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgit rebase master\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgit fetch -b master"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-18T15:09:00.000Z",
        "voteCount": 2,
        "content": "Rebasing\nIn Git, there are two main ways to integrate changes from one branch into another: the merge and the rebase. In this section you\u2019ll learn what rebasing is, how to do it, why it\u2019s a pretty amazing tool, and in what cases you won\u2019t want to use it.\n\nThe Basic Rebase\nIf you go back to an earlier example from Basic Merging, you can see that you diverged your work and made commits on two different branches.\n\nAnswer C"
      },
      {
        "date": "2024-08-14T06:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2024-08-14T06:08:00.000Z",
        "voteCount": 2,
        "content": "Option C: git rebase maste"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/amazon/view/145119-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company stores employee data in Amazon Resdshift. A table names Employee uses columns named Region ID, Department ID, and Role ID as a compound sort key.<br><br>Which queries will MOST increase the speed of query by using a compound sort key of the table? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect *from Employee where Region ID=\u2019North America\u2019;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect *from Employee where Region ID=\u2019North America\u2019 and Department ID=20;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect *from Employee where Department ID=20 and Region ID=\u2019North America\u2019;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect *from Employee where Role ID=50;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect *from Employee where Region ID=\u2019North America\u2019 and Role ID=50;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T10:29:00.000Z",
        "voteCount": 5,
        "content": "To maximize the speed of queries by using the compound sort key (Region ID, Department ID, and Role ID) in the Employee table in Amazon Redshift, the queries should align with the order of the columns in the sort key."
      },
      {
        "date": "2024-08-12T23:08:00.000Z",
        "voteCount": 3,
        "content": "To maximize the speed of queries using a compound sort key in Amazon Redshift, you should structure your queries to take advantage of the order of the columns in the sort key. The most efficient queries will filter or join on the columns in the same order as the sort key. Saying that, the most efficient queries would be:\nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1' \n  AND Department_ID = 'dept1' \n  AND Role_ID = 'role1';\nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1' \n  AND Department_ID = 'dept1'; \nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1';"
      },
      {
        "date": "2024-08-06T00:07:00.000Z",
        "voteCount": 2,
        "content": "Based on the order of the compound sort key columns."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/amazon/view/145729-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company receives test results from testing facilities that are located around the world. The company stores the test results in millions of 1 KB JSON files in an Amazon S3 bucket. A data engineer needs to process the files, convert them into Apache Parquet format, and load them into Amazon Redshift tables. The data engineer uses AWS Glue to process the files, AWS Step Functions to orchestrate the processes, and Amazon EventBridge to schedule jobs.<br><br>The company recently added more testing facilities. The time required to process files is increasing. The data engineer must reduce the data processing time.<br><br>Which solution will MOST reduce the data processing time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda to group the raw input files into larger files. Write the larger files back to Amazon S3. Use AWS Glue to process the files. Load the files into the Amazon Redshift tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue dynamic frame file-grouping option to ingest the raw input files. Process the files. Load the files into the Amazon Redshift tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Redshift COPY command to move the raw input files from Amazon S3 directly into the Amazon Redshift tables. Process the files in Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EMR instead of AWS Glue to group the raw input files. Process the files in Amazon EMR. Load the files into the Amazon Redshift tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T06:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-08-14T06:09:00.000Z",
        "voteCount": 1,
        "content": "Option B: Use the AWS Glue dynamic frame file-grouping option to ingest the raw input files. Process the files. Load the files into the Amazon Redshift tables."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/amazon/view/145096-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A data engineer uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to run data pipelines in an AWS account.<br><br>A workflow recently failed to run. The data engineer needs to use Apache Airflow logs to diagnose the failure of the workflow.<br><br>Which log type should the data engineer use to diagnose the cause of the failure?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYourEnvironmentName-WebServer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYourEnvironmentName-Scheduler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYourEnvironmentName-DAGProcessing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYourEnvironmentName-Task\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-12T01:11:00.000Z",
        "voteCount": 5,
        "content": "https://pupuweb.com/amazon-dea-c01-which-apache-airflow-log-type-should-you-use-to-diagnose-workflow-failures-in-amazon-mwaa/\n\nWhen a workflow fails to run in Amazon MWAA, the task logs (YourEnvironmentName-Task) are the most relevant for diagnosing the issue. Task logs contain detailed information about the execution of individual tasks within the workflow, including any error messages or stack traces that can help pinpoint the cause of the failure."
      },
      {
        "date": "2024-08-21T01:51:00.000Z",
        "voteCount": 1,
        "content": "Tasks within the workflow but the workflow failed to run from the question. It could be that workflow didn't start at all. I think C is most suitable"
      },
      {
        "date": "2024-08-12T23:18:00.000Z",
        "voteCount": 3,
        "content": "Agree with D based on 150b64e comments"
      },
      {
        "date": "2024-08-05T23:12:00.000Z",
        "voteCount": 2,
        "content": "Reference --&gt; https://aws.amazon.com/managed-workflows-for-apache-airflow/"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/amazon/view/147826-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A finance company uses Amazon Redshift as a data warehouse. The company stores the data in a shared Amazon S3 bucket. The company uses Amazon Redshift Spectrum to access the data that is stored in the S3 bucket. The data comes from certified third-party data providers. Each third-party data provider has unique connection details.<br><br>To comply with regulations, the company must ensure that none of the data is accessible from outside the company's AWS environment.<br><br>Which combination of steps should the company take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the existing Redshift cluster with a new Redshift cluster that is in a private subnet. Use an interface VPC endpoint to connect to the Redshift cluster. Use a NAT gateway to give Redshift access to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudHSM hardware security module (HSM) for each data provider. Encrypt each data provider's data by using the corresponding HSM for each data provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on enhanced VPC routing for the Amazon Redshift cluster. Set up an AWS Direct Connect connection and configure a connection between each data provider and the finance company\u2019s VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine table constraints for the primary keys and the foreign keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse federated queries to access the data from each data provider. Do not upload the data to the S3 bucket. Perform the federated queries through a gateway VPC endpoint."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T19:11:00.000Z",
        "voteCount": 2,
        "content": "A. Replace the existing Redshift cluster with a new Redshift cluster that is in a private subnet. Use an interface VPC endpoint to connect to the Redshift cluster. Use a NAT gateway to give Redshift access to the S3 bucket.\nC. Turn on enhanced VPC routing for the Amazon Redshift cluster. Set up an AWS Direct Connect connection and configure a connection between each data provider and the finance company\u2019s VPC."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/amazon/view/146986-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "Files from multiple data sources arrive in an Amazon S3 bucket on a regular basis. A data engineer wants to ingest new files into Amazon Redshift in near real time when the new files arrive in the S3 bucket.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the query editor v2 to schedule a COPY command to load new files into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the zero-ETL integration between Amazon Aurora and Amazon Redshift to load new files into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue job bookmarks to extract, transform, and load (ETL) load new files into Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Event Notifications to invoke an AWS Lambda function that loads new files into Amazon Redshift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-04T23:47:00.000Z",
        "voteCount": 3,
        "content": "Seems like the trigger on upload would be the fastest option"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/amazon/view/146967-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A technology company currently uses Amazon Kinesis Data Streams to collect log data in real time. The company wants to use Amazon Redshift for downstream real-time queries and to enrich the log data.<br><br>Which solution will ingest data into Amazon Redshift with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Kinesis Data Firehose delivery stream to send data to a Redshift provisioned cluster table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an Amazon Kinesis Data Firehose delivery stream to send data to Amazon S3. Configure a Redshift provisioned cluster to load data every minute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to send data directly to a Redshift provisioned cluster table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-04T23:56:00.000Z",
        "voteCount": 2,
        "content": "Amazon Redshift supports streaming ingestion from Amazon Kinesis Data Streams. The Amazon Redshift streaming ingestion feature provides low-latency, high-speed ingestion of streaming data from Amazon Kinesis Data Streams into an Amazon Redshift materialized view. Amazon Redshift streaming ingestion removes the need to stage data in Amazon S3before ingesting into Amazon Redshift.\n\nlink: https://docs.aws.amazon.com/streams/latest/dev/using-other-services-redshift.html"
      },
      {
        "date": "2024-09-04T19:02:00.000Z",
        "voteCount": 1,
        "content": "D. Use Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view."
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/amazon/view/147821-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company maintains a data warehouse in an on-premises Oracle database. The company wants to build a data lake on AWS. The company wants to load data warehouse tables into Amazon S3 and synchronize the tables with incremental data that arrives from the data warehouse every day.<br><br>Each table has a column that contains monotonically increasing values. The size of each table is less than 50 GB. The data warehouse tables are refreshed every night between 1 AM and 2 AM. A business intelligence team queries the tables between 10 AM and 8 PM every day.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Database Migration Service (AWS DMS) full load plus CDC job to load tables that contain monotonically increasing data columns from the on-premises data warehouse to Amazon S3. Use custom logic in AWS Glue to append the daily incremental data to a full-load copy that is in Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Glue Java Database Connectivity (JDBC) connection. Configure a job bookmark for a column that contains monotonically increasing values. Write custom logic to append the daily incremental data to a full-load copy that is in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Database Migration Service (AWS DMS) full load migration to load the data warehouse tables into Amazon S3 every day. Overwrite the previous day's full-load copy every day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue to load a full copy of the data warehouse tables into Amazon S3 every day. Overwrite the previous day's full-load copy every day."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-28T12:42:00.000Z",
        "voteCount": 1,
        "content": "A seems to be an overkill using custom logic"
      },
      {
        "date": "2024-09-18T18:00:00.000Z",
        "voteCount": 1,
        "content": "DMS is definitely the service, and C is obviously wrong"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/amazon/view/146993-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A company is building a data lake for a new analytics team. The company is using Amazon S3 for storage and Amazon Athena for query analysis. All data that is in Amazon S3 is in Apache Parquet format.<br><br>The company is running a new Oracle database as a source system in the company\u2019s data center. The company has 70 tables in the Oracle database. All the tables have primary keys. Data can occasionally change in the source system. The company wants to ingest the tables every day into the data lake.<br><br>Which solution will meet this requirement with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Apache Sqoop job in Amazon EMR to read the data from the Oracle database. Configure the Sqoop job to write the data to Amazon S3 in Parquet format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue connection to the Oracle database. Create an AWS Glue bookmark job to ingest the data incrementally and to write the data to Amazon S3 in Parquet format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Database Migration Service (AWS DMS) task for ongoing replication. Set the Oracle database as the source. Set Amazon S3 as the target. Configure the task to write the data in Parquet format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Oracle database in Amazon RDS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises Oracle database to Amazon RDS. Configure triggers on the tables to invoke AWS Lambda functions to write changed records to Amazon S3 in Parquet format."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-05T00:14:00.000Z",
        "voteCount": 6,
        "content": "C: You can use S3 as a target and configure files to be in Parquet format https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"
      },
      {
        "date": "2024-09-06T07:19:00.000Z",
        "voteCount": 1,
        "content": "VOTE B"
      },
      {
        "date": "2024-09-18T07:08:00.000Z",
        "voteCount": 1,
        "content": "You can no see any updates to the data after the records are ingested by Glue. B is not correct"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/amazon/view/147168-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A transportation company wants to track vehicle movements by capturing geolocation records. The records are 10 bytes in size. The company receives up to 10.000 records every second. Data transmission delays of a few minutes are acceptable because of unreliable network conditions.<br><br>The transportation company wants to use Amazon Kinesis Data Streams to ingest the geolocation data. The company needs a reliable mechanism to send data to Kinesis Data Streams. The company needs to maximize the throughput efficiency of the Kinesis shards.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis Agent",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis Producer Library (KPL)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon Kinesis Data Firehose",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKinesis SDK"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-08T01:38:00.000Z",
        "voteCount": 2,
        "content": "B. Kinesis Producer Library (KPL)"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/amazon/view/147823-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "An investment company needs to manage and extract insights from a volume of semi-structured data that grows continuously.<br><br>A data engineer needs to deduplicate the semi-structured data, remove records that are duplicates, and remove common misspellings of duplicates.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the FindMatches feature of AWS Glue to remove duplicate records.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse non-Windows functions in Amazon Athena to remove duplicate records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Neptune ML and an Apache Gremlin script to remove duplicate records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the global tables feature of Amazon DynamoDB to prevent duplicate data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T18:04:00.000Z",
        "voteCount": 1,
        "content": "A - The other options are dumb and hardly make sense"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/amazon/view/147824-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "body": "A mobile gaming company wants to capture data from its gaming app. The company wants to make the data available to three internal consumers of the data. The data records are approximately 20 KB in size.<br><br>The company wants to achieve optimal throughput from each device that runs the gaming app. Additionally, the company wants to develop an application to process data streams. The stream-processing application must have dedicated throughput for each internal consumer.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature with a stream for each internal consumer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the mobile app to call the PutRecordBatch API operation to send data to Amazon Kinesis Data Firehose. Submit an AWS Support case to turn on dedicated throughput for the company\u2019s AWS account. Allow each internal consumer to access the stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the mobile app to use the Amazon Kinesis Producer Library (KPL) to send data to Amazon Kinesis Data Firehose. Use the enhanced fan-out feature with a stream for each internal consumer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Host the stream-processing application for each internal consumer on Amazon EC2 instances. Configure auto scaling for the EC2 instances."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-25T19:46:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/streams/latest/dev/kpl-with-firehose.html\n\nKPL does work with firehose"
      },
      {
        "date": "2024-09-25T19:47:00.000Z",
        "voteCount": 1,
        "content": "Sorry meant to select option C"
      },
      {
        "date": "2024-09-18T18:14:00.000Z",
        "voteCount": 2,
        "content": "Seems to be A - Since KPL does not work into firehouse and only streams, and additionally the dedicated throughput is solved through fan-out"
      }
    ],
    "examNameCode": "aws-certified-data-engineer-associate-dea-c01",
    "topicNumber": "1"
  }
]